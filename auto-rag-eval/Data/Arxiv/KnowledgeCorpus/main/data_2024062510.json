[{"title":"Learning to Infer Semantic Parameters for 3D Shape Editing","source":"Fangyin Wei, Elena Sizikova, Avneesh Sud, Szymon Rusinkiewicz, Thomas\n  Funkhouser","docs_id":"2011.04755","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Learning to Infer Semantic Parameters for 3D Shape Editing. Many applications in 3D shape design and augmentation require the ability to make specific edits to an object's semantic parameters (e.g., the pose of a person's arm or the length of an airplane's wing) while preserving as much existing details as possible. We propose to learn a deep network that infers the semantic parameters of an input shape and then allows the user to manipulate those parameters. The network is trained jointly on shapes from an auxiliary synthetic template and unlabeled realistic models, ensuring robustness to shape variability while relieving the need to label realistic exemplars. At testing time, edits within the parameter space drive deformations to be applied to the original shape, which provides semantically-meaningful manipulation while preserving the details. This is in contrast to prior methods that either use autoencoders with a limited latent-space dimensionality, failing to preserve arbitrary detail, or drive deformations with purely-geometric controls, such as cages, losing the ability to update local part regions. Experiments with datasets of chairs, airplanes, and human bodies demonstrate that our method produces more natural edits than prior work."},{"title":"Asymptotic behaviour of learning rates in Armijo's condition","source":"Tuyen Trung Truong, Tuan Hang Nguyen","docs_id":"2007.03618","section":["math.OC","cs.LG","cs.NA","math.DS","math.NA"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Asymptotic behaviour of learning rates in Armijo's condition. Fix a constant $0<\\alpha <1$. For a $C^1$ function $f:\\mathbb{R}^k\\rightarrow \\mathbb{R}$, a point $x$ and a positive number $\\delta >0$, we say that Armijo's condition is satisfied if $f(x-\\delta \\nabla f(x))-f(x)\\leq -\\alpha \\delta ||\\nabla f(x)||^2$. It is a basis for the well known Backtracking Gradient Descent (Backtracking GD) algorithm. Consider a sequence $\\{x_n\\}$ defined by $x_{n+1}=x_n-\\delta _n\\nabla f(x_n)$, for positive numbers $\\delta _n$ for which Armijo's condition is satisfied. We show that if $\\{x_n\\}$ converges to a non-degenerate critical point, then $\\{\\delta _n\\}$ must be bounded. Moreover this boundedness can be quantified in terms of the norms of the Hessian $\\nabla ^2f$ and its inverse at the limit point. This complements the first author's results on Unbounded Backtracking GD, and shows that in case of convergence to a non-degenerate critical point the behaviour of Unbounded Backtracking GD is not too different from that of usual Backtracking GD. On the other hand, in case of convergence to a degenerate critical point the behaviours can be very much different. We run some experiments to illustrate that both scenrios can really happen. In another part of the paper, we argue that Backtracking GD has the correct unit (according to a definition by Zeiler in his Adadelta's paper). The main point is that since learning rate in Backtracking GD is bound by Armijo's condition, it is not unitless."},{"title":"Contextual Media Retrieval Using Natural Language Queries","source":"Sreyasi Nag Chowdhury, Mateusz Malinowski, Andreas Bulling, Mario\n  Fritz","docs_id":"1602.04983","section":["cs.IR","cs.AI","cs.CL","cs.CV","cs.HC"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Contextual Media Retrieval Using Natural Language Queries. The widespread integration of cameras in hand-held and head-worn devices as well as the ability to share content online enables a large and diverse visual capture of the world that millions of users build up collectively every day. We envision these images as well as associated meta information, such as GPS coordinates and timestamps, to form a collective visual memory that can be queried while automatically taking the ever-changing context of mobile users into account. As a first step towards this vision, in this work we present Xplore-M-Ego: a novel media retrieval system that allows users to query a dynamic database of images and videos using spatio-temporal natural language queries. We evaluate our system using a new dataset of real user queries as well as through a usability study. One key finding is that there is a considerable amount of inter-user variability, for example in the resolution of spatial relations in natural language utterances. We show that our retrieval system can cope with this variability using personalisation through an online learning-based retrieval formulation."},{"title":"Optimal Energy-Efficient Regular Delivery of Packets in Cyber-Physical\n  Systems","source":"Xueying Guo, Rahul Singh, P.R. Kumar and Zhisheng Niu","docs_id":"1502.07809","section":["cs.SY","cs.NI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Optimal Energy-Efficient Regular Delivery of Packets in Cyber-Physical\n  Systems. In cyber-physical systems such as in-vehicle wireless sensor networks, a large number of sensor nodes continually generate measurements that should be received by other nodes such as actuators in a regular fashion. Meanwhile, energy-efficiency is also important in wireless sensor networks. Motivated by these, we develop scheduling policies which are energy efficient and simultaneously maintain \"regular\" deliveries of packets. A tradeoff parameter is introduced to balance these two conflicting objectives. We employ a Markov Decision Process (MDP) model where the state of each client is the time-since-last-delivery of its packet, and reduce it into an equivalent finite-state MDP problem. Although this equivalent problem can be solved by standard dynamic programming techniques, it suffers from a high-computational complexity. Thus we further pose the problem as a restless multi-armed bandit problem and employ the low-complexity Whittle Index policy. It is shown that this problem is indexable and the Whittle indexes are derived. Also, we prove the Whittle Index policy is asymptotically optimal and validate its optimality via extensive simulations."},{"title":"Measurement Setup Consideration and Implementation for Inductively\n  Coupled Online Impedance Extraction","source":"Zhenyu Zhao","docs_id":"2108.13102","section":["physics.ins-det","cs.SY","eess.SY"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Measurement Setup Consideration and Implementation for Inductively\n  Coupled Online Impedance Extraction. This thesis is organized as follows: Chapter 1 introduces the background, motivation, objectives, and contributions of this thesis. Chapter 2 presents a review of existing online impedance extraction approaches. Chapter 3 proposes the improved measurement setup of the inductive coupling approach and introduces the theory behind time-variant online impedance extraction. Chapter 4 develops a three-term calibration technique for the proposed measurement setup to deembed the effect of the probe-to-probe coupling between the inductive probes with the objective to improve the accuracy of online impedance extraction. Chapter 5 discusses the additional measurement setup consideration in industrial applications where significant electrical noise and power surges are present. Chapter 6 discusses and demonstrates the application of the inductive coupling approach in online detection of the incipient stator faults in the inverter-fed induction motor. Chapter 7 further extends the application of this approach for non-intrusive extraction of the voltage-dependent capacitances of the silicon carbide (SiC) power metal-oxide-semiconductor field-effect transistor (MOSFET). Finally, Chapter 8 concludes this thesis and proposes future works that are worth exploring."},{"title":"Second-order Quantile Methods for Experts and Combinatorial Games","source":"Wouter M. Koolen and Tim van Erven","docs_id":"1502.08009","section":["cs.LG","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Second-order Quantile Methods for Experts and Combinatorial Games. We aim to design strategies for sequential decision making that adjust to the difficulty of the learning problem. We study this question both in the setting of prediction with expert advice, and for more general combinatorial decision tasks. We are not satisfied with just guaranteeing minimax regret rates, but we want our algorithms to perform significantly better on easy data. Two popular ways to formalize such adaptivity are second-order regret bounds and quantile bounds. The underlying notions of 'easy data', which may be paraphrased as \"the learning problem has small variance\" and \"multiple decisions are useful\", are synergetic. But even though there are sophisticated algorithms that exploit one of the two, no existing algorithm is able to adapt to both. In this paper we outline a new method for obtaining such adaptive algorithms, based on a potential function that aggregates a range of learning rates (which are essential tuning parameters). By choosing the right prior we construct efficient algorithms and show that they reap both benefits by proving the first bounds that are both second-order and incorporate quantiles."},{"title":"Coloring Drawings of Graphs","source":"Christoph Hertrich, Felix Schr\\\"oder and Raphael Steiner","docs_id":"2008.09692","section":["math.CO","cs.DM"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Coloring Drawings of Graphs. We consider face-colorings of drawings of graphs in the plane. Given a multi-graph $G$ together with a drawing $\\Gamma(G)$ in the plane with only finitely many crossings, we define a face-$k$-coloring of $\\Gamma(G)$ to be a coloring of the maximal connected regions of the drawing, the faces, with $k$ colors such that adjacent faces have different colors. By the $4$-color theorem, every drawing of a bridgeless graph has a face-$4$-coloring. A drawing of a graph is facially $2$-colorable if and only if the underlying graph is Eulerian. We show that every graph without degree 1 vertices admits a $3$-colorable drawing. This leads to the natural question which graphs $G$ have the property that each of its drawings has a $3$-coloring. We say that such a graph $G$ is facially $3$-colorable. We derive several sufficient and necessary conditions for this property: we show that every $4$-edge-connected graph and every graph admitting a nowhere-zero $3$-flow is facially $3$-colorable. We also discuss circumstances under which facial $3$-colorability guarantees the existence of a nowhere-zero $3$-flow. On the negative side, we present an infinite family of facially $3$-colorable graphs without a nowhere-zero $3$-flow. On the positive side, we formulate a conjecture which has a surprising relation to a famous open problem by Tutte known as the $3$-flow-conjecture. We prove our conjecture for subcubic and for $K_{3,3}$-minor-free graphs."},{"title":"Computing Distances between Probabilistic Automata","source":"Mathieu Tracol (IST Austria), Jos\\'ee Desharnais (Departement\n  d'informatique et de g\\'enie logiciel, Universit\\'e Laval, Qu\\'ebec, Canada),\n  Abir Zhioua (Departement d'informatique et de g\\'enie logiciel, Universit\\'e\n  Laval, Qu\\'ebec, Canada)","docs_id":"1107.1206","section":["cs.FL","cs.LO"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Computing Distances between Probabilistic Automata. We present relaxed notions of simulation and bisimulation on Probabilistic Automata (PA), that allow some error epsilon. When epsilon is zero we retrieve the usual notions of bisimulation and simulation on PAs. We give logical characterisations of these notions by choosing suitable logics which differ from the elementary ones, L with negation and L without negation, by the modal operator. Using flow networks, we show how to compute the relations in PTIME. This allows the definition of an efficiently computable non-discounted distance between the states of a PA. A natural modification of this distance is introduced, to obtain a discounted distance, which weakens the influence of long term transitions. We compare our notions of distance to others previously defined and illustrate our approach on various examples. We also show that our distance is not expansive with respect to process algebra operators. Although L without negation is a suitable logic to characterise epsilon-(bi)simulation on deterministic PAs, it is not for general PAs; interestingly, we prove that it does characterise weaker notions, called a priori epsilon-(bi)simulation, which we prove to be NP-difficult to decide."},{"title":"Modular Pipe Climber","source":"Rama Vadapalli, Kartik Suryavanshi, Ruchita Vucha, Abhishek Sarkar, K\n  Madhava Krishna","docs_id":"1909.10195","section":["cs.RO"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Modular Pipe Climber. This paper discusses the design and implementation of the Modular Pipe Climber inside ASTM D1785 - 15e1 standard pipes [1]. The robot has three tracks which operate independently and are mounted on three modules which are oriented at 120{\\deg} to each other. The tracks provide for greater surface traction compared to wheels [2]. The tracks are pushed onto the inner wall of the pipe by passive springs which help in maintaining the contact with the pipe during vertical climb and while turning in bends. The modules have the provision to compress asymmetrically, which helps the robot to take turns in bends in all directions. The motor torque required by the robot and the desired spring stiffness are calculated at quasistatic and static equilibriums when the pipe climber is in a vertical climb. The springs were further simulated and analyzed in ADAMS MSC. The prototype built based on these obtained values was experimented on, in complex pipe networks. Differential speed is employed when turning in bends to improve the efficiency and reduce the stresses experienced by the robot."},{"title":"Practical Speech Recognition with HTK","source":"Zulkarnaen Hatala","docs_id":"1908.02119","section":["eess.AS","cs.HC","cs.LG","cs.SD"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Practical Speech Recognition with HTK. The practical aspects of developing an Automatic Speech Recognition System (ASR) with HTK are reviewed. Steps are explained concerning hardware, software, libraries, applications and computer programs used. The common procedure to rapidly apply speech recognition system is summarized. The procedure is illustrated, to implement a speech based electrical switch in home automation for the Indonesian language. The main key of the procedure is to match the environment for training and testing using the training data recorded from the testing program, HVite. Often the silence detector of HTK is wrongly triggered by noises because the microphone is too sensitive. This problem is mitigated by simply scaling down the volume. In this sub-word phone-based speech recognition, noise is included in the training database and labelled particularly. Illustration of the procedure is applied to a home automation application. Electrical switches are controlled by Indonesian speech recognizer. The results show 100% command completion rate."},{"title":"Solving a fractional parabolic-hyperbolic free boundary problem which\n  models the growth of tumor with drug application using finite\n  difference-spectral method","source":"Sakine Esmaili, F. Nasresfahani, M.R. Eslahchi","docs_id":"1908.07386","section":["math.NA","cs.NA"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Solving a fractional parabolic-hyperbolic free boundary problem which\n  models the growth of tumor with drug application using finite\n  difference-spectral method. In this paper, a free boundary problem modelling the growth of tumor is considered. The model includes two reaction-diffusion equations modelling the diffusion of nutrient and drug in the tumor and three hyperbolic equations describing the evolution of three types of cells (i.e. proliferative cells, quiescent cells and dead cells) considered in the tumor. Due to the fact that in the real situation, the subdiffusion of nutrient and drug in the tumor can be found, we have changed the reaction-diffusion equations to the fractional ones to consider other conditions and study a more general and reliable model of tumor growth. Since it is important to solve a problem to have a clear vision of the dynamic of tumor growth under the effect of the nutrient and drug, we have solved the fractional free boundary problem. We have solved the fractional parabolic equations employing a combination of spectral and finite difference methods and the hyperbolic equations are solved using characteristic equation and finite difference method. It is proved that the presented method is unconditionally convergent and stable to be sure that we have a correct vision of tumor growth dynamic. Finally, by presenting some numerical examples and showing the results, the theoretical statements are justified."},{"title":"High-resolution medical image synthesis using progressively grown\n  generative adversarial networks","source":"Andrew Beers, James Brown, Ken Chang, J. Peter Campbell, Susan Ostmo,\n  Michael F. Chiang, and Jayashree Kalpathy-Cramer","docs_id":"1805.03144","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"High-resolution medical image synthesis using progressively grown\n  generative adversarial networks. Generative adversarial networks (GANs) are a class of unsupervised machine learning algorithms that can produce realistic images from randomly-sampled vectors in a multi-dimensional space. Until recently, it was not possible to generate realistic high-resolution images using GANs, which has limited their applicability to medical images that contain biomarkers only detectable at native resolution. Progressive growing of GANs is an approach wherein an image generator is trained to initially synthesize low resolution synthetic images (8x8 pixels), which are then fed to a discriminator that distinguishes these synthetic images from real downsampled images. Additional convolutional layers are then iteratively introduced to produce images at twice the previous resolution until the desired resolution is reached. In this work, we demonstrate that this approach can produce realistic medical images in two different domains; fundus photographs exhibiting vascular pathology associated with retinopathy of prematurity (ROP), and multi-modal magnetic resonance images of glioma. We also show that fine-grained details associated with pathology, such as retinal vessels or tumor heterogeneity, can be preserved and enhanced by including segmentation maps as additional channels. We envisage several applications of the approach, including image augmentation and unsupervised classification of pathology."},{"title":"Deep learning-based development of personalized human head model with\n  non-uniform conductivity for brain stimulation","source":"Essam A. Rashed, Jose Gomez-Tames, Akimasa Hirata","docs_id":"1910.02420","section":["cs.LG","cs.CV","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Deep learning-based development of personalized human head model with\n  non-uniform conductivity for brain stimulation. Electromagnetic stimulation of the human brain is a key tool for the neurophysiological characterization and diagnosis of several neurological disorders. Transcranial magnetic stimulation (TMS) is one procedure that is commonly used clinically. However, personalized TMS requires a pipeline for accurate head model generation to provide target-specific stimulation. This process includes intensive segmentation of several head tissues based on magnetic resonance imaging (MRI), which has significant potential for segmentation error, especially for low-contrast tissues. Additionally, a uniform electrical conductivity is assigned to each tissue in the model, which is an unrealistic assumption based on conventional volume conductor modeling. This paper proposes a novel approach to the automatic estimation of electric conductivity in the human head for volume conductor models without anatomical segmentation. A convolutional neural network is designed to estimate personalized electrical conductivity values based on anatomical information obtained from T1- and T2-weighted MRI scans. This approach can avoid the time-consuming process of tissue segmentation and maximize the advantages of position-dependent conductivity assignment based on water content values estimated from MRI intensity values. The computational results of the proposed approach provide similar but smoother electric field results for the brain when compared to conventional approaches."},{"title":"Some variations on Lyndon words","source":"Francesco Dolce, Antonio Restivo, Christophe Reutenauer","docs_id":"1904.00954","section":["cs.DM","math.CO"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Some variations on Lyndon words. In this paper we compare two finite words $u$ and $v$ by the lexicographical order of the infinite words $u^\\omega$ and $v^\\omega$. Informally, we say that we compare $u$ and $v$ by the infinite order. We show several properties of Lyndon words expressed using this infinite order. The innovative aspect of this approach is that it allows to take into account also non trivial conditions on the prefixes of a word, instead that only on the suffixes. In particular, we derive a result of Ufnarovskij [V. Ufnarovskij, \"Combinatorial and asymptotic methods in algebra\", 1995] that characterizes a Lyndon word as a word which is greater, with respect to the infinite order, than all its prefixes. Motivated by this result, we introduce the prefix standard permutation of a Lyndon word and the corresponding (left) Cartesian tree. We prove that the left Cartesian tree is equal to the left Lyndon tree, defined by the left standard factorization of Viennot [G. Viennot, \"Alg\\`ebres de Lie libres et mono\\\"ides libres\", 1978]. This result is dual with respect to a theorem of Hohlweg and Reutenauer [C. Hohlweg and C. Reutenauer, \"Lyndon words, permutations and trees\", 2003]."},{"title":"Good Colour Maps: How to Design Them","source":"Peter Kovesi","docs_id":"1509.03700","section":["cs.GR"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Good Colour Maps: How to Design Them. Many colour maps provided by vendors have highly uneven perceptual contrast over their range. It is not uncommon for colour maps to have perceptual flat spots that can hide a feature as large as one tenth of the total data range. Colour maps may also have perceptual discontinuities that induce the appearance of false features. Previous work in the design of perceptually uniform colour maps has mostly failed to recognise that CIELAB space is only designed to be perceptually uniform at very low spatial frequencies. The most important factor in designing a colour map is to ensure that the magnitude of the incremental change in perceptual lightness of the colours is uniform. The specific requirements for linear, diverging, rainbow and cyclic colour maps are developed in detail. To support this work two test images for evaluating colour maps are presented. The use of colour maps in combination with relief shading is considered and the conditions under which colour can enhance or disrupt relief shading are identified. Finally, a set of new basis colours for the construction of ternary images are presented. Unlike the RGB primaries these basis colours produce images whereby the salience of structures are consistent irrespective of the assignment of basis colours to data channels."},{"title":"Spatial SINR Games of Base Station Placement and Mobile Association","source":"Eitan Altman, Anurag Kumar, Chandramani Singh and Rajesh Sundaresan","docs_id":"1102.3561","section":["cs.NI","cs.GT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Spatial SINR Games of Base Station Placement and Mobile Association. We study the question of determining locations of base stations that may belong to the same or to competing service providers. We take into account the impact of these decisions on the behavior of intelligent mobile terminals who can connect to the base station that offers the best utility. The signal to interference and noise ratio is used as the quantity that determines the association. We first study the SINR association-game: we determine the cells corresponding to each base stations, i.e., the locations at which mobile terminals prefer to connect to a given base station than to others. We make some surprising observations: (i) displacing a base station a little in one direction may result in a displacement of the boundary of the corresponding cell to the opposite direction; (ii) A cell corresponding to a BS may be the union of disconnected sub-cells. We then study the hierarchical equilibrium in the combined BS location and mobile association problem: we determine where to locate the BSs so as to maximize the revenues obtained at the induced SINR mobile association game. We consider the cases of single frequency band and two frequency bands of operation. Finally, we also consider hierarchical equilibria in two frequency systems with successive interference cancellation."},{"title":"Seed Stocking Via Multi-Task Learning","source":"Yunhe Feng and Wenjun Zhou","docs_id":"2101.04333","section":["cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Seed Stocking Via Multi-Task Learning. Sellers of crop seeds need to plan for the variety and quantity of seeds to stock at least a year in advance. There are a large number of seed varieties of one crop, and each can perform best under different growing conditions. Given the unpredictability of weather, farmers need to make decisions that balance high yield and low risk. A seed vendor needs to be able to anticipate the needs of farmers and have them ready. In this study, we propose an analytical framework for estimating seed demand with three major steps. First, we will estimate the yield and risk of each variety as if they were planted at each location. Since past experiments performed with different seed varieties are highly unbalanced across varieties, and the combination of growing conditions is sparse, we employ multi-task learning to borrow information from similar varieties. Second, we will determine the best mix of seeds for each location by seeking a tradeoff between yield and risk. Third, we will aggregate such mix and pick the top five varieties to re-balance the yield and risk for each growing location. We find that multi-task learning provides a viable solution for yield prediction, and our overall analytical framework has resulted in a good performance."},{"title":"On the Exponentially Weighted Aggregate with the Laplace Prior","source":"Arnak S. Dalalyan, Edwin Grappin, Quentin Paris","docs_id":"1611.08483","section":["math.ST","cs.LG","stat.TH"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"On the Exponentially Weighted Aggregate with the Laplace Prior. In this paper, we study the statistical behaviour of the Exponentially Weighted Aggregate (EWA) in the problem of high-dimensional regression with fixed design. Under the assumption that the underlying regression vector is sparse, it is reasonable to use the Laplace distribution as a prior. The resulting estimator and, specifically, a particular instance of it referred to as the Bayesian lasso, was already used in the statistical literature because of its computational convenience, even though no thorough mathematical analysis of its statistical properties was carried out. The present work fills this gap by establishing sharp oracle inequalities for the EWA with the Laplace prior. These inequalities show that if the temperature parameter is small, the EWA with the Laplace prior satisfies the same type of oracle inequality as the lasso estimator does, as long as the quality of estimation is measured by the prediction loss. Extensions of the proposed methodology to the problem of prediction with low-rank matrices are considered."},{"title":"Additive Tree-Structured Conditional Parameter Spaces in Bayesian\n  Optimization: A Novel Covariance Function and a Fast Implementation","source":"Xingchen Ma, Matthew B. Blaschko","docs_id":"2010.03171","section":["stat.ML","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Additive Tree-Structured Conditional Parameter Spaces in Bayesian\n  Optimization: A Novel Covariance Function and a Fast Implementation. Bayesian optimization (BO) is a sample-efficient global optimization algorithm for black-box functions which are expensive to evaluate. Existing literature on model based optimization in conditional parameter spaces are usually built on trees. In this work, we generalize the additive assumption to tree-structured functions and propose an additive tree-structured covariance function, showing improved sample-efficiency, wider applicability and greater flexibility. Furthermore, by incorporating the structure information of parameter spaces and the additive assumption in the BO loop, we develop a parallel algorithm to optimize the acquisition function and this optimization can be performed in a low dimensional space. We demonstrate our method on an optimization benchmark function, on a neural network compression problem, on pruning pre-trained VGG16 and ResNet50 models as well as on searching activation functions of ResNet20. Experimental results show our approach significantly outperforms the current state of the art for conditional parameter optimization including SMAC, TPE and Jenatton et al. (2017)."},{"title":"Area-Delay-Efficeint FPGA Design of 32-bit Euclid's GCD based on Sum of\n  Absolute Difference","source":"Saeideh Nabipour, Masoume Gholizade, Nima Nabipour","docs_id":"2107.02762","section":["cs.AR","cs.CR","eess.SP"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Area-Delay-Efficeint FPGA Design of 32-bit Euclid's GCD based on Sum of\n  Absolute Difference. Euclids algorithm is widely used in calculating of GCD (Greatest Common Divisor) of two positive numbers. There are various fields where this division is used such as channel coding, cryptography, and error correction codes. This makes the GCD a fundamental algorithm in number theory, so a number of methods have been discovered to efficiently compute it. The main contribution of this paper is to investigate a method that computes the GCD of two 32-bit numbers based on Euclidean algorithm which targets six different Xilinx chips. The complexity of this method that we call Optimized_GCDSAD is achieved by utilizing Sum of Absolute Difference (SAD) block which is based on a fast carry-out generation function. The efficiency of the proposed architecture is evaluated based on criteria such as time (latency), area delay product (ADP) and space (slice number) complexity. The VHDL codes of these architectures have been implemented and synthesized through ISE 14.7. A detailed comparative analysis indicates that the proposed Optimized_GCDSAD method based on SAD block outperforms previously known results."},{"title":"Selfless Sequential Learning","source":"Rahaf Aljundi, Marcus Rohrbach and Tinne Tuytelaars","docs_id":"1806.05421","section":["stat.ML","cs.AI","cs.CV","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Selfless Sequential Learning. Sequential learning, also called lifelong learning, studies the problem of learning tasks in a sequence with access restricted to only the data of the current task. In this paper we look at a scenario with fixed model capacity, and postulate that the learning process should not be selfish, i.e. it should account for future tasks to be added and thus leave enough capacity for them. To achieve Selfless Sequential Learning we study different regularization strategies and activation functions. We find that imposing sparsity at the level of the representation (i.e.~neuron activations) is more beneficial for sequential learning than encouraging parameter sparsity. In particular, we propose a novel regularizer, that encourages representation sparsity by means of neural inhibition. It results in few active neurons which in turn leaves more free neurons to be utilized by upcoming tasks. As neural inhibition over an entire layer can be too drastic, especially for complex tasks requiring strong representations, our regularizer only inhibits other neurons in a local neighbourhood, inspired by lateral inhibition processes in the brain. We combine our novel regularizer, with state-of-the-art lifelong learning methods that penalize changes to important previously learned parts of the network. We show that our new regularizer leads to increased sparsity which translates in consistent performance improvement %over alternative regularizers we studied on diverse datasets."},{"title":"Wireless Communication via Double IRS: Channel Estimation and Passive\n  Beamforming Designs","source":"Changsheng You, Beixiong Zheng, and Rui Zhang","docs_id":"2008.11439","section":["cs.IT","cs.NI","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Wireless Communication via Double IRS: Channel Estimation and Passive\n  Beamforming Designs. In this letter, we study efficient channel estimation and passive beamforming designs for a double-intelligent reflecting surface (IRS) aided single-user communication system, where a user communicates with an access point (AP) via the cascaded user-IRS 1-IRS 2-AP double-reflection link. First, a general channel estimation scheme is proposed for the system under any arbitrary inter-IRS channel, where all coefficients of the cascaded channel are estimated. Next, for the typical scenario with a line-of-sight (LoS)-dominant inter-IRS channel, we propose another customized scheme to estimate two signature vectors of the rank-one cascaded channel with significantly less channel training time than the first scheme. For the two proposed channel estimation schemes, we further optimize their corresponding cooperative passive beamforming for data transmission to maximize the achievable rate with the training overhead and channel estimation error taken into account. Numerical results show that deploying two cooperative IRSs with the proposed channel estimation and passive beamforming designs achieves significant rate enhancement as compared to the conventional case of single IRS deployment."},{"title":"Exploiting Unlabeled Data in CNNs by Self-supervised Learning to Rank","source":"Xialei Liu, Joost van de Weijer, Andrew D. Bagdanov","docs_id":"1902.06285","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Exploiting Unlabeled Data in CNNs by Self-supervised Learning to Rank. For many applications the collection of labeled data is expensive laborious. Exploitation of unlabeled data during training is thus a long pursued objective of machine learning. Self-supervised learning addresses this by positing an auxiliary task (different, but related to the supervised task) for which data is abundantly available. In this paper, we show how ranking can be used as a proxy task for some regression problems. As another contribution, we propose an efficient backpropagation technique for Siamese networks which prevents the redundant computation introduced by the multi-branch network architecture. We apply our framework to two regression problems: Image Quality Assessment (IQA) and Crowd Counting. For both we show how to automatically generate ranked image sets from unlabeled data. Our results show that networks trained to regress to the ground truth targets for labeled data and to simultaneously learn to rank unlabeled data obtain significantly better, state-of-the-art results for both IQA and crowd counting. In addition, we show that measuring network uncertainty on the self-supervised proxy task is a good measure of informativeness of unlabeled data. This can be used to drive an algorithm for active learning and we show that this reduces labeling effort by up to 50%."},{"title":"Chat More If You Like: Dynamic Cue Words Planning to Flow Longer\n  Conversations","source":"Lili Yao, Ruijian Xu, Chao Li, Dongyan Zhao and Rui Yan","docs_id":"1811.07631","section":["cs.CL"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Chat More If You Like: Dynamic Cue Words Planning to Flow Longer\n  Conversations. To build an open-domain multi-turn conversation system is one of the most interesting and challenging tasks in Artificial Intelligence. Many research efforts have been dedicated to building such dialogue systems, yet few shed light on modeling the conversation flow in an ongoing dialogue. Besides, it is common for people to talk about highly relevant aspects during a conversation. And the topics are coherent and drift naturally, which demonstrates the necessity of dialogue flow modeling. To this end, we present the multi-turn cue-words driven conversation system with reinforcement learning method (RLCw), which strives to select an adaptive cue word with the greatest future credit, and therefore improve the quality of generated responses. We introduce a new reward to measure the quality of cue words in terms of effectiveness and relevance. To further optimize the model for long-term conversations, a reinforcement approach is adopted in this paper. Experiments on real-life dataset demonstrate that our model consistently outperforms a set of competitive baselines in terms of simulated turns, diversity and human evaluation."},{"title":"Asynchronous Convolutional-Coded Physical-Layer Network Coding","source":"Qing Yang, Soung Chang Liew","docs_id":"1312.1447","section":["cs.IT","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Asynchronous Convolutional-Coded Physical-Layer Network Coding. This paper investigates the decoding process of asynchronous convolutional-coded physical-layer network coding (PNC) systems. Specifically, we put forth a layered decoding framework for convolutional-coded PNC consisting of three layers: symbol realignment layer, codeword realignment layer, and joint channel-decoding network coding (Jt-CNC) decoding layer. Our framework can deal with phase asynchrony and symbol arrival-time asynchrony between the signals simultaneously transmitted by multiple sources. A salient feature of this framework is that it can handle both fractional and integral symbol offsets; previously proposed PNC decoding algorithms (e.g., XOR-CD and reduced-state Viterbi algorithms) can only deal with fractional symbol offset. Moreover, the Jt-CNC algorithm, based on belief propagation (BP), is BER-optimal for synchronous PNC and near optimal for asynchronous PNC. Extending beyond convolutional codes, we further generalize the Jt-CNC decoding algorithm for all cyclic codes. Our simulation shows that Jt-CNC outperforms the previously proposed XOR-CD algorithm and reduced-state Viterbi algorithm by 2dB for synchronous PNC. For phase-asynchronous PNC, Jt-CNC is 4dB better than the other two algorithms. Importantly, for real wireless environment testing, we have also implemented our decoding algorithm in a PNC system built on the USRP software radio platform. Our experiment shows that the proposed Jt-CNC decoder works well in practice."},{"title":"Statistical Deformation Reconstruction Using Multi-organ Shape Features\n  for Pancreatic Cancer Localization","source":"Megumi Nakao, Mitsuhiro Nakamura, Takashi Mizowaki, Tetsuya Matsuda","docs_id":"1911.05439","section":["cs.CV","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Statistical Deformation Reconstruction Using Multi-organ Shape Features\n  for Pancreatic Cancer Localization. Respiratory motion and the associated deformations of abdominal organs and tumors are essential information in clinical applications. However, inter- and intra-patient multi-organ deformations are complex and have not been statistically formulated, whereas single organ deformations have been widely studied. In this paper, we introduce a multi-organ deformation library and its application to deformation reconstruction based on the shape features of multiple abdominal organs. Statistical multi-organ motion\/deformation models of the stomach, liver, left and right kidneys, and duodenum were generated by shape matching their region labels defined on four-dimensional computed tomography images. A total of 250 volumes were measured from 25 pancreatic cancer patients. This paper also proposes a per-region-based deformation learning using the reproducing kernel to predict the displacement of pancreatic cancer for adaptive radiotherapy. The experimental results show that the proposed concept estimates deformations better than general per-patient-based learning models and achieves a clinically acceptable estimation error with a mean distance of 1.2 $\\pm$ 0.7 mm and a Hausdorff distance of 4.2 $\\pm$ 2.3 mm throughout the respiratory motion."},{"title":"Okapi: Causally Consistent Geo-Replication Made Faster, Cheaper and More\n  Available","source":"Diego Didona, Kristina Spirovska, Willy Zwaenepoel","docs_id":"1702.04263","section":["cs.DC"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Okapi: Causally Consistent Geo-Replication Made Faster, Cheaper and More\n  Available. Okapi is a new causally consistent geo-replicated key- value store. Okapi leverages two key design choices to achieve high performance. First, it relies on hybrid logical\/physical clocks to achieve low latency even in the presence of clock skew. Second, Okapi achieves higher resource efficiency and better availability, at the expense of a slight increase in update visibility latency. To this end, Okapi implements a new stabilization protocol that uses a combination of vector and scalar clocks and makes a remote update visible when its delivery has been acknowledged by every data center. We evaluate Okapi with different workloads on Amazon AWS, using three geographically distributed regions and 96 nodes. We compare Okapi with two recent approaches to causal consistency, Cure and GentleRain. We show that Okapi delivers up to two orders of magnitude better performance than GentleRain and that Okapi achieves up to 3.5x lower latency and a 60% reduction of the meta-data overhead with respect to Cure."},{"title":"Deep Robust Subjective Visual Property Prediction in Crowdsourcing","source":"Qianqian Xu, Zhiyong Yang, Yangbangyan Jiang, Xiaochun Cao, Qingming\n  Huang, Yuan Yao","docs_id":"1903.03956","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Deep Robust Subjective Visual Property Prediction in Crowdsourcing. The problem of estimating subjective visual properties (SVP) of images (e.g., Shoes A is more comfortable than B) is gaining rising attention. Due to its highly subjective nature, different annotators often exhibit different interpretations of scales when adopting absolute value tests. Therefore, recent investigations turn to collect pairwise comparisons via crowdsourcing platforms. However, crowdsourcing data usually contains outliers. For this purpose, it is desired to develop a robust model for learning SVP from crowdsourced noisy annotations. In this paper, we construct a deep SVP prediction model which not only leads to better detection of annotation outliers but also enables learning with extremely sparse annotations. Specifically, we construct a comparison multi-graph based on the collected annotations, where different labeling results correspond to edges with different directions between two vertexes. Then, we propose a generalized deep probabilistic framework which consists of an SVP prediction module and an outlier modeling module that work collaboratively and are optimized jointly. Extensive experiments on various benchmark datasets demonstrate that our new approach guarantees promising results."},{"title":"Webly Supervised Image Classification with Self-Contained Confidence","source":"Jingkang Yang, Litong Feng, Weirong Chen, Xiaopeng Yan, Huabin Zheng,\n  Ping Luo, Wayne Zhang","docs_id":"2008.11894","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Webly Supervised Image Classification with Self-Contained Confidence. This paper focuses on webly supervised learning (WSL), where datasets are built by crawling samples from the Internet and directly using search queries as web labels. Although WSL benefits from fast and low-cost data collection, noises in web labels hinder better performance of the image classification model. To alleviate this problem, in recent works, self-label supervised loss $\\mathcal{L}_s$ is utilized together with webly supervised loss $\\mathcal{L}_w$. $\\mathcal{L}_s$ relies on pseudo labels predicted by the model itself. Since the correctness of the web label or pseudo label is usually on a case-by-case basis for each web sample, it is desirable to adjust the balance between $\\mathcal{L}_s$ and $\\mathcal{L}_w$ on sample level. Inspired by the ability of Deep Neural Networks (DNNs) in confidence prediction, we introduce Self-Contained Confidence (SCC) by adapting model uncertainty for WSL setting, and use it to sample-wisely balance $\\mathcal{L}_s$ and $\\mathcal{L}_w$. Therefore, a simple yet effective WSL framework is proposed. A series of SCC-friendly regularization approaches are investigated, among which the proposed graph-enhanced mixup is the most effective method to provide high-quality confidence to enhance our framework. The proposed WSL framework has achieved the state-of-the-art results on two large-scale WSL datasets, WebVision-1000 and Food101-N. Code is available at https:\/\/github.com\/bigvideoresearch\/SCC."},{"title":"HACS: Human Action Clips and Segments Dataset for Recognition and\n  Temporal Localization","source":"Hang Zhao, Antonio Torralba, Lorenzo Torresani, Zhicheng Yan","docs_id":"1712.09374","section":["cs.CV","cs.AI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"HACS: Human Action Clips and Segments Dataset for Recognition and\n  Temporal Localization. This paper presents a new large-scale dataset for recognition and temporal localization of human actions collected from Web videos. We refer to it as HACS (Human Action Clips and Segments). We leverage both consensus and disagreement among visual classifiers to automatically mine candidate short clips from unlabeled videos, which are subsequently validated by human annotators. The resulting dataset is dubbed HACS Clips. Through a separate process we also collect annotations defining action segment boundaries. This resulting dataset is called HACS Segments. Overall, HACS Clips consists of 1.5M annotated clips sampled from 504K untrimmed videos, and HACS Seg-ments contains 139K action segments densely annotatedin 50K untrimmed videos spanning 200 action categories. HACS Clips contains more labeled examples than any existing video benchmark. This renders our dataset both a large scale action recognition benchmark and an excellent source for spatiotemporal feature learning. In our transferlearning experiments on three target datasets, HACS Clips outperforms Kinetics-600, Moments-In-Time and Sports1Mas a pretraining source. On HACS Segments, we evaluate state-of-the-art methods of action proposal generation and action localization, and highlight the new challenges posed by our dense temporal annotations."},{"title":"A Survey on Contrastive Self-supervised Learning","source":"Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya\n  Banerjee, Fillia Makedon","docs_id":"2011.00362","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A Survey on Contrastive Self-supervised Learning. Self-supervised learning has gained popularity because of its ability to avoid the cost of annotating large-scale datasets. It is capable of adopting self-defined pseudo labels as supervision and use the learned representations for several downstream tasks. Specifically, contrastive learning has recently become a dominant component in self-supervised learning methods for computer vision, natural language processing (NLP), and other domains. It aims at embedding augmented versions of the same sample close to each other while trying to push away embeddings from different samples. This paper provides an extensive review of self-supervised methods that follow the contrastive approach. The work explains commonly used pretext tasks in a contrastive learning setup, followed by different architectures that have been proposed so far. Next, we have a performance comparison of different methods for multiple downstream tasks such as image classification, object detection, and action recognition. Finally, we conclude with the limitations of the current methods and the need for further techniques and future directions to make substantial progress."},{"title":"CAT: CRF-based ASR Toolkit","source":"Keyu An, Hongyu Xiang, Zhijian Ou","docs_id":"1911.08747","section":["cs.LG","cs.SD","eess.AS","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"CAT: CRF-based ASR Toolkit. In this paper, we present a new open source toolkit for automatic speech recognition (ASR), named CAT (CRF-based ASR Toolkit). A key feature of CAT is discriminative training in the framework of conditional random field (CRF), particularly with connectionist temporal classification (CTC) inspired state topology. CAT contains a full-fledged implementation of CTC-CRF and provides a complete workflow for CRF-based end-to-end speech recognition. Evaluation results on Chinese and English benchmarks such as Switchboard and Aishell show that CAT obtains the state-of-the-art results among existing end-to-end models with less parameters, and is competitive compared with the hybrid DNN-HMM models. Towards flexibility, we show that i-vector based speaker-adapted recognition and latency control mechanism can be explored easily and effectively in CAT. We hope CAT, especially the CRF-based framework and software, will be of broad interest to the community, and can be further explored and improved."},{"title":"Teaching Agents how to Map: Spatial Reasoning for Multi-Object\n  Navigation","source":"Pierre Marza, Laetitia Matignon, Olivier Simonin, Christian Wolf","docs_id":"2107.06011","section":["cs.CV","cs.LG","cs.RO"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Teaching Agents how to Map: Spatial Reasoning for Multi-Object\n  Navigation. In the context of visual navigation, the capacity to map a novel environment is necessary for an agent to exploit its observation history in the considered place and efficiently reach known goals. This ability can be associated with spatial reasoning, where an agent is able to perceive spatial relationships and regularities, and discover object characteristics. In classical Reinforcement Learning (RL) setups, this capacity is learned from reward alone. We introduce supplementary supervision in the form of auxiliary tasks designed to favor the emergence of spatial perception capabilities in agents trained for a goal-reaching downstream objective. We show that learning to estimate metrics quantifying the spatial relationships between an agent at a given location and a goal to reach has a high positive impact in Multi-Object Navigation settings. Our method significantly improves the performance of different baseline agents, that either build an explicit or implicit representation of the environment, even matching the performance of incomparable oracle agents taking ground-truth maps as input."},{"title":"Assessing Individual and Community Vulnerability to Fake News in Social\n  Networks","source":"Bhavtosh Rath, Wei Gao, Jaideep Srivastava","docs_id":"2102.02434","section":["cs.SI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Assessing Individual and Community Vulnerability to Fake News in Social\n  Networks. The plague of false information, popularly called fake news has affected lives of news consumers ever since the prevalence of social media. Thus understanding the spread of false information in social networks has gained a lot of attention in the literature. While most proposed models do content analysis of the information, no much work has been done by exploring the community structures that also play an important role in determining how people get exposed to it. In this paper we base our idea on Computational Trust in social networks to propose a novel Community Health Assessment model against fake news. Based on the concepts of neighbor, boundary and core nodes of a community, we propose novel evaluation metrics to quantify the vulnerability of nodes (individual-level) and communities (group-level) to spreading false information. Our model hypothesizes that if the boundary nodes trust the neighbor nodes of a community who are spreaders, the densely-connected core nodes of the community are highly likely to become spreaders. We test our model with communities generated using three popular community detection algorithms based on two new datasets of information spreading networks collected from Twitter. Our experimental results show that the proposed metrics perform clearly better on the networks spreading false information than on those spreading true ones, indicating our community health assessment model is effective."},{"title":"A convolutional neural-network model of human cochlear mechanics and\n  filter tuning for real-time applications","source":"Deepak Baby, Arthur Van Den Broucke, Sarah Verhulst","docs_id":"2004.14832","section":["eess.AS","cs.CE","cs.LG","cs.SD"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A convolutional neural-network model of human cochlear mechanics and\n  filter tuning for real-time applications. Auditory models are commonly used as feature extractors for automatic speech-recognition systems or as front-ends for robotics, machine-hearing and hearing-aid applications. Although auditory models can capture the biophysical and nonlinear properties of human hearing in great detail, these biophysical models are computationally expensive and cannot be used in real-time applications. We present a hybrid approach where convolutional neural networks are combined with computational neuroscience to yield a real-time end-to-end model for human cochlear mechanics, including level-dependent filter tuning (CoNNear). The CoNNear model was trained on acoustic speech material and its performance and applicability were evaluated using (unseen) sound stimuli commonly employed in cochlear mechanics research. The CoNNear model accurately simulates human cochlear frequency selectivity and its dependence on sound intensity, an essential quality for robust speech intelligibility at negative speech-to-background-noise ratios. The CoNNear architecture is based on parallel and differentiable computations and has the power to achieve real-time human performance. These unique CoNNear features will enable the next generation of human-like machine-hearing applications."},{"title":"Binarized Neural Architecture Search","source":"Hanlin Chen, Li'an Zhuo, Baochang Zhang, Xiawu Zheng, Jianzhuang Liu,\n  David Doermann, Rongrong Ji","docs_id":"1911.10862","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Binarized Neural Architecture Search. Neural architecture search (NAS) can have a significant impact in computer vision by automatically designing optimal neural network architectures for various tasks. A variant, binarized neural architecture search (BNAS), with a search space of binarized convolutions, can produce extremely compressed models. Unfortunately, this area remains largely unexplored. BNAS is more challenging than NAS due to the learning inefficiency caused by optimization requirements and the huge architecture space. To address these issues, we introduce channel sampling and operation space reduction into a differentiable NAS to significantly reduce the cost of searching. This is accomplished through a performance-based strategy used to abandon less potential operations. Two optimization methods for binarized neural networks are used to validate the effectiveness of our BNAS. Extensive experiments demonstrate that the proposed BNAS achieves a performance comparable to NAS on both CIFAR and ImageNet databases. An accuracy of $96.53\\%$ vs. $97.22\\%$ is achieved on the CIFAR-10 dataset, but with a significantly compressed model, and a $40\\%$ faster search than the state-of-the-art PC-DARTS."},{"title":"Learning Logic Rules for Document-level Relation Extraction","source":"Dongyu Ru and Changzhi Sun and Jiangtao Feng and Lin Qiu and Hao Zhou\n  and Weinan Zhang and Yong Yu and Lei Li","docs_id":"2111.05407","section":["cs.CL"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Learning Logic Rules for Document-level Relation Extraction. Document-level relation extraction aims to identify relations between entities in a whole document. Prior efforts to capture long-range dependencies have relied heavily on implicitly powerful representations learned through (graph) neural networks, which makes the model less transparent. To tackle this challenge, in this paper, we propose LogiRE, a novel probabilistic model for document-level relation extraction by learning logic rules. LogiRE treats logic rules as latent variables and consists of two modules: a rule generator and a relation extractor. The rule generator is to generate logic rules potentially contributing to final predictions, and the relation extractor outputs final predictions based on the generated logic rules. Those two modules can be efficiently optimized with the expectation-maximization (EM) algorithm. By introducing logic rules into neural networks, LogiRE can explicitly capture long-range dependencies as well as enjoy better interpretation. Empirical results show that LogiRE significantly outperforms several strong baselines in terms of relation performance (1.8 F1 score) and logical consistency (over 3.3 logic score). Our code is available at https:\/\/github.com\/rudongyu\/LogiRE."},{"title":"Bayesian\/Graphoid intersection property for factorisation spaces","source":"Gr\\'egoire Sergeant-Perthuis","docs_id":"1903.06026","section":["math.ST","cs.IT","math.IT","physics.data-an","stat.TH"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Bayesian\/Graphoid intersection property for factorisation spaces. We remark that Pearl's Graphoid intersection property, also called intersection property in Bayesian networks, is a particular case of a general intersection property, in the sense of intersection of coverings, for factorisation spaces, also coined as factorisation models, factor graphs or by Lauritzen in his reference book 'Graphical Models' as hierarchical model subspaces. A particular case of this intersection property appears in Lauritzen's book as a consequence of the decomposition into interaction subspaces; the novel proof that we give of this result allows us to extend it in the most general setting. It also allows us to give a direct and new proof of the Hammersley-Clifford theorem transposing and reducing it to a corresponding statement for graphs, justifying formally the geometric intuition of independency, and extending it to non finite graphs. This intersection property is the starting point for a generalization of the decomposition into interaction subspaces to collections of vector spaces."},{"title":"Feature Tracking Cardiac Magnetic Resonance via Deep Learning and Spline\n  Optimization","source":"Davis M. Vigneault, Weidi Xie, David A. Bluemke, and J. Alison Noble","docs_id":"1704.03660","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Feature Tracking Cardiac Magnetic Resonance via Deep Learning and Spline\n  Optimization. Feature tracking Cardiac Magnetic Resonance (CMR) has recently emerged as an area of interest for quantification of regional cardiac function from balanced, steady state free precession (SSFP) cine sequences. However, currently available techniques lack full automation, limiting reproducibility. We propose a fully automated technique whereby a CMR image sequence is first segmented with a deep, fully convolutional neural network (CNN) architecture, and quadratic basis splines are fitted simultaneously across all cardiac frames using least squares optimization. Experiments are performed using data from 42 patients with hypertrophic cardiomyopathy (HCM) and 21 healthy control subjects. In terms of segmentation, we compared state-of-the-art CNN frameworks, U-Net and dilated convolution architectures, with and without temporal context, using cross validation with three folds. Performance relative to expert manual segmentation was similar across all networks: pixel accuracy was ~97%, intersection-over-union (IoU) across all classes was ~87%, and IoU across foreground classes only was ~85%. Endocardial left ventricular circumferential strain calculated from the proposed pipeline was significantly different in control and disease subjects (-25.3% vs -29.1%, p = 0.006), in agreement with the current clinical literature."},{"title":"MemTorch: An Open-source Simulation Framework for Memristive Deep\n  Learning Systems","source":"Corey Lammie, Wei Xiang, Bernab\\'e Linares-Barranco, Mostafa Rahimi\n  Azghadi","docs_id":"2004.10971","section":["cs.ET"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"MemTorch: An Open-source Simulation Framework for Memristive Deep\n  Learning Systems. Memristive devices have shown great promise to facilitate the acceleration and improve the power efficiency of Deep Learning (DL) systems. Crossbar architectures constructed using these Resistive Random-Access Memory (RRAM) devices can be used to efficiently implement various in-memory computing operations, such as Multiply Accumulate (MAC) and unrolled-convolutions, which are used extensively in Deep Neural Networks (DNNs) and Convolutional Neural Networks (CNNs). However, memristive devices face concerns of aging and non-idealities, which limit the accuracy, reliability, and robustness of Memristive Deep Learning Systems (MDLSs), that should be considered prior to circuit-level realization. This Original Software Publication (OSP) presents MemTorch, an open-source framework for customized large-scale memristive DL simulations, with a refined focus on the co-simulation of device non-idealities. MemTorch also facilitates co-modelling of key crossbar peripheral circuitry. MemTorch adopts a modernized soft-ware engineering methodology and integrates directly with the well-known PyTorch Machine Learning (ML) library"},{"title":"Perception-Based Temporal Logic Planning in Uncertain Semantic Maps","source":"Yiannis Kantaros, Samarth Kalluraya, Qi Jin, and George J. Pappas","docs_id":"2012.10490","section":["cs.RO"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Perception-Based Temporal Logic Planning in Uncertain Semantic Maps. This paper addresses a multi-robot planning problem in partially unknown semantic environments. The environment is assumed to have known geometric structure (e.g., walls) and to be occupied by static labeled landmarks with uncertain positions and classes. This modeling approach gives rise to an uncertain semantic map generated by semantic SLAM algorithms. Our goal is to design control policies for robots equipped with noisy perception systems so that they can accomplish collaborative tasks captured by global temporal logic specifications. To account for environmental and perceptual uncertainty, we extend a fragment of Linear Temporal Logic (LTL), called co-safe LTL, by including perception-based atomic predicates allowing us to incorporate uncertainty-wise and probabilistic satisfaction requirements directly into the task specification. The perception-based LTL planning problem gives rise to an optimal control problem, solved by a novel sampling-based algorithm, that generates open-loop control policies that are updated online to adapt to a continuously learned semantic map. We provide extensive experiments to demonstrate the efficiency of the proposed planning architecture."},{"title":"Restoration of Non-rigidly Distorted Underwater Images using a\n  Combination of Compressive Sensing and Local Polynomial Image Representations","source":"Jerin Geo James, Pranay Agrawal, Ajit Rajwade","docs_id":"1908.01940","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Restoration of Non-rigidly Distorted Underwater Images using a\n  Combination of Compressive Sensing and Local Polynomial Image Representations. Images of static scenes submerged beneath a wavy water surface exhibit severe non-rigid distortions. The physics of water flow suggests that water surfaces possess spatio-temporal smoothness and temporal periodicity. Hence they possess a sparse representation in the 3D discrete Fourier (DFT) basis. Motivated by this, we pose the task of restoration of such video sequences as a compressed sensing (CS) problem. We begin by tracking a few salient feature points across the frames of a video sequence of the submerged scene. Using these point trajectories, we show that the motion fields at all other (non-tracked) points can be effectively estimated using a typical CS solver. This by itself is a novel contribution in the field of non-rigid motion estimation. We show that this method outperforms state of the art algorithms for underwater image restoration. We further consider a simple optical flow algorithm based on local polynomial expansion of the image frames (PEOF). Surprisingly, we demonstrate that PEOF is more efficient and often outperforms all the state of the art methods in terms of numerical measures. Finally, we demonstrate that a two-stage approach consisting of the CS step followed by PEOF much more accurately preserves the image structure and improves the (visual as well as numerical) video quality as compared to just the PEOF stage."},{"title":"Graph Universal Adversarial Attacks: A Few Bad Actors Ruin Graph\n  Learning Models","source":"Xiao Zang, Yi Xie, Jie Chen, Bo Yuan","docs_id":"2002.04784","section":["cs.LG","cs.CR","cs.SI","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Graph Universal Adversarial Attacks: A Few Bad Actors Ruin Graph\n  Learning Models. Deep neural networks, while generalize well, are known to be sensitive to small adversarial perturbations. This phenomenon poses severe security threat and calls for in-depth investigation of the robustness of deep learning models. With the emergence of neural networks for graph structured data, similar investigations are urged to understand their robustness. It has been found that adversarially perturbing the graph structure and\/or node features may result in a significant degradation of the model performance. In this work, we show from a different angle that such fragility similarly occurs if the graph contains a few bad-actor nodes, which compromise a trained graph neural network through flipping the connections to any targeted victim. Worse, the bad actors found for one graph model severely compromise other models as well. We call the bad actors ``anchor nodes'' and propose an algorithm, named GUA, to identify them. Thorough empirical investigations suggest an interesting finding that the anchor nodes often belong to the same class; and they also corroborate the intuitive trade-off between the number of anchor nodes and the attack success rate. For the dataset Cora which contains 2708 nodes, as few as six anchor nodes will result in an attack success rate higher than 80\\% for GCN and other three models."},{"title":"Neural Machine Translation for Low-Resource Languages: A Survey","source":"Surangika Ranathunga, En-Shiun Annie Lee, Marjana Prifti Skenduli,\n  Ravi Shekhar, Mehreen Alam and Rishemjit Kaur","docs_id":"2106.15115","section":["cs.CL","cs.AI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Neural Machine Translation for Low-Resource Languages: A Survey. Neural Machine Translation (NMT) has seen a tremendous spurt of growth in less than ten years, and has already entered a mature phase. While considered as the most widely used solution for Machine Translation, its performance on low-resource language pairs still remains sub-optimal compared to the high-resource counterparts, due to the unavailability of large parallel corpora. Therefore, the implementation of NMT techniques for low-resource language pairs has been receiving the spotlight in the recent NMT research arena, thus leading to a substantial amount of research reported on this topic. This paper presents a detailed survey of research advancements in low-resource language NMT (LRL-NMT), along with a quantitative analysis aimed at identifying the most popular solutions. Based on our findings from reviewing previous work, this survey paper provides a set of guidelines to select the possible NMT technique for a given LRL data setting. It also presents a holistic view of the LRL-NMT research landscape and provides a list of recommendations to further enhance the research efforts on LRL-NMT."},{"title":"DropBlock: A regularization method for convolutional networks","source":"Golnaz Ghiasi, Tsung-Yi Lin, Quoc V. Le","docs_id":"1810.12890","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"DropBlock: A regularization method for convolutional networks. Deep neural networks often work well when they are over-parameterized and trained with a massive amount of noise and regularization, such as weight decay and dropout. Although dropout is widely used as a regularization technique for fully connected layers, it is often less effective for convolutional layers. This lack of success of dropout for convolutional layers is perhaps due to the fact that activation units in convolutional layers are spatially correlated so information can still flow through convolutional networks despite dropout. Thus a structured form of dropout is needed to regularize convolutional networks. In this paper, we introduce DropBlock, a form of structured dropout, where units in a contiguous region of a feature map are dropped together. We found that applying DropbBlock in skip connections in addition to the convolution layers increases the accuracy. Also, gradually increasing number of dropped units during training leads to better accuracy and more robust to hyperparameter choices. Extensive experiments show that DropBlock works better than dropout in regularizing convolutional networks. On ImageNet classification, ResNet-50 architecture with DropBlock achieves $78.13\\%$ accuracy, which is more than $1.6\\%$ improvement on the baseline. On COCO detection, DropBlock improves Average Precision of RetinaNet from $36.8\\%$ to $38.4\\%$."},{"title":"Topological Interference Management with Confidential Messages","source":"Jean de Dieu Mutangana, Ravi Tandon","docs_id":"2010.14503","section":["cs.IT","cs.CR","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Topological Interference Management with Confidential Messages. The topological interference management (TIM) problem refers to the study of the K-user partially connected interference networks with no channel state information at the transmitters (CSIT), except for the knowledge of network topology. In this paper, we study the TIM problem with confidential messages (TIM-CM), where message confidentiality must be satisfied in addition to reliability constraints. In particular, each transmitted message must be decodable at its intended receiver and remain confidential at the remaining (K-1) receivers. Our main contribution is to present a comprehensive set of results for the TIM-CM problem by studying the symmetric secure degrees of freedom (SDoF). To this end, we first characterize necessary and sufficient conditions for feasibility of positive symmetric SDoF for any arbitrary topology. We next present two achievable schemes for the TIM-CM problem: For the first scheme, we use the concept of secure partition and, for the second one, we use the concept of secure maximal independent sets. We also present outer bounds on symmetric SDoF for any arbitrary network topology. Using these bounds, we characterize the optimal symmetric SDoF of all K=2-user and K=3-user network topologies."},{"title":"A Dynamical Model of Twitter Activity Profiles","source":"Hoai Nguyen Huynh, Erika Fille Legara, Christopher Monterola","docs_id":"1508.07097","section":["cs.SI","cs.CY","cs.HC","physics.soc-ph"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A Dynamical Model of Twitter Activity Profiles. The advent of the era of Big Data has allowed many researchers to dig into various socio-technical systems, including social media platforms. In particular, these systems have provided them with certain verifiable means to look into certain aspects of human behavior. In this work, we are specifically interested in the behavior of individuals on social media platforms---how they handle the information they get, and how they share it. We look into Twitter to understand the dynamics behind the users' posting activities---tweets and retweets---zooming in on topics that peaked in popularity. Three mechanisms are considered: endogenous stimuli, exogenous stimuli, and a mechanism that dictates the decay of interest of the population in a topic. We propose a model involving two parameters $\\eta^\\star$ and $\\lambda$ describing the tweeting behaviour of users, which allow us to reconstruct the findings of Lehmann et al. (2012) on the temporal profiles of popular Twitter hashtags. With this model, we are able to accurately reproduce the temporal profile of user engagements on Twitter. Furthermore, we introduce an alternative in classifying the collective activities on the socio-technical system based on the model."},{"title":"Time Segmentation Approach Allowing QoS and Energy Saving for Wireless\n  Sensor Networks","source":"Gerard Chalhoub, Fran\\c{c}ois Delobel and Michel Misson","docs_id":"1005.5118","section":["cs.NI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Time Segmentation Approach Allowing QoS and Energy Saving for Wireless\n  Sensor Networks. Wireless sensor networks are conceived to monitor a certain application or physical phenomena and are supposed to function for several years without any human intervention for maintenance. Thus, the main issue in sensor networks is often to extend the lifetime of the network by reducing energy consumption. On the other hand, some applications have high priority traffic that needs to be transferred within a bounded end-to-end delay while maintaining an energy efficient behavior. We propose MaCARI, a time segmentation protocol that saves energy, improves the overall performance of the network and enables quality of service in terms of guaranteed access to the medium and end-to-end delays. This time segmentation is achieved by synchronizing the activity of nodes using a tree-based beacon propagation and allocating activity periods for each cluster of nodes. The tree-based topology is inspired from the cluster-tree proposed by the ZigBee standard. The efficiency of our protocol is proven analytically, by simulation and through real testbed measurements."},{"title":"Differentiable Channel Sparsity Search via Weight Sharing within Filters","source":"Yu Zhao, Chung-Kuei Lee","docs_id":"2010.14714","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Differentiable Channel Sparsity Search via Weight Sharing within Filters. In this paper, we propose the differentiable channel sparsity search (DCSS) for convolutional neural networks. Unlike traditional channel pruning algorithms which require users to manually set prune ratios for each convolutional layer, DCSS automatically searches the optimal combination of sparsities. Inspired by the differentiable architecture search (DARTS), we draw lessons from the continuous relaxation and leverage the gradient information to balance the computational cost and metrics. Since directly applying the scheme of DARTS causes shape mismatching and excessive memory consumption, we introduce a novel technique called weight sharing within filters. This technique elegantly eliminates the problem of shape mismatching with negligible additional resources. We conduct comprehensive experiments on not only image classification but also find-grained tasks including semantic segmentation and image super resolution to verify the effectiveness of DCSS. Compared with previous network pruning approaches, DCSS achieves state-of-the-art results for image classification. Experimental results of semantic segmentation and image super resolution indicate that task-specific search achieves better performance than transferring slim models, demonstrating the wide applicability and high efficiency of DCSS."},{"title":"Superpixels Based Segmentation and SVM Based Classification Method to\n  Distinguish Five Diseases from Normal Regions in Wireless Capsule Endoscopy","source":"Omid Haji Maghsoudi","docs_id":"1711.06616","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Superpixels Based Segmentation and SVM Based Classification Method to\n  Distinguish Five Diseases from Normal Regions in Wireless Capsule Endoscopy. Wireless Capsule Endoscopy (WCE) is relatively a new technology to examine the entire GI trace. During an examination, it captures more than 55,000 frames. Reviewing all these images is time-consuming and prone to human error. It has been a challenge to develop intelligent methods assisting physicians to review the frames. The WCE frames are captured in 8-bit color depths which provides enough a color range to detect abnormalities. Here, superpixel based methods are proposed to segment five diseases including: bleeding, Crohn's disease, Lymphangiectasia, Xanthoma, and Lymphoid hyperplasia. Two superpixels methods are compared to provide semantic segmentation of these prolific diseases: simple linear iterative clustering (SLIC) and quick shift (QS). The segmented superpixels were classified into two classes (normal and abnormal) by support vector machine (SVM) using texture and color features. For both superpixel methods, the accuracy, specificity, sensitivity, and precision (SLIC, QS) were around 92%, 93%, 93%, and 88%, respectively. However, SLIC was dramatically faster than QS."},{"title":"Machine Learning for Massive Industrial Internet of Things","source":"Hui Zhou, Changyang She, Yansha Deng, Mischa Dohler, and Arumugam\n  Nallanathan","docs_id":"2103.08308","section":["cs.LG","cs.NI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Machine Learning for Massive Industrial Internet of Things. Industrial Internet of Things (IIoT) revolutionizes the future manufacturing facilities by integrating the Internet of Things technologies into industrial settings. With the deployment of massive IIoT devices, it is difficult for the wireless network to support the ubiquitous connections with diverse quality-of-service (QoS) requirements. Although machine learning is regarded as a powerful data-driven tool to optimize wireless network, how to apply machine learning to deal with the massive IIoT problems with unique characteristics remains unsolved. In this paper, we first summarize the QoS requirements of the typical massive non-critical and critical IIoT use cases. We then identify unique characteristics in the massive IIoT scenario, and the corresponding machine learning solutions with its limitations and potential research directions. We further present the existing machine learning solutions for individual layer and cross-layer problems in massive IIoT. Last but not the least, we present a case study of massive access problem based on deep neural network and deep reinforcement learning techniques, respectively, to validate the effectiveness of machine learning in massive IIoT scenario."},{"title":"Deep Learning for Functional Data Analysis with Adaptive Basis Layers","source":"Junwen Yao, Jonas Mueller, Jane-Ling Wang","docs_id":"2106.10414","section":["stat.ML","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Deep Learning for Functional Data Analysis with Adaptive Basis Layers. Despite their widespread success, the application of deep neural networks to functional data remains scarce today. The infinite dimensionality of functional data means standard learning algorithms can be applied only after appropriate dimension reduction, typically achieved via basis expansions. Currently, these bases are chosen a priori without the information for the task at hand and thus may not be effective for the designated task. We instead propose to adaptively learn these bases in an end-to-end fashion. We introduce neural networks that employ a new Basis Layer whose hidden units are each basis functions themselves implemented as a micro neural network. Our architecture learns to apply parsimonious dimension reduction to functional inputs that focuses only on information relevant to the target rather than irrelevant variation in the input function. Across numerous classification\/regression tasks with functional data, our method empirically outperforms other types of neural networks, and we prove that our approach is statistically consistent with low generalization error. Code is available at: \\url{https:\/\/github.com\/jwyyy\/AdaFNN}."},{"title":"Bounds on Asymptotic Rate of Capacitive Crosstalk Avoidance Codes for\n  On-chip Buses","source":"Tadashi Wadayama and Taisuke Izumi","docs_id":"1601.06880","section":["cs.IT","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Bounds on Asymptotic Rate of Capacitive Crosstalk Avoidance Codes for\n  On-chip Buses. In order to prevent the capacitive crosstalk in on-chip buses, several types of capacitive crosstalk avoidance codes have been devised. These codes are designed to prohibit transition patterns prone to the capacity crosstalk from any consecutive two words transmitted to on-chip buses. This paper provides a rigorous analysis on the asymptotic rate of (p,q)-transition free word sequences under the assumption that coding is based on a pair of a stateful encoder and a stateless decoder. The symbols p and q represent k-bit transition patterns that should not be appeared in any consecutive two words at the same adjacent k-bit positions. It is proved that the maximum rate of the sequences equals to the subgraph domatic number of (p,q)-transition free graph. Based on the theoretical results on the subgraph domatic partition problem, a pair of lower and upper bounds on the asymptotic rate is derived. We also present that the asymptotic rate 0.8325 is achievable for the (10,01)-transition free word sequences."},{"title":"Non-Local Graph-Based Prediction For Reversible Data Hiding In Images","source":"Qi Chang and Gene Cheung and Yao Zhao and Xiaolong Li and Rongrong Ni","docs_id":"1802.06935","section":["eess.IV","cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Non-Local Graph-Based Prediction For Reversible Data Hiding In Images. Reversible data hiding (RDH) is desirable in applications where both the hidden message and the cover medium need to be recovered without loss. Among many RDH approaches is prediction-error expansion (PEE), containing two steps: i) prediction of a target pixel value, and ii) embedding according to the value of prediction-error. In general, higher prediction performance leads to larger embedding capacity and\/or lower signal distortion. Leveraging on recent advances in graph signal processing (GSP), we pose pixel prediction as a graph-signal restoration problem, where the appropriate edge weights of the underlying graph are computed using a similar patch searched in a semi-local neighborhood. Specifically, for each candidate patch, we first examine eigenvalues of its structure tensor to estimate its local smoothness. If sufficiently smooth, we pose a maximum a posteriori (MAP) problem using either a quadratic Laplacian regularizer or a graph total variation (GTV) term as signal prior. While the MAP problem using the first prior has a closed-form solution, we design an efficient algorithm for the second prior using alternating direction method of multipliers (ADMM) with nested proximal gradient descent. Experimental results show that with better quality GSP-based prediction, at low capacity the visual quality of the embedded image exceeds state-of-the-art methods noticeably."},{"title":"PrivacyProxy: Leveraging Crowdsourcing and In Situ Traffic Analysis to\n  Detect and Mitigate Information Leakage","source":"Gaurav Srivastava, Kunal Bhuwalka, Swarup Kumar Sahoo, Saksham\n  Chitkara, Kevin Ku, Matt Fredrikson, Jason Hong, Yuvraj Agarwal","docs_id":"1708.06384","section":["cs.CR"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"PrivacyProxy: Leveraging Crowdsourcing and In Situ Traffic Analysis to\n  Detect and Mitigate Information Leakage. Many smartphone apps transmit personally identifiable information (PII), often without the users knowledge. To address this issue, we present PrivacyProxy, a system that monitors outbound network traffic and generates app-specific signatures to represent sensitive data being shared. PrivacyProxy uses a crowd-based approach to detect likely PII in an adaptive and scalable manner by anonymously combining signatures from different users of the same app. Furthermore, we do not observe users network traffic and instead rely on hashed signatures. We present the design and implementation of PrivacyProxy and evaluate it with a lab study, a field deployment, a user survey, and a comparison against prior work. Our field study shows PrivacyProxy can automatically detect PII with an F1 score of 0.885. PrivacyProxy also achieves an F1 score of 0.759 in our controlled experiment for the 500 most popular apps. The F1 score also improves to 0.866 with additional training data for 40 apps that initially had the most false positives. We also show performance overhead of using PrivacyProxy is between 8.6% to 14.2%, slightly more than using a standard unmodified VPN, and most users report no perceptible impact on battery life or the network."},{"title":"Modelling System of Systems Interface Contract Behaviour","source":"Oldrich Faldik (Mendel University, Brno, Czech Republic), Richard\n  Payne (Newcastle University, Newcastle upon Tyne, United Kingdom), John\n  Fitzgerald (Newcastle University, Newcastle upon Tyne, United Kingdom),\n  Barbora Buhnova (Masaryk University, Brno, Czech Republic)","docs_id":"1703.07037","section":["cs.SE","cs.FL"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Modelling System of Systems Interface Contract Behaviour. A key challenge in System of Systems (SoS) engineering is the analysis and maintenance of global properties under SoS evolution, and the integration of new constituent elements. There is a need to model the constituent systems composing a SoS in order to allow the analysis of emergent behaviours at the SoS boundary. The Contract pattern allows the engineer to specify constrained behaviours to which constituent systems are required to conform in order to be a part of the SoS. However, the Contract pattern faces some limitations in terms of its accessibility and suitability for verifying contract compatibility. To address these deficiencies, we propose the enrichment of the Contract pattern, which hitherto has been defined using SysML and the COMPASS Modelling Language (CML), by utilising SysML and Object Constraint Language (OCL). In addition, we examine the potential of interface automata, a notation for improving loose coupling between interfaces of constituent systems defined according to the contract, as a means of enabling the verification of contract compatibility. The approach is demonstrated using a case study in audio\/video content streaming."},{"title":"Expert-Guided Symmetry Detection in Markov Decision Processes","source":"Giorgio Angelotti, Nicolas Drougard, Caroline P. C. Chanel","docs_id":"2111.10297","section":["cs.LG","cs.AI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Expert-Guided Symmetry Detection in Markov Decision Processes. Learning a Markov Decision Process (MDP) from a fixed batch of trajectories is a non-trivial task whose outcome's quality depends on both the amount and the diversity of the sampled regions of the state-action space. Yet, many MDPs are endowed with invariant reward and transition functions with respect to some transformations of the current state and action. Being able to detect and exploit these structures could benefit not only the learning of the MDP but also the computation of its subsequent optimal control policy. In this work we propose a paradigm, based on Density Estimation methods, that aims to detect the presence of some already supposed transformations of the state-action space for which the MDP dynamics is invariant. We tested the proposed approach in a discrete toroidal grid environment and in two notorious environments of OpenAI's Gym Learning Suite. The results demonstrate that the model distributional shift is reduced when the dataset is augmented with the data obtained by using the detected symmetries, allowing for a more thorough and data-efficient learning of the transition functions."},{"title":"Logic as a distributive law","source":"Mike Stay, Lucius Gregory Meredith","docs_id":"1610.02247","section":["cs.LO"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Logic as a distributive law. We present an algorithm for deriving a spatial-behavioral type system from a formal presentation of a computational calculus. Given a 2-monad Calc: Catv$\\to$ Cat for the free calculus on a category of terms and rewrites and a 2-monad BoolAlg for the free Boolean algebra on a category, we get a 2-monad Form = BoolAlg + Calc for the free category of formulae and proofs. We also get the 2-monad BoolAlg $\\circ$ Calc for subsets of terms. The interpretation of formulae is a natural transformation $\\interp{-}$: Form $\\Rightarrow$ BoolAlg $\\circ$ Calc defined by the units and multiplications of the monads and a distributive law transformation $\\delta$: Calc $\\circ$ BoolAlg $\\Rightarrow$ BoolAlg $\\circ$ Calc. This interpretation is consistent both with the Curry-Howard isomorphism and with realizability. We give an implementation of the \"possibly\" modal operator parametrized by a two-hole term context and show that, surprisingly, the arrow type constructor in the $\\lambda$-calculus is a specific case. We also exhibit nontrivial formulae encoding confinement and liveness properties for a reflective higher-order variant of the $\\pi$-calculus."},{"title":"Enumerating all maximal biclusters in numerical datasets","source":"Rosana Veroneze, Arindam Banerjee and Fernando J. Von Zuben","docs_id":"1403.3562","section":["cs.DM"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Enumerating all maximal biclusters in numerical datasets. Biclustering has proved to be a powerful data analysis technique due to its wide success in various application domains. However, the existing literature presents efficient solutions only for enumerating maximal biclusters with constant values, or heuristic-based approaches which can not find all biclusters or even support the maximality of the obtained biclusters. Here, we present a general family of biclustering algorithms for enumerating all maximal biclusters with (i) constant values on rows, (ii) constant values on columns, or (iii) coherent values. Versions for perfect and for perturbed biclusters are provided. Our algorithms have four key properties (just the algorithm for perturbed biclusters with coherent values fails to exhibit the first property): they are (1) efficient (take polynomial time per pattern), (2) complete (find all maximal biclusters), (3) correct (all biclusters attend the user-defined measure of similarity), and (4) non-redundant (all the obtained biclusters are maximal and the same bicluster is not enumerated twice). They are based on a generalization of an efficient formal concept analysis algorithm called In-Close2. Experimental results point to the necessity of having efficient enumerative biclustering algorithms and provide a valuable insight into the scalability of our family of algorithms and its sensitivity to user-defined parameters."},{"title":"A Deep Reinforced Model for Zero-Shot Cross-Lingual Summarization with\n  Bilingual Semantic Similarity Rewards","source":"Zi-Yi Dou, Sachin Kumar, Yulia Tsvetkov","docs_id":"2006.15454","section":["cs.CL"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A Deep Reinforced Model for Zero-Shot Cross-Lingual Summarization with\n  Bilingual Semantic Similarity Rewards. Cross-lingual text summarization aims at generating a document summary in one language given input in another language. It is a practically important but under-explored task, primarily due to the dearth of available data. Existing methods resort to machine translation to synthesize training data, but such pipeline approaches suffer from error propagation. In this work, we propose an end-to-end cross-lingual text summarization model. The model uses reinforcement learning to directly optimize a bilingual semantic similarity metric between the summaries generated in a target language and gold summaries in a source language. We also introduce techniques to pre-train the model leveraging monolingual summarization and machine translation objectives. Experimental results in both English--Chinese and English--German cross-lingual summarization settings demonstrate the effectiveness of our methods. In addition, we find that reinforcement learning models with bilingual semantic similarity as rewards generate more fluent sentences than strong baselines."},{"title":"Kernel Two-Sample Tests for Manifold Data","source":"Xiuyuan Cheng, Yao Xie","docs_id":"2105.03425","section":["stat.ML","cs.LG","math.ST","stat.TH"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Kernel Two-Sample Tests for Manifold Data. We present a study of kernel based two-sample test statistic, which is related to the Maximum Mean Discrepancy (MMD), in the manifold data setting, assuming that high-dimensional observations are close to a low-dimensional manifold. We characterize the test level and power in relation to the kernel bandwidth, the number of samples, and the intrinsic dimensionality of the manifold. Specifically, we show that when data densities are supported on a $d$-dimensional sub-manifold $\\mathcal{M}$ embedded in an $m$-dimensional space, the kernel two-sample test for data sampled from a pair of distributions $(p, q)$ that are H\\\"older with order $\\beta$ is consistent and powerful when the number of samples $n$ is greater than $\\delta_2(p,q)^{-2-d\/\\beta}$ up to certain constant, where $\\delta_2$ is the squared $\\ell_2$-divergence between two distributions on manifold. Moreover, to achieve testing consistency under this scaling of $n$, our theory suggests that the kernel bandwidth $\\gamma$ scales with $n^{-1\/(d+2\\beta)}$. These results indicate that the kernel two-sample test does not have a curse-of-dimensionality when the data lie on a low-dimensional manifold. We demonstrate the validity of our theory and the property of the kernel test for manifold data using several numerical experiments."},{"title":"ParPaRaw: Massively Parallel Parsing of Delimiter-Separated Raw Data","source":"Elias Stehle and Hans-Arno Jacobsen","docs_id":"1905.13415","section":["cs.DB","cs.DC","cs.DS"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"ParPaRaw: Massively Parallel Parsing of Delimiter-Separated Raw Data. Parsing is essential for a wide range of use cases, such as stream processing, bulk loading, and in-situ querying of raw data. Yet, the compute-intense step often constitutes a major bottleneck in the data ingestion pipeline, since parsing of inputs that require more involved parsing rules is challenging to parallelise. This work proposes a massively parallel algorithm for parsing delimiter-separated data formats on GPUs. Other than the state-of-the-art, the proposed approach does not require an initial sequential pass over the input to determine a thread's parsing context. That is, how a thread, beginning somewhere in the middle of the input, should interpret a certain symbol (e.g., whether to interpret a comma as a delimiter or as part of a larger string enclosed in double-quotes). Instead of tailoring the approach to a single format, we are able to perform a massively parallel FSM simulation, which is more flexible and powerful, supporting more expressive parsing rules with general applicability. Achieving a parsing rate of as much as 14.2 GB\/s, our experimental evaluation on a GPU with 3584 cores shows that the presented approach is able to scale to thousands of cores and beyond. With an end-to-end streaming approach, we are able to exploit the full-duplex capabilities of the PCIe bus and hide latency from data transfers. Considering the end-to-end performance, the algorithm parses 4.8 GB in as little as 0.44 seconds, including data transfers."},{"title":"The FastMap Algorithm for Shortest Path Computations","source":"Liron Cohen, Tansel Uras, Shiva Jahangiri, Aliyah Arunasalam, Sven\n  Koenig, T.K. Satish Kumar","docs_id":"1706.02792","section":["cs.AI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"The FastMap Algorithm for Shortest Path Computations. We present a new preprocessing algorithm for embedding the nodes of a given edge-weighted undirected graph into a Euclidean space. The Euclidean distance between any two nodes in this space approximates the length of the shortest path between them in the given graph. Later, at runtime, a shortest path between any two nodes can be computed with A* search using the Euclidean distances as heuristic. Our preprocessing algorithm, called FastMap, is inspired by the data mining algorithm of the same name and runs in near-linear time. Hence, FastMap is orders of magnitude faster than competing approaches that produce a Euclidean embedding using Semidefinite Programming. FastMap also produces admissible and consistent heuristics and therefore guarantees the generation of shortest paths. Moreover, FastMap applies to general undirected graphs for which many traditional heuristics, such as the Manhattan Distance heuristic, are not well defined. Empirically, we demonstrate that A* search using the FastMap heuristic is competitive with A* search using other state-of-the-art heuristics, such as the Differential heuristic."},{"title":"A novel shape matching descriptor for real-time hand gesture recognition","source":"Michalis Lazarou, Bo Li, Tania Stathaki","docs_id":"2101.03923","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A novel shape matching descriptor for real-time hand gesture recognition. The current state-of-the-art hand gesture recognition methodologies heavily rely in the use of machine learning. However there are scenarios that machine learning cannot be applied successfully, for example in situations where data is scarce. This is the case when one-to-one matching is required between a query and a dataset of hand gestures where each gesture represents a unique class. In situations where learning algorithms cannot be trained, classic computer vision techniques such as feature extraction can be used to identify similarities between objects. Shape is one of the most important features that can be extracted from images, however the most accurate shape matching algorithms tend to be computationally inefficient for real-time applications. In this work we present a novel shape matching methodology for real-time hand gesture recognition. Extensive experiments were carried out comparing our method with other shape matching methods with respect to accuracy and computational complexity using our own collected hand gesture dataset and a modified version of the MPEG-7 dataset.%that is widely used for comparing 2D shape matching algorithms. Our method outperforms the other methods and provides a good combination of accuracy and computational efficiency for real-time applications."},{"title":"Supplementary Variable Method for Developing Structure-Preserving\n  Numerical Approximations to Thermodynamically Consistent Partial Differential\n  Equations","source":"Yuezheng Gong, Qi Hong and Qi Wang","docs_id":"2006.04348","section":["math.NA","cs.NA"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Supplementary Variable Method for Developing Structure-Preserving\n  Numerical Approximations to Thermodynamically Consistent Partial Differential\n  Equations. We present a new temporal discretization paradigm for developing energy-production-rate preserving numerical approximations to thermodynamically consistent partial differential equation systems, called the supplementary variable method. The central idea behind it is to introduce a supplementary variable to the thermodynamically consistent model to make the over-determined equation system, consisting of the thermodynamically consistent PDE system, the energy definition and the energy dissipation equation, structurally stable. The supplementary variable allows one to retain the consistency between the energy dissipation equation and the PDE system after the temporal discretization. We illustrate the method using a dissipative gradient flow model. Among virtually infinite many possibilities, we present two ways to add the supplementary variable in the gradient flow model to develop energy-dissipation-rate preserving algorithms. Spatial discretizations are carried out using the pseudo-spectral method. We then compare the two new schemes with the energy stable SAV scheme and the fully implicit Crank-Nicolson scheme. The results favor the new schemes in the overall performance. This new numerical paradigm can be applied to any thermodynamically consistent models."},{"title":"A Domain-Independent Algorithm for Plan Adaptation","source":"S. Hanks, D. S. Weld","docs_id":"cs\/9501102","section":["cs.AI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A Domain-Independent Algorithm for Plan Adaptation. The paradigms of transformational planning, case-based planning, and plan debugging all involve a process known as plan adaptation - modifying or repairing an old plan so it solves a new problem. In this paper we provide a domain-independent algorithm for plan adaptation, demonstrate that it is sound, complete, and systematic, and compare it to other adaptation algorithms in the literature. Our approach is based on a view of planning as searching a graph of partial plans. Generative planning starts at the graph's root and moves from node to node using plan-refinement operators. In planning by adaptation, a library plan - an arbitrary node in the plan graph - is the starting point for the search, and the plan-adaptation algorithm can apply both the same refinement operators available to a generative planner and can also retract constraints and steps from the plan. Our algorithm's completeness ensures that the adaptation algorithm will eventually search the entire graph and its systematicity ensures that it will do so without redundantly searching any parts of the graph."},{"title":"Estimating Maximally Probable Constrained Relations by Mathematical\n  Programming","source":"Lizhen Qu and Bjoern Andres","docs_id":"1408.0838","section":["cs.LG","cs.NA","math.OC","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Estimating Maximally Probable Constrained Relations by Mathematical\n  Programming. Estimating a constrained relation is a fundamental problem in machine learning. Special cases are classification (the problem of estimating a map from a set of to-be-classified elements to a set of labels), clustering (the problem of estimating an equivalence relation on a set) and ranking (the problem of estimating a linear order on a set). We contribute a family of probability measures on the set of all relations between two finite, non-empty sets, which offers a joint abstraction of multi-label classification, correlation clustering and ranking by linear ordering. Estimating (learning) a maximally probable measure, given (a training set of) related and unrelated pairs, is a convex optimization problem. Estimating (inferring) a maximally probable relation, given a measure, is a 01-linear program. It is solved in linear time for maps. It is NP-hard for equivalence relations and linear orders. Practical solutions for all three cases are shown in experiments with real data. Finally, estimating a maximally probable measure and relation jointly is posed as a mixed-integer nonlinear program. This formulation suggests a mathematical programming approach to semi-supervised learning."},{"title":"Policy documents as sources for measuring societal impact: How often is\n  climate change research mentioned in policy-related documents?","source":"Lutz Bornmann, Robin Haunschild, Werner Marx","docs_id":"1512.07071","section":["physics.soc-ph","cs.DL"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Policy documents as sources for measuring societal impact: How often is\n  climate change research mentioned in policy-related documents?. In the current UK Research Excellence Framework (REF) and the Excellence in Research for Australia (ERA) societal impact measurements are inherent parts of the national evaluation systems. In this study, we deal with a relatively new form of societal impact measurements. Recently, Altmetric - a start-up providing publication level metrics - started to make data for publications available which have been mentioned in policy documents. We regard this data source as an interesting possibility to specifically measure the (societal) impact of research. Using a comprehensive dataset with publications on climate change as an example, we study the usefulness of the new data source for impact measurement. Only 1.2% (n=2,341) out of 191,276 publications on climate change in the dataset have at least one policy mention. We further reveal that papers published in Nature and Science as well as from the areas \"Earth and related environmental sciences\" and \"Social and economic geography\" are especially relevant in the policy context. Given the low coverage of the climate change literature in policy documents, this study can be only a first attempt to study this new source of altmetric data. Further empirical studies are necessary in upcoming years, because mentions in policy documents are of special interest in the use of altmetric data for measuring target-oriented the broader impact of research."},{"title":"Managing Recurrent Virtual Network Updates in Multi-Tenant Datacenters:\n  A System Perspective","source":"Zhuotao Liu and Yuan Cao and Xuewu Zhang and Changping Zhu and Fan\n  Zhang","docs_id":"1903.09465","section":["cs.CR","cs.NI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Managing Recurrent Virtual Network Updates in Multi-Tenant Datacenters:\n  A System Perspective. With the advent of software-defined networking, network configuration through programmable interfaces becomes practical, leading to various on-demand opportunities for network routing update in multi-tenant datacenters, where tenants have diverse requirements on network routings such as short latency, low path inflation, large bandwidth, high reliability, etc. Conventional solutions that rely on topology search coupled with an objective function https:\/\/ www.overleaf.com\/project\/5beb742041ab9c0e3caec84f to find desired routings have at least two shortcomings: (i) they run into scalability issues when handling consistent and frequent routing updates and (ii) they restrict the flexibility and capability to satisfy various routing requirements. To address these issues, this paper proposes a novel search and optimization decoupled design, which not only saves considerable topology search costs via search result reuse, but also avoids possible sub-optimality in greedy routing search algorithms by making decisions based on the global view of all possible routings. We implement a prototype of our proposed system, OpReduce, and perform extensive evaluations to validate its design goals."},{"title":"A 3-D Spatial Model for In-building Wireless Networks with Correlated\n  Shadowing","source":"Junse Lee, Xinchen Zhang, and Francois Baccelli","docs_id":"1603.07072","section":["cs.IT","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A 3-D Spatial Model for In-building Wireless Networks with Correlated\n  Shadowing. Consider orthogonal planes in the 3-D space representing floors and walls in a large building. These planes divide the space into rooms where a wireless infrastructure is deployed. This paper is focused on the analysis of the correlated shadowing field created by this wireless infrastructure through the set of walls and floors. When the locations of the planes and of the wireless nodes are governed by Poisson processes, we obtain a simple stochastic model which captures the non-uniform nature of node deployment and room sizes. This model, which we propose to call the Poisson building, captures the complex in-building shadowing correlations, is scalable in the number of dimensions and is tractable for network performance analysis. It allows an exact mathematical characterization of the interference distribution in both infinite and finite buildings, which further leads to closed-form expressions for the coverage probabilities in in-building cellular networks and the success probability of in-building underlay D2D transmissions."},{"title":"Solving Target Set Selection with Bounded Thresholds Faster than $2^n$","source":"Ivan Bliznets and Danil Sagunov","docs_id":"1807.10789","section":["cs.DS"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Solving Target Set Selection with Bounded Thresholds Faster than $2^n$. In this paper we consider the Target Set Selection problem. The problem naturally arises in many fields like economy, sociology, medicine. In the Target Set Selection problem one is given a graph $G$ with a function $\\operatorname{thr}: V(G) \\to \\mathbb{N} \\cup \\{0\\}$ and integers $k, \\ell$. The goal of the problem is to activate at most $k$ vertices initially so that at the end of the activation process there is at least $\\ell$ activated vertices. The activation process occurs in the following way: (i) once activated, a vertex stays activated forever; (ii) vertex $v$ becomes activated if at least $\\operatorname{thr}(v)$ of its neighbours are activated. The problem and its different special cases were extensively studied from approximation and parameterized points of view. For example, parameterizations by the following parameters were studied: treewidth, feedback vertex set, diameter, size of target set, vertex cover, cluster editing number and others. Despite the extensive study of the problem it is still unknown whether the problem can be solved in $\\mathcal{O}^*((2-\\epsilon)^n)$ time for some $\\epsilon >0$. We partially answer this question by presenting several faster-than-trivial algorithms that work in cases of constant thresholds, constant dual thresholds or when the threshold value of each vertex is bounded by one-third of its degree. Also, we show that the problem parameterized by $\\ell$ is W[1]-hard even when all thresholds are constant."},{"title":"Short Packet Structure for Ultra-Reliable Machine-type Communication:\n  Tradeoff between Detection and Decoding","source":"Alexandru-Sabin Bana, Kasper Fl{\\o}e Trillingsgaard, Petar Popovski,\n  Elisabeth de Carvalho","docs_id":"1802.10407","section":["cs.IT","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Short Packet Structure for Ultra-Reliable Machine-type Communication:\n  Tradeoff between Detection and Decoding. Machine-type communication requires rethinking of the structure of short packets due to the coding limitations and the significant role of the control information. In ultra-reliable low-latency communication (URLLC), it is crucial to optimally use the limited degrees of freedom (DoFs) to send data and control information. We consider a URLLC model for short packet transmission with acknowledgement (ACK). We compare the detection\/decoding performance of two short packet structures: (1) time-multiplexed detection sequence and data; and (2) structure in which both packet detection and data decoding use all DoFs. Specifically, as an instance of the second structure we use superimposed sequences for detection and data. We derive the probabilities of false alarm and misdetection for an AWGN channel and numerically minimize the packet error probability (PER), showing that for delay-constrained data and ACK exchange, there is a tradeoff between the resources spent for detection and decoding. We show that the optimal PER for the superimposed structure is achieved for higher detection overhead. For this reason, the PER is also higher than in the preamble case. However, the superimposed structure is advantageous due to its flexibility to achieve optimal operation without the need to use multiple codebooks."},{"title":"No-Reference Color Image Quality Assessment: From Entropy to Perceptual\n  Quality","source":"Xiaoqiao Chen, Qingyi Zhang, Manhui Lin, Guangyi Yang, Chu He","docs_id":"1812.10695","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"No-Reference Color Image Quality Assessment: From Entropy to Perceptual\n  Quality. This paper presents a high-performance general-purpose no-reference (NR) image quality assessment (IQA) method based on image entropy. The image features are extracted from two domains. In the spatial domain, the mutual information between the color channels and the two-dimensional entropy are calculated. In the frequency domain, the two-dimensional entropy and the mutual information of the filtered sub-band images are computed as the feature set of the input color image. Then, with all the extracted features, the support vector classifier (SVC) for distortion classification and support vector regression (SVR) are utilized for the quality prediction, to obtain the final quality assessment score. The proposed method, which we call entropy-based no-reference image quality assessment (ENIQA), can assess the quality of different categories of distorted images, and has a low complexity. The proposed ENIQA method was assessed on the LIVE and TID2013 databases and showed a superior performance. The experimental results confirmed that the proposed ENIQA method has a high consistency of objective and subjective assessment on color images, which indicates the good overall performance and generalization ability of ENIQA. The source code is available on github https:\/\/github.com\/jacob6\/ENIQA."},{"title":"Preserving general physical properties in model reduction of dynamical\n  systems via constrained-optimization projection","source":"A. Schein, K. T. Carlberg, M. J. Zahr","docs_id":"2011.13998","section":["cs.CE"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Preserving general physical properties in model reduction of dynamical\n  systems via constrained-optimization projection. Model-reduction techniques aim to reduce the computational complexity of simulating dynamical systems by applying a (Petrov-)Galerkin projection process that enforces the dynamics to evolve in a low-dimensional subspace of the original state space. Frequently, the resulting reduced-order model (ROM) violates intrinsic physical properties of the original full-order model (FOM) (e.g., global conservation, Lagrangian structure, state-variable bounds) because the projection process does not generally ensure preservation of these properties. However, in many applications, ensuring the ROM preserves such intrinsic properties can enable the ROM to retain physical meaning and lead to improved accuracy and stability properties. In this work, we present a general constrained-optimization formulation for projection-based model reduction that can be used as a template to enforce the ROM to satisfy specific properties on the kinematics and dynamics. We introduce constrained-optimization formulations at both the time-continuous (i.e., ODE) level, which leads to a constrained Galerkin projection, and at the time-discrete level, which leads to a least-squares Petrov-Galerkin (LSPG) projection, in the context of linear multistep schemes. We demonstrate the ability of the proposed formulations to equip ROMs with desired properties such as global energy conservation and bounds on the total variation."},{"title":"Need-based Communication for Smart Grid: When to Inquire Power Price?","source":"Husheng Li and Robert C. Qiu","docs_id":"1003.2138","section":["cs.IT","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Need-based Communication for Smart Grid: When to Inquire Power Price?. In smart grid, a home appliance can adjust its power consumption level according to the realtime power price obtained from communication channels. Most studies on smart grid do not consider the cost of communications which cannot be ignored in many situations. Therefore, the total cost in smart grid should be jointly optimized with the communication cost. In this paper, a probabilistic mechanism of locational margin price (LMP) is applied and a model for the stochastic evolution of the underlying load which determines the power price is proposed. Based on this framework of power price, the problem of determining when to inquire the power price is formulated as a Markov decision process and the corresponding elements, namely the action space, system state and reward function, are defined. Dynamic programming is then applied to obtain the optimal strategy. A simpler myopic approach is proposed by comparing the cost of communications and the penalty incurred by using the old value of power price. Numerical results show the significant performance gain of the optimal strategy of price inquiry, as well as the near-optimality of the myopic approach."},{"title":"A Highly Accelerated Parallel Multi-GPU based Reconstruction Algorithm\n  for Generating Accurate Relative Stopping Powers","source":"Paniz Karbasi, Ritchie Cai, Blake Schultze, Hanh Nguyen, Jones Reed,\n  Patrick Hall, Valentina Giacometti, Vladimir Bashkirov, Robert Johnson, Nick\n  Karonis, Jeffrey Olafsen, Caesar Ordonez, Keith E. Schubert, Reinhard W.\n  Schulte","docs_id":"1802.01070","section":["physics.med-ph","cs.DC"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A Highly Accelerated Parallel Multi-GPU based Reconstruction Algorithm\n  for Generating Accurate Relative Stopping Powers. Low-dose Proton Computed Tomography (pCT) is an evolving imaging modality that is used in proton therapy planning which addresses the range uncertainty problem. The goal of pCT is generating a 3D map of Relative Stopping Power (RSP) measurements with high accuracy within clinically required time frames. Generating accurate RSP values within the shortest amount of time is considered a key goal when developing a pCT software. The existing pCT softwares have successfully met this time frame and even succeeded this time goal, but requiring clusters with hundreds of processors. This paper describes a novel reconstruction technique using two Graphics Processing Unit (GPU) cores, such as is available on a single Nvidia P100. The proposed reconstruction technique is tested on both simulated and experimental datasets and on two different systems namely Nvidia K40 and P100 GPUs from IBM and Cray. The experimental results demonstrate that our proposed reconstruction method meets both the timing and accuracy with the benefit of having reasonable cost, and efficient use of power."},{"title":"Leveraging Predictions in Smoothed Online Convex Optimization via\n  Gradient-based Algorithms","source":"Yingying Li and Na Li","docs_id":"2011.12539","section":["cs.LG","cs.SY","eess.SY","math.OC"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Leveraging Predictions in Smoothed Online Convex Optimization via\n  Gradient-based Algorithms. We consider online convex optimization with time-varying stage costs and additional switching costs. Since the switching costs introduce coupling across all stages, multi-step-ahead (long-term) predictions are incorporated to improve the online performance. However, longer-term predictions tend to suffer from lower quality. Thus, a critical question is: how to reduce the impact of long-term prediction errors on the online performance? To address this question, we introduce a gradient-based online algorithm, Receding Horizon Inexact Gradient (RHIG), and analyze its performance by dynamic regrets in terms of the temporal variation of the environment and the prediction errors. RHIG only considers at most $W$-step-ahead predictions to avoid being misled by worse predictions in the longer term. The optimal choice of $W$ suggested by our regret bounds depends on the tradeoff between the variation of the environment and the prediction accuracy. Additionally, we apply RHIG to a well-established stochastic prediction error model and provide expected regret and concentration bounds under correlated prediction errors. Lastly, we numerically test the performance of RHIG on quadrotor tracking problems."},{"title":"Accelerating Beam Sweeping in mmWave Standalone 5G New Radios using\n  Recurrent Neural Networks","source":"Asim Mazin, Mohamed Elkourdi, and Richard D. Gitlin","docs_id":"1809.01096","section":["eess.SP","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Accelerating Beam Sweeping in mmWave Standalone 5G New Radios using\n  Recurrent Neural Networks. Millimeter wave (mmWave) is a key technology to support high data rate demands for 5G applications. Highly directional transmissions are crucial at these frequencies to compensate for high isotropic pathloss. This reliance on di- rectional beamforming, however, makes the cell discovery (cell search) challenging since both base station (gNB) and user equipment (UE) jointly perform a search over angular space to locate potential beams to initiate communication. In the cell discovery phase, sequential beam sweeping is performed through the angular coverage region in order to transmit synchronization signals. The sweeping pattern can either be a linear rotation or a hopping pattern that makes use of additional information. This paper proposes beam sweeping pattern prediction, based on the dynamic distribution of user traffic, using a form of recurrent neural networks (RNNs) called Gated Recurrent Unit (GRU). The spatial distribution of users is inferred from data in call detail records (CDRs) of the cellular network. Results show that the users spatial distribution and their approximate location (direction) can be accurately predicted based on CDRs data using GRU, which is then used to calculate the sweeping pattern in the angular domain during cell search."},{"title":"The Gaussian Many-to-1 Interference Channel with Confidential Messages","source":"Xiang He and Aylin Yener","docs_id":"1005.0624","section":["cs.IT","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"The Gaussian Many-to-1 Interference Channel with Confidential Messages. The many-to-one interference channel has received interest by virtue of embodying the essence of an interference network while being more tractable than the general K-user interference channel. In this paper, we introduce information theoretic secrecy to this model and consider the many-to-one interference channel with confidential messages, in which each receiver, in particular, the one subject to interference, is also one from which the interfering users' messages need to be kept secret from. We derive the achievable secrecy sum rate for this channel using nested lattice codes, as well as an upper bound on the secrecy sum rate for all possible channel gain configurations. We identify several nontrivial cases where the gap between the upper bound and the achieved secrecy sum rate is only a function of the number of the users K, and is uniform over all possible channel gain configurations in each case. In addition, we identify the secure degree of freedom for this channel and show it to be equivalent to its degree of freedom, i.e., the secrecy in high SNR comes for free."},{"title":"Teaching Electronics and Programming in Norwegian Schools Using the\n  air:bit Sensor Kit","source":"Bj{\\o}rn Fjukstad, Nina Angelvik, Morten Gr{\\o}nnesby, Maria Wulff\n  Hauglann, Hedinn Gunhildrud, Fredrik H{\\o}is{\\ae}ther Rasch, Julianne\n  Iversen, Margaret Dalseng, Lars Ailo Bongo","docs_id":"1901.05240","section":["cs.CY"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Teaching Electronics and Programming in Norwegian Schools Using the\n  air:bit Sensor Kit. We describe lessons learned from using the air:bit project to introduce more than 150 students in the Norwegian upper secondary school to computer programming, engineering and environmental sciences. In the air:bit project, students build and code a portable air quality sensor kits, and use their air:bit to collect data to investigate patterns in air quality in their local environment. When the project ended students had collected more than 400,000 measurements with their air:bit kits, and could describe local patterns in air quality. Students participate in all parts of the project, from soldering components and programming the sensors, to analyzing the air quality measurements. We conducted a survey after the project and describe our lessons learned from the project. The results show that the project successfully taught the students fundamental concepts in computer programming, electronics, and the scientific method. In addition, all the participating teachers reported that their students had showed good learning outcomes."},{"title":"Fast-Convergent Dynamics for Distributed Allocation of Resources Over\n  Switching Sparse Networks with Quantized Communication Links","source":"Mohammadreza Doostmohammadian, Alireza Aghasi, Mohammad Pirani, Ehsan\n  Nekouei, Usman A. Khan, Themistoklis Charalambous","docs_id":"2012.08181","section":["eess.SY","cs.LG","cs.MA","cs.SI","cs.SY"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Fast-Convergent Dynamics for Distributed Allocation of Resources Over\n  Switching Sparse Networks with Quantized Communication Links. This paper proposes networked dynamics to solve resource allocation problems over time-varying multi-agent networks. The state of each agent represents the amount of used resources (or produced utilities) while the total amount of resources is fixed. The idea is to optimally allocate the resources among the group of agents by minimizing the overall cost function subject to fixed sum of resources. Each agents' information is restricted to its own state and cost function and those of its immediate in-neighbors. This is motivated by distributed applications such as mobile edge-computing, economic dispatch over smart grids, and multi-agent coverage control. This work provides a fast convergent solution (in comparison with linear dynamics) while considering relaxed network connectivity with quantized communication links. The proposed dynamics reaches optimal solution over switching (possibly disconnected) undirected networks as far as their union over some bounded non-overlapping time-intervals has a spanning-tree. We prove feasibility of the solution, uniqueness of the optimal state, and convergence to the optimal value under the proposed dynamics, where the analysis is applicable to similar 1st-order allocation dynamics with strongly sign-preserving nonlinearities, such as actuator saturation."},{"title":"Using AoI Forecasts in Communicating and Robust Distributed\n  Model-Predictive Control","source":"Jannik Hahn, Richard Schoeffauer, Gerhard Wunder, Olaf Stursberg","docs_id":"2103.05526","section":["eess.SY","cs.SY"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Using AoI Forecasts in Communicating and Robust Distributed\n  Model-Predictive Control. In order to enhance the performance of cyber-physical systems, this paper proposes the integrated de-sign of distributed controllers for distributed plants andthe control of the communication network. Conventionaldesign methods use static interfaces between both enti-ties and therefore rely on worst-case estimations of com-munication delay, often leading to conservative behaviorof the overall system. By contrast, the present approachestablishes a robust distributed model-predictive controlscheme, in which the local subsystem controllers oper-ate under the assumption of a variable communicationschedule that is predicted by a network controller. Us-ing appropriate models for the communication network,the network controller applies a predictive network policyfor scheduling the communication among the subsystemcontrollers across the network. Given the resulting time-varying predictions of the age of information, the papershows under which conditions the subsystem controllerscan robustly stabilize the distributed system. To illustratethe approach, the paper also reports on the application to avehicle platooning scenario."},{"title":"A Note on the Periodicity and the Output Rate of Bit Search Type\n  Generators","source":"Yucel Altug, N. Polat Ayerden, M. Kivanc Mihcak, Emin Anarim","docs_id":"cs\/0702092","section":["cs.CR"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A Note on the Periodicity and the Output Rate of Bit Search Type\n  Generators. We investigate the bit-search type irregular decimation algorithms that are used within LFSR-based stream ciphers. In particular, we concentrate on BSG and ABSG, and consider two different setups for the analysis. In the first case, the input is assumed to be a m-sequence; we show that all possible output sequences can be classified into two sets, each of which is characterized by the equivalence of their elements up to shifts. Furthermore, we prove that the cardinality of each of these sets is equal to the period of one of its elements and subsequently derive the first known bounds on the expected output period (assuming that no subperiods exist). In the second setup, we work in a probabilistic framework and assume that the input sequence is evenly distributed (i.e., independent identically distributed Bernoulli process with probability 1\/2). Under these assumptions, we derive closed-form expressions for the distribution of the output length and the output rate, which is shown to be asymptotically Gaussian-distributed and concentrated around the mean with exponential tightness."},{"title":"Incentivizing High-quality Content from Heterogeneous Users: On the\n  Existence of Nash Equilibrium","source":"Yingce Xia, Tao Qin, Nenghai Yu, Tie-Yan Liu","docs_id":"1404.5155","section":["cs.GT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Incentivizing High-quality Content from Heterogeneous Users: On the\n  Existence of Nash Equilibrium. We study the existence of pure Nash equilibrium (PNE) for the mechanisms used in Internet services (e.g., online reviews and question-answer websites) to incentivize users to generate high-quality content. Most existing work assumes that users are homogeneous and have the same ability. However, real-world users are heterogeneous and their abilities can be very different from each other due to their diverse background, culture, and profession. In this work, we consider heterogeneous users with the following framework: (1) the users are heterogeneous and each of them has a private type indicating the best quality of the content she can generate; (2) there is a fixed amount of reward to allocate to the participated users. Under this framework, we study the existence of pure Nash equilibrium of several mechanisms composed by different allocation rules, action spaces, and information settings. We prove the existence of PNE for some mechanisms and the non-existence of PNE for some mechanisms. We also discuss how to find a PNE for those mechanisms with PNE either through a constructive way or a search algorithm."},{"title":"Frequency-Domain Group-based Shrinkage Estimators for UWB Systems","source":"Sheng Li, Rodrigo C. de Lamare and Martin Haardt","docs_id":"1304.5817","section":["cs.IT","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Frequency-Domain Group-based Shrinkage Estimators for UWB Systems. In this work, we propose low-complexity adaptive biased estimation algorithms, called group-based shrinkage estimators (GSEs), for parameter estimation and interference suppression scenarios with mechanisms to automatically adjust the shrinkage factors. The proposed estimation algorithms divide the target parameter vector into a number of groups and adaptively calculate one shrinkage factor for each group. GSE schemes improve the performance of the conventional least squares (LS) estimator in terms of the mean-squared error (MSE), while requiring a very modest increase in complexity. An MSE analysis is presented which indicates the lower bounds of the GSE schemes with different group sizes. We prove that our proposed schemes outperform the biased estimation with only one shrinkage factor and the best performance of GSE can be obtained with the maximum number of groups. Then, we consider an application of the proposed algorithms to single-carrier frequency-domain equalization (SC-FDE) of direct-sequence ultra-wideband (DS-UWB) systems, in which the structured channel estimation (SCE) algorithm and the frequency domain receiver employ the GSE. The simulation results show that the proposed algorithms significantly outperform the conventional unbiased estimator in the analyzed scenarios."},{"title":"Semi-discrete optimization through semi-discrete optimal transport: a\n  framework for neural architecture search","source":"Nicolas Garcia Trillos, Javier Morales","docs_id":"2006.15221","section":["math.AP","cs.LG","math.OC","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Semi-discrete optimization through semi-discrete optimal transport: a\n  framework for neural architecture search. In this paper we introduce a theoretical framework for semi-discrete optimization using ideas from optimal transport. Our primary motivation is in the field of deep learning, and specifically in the task of neural architecture search. With this aim in mind, we discuss the geometric and theoretical motivation for new techniques for neural architecture search (in the companion work \\cite{practical}; we show that algorithms inspired by our framework are competitive with contemporaneous methods). We introduce a Riemannian like metric on the space of probability measures over a semi-discrete space $\\mathbb{R}^d \\times \\mathcal{G}$ where $\\mathcal{G}$ is a finite weighted graph. With such Riemmanian structure in hand, we derive formal expressions for the gradient flow of a relative entropy functional, as well as second order dynamics for the optimization of said energy. Then, with the aim of providing a rigorous motivation for the gradient flow equations derived formally we also consider an iterative procedure known as minimizing movement scheme (i.e., Implicit Euler scheme, or JKO scheme) and apply it to the relative entropy with respect to a suitable cost function. For some specific choices of metric and cost, we rigorously show that the minimizing movement scheme of the relative entropy functional converges to the gradient flow process provided by the formal Riemannian structure. This flow coincides with a system of reaction-diffusion equations on $\\mathbb{R}^d$."},{"title":"Neural Network Training Techniques Regularize Optimization Trajectory:\n  An Empirical Study","source":"Cheng Chen, Junjie Yang, Yi Zhou","docs_id":"2011.06702","section":["cs.LG","math.OC"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Neural Network Training Techniques Regularize Optimization Trajectory:\n  An Empirical Study. Modern deep neural network (DNN) trainings utilize various training techniques, e.g., nonlinear activation functions, batch normalization, skip-connections, etc. Despite their effectiveness, it is still mysterious how they help accelerate DNN trainings in practice. In this paper, we provide an empirical study of the regularization effect of these training techniques on DNN optimization. Specifically, we find that the optimization trajectories of successful DNN trainings consistently obey a certain regularity principle that regularizes the model update direction to be aligned with the trajectory direction. Theoretically, we show that such a regularity principle leads to a convergence guarantee in nonconvex optimization and the convergence rate depends on a regularization parameter. Empirically, we find that DNN trainings that apply the training techniques achieve a fast convergence and obey the regularity principle with a large regularization parameter, implying that the model updates are well aligned with the trajectory. On the other hand, DNN trainings without the training techniques have slow convergence and obey the regularity principle with a small regularization parameter, implying that the model updates are not well aligned with the trajectory. Therefore, different training techniques regularize the model update direction via the regularity principle to facilitate the convergence."},{"title":"Kara: A System for Visualising and Visual Editing of Interpretations for\n  Answer-Set Programs","source":"Christian Kloim\\\"ullner, Johannes Oetsch, J\\\"org P\\\"uhrer, and Hans\n  Tompits","docs_id":"1109.4095","section":["cs.LO","cs.AI","cs.GR","cs.PL"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Kara: A System for Visualising and Visual Editing of Interpretations for\n  Answer-Set Programs. In answer-set programming (ASP), the solutions of a problem are encoded in dedicated models, called answer sets, of a logical theory. These answer sets are computed from the program that represents the theory by means of an ASP solver and returned to the user as sets of ground first-order literals. As this type of representation is often cumbersome for the user to interpret, tools like ASPVIZ and IDPDraw were developed that allow for visualising answer sets. The tool Kara, introduced in this paper, follows these approaches, using ASP itself as a language for defining visualisations of interpretations. Unlike existing tools that position graphic primitives according to static coordinates only, Kara allows for more high-level specifications, supporting graph structures, grids, and relative positioning of graphical elements. Moreover, generalising the functionality of previous tools, Kara provides modifiable visualisations such that interpretations can be manipulated by graphically editing their visualisations. This is realised by resorting to abductive reasoning techniques. Kara is part of SeaLion, a forthcoming integrated development environment (IDE) for ASP."},{"title":"Causal Feature Selection with Dimension Reduction for Interpretable Text\n  Classification","source":"Guohou Shan, James Foulds, Shimei Pan","docs_id":"2010.04609","section":["cs.LG","cs.CL","cs.IR"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Causal Feature Selection with Dimension Reduction for Interpretable Text\n  Classification. Text features that are correlated with class labels, but do not directly cause them, are sometimesuseful for prediction, but they may not be insightful. As an alternative to traditional correlation-basedfeature selection, causal inference could reveal more principled, meaningful relationships betweentext features and labels. To help researchers gain insight into text data, e.g. for social scienceapplications, in this paper we investigate a class of matching-based causal inference methods fortext feature selection. Features used in document classification are often high dimensional, howeverexisting causal feature selection methods use Propensity Score Matching (PSM) which is known to beless effective in high-dimensional spaces. We propose a new causal feature selection framework thatcombines dimension reduction with causal inference to improve text feature selection. Experiments onboth synthetic and real-world data demonstrate the promise of our methods in improving classificationand enhancing interpretability."},{"title":"Topic Modeling on Health Journals with Regularized Variational Inference","source":"Robert Giaquinto and Arindam Banerjee","docs_id":"1801.04958","section":["cs.CL","cs.LG","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Topic Modeling on Health Journals with Regularized Variational Inference. Topic modeling enables exploration and compact representation of a corpus. The CaringBridge (CB) dataset is a massive collection of journals written by patients and caregivers during a health crisis. Topic modeling on the CB dataset, however, is challenging due to the asynchronous nature of multiple authors writing about their health journeys. To overcome this challenge we introduce the Dynamic Author-Persona topic model (DAP), a probabilistic graphical model designed for temporal corpora with multiple authors. The novelty of the DAP model lies in its representation of authors by a persona --- where personas capture the propensity to write about certain topics over time. Further, we present a regularized variational inference algorithm, which we use to encourage the DAP model's personas to be distinct. Our results show significant improvements over competing topic models --- particularly after regularization, and highlight the DAP model's unique ability to capture common journeys shared by different authors."},{"title":"SE-MelGAN -- Speaker Agnostic Rapid Speech Enhancement","source":"Luka Chkhetiani, Levan Bejanidze","docs_id":"2006.07637","section":["eess.AS","cs.CL","cs.SD"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"SE-MelGAN -- Speaker Agnostic Rapid Speech Enhancement. Recent advancement in Generative Adversarial Networks in speech synthesis domain[3],[2] have shown, that it's possible to train GANs [8] in a reliable manner for high quality coherent waveform generation from mel-spectograms. We propose that it is possible to transfer the MelGAN's [3] robustness in learning speech features to speech enhancement and noise reduction domain without any model modification tasks. Our proposed method generalizes over multi-speaker speech dataset and is able to robustly handle unseen background noises during the inference. Also, we show that by increasing the batch size for this particular approach not only yields better speech results, but generalizes over multi-speaker dataset easily and leads to faster convergence. Additionally, it outperforms previous state of the art GAN approach for speech enhancement SEGAN [5] in two domains: 1. quality ; 2. speed. Proposed method runs at more than 100x faster than realtime on GPU and more than 2x faster than real time on CPU without any hardware optimization tasks, right at the speed of MelGAN [3]."},{"title":"Fast and Robust Registration of Aerial Images and LiDAR data Based on\n  Structrual Features and 3D Phase Correlation","source":"Bai Zhu, Yuanxin Ye, Chao Yang, Liang Zhou, Huiyu Liu, Yungang Cao","docs_id":"2004.09811","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Fast and Robust Registration of Aerial Images and LiDAR data Based on\n  Structrual Features and 3D Phase Correlation. Co-Registration of aerial imagery and Light Detection and Ranging (LiDAR) data is quilt challenging because the different imaging mechanism causes significant geometric and radiometric distortions between such data. To tackle the problem, this paper proposes an automatic registration method based on structural features and three-dimension (3D) phase correlation. In the proposed method, the LiDAR point cloud data is first transformed into the intensity map, which is used as the reference image. Then, we employ the Fast operator to extract uniformly distributed interest points in the aerial image by a partition strategy and perform a local geometric correction by using the collinearity equation to eliminate scale and rotation difference between images. Subsequently, a robust structural feature descriptor is build based on dense gradient features, and the 3D phase correlation is used to detect control points (CPs) between aerial images and LiDAR data in the frequency domain, where the image matching is accelerated by the 3D Fast Fourier Transform (FFT). Finally, the obtained CPs are employed to correct the exterior orientation elements, which is used to achieve co-registration of aerial images and LiDAR data. Experiments with two datasets of aerial images and LiDAR data show that the proposed method is much faster and more robust than state of the art methods"},{"title":"Data Driven Validation Framework for Multi-agent Activity-based Models","source":"Jan Drchal, Michal \\v{C}ertick\\'y, Michal Jakob","docs_id":"1502.07601","section":["cs.MA"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Data Driven Validation Framework for Multi-agent Activity-based Models. Activity-based models, as a specific instance of agent-based models, deal with agents that structure their activity in terms of (daily) activity schedules. An activity schedule consists of a sequence of activity instances, each with its assigned start time, duration and location, together with transport modes used for travel between subsequent activity locations. A critical step in the development of simulation models is validation. Despite the growing importance of activity-based models in modelling transport and mobility, there has been so far no work focusing specifically on statistical validation of such models. In this paper, we propose a six-step Validation Framework for Activity-based Models (VALFRAM) that allows exploiting historical real-world data to assess the validity of activity-based models. The framework compares temporal and spatial properties and the structure of activity schedules against real-world travel diaries and origin-destination matrices. We confirm the usefulness of the framework on three real-world activity-based transport models."},{"title":"A Range Matching CAM for Hierarchical Defect Tolerance Technique in NRAM\n  Structures","source":"Hossein Pourmeidani, Mehdi Habibi","docs_id":"1907.04504","section":["cs.AR"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A Range Matching CAM for Hierarchical Defect Tolerance Technique in NRAM\n  Structures. Due to the small size of nanoscale devices, they are highly prone to process disturbances which results in manufacturing defects. Some of the defects are randomly distributed throughout the nanodevice layer. Other disturbances tend to be local and lead to cluster defects caused by factors such as layer misintegration and line width variations. In this paper, we propose a method for identifying cluster defects from random ones. The motivation is to repair the cluster defects using rectangular ranges in a range matching content-addressable memory (RM-CAM) and random defects using triple-modular redundancy (TMR). It is believed a combination of these two approaches is more effective for repairing defects at high error rate with less resource. With the proposed fault repairing technique, defect recovery results are examined for different fault distribution scenarios. Also the mapping circuit structure required for two conceptual 32*32 and 64*64 bit RAMs are presented and their speed, power and transistor count are reported."},{"title":"Pushing for weighted tree automata","source":"Thomas Hanneforth and Andreas Maletti and Daniel Quernheim","docs_id":"1702.00304","section":["cs.FL"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Pushing for weighted tree automata. A weight normalization procedure, commonly called pushing, is introduced for weighted tree automata (wta) over commutative semifields. The normalization preserves the recognized weighted tree language even for nondeterministic wta, but it is most useful for bottom-up deterministic wta, where it can be used for minimization and equivalence testing. In both applications a careful selection of the weights to be redistributed followed by normalization allows a reduction of the general problem to the corresponding problem for bottom-up deterministic unweighted tree automata. This approach was already successfully used by Mohri and Eisner for the minimization of deterministic weighted string automata. Moreover, the new equivalence test for two wta $M$ and $M'$ runs in time $\\mathcal O((\\lvert M \\rvert + \\lvert M'\\rvert) \\cdot \\log {(\\lvert Q\\rvert + \\lvert Q'\\rvert)})$, where $Q$ and $Q'$ are the states of $M$ and $M'$, respectively, which improves the previously best run-time $\\mathcal O(\\lvert M \\rvert \\cdot \\lvert M'\\rvert)$."},{"title":"A Map of Science in Wikipedia","source":"Puyu Yang and Giovanni Colavizza","docs_id":"2110.13790","section":["cs.DL","cs.CY"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A Map of Science in Wikipedia. In recent decades, the rapid growth of Internet adoption is offering opportunities for convenient and inexpensive access to scientific information. Wikipedia, one of the largest encyclopedias worldwide, has become a reference in this respect, and has attracted widespread attention from scholars. However, a clear understanding of the scientific sources underpinning Wikipedia's contents remains elusive. In this work, we explore Wikipedia's role in the public understanding of science from the perspective of its scientific sources. We rely on an open dataset of citations from Wikipedia, and use network analysis to map the relationship between Wikipedia articles and scientific journal articles. We find that most journal articles cited from Wikipedia belong to STEM fields, in particular biology and medicine ($47.6$\\% of citations; $46.1$\\% of cited articles). Furthermore, Wikipedia's biographies play an important role in connecting STEM fields with the humanities, in particular history. Our results provide valuable insights into the reliance of Wikipedia on scientific sources, and its role in interconnecting knowledge across different topics."},{"title":"Dual-domain Cascade of U-nets for Multi-channel Magnetic Resonance Image\n  Reconstruction","source":"Roberto Souza, Mariana Bento, Nikita Nogovitsyn, Kevin J. Chung, R.\n  Marc Lebel and Richard Frayne","docs_id":"1911.01458","section":["eess.IV","cs.LG","physics.med-ph","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Dual-domain Cascade of U-nets for Multi-channel Magnetic Resonance Image\n  Reconstruction. The U-net is a deep-learning network model that has been used to solve a number of inverse problems. In this work, the concatenation of two-element U-nets, termed the W-net, operating in k-space (K) and image (I) domains, were evaluated for multi-channel magnetic resonance (MR) image reconstruction. The two element network combinations were evaluated for the four possible image-k-space domain configurations: a) W-net II, b) W-net KK, c) W-net IK, and d) W-net KI were evaluated. Selected promising four element networks (WW-nets) were also examined. Two configurations of each network were compared: 1) Each coil channel processed independently, and 2) all channels processed simultaneously. One hundred and eleven volumetric, T1-weighted, 12-channel coil k-space datasets were used in the experiments. Normalized root mean squared error, peak signal to noise ratio, visual information fidelity and visual inspection were used to assess the reconstructed images against the fully sampled reference images. Our results indicated that networks that operate solely in the image domain are better suited when processing individual channels of multi-channel data independently. Dual domain methods are more advantageous when simultaneously reconstructing all channels of multi-channel data. Also, the appropriate cascade of U-nets compared favorably (p < 0.01) to the previously published, state-of-the-art Deep Cascade model in in three out of four experiments."},{"title":"CEVO: Comprehensive EVent Ontology Enhancing Cognitive Annotation","source":"Saeedeh Shekarpour, Faisal Alshargi, Valerie Shalin, Krishnaprasad\n  Thirunarayan, Amit P. Sheth","docs_id":"1701.05625","section":["cs.CL"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"CEVO: Comprehensive EVent Ontology Enhancing Cognitive Annotation. While the general analysis of named entities has received substantial research attention on unstructured as well as structured data, the analysis of relations among named entities has received limited focus. In fact, a review of the literature revealed a deficiency in research on the abstract conceptualization required to organize relations. We believe that such an abstract conceptualization can benefit various communities and applications such as natural language processing, information extraction, machine learning, and ontology engineering. In this paper, we present Comprehensive EVent Ontology (CEVO), built on Levin's conceptual hierarchy of English verbs that categorizes verbs with shared meaning, and syntactic behavior. We present the fundamental concepts and requirements for this ontology. Furthermore, we present three use cases employing the CEVO ontology on annotation tasks: (i) annotating relations in plain text, (ii) annotating ontological properties, and (iii) linking textual relations to ontological properties. These use-cases demonstrate the benefits of using CEVO for annotation: (i) annotating English verbs from an abstract conceptualization, (ii) playing the role of an upper ontology for organizing ontological properties, and (iii) facilitating the annotation of text relations using any underlying vocabulary. This resource is available at https:\/\/shekarpour.github.io\/cevo.io\/ using https:\/\/w3id.org\/cevo namespace."},{"title":"Secure Polar Coding for the Two-Way Wiretap Channel","source":"Mengfan Zheng, Meixia Tao, Wen Chen and Cong Ling","docs_id":"1612.00130","section":["cs.IT","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Secure Polar Coding for the Two-Way Wiretap Channel. We consider the problem of polar coding for secure communications over the two-way wiretap channel, where two legitimate users communicate with each other simultaneously while a passive eavesdropper overhears a combination of their exchanged signals. The legitimate users wish to design a cooperative jamming code such that the interference between their codewords can jam the eavesdropper. In this paper, we design a polar coded cooperative jamming scheme that achieves the whole secrecy rate region of the general two-way wiretap channel under the strong secrecy criterion. The chaining method is used to make proper alignment of polar indices. The randomness required to be shared between two legitimate users is treated as a limited resource and we show that its rate can be made negligible by increasing the blocklength and the number of chained blocks. For the special case when the eavesdropper channel is degraded with respect to the legitimate ones, a simplified scheme is proposed which can simultaneously ensure reliability and weak secrecy within a single transmission block. An example of the binary erasure channel case is given to demonstrate the performance of our scheme."},{"title":"Stability Analysis of Gradient-Based Distributed Formation Control with\n  Heterogeneous Sensing Mechanism: Two and Three Robot Case","source":"Nelson P.K. Chan and Bayu Jayawardhana and Hector Garcia de Marina","docs_id":"2010.10559","section":["eess.SY","cs.SY","math.OC"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Stability Analysis of Gradient-Based Distributed Formation Control with\n  Heterogeneous Sensing Mechanism: Two and Three Robot Case. This paper focuses on the stability analysis of a formation shape displayed by a team of mobile robots that uses heterogeneous sensing mechanism. Depending on the convenience and reliability of the local information, each robot utilizes the popular gradient-based control law which, in this paper, is either the distance-based or the bearing-only formation control. For the two and three robot case, we show that the use of heterogeneous gradient-based control laws can give rise to an undesired invariant set where a distorted formation shape is moving at a constant velocity. The (in)stability of such an invariant set is dependent on the specified distance and bearing constraints. For the two robot case, we prove almost global stability of the desired equilibrium set while for the three robot case, we guarantee local asymptotic stability for the correct formation shape. We also derive conditions for the three robot case in which the undesired invariant set is locally attractive. Numerical simulations are presented for illustrating the theoretical results in the three robot case."},{"title":"The $\\aleph$ Calculus","source":"Hannah Earley","docs_id":"2011.14989","section":["cs.PL"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"The $\\aleph$ Calculus. Motivated by a need for a model of reversible computation appropriate for a Brownian molecular architecture, the $\\aleph$ calculus is introduced. This novel model is declarative, concurrent, and term-based--encapsulating all information about the program data and state within a single structure in order to obviate the need for a von Neumann-style discrete computational 'machine', a challenge in a molecular environment. The name is inspired by the Greek for 'not forgotten', due to the emphasis on (reversibly) learning and un-learning knowledge of different variables. To demonstrate its utility for this purpose, as well as its elegance as a programming language, a number of examples are presented; two of these examples, addition\/subtraction and squaring\/square-rooting, are furnished with designs for abstract molecular implementations. A natural by-product of these examples and accompanying syntactic sugar is the design of a fully-fledged programming language, alethe, which is also presented along with an interpreter. Efficiently simulating $\\aleph$ on a deterministic computer necessitates some static analysis of programs within the alethe interpreter in order to render the declarative programs sequential. Finally, work towards a type system appropriate for such a reversible, declarative model of computation is presented."},{"title":"Energy Disaggregation using Variational Autoencoders","source":"Antoine Langevin, Marc-Andr\\'e Carbonneau, Mohamed Cheriet, Ghyslain\n  Gagnon","docs_id":"2103.12177","section":["cs.LG","eess.SP"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Energy Disaggregation using Variational Autoencoders. Non-intrusive load monitoring (NILM) is a technique that uses a single sensor to measure the total power consumption of a building. Using an energy disaggregation method, the consumption of individual appliances can be estimated from the aggregate measurement. Recent disaggregation algorithms have significantly improved the performance of NILM systems. However, the generalization capability of these methods to different houses as well as the disaggregation of multi-state appliances are still major challenges. In this paper we address these issues and propose an energy disaggregation approach based on the variational autoencoders framework. The probabilistic encoder makes this approach an efficient model for encoding information relevant to the reconstruction of the target appliance consumption. In particular, the proposed model accurately generates more complex load profiles, thus improving the power signal reconstruction of multi-state appliances. Moreover, its regularized latent space improves the generalization capabilities of the model across different houses. The proposed model is compared to state-of-the-art NILM approaches on the UK-DALE and REFIT datasets, and yields competitive results. The mean absolute error reduces by 18% on average across all appliances compared to the state-of-the-art. The F1-Score increases by more than 11%, showing improvements for the detection of the target appliance in the aggregate measurement."},{"title":"Light-Weight DDoS Mitigation at Network Edge with Limited Resources","source":"Ryo Yaegashi, Daisuke Hisano, Yu Nakayama","docs_id":"2010.15786","section":["cs.NI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Light-Weight DDoS Mitigation at Network Edge with Limited Resources. The Internet of Things (IoT) has been growing rapidly in recent years. With the appearance of 5G, it is expected to become even more indispensable to people's lives. In accordance with the increase of Distributed Denial-of-Service (DDoS) attacks from IoT devices, DDoS defense has become a hot research topic. DDoS detection mechanisms executed on routers and SDN environments have been intensely studied. However, these methods have the disadvantage of requiring the cost and performance of the devices. In addition, there is no existing DDoS mitigation algorithm on the network edge that can be performed with the low-cost and low performance equipments. Therefore, this paper proposes a light-weight DDoS mitigation scheme at the network edge using limited resources of inexpensive devices such as home gateways. The goal of the proposed scheme is to simply detect and mitigate flooding attacks. It utilizes unused queue resources to detect malicious flows by random shuffling of queue allocation and discard the packets of the detected flows. The performance of the proposed scheme was confirmed via theoretical analysis and computer simulation. The simulation results match the theoretical results and the proposed algorithm can efficiently detect malicious flows using limited resources."},{"title":"Sequence-based Person Attribute Recognition with Joint CTC-Attention\n  Model","source":"Hao Liu and Jingjing Wu and Jianguo Jiang and Meibin Qi and Bo Ren","docs_id":"1811.08115","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Sequence-based Person Attribute Recognition with Joint CTC-Attention\n  Model. Attribute recognition has become crucial because of its wide applications in many computer vision tasks, such as person re-identification. Like many object recognition problems, variations in viewpoints, illumination, and recognition at far distance, all make this task challenging. In this work, we propose a joint CTC-Attention model (JCM), which maps attribute labels into sequences to learn the semantic relationship among attributes. Besides, this network uses neural network to encode images into sequences, and employs connectionist temporal classification (CTC) loss to train the network with the aim of improving the encoding performance of the network. At the same time, it adopts the attention model to decode the sequences, which can realize aligning the sequences and better learning the semantic information from attributes. Extensive experiments on three public datasets, i.e., Market-1501 attribute dataset, Duke attribute dataset and PETA dataset, demonstrate the effectiveness of the proposed method."},{"title":"Network dismantling","source":"Alfredo Braunstein, Luca Dall'Asta, Guilhem Semerjian, Lenka\n  Zdeborov\\'a","docs_id":"1603.08883","section":["physics.soc-ph","cond-mat.stat-mech","cs.DS"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Network dismantling. We study the network dismantling problem, which consists in determining a minimal set of vertices whose removal leaves the network broken into connected components of sub-extensive size. For a large class of random graphs, this problem is tightly connected to the decycling problem (the removal of vertices leaving the graph acyclic). Exploiting this connection and recent works on epidemic spreading we present precise predictions for the minimal size of a dismantling set in a large random graph with a prescribed (light-tailed) degree distribution. Building on the statistical mechanics perspective we propose a three-stage Min-Sum algorithm for efficiently dismantling networks, including heavy-tailed ones for which the dismantling and decycling problems are not equivalent. We also provide further insights into the dismantling problem concluding that it is an intrinsically collective problem and that optimal dismantling sets cannot be viewed as a collection of individually well performing nodes."},{"title":"Characterizing A Database of Sequential Behaviors with Latent Dirichlet\n  Hidden Markov Models","source":"Yin Song, Longbing Cao, Xuhui Fan, Wei Cao and Jian Zhang","docs_id":"1305.5734","section":["stat.ML","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Characterizing A Database of Sequential Behaviors with Latent Dirichlet\n  Hidden Markov Models. This paper proposes a generative model, the latent Dirichlet hidden Markov models (LDHMM), for characterizing a database of sequential behaviors (sequences). LDHMMs posit that each sequence is generated by an underlying Markov chain process, which are controlled by the corresponding parameters (i.e., the initial state vector, transition matrix and the emission matrix). These sequence-level latent parameters for each sequence are modeled as latent Dirichlet random variables and parameterized by a set of deterministic database-level hyper-parameters. Through this way, we expect to model the sequence in two levels: the database level by deterministic hyper-parameters and the sequence-level by latent parameters. To learn the deterministic hyper-parameters and approximate posteriors of parameters in LDHMMs, we propose an iterative algorithm under the variational EM framework, which consists of E and M steps. We examine two different schemes, the fully-factorized and partially-factorized forms, for the framework, based on different assumptions. We present empirical results of behavior modeling and sequence classification on three real-world data sets, and compare them to other related models. The experimental results prove that the proposed LDHMMs produce better generalization performance in terms of log-likelihood and deliver competitive results on the sequence classification problem."},{"title":"Compressed Sensing: How sharp is the Restricted Isometry Property","source":"Jeffrey D. Blanchard, Coralia Cartis, and Jared Tanner","docs_id":"1004.5026","section":["cs.IT","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Compressed Sensing: How sharp is the Restricted Isometry Property. Compressed Sensing (CS) seeks to recover an unknown vector with $N$ entries by making far fewer than $N$ measurements; it posits that the number of compressed sensing measurements should be comparable to the information content of the vector, not simply $N$. CS combines the important task of compression directly with the measurement task. Since its introduction in 2004 there have been hundreds of manuscripts on CS, a large fraction of which develop algorithms to recover a signal from its compressed measurements. Because of the paradoxical nature of CS -- exact reconstruction from seemingly undersampled measurements -- it is crucial for acceptance of an algorithm that rigorous analyses verify the degree of undersampling the algorithm permits. The Restricted Isometry Property (RIP) has become the dominant tool used for the analysis in such cases. We present here an asymmetric form of RIP which gives tighter bounds than the usual symmetric one. We give the best known bounds on the RIP constants for matrices from the Gaussian ensemble. Our derivations illustrate the way in which the combinatorial nature of CS is controlled. Our quantitative bounds on the RIP allow precise statements as to how aggressively a signal can be undersampled, the essential question for practitioners. We also document the extent to which RIP gives precise information about the true performance limits of CS, by comparing with approaches from high-dimensional geometry."},{"title":"On the Power of Simple Reductions for the Maximum Independent Set\n  Problem","source":"Darren Strash","docs_id":"1608.00724","section":["cs.DS"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"On the Power of Simple Reductions for the Maximum Independent Set\n  Problem. Reductions---rules that reduce input size while maintaining the ability to compute an optimal solution---are critical for developing efficient maximum independent set algorithms in both theory and practice. While several simple reductions have previously been shown to make small domain-specific instances tractable in practice, it was only recently shown that advanced reductions (in a measure-and-conquer approach) can be used to solve real-world networks on millions of vertices [Akiba and Iwata, TCS 2016]. In this paper we compare these state-of-the-art reductions against a small suite of simple reductions, and come to two conclusions: just two simple reductions---vertex folding and isolated vertex removal---are sufficient for many real-world instances, and further, the power of the advanced rules comes largely from their initial application (i.e., kernelization), and not their repeated application during branch-and-bound. As a part of our comparison, we give the first experimental evaluation of a reduction based on maximum critical independent sets, and show it is highly effective in practice for medium-sized networks."},{"title":"A Multimodal Memes Classification: A Survey and Open Research Issues","source":"Tariq Habib Afridi, Aftab Alam, Muhammad Numan Khan, Jawad Khan,\n  Young-Koo Lee","docs_id":"2009.08395","section":["cs.CV","cs.AI","cs.CL","cs.LG","cs.MM"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A Multimodal Memes Classification: A Survey and Open Research Issues. Memes are graphics and text overlapped so that together they present concepts that become dubious if one of them is absent. It is spread mostly on social media platforms, in the form of jokes, sarcasm, motivating, etc. After the success of BERT in Natural Language Processing (NLP), researchers inclined to Visual-Linguistic (VL) multimodal problems like memes classification, image captioning, Visual Question Answering (VQA), and many more. Unfortunately, many memes get uploaded each day on social media platforms that need automatic censoring to curb misinformation and hate. Recently, this issue has attracted the attention of researchers and practitioners. State-of-the-art methods that performed significantly on other VL dataset, tends to fail on memes classification. In this context, this work aims to conduct a comprehensive study on memes classification, generally on the VL multimodal problems and cutting edge solutions. We propose a generalized framework for VL problems. We cover the early and next-generation works on VL problems. Finally, we identify and articulate several open research issues and challenges. This is the first study that presents the generalized view of the advanced classification techniques concerning memes classification to the best of our knowledge. We believe this study presents a clear road-map for the Machine Learning (ML) research community to implement and enhance memes classification techniques."},{"title":"Deep Generalized Method of Moments for Instrumental Variable Analysis","source":"Andrew Bennett, Nathan Kallus, Tobias Schnabel","docs_id":"1905.12495","section":["stat.ML","cs.LG","econ.EM"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Deep Generalized Method of Moments for Instrumental Variable Analysis. Instrumental variable analysis is a powerful tool for estimating causal effects when randomization or full control of confounders is not possible. The application of standard methods such as 2SLS, GMM, and more recent variants are significantly impeded when the causal effects are complex, the instruments are high-dimensional, and\/or the treatment is high-dimensional. In this paper, we propose the DeepGMM algorithm to overcome this. Our algorithm is based on a new variational reformulation of GMM with optimal inverse-covariance weighting that allows us to efficiently control very many moment conditions. We further develop practical techniques for optimization and model selection that make it particularly successful in practice. Our algorithm is also computationally tractable and can handle large-scale datasets. Numerical results show our algorithm matches the performance of the best tuned methods in standard settings and continues to work in high-dimensional settings where even recent methods break."},{"title":"It's Moving! A Probabilistic Model for Causal Motion Segmentation in\n  Moving Camera Videos","source":"Pia Bideau, Erik Learned-Miller","docs_id":"1604.00136","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"It's Moving! A Probabilistic Model for Causal Motion Segmentation in\n  Moving Camera Videos. The human ability to detect and segment moving objects works in the presence of multiple objects, complex background geometry, motion of the observer, and even camouflage. In addition to all of this, the ability to detect motion is nearly instantaneous. While there has been much recent progress in motion segmentation, it still appears we are far from human capabilities. In this work, we derive from first principles a new likelihood function for assessing the probability of an optical flow vector given the 3D motion direction of an object. This likelihood uses a novel combination of the angle and magnitude of the optical flow to maximize the information about the true motions of objects. Using this new likelihood and several innovations in initialization, we develop a motion segmentation algorithm that beats current state-of-the-art methods by a large margin. We compare to five state-of-the-art methods on two established benchmarks, and a third new data set of camouflaged animals, which we introduce to push motion segmentation to the next level."},{"title":"Generalized roof duality and bisubmodular functions","source":"Vladimir Kolmogorov","docs_id":"1005.2305","section":["cs.DM"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Generalized roof duality and bisubmodular functions. Consider a convex relaxation $\\hat f$ of a pseudo-boolean function $f$. We say that the relaxation is {\\em totally half-integral} if $\\hat f(x)$ is a polyhedral function with half-integral extreme points $x$, and this property is preserved after adding an arbitrary combination of constraints of the form $x_i=x_j$, $x_i=1-x_j$, and $x_i=\\gamma$ where $\\gamma\\in\\{0, 1, 1\/2}$ is a constant. A well-known example is the {\\em roof duality} relaxation for quadratic pseudo-boolean functions $f$. We argue that total half-integrality is a natural requirement for generalizations of roof duality to arbitrary pseudo-boolean functions. Our contributions are as follows. First, we provide a complete characterization of totally half-integral relaxations $\\hat f$ by establishing a one-to-one correspondence with {\\em bisubmodular functions}. Second, we give a new characterization of bisubmodular functions. Finally, we show some relationships between general totally half-integral relaxations and relaxations based on the roof duality."},{"title":"Clue Me In: Semi-Supervised FGVC with Out-of-Distribution Data","source":"Ruoyi Du, Dongliang Chang, Zhanyu Ma, Yi-Zhe Song, Jun Guo","docs_id":"2112.02825","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Clue Me In: Semi-Supervised FGVC with Out-of-Distribution Data. Despite great strides made on fine-grained visual classification (FGVC), current methods are still heavily reliant on fully-supervised paradigms where ample expert labels are called for. Semi-supervised learning (SSL) techniques, acquiring knowledge from unlabeled data, provide a considerable means forward and have shown great promise for coarse-grained problems. However, exiting SSL paradigms mostly assume in-distribution (i.e., category-aligned) unlabeled data, which hinders their effectiveness when re-proposed on FGVC. In this paper, we put forward a novel design specifically aimed at making out-of-distribution data work for semi-supervised FGVC, i.e., to \"clue them in\". We work off an important assumption that all fine-grained categories naturally follow a hierarchical structure (e.g., the phylogenetic tree of \"Aves\" that covers all bird species). It follows that, instead of operating on individual samples, we can instead predict sample relations within this tree structure as the optimization goal of SSL. Beyond this, we further introduced two strategies uniquely brought by these tree structures to achieve inter-sample consistency regularization and reliable pseudo-relation. Our experimental results reveal that (i) the proposed method yields good robustness against out-of-distribution data, and (ii) it can be equipped with prior arts, boosting their performance thus yielding state-of-the-art results. Code is available at https:\/\/github.com\/PRIS-CV\/RelMatch."},{"title":"An Algorithm for Road Coloring","source":"A.N. Trahtman","docs_id":"0801.2838","section":["cs.DM"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"An Algorithm for Road Coloring. A coloring of edges of a finite directed graph turns the graph into finite-state automaton. The synchronizing word of a deterministic automaton is a word in the alphabet of colors (considered as letters) of its edges that maps the automaton to a single state. A coloring of edges of a directed graph of uniform outdegree (constant outdegree of any vertex) is synchronizing if the coloring turns the graph into a deterministic finite automaton possessing a synchronizing word. The road coloring problem is the problem of synchronizing coloring of a directed finite strongly connected graph of uniform outdegree if the greatest common divisor of the lengths of all its cycles is one. The problem posed in 1970 had evoked a noticeable interest among the specialists in the theory of graphs, automata, codes, symbolic dynamics as well as among the wide mathematical community. A polynomial time algorithm of $O(n^3)$ complexity in the most worst case and quadratic in majority of studied cases for the road coloring of the considered graph is presented below. The work is based on recent positive solution of the road coloring problem. The algorithm was implemented in the package TESTAS"},{"title":"Ultra-Fast Shapelets for Time Series Classification","source":"Martin Wistuba, Josif Grabocka, Lars Schmidt-Thieme","docs_id":"1503.05018","section":["cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Ultra-Fast Shapelets for Time Series Classification. Time series shapelets are discriminative subsequences and their similarity to a time series can be used for time series classification. Since the discovery of time series shapelets is costly in terms of time, the applicability on long or multivariate time series is difficult. In this work we propose Ultra-Fast Shapelets that uses a number of random shapelets. It is shown that Ultra-Fast Shapelets yield the same prediction quality as current state-of-the-art shapelet-based time series classifiers that carefully select the shapelets by being by up to three orders of magnitudes. Since this method allows a ultra-fast shapelet discovery, using shapelets for long multivariate time series classification becomes feasible. A method for using shapelets for multivariate time series is proposed and Ultra-Fast Shapelets is proven to be successful in comparison to state-of-the-art multivariate time series classifiers on 15 multivariate time series datasets from various domains. Finally, time series derivatives that have proven to be useful for other time series classifiers are investigated for the shapelet-based classifiers. It is shown that they have a positive impact and that they are easy to integrate with a simple preprocessing step, without the need of adapting the shapelet discovery algorithm."},{"title":"Picturing Bivariate Separable-Features for Univariate Vector Magnitudes\n  in Large-Magnitude-Range Quantum Physics Data","source":"Henan Zhao and Jian Chen","docs_id":"1905.02586","section":["cs.GR"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Picturing Bivariate Separable-Features for Univariate Vector Magnitudes\n  in Large-Magnitude-Range Quantum Physics Data. We present study results from two experiments to empirically validate that separable bivariate pairs for univariate representations of large-magnitude-range vectors are more efficient than integral pairs. The first experiment with 20 participants compared: one integral pair, three separable pairs, and one redundant pair, which is a mix of the integral and separable features. Participants performed three local tasks requiring reading numerical values, estimating ratio, and comparing two points. The second 18-participant study compared three separable pairs using three global tasks when participants must look at the entire field to get an answer: find a specific target in 20 seconds, find the maximum magnitude in 20 seconds, and estimate the total number of vector exponents within 2 seconds. Our results also reveal the following: separable pairs led to the most accurate answers and the shortest task execution time, while integral dimensions were among the least accurate; it achieved high performance only when a pop-out separable feature (here color) was added. To reconcile this finding with the existing literature, our second experiment suggests that the higher the separability, the higher the accuracy; the reason is probably that the emergent global scene created by the separable pairs reduces the subsequent search space."},{"title":"Climate Modeling with Neural Diffusion Equations","source":"Jeehyun Hwang, Jeongwhan Choi, Hwangyong Choi, Kookjin Lee, Dongeun\n  Lee, Noseong Park","docs_id":"2111.06011","section":["cs.LG","cs.AI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Climate Modeling with Neural Diffusion Equations. Owing to the remarkable development of deep learning technology, there have been a series of efforts to build deep learning-based climate models. Whereas most of them utilize recurrent neural networks and\/or graph neural networks, we design a novel climate model based on the two concepts, the neural ordinary differential equation (NODE) and the diffusion equation. Many physical processes involving a Brownian motion of particles can be described by the diffusion equation and as a result, it is widely used for modeling climate. On the other hand, neural ordinary differential equations (NODEs) are to learn a latent governing equation of ODE from data. In our presented method, we combine them into a single framework and propose a concept, called neural diffusion equation (NDE). Our NDE, equipped with the diffusion equation and one more additional neural network to model inherent uncertainty, can learn an appropriate latent governing equation that best describes a given climate dataset. In our experiments with two real-world and one synthetic datasets and eleven baselines, our method consistently outperforms existing baselines by non-trivial margins."},{"title":"Gamifying the Escape from the Engineering Method Prison - An Innovative\n  Board Game to Teach the Essence Theory to Future Project Managers and\n  Software Engineers","source":"Kai-Kristian Kemell, Juhani Risku, Arthur Evensen, Pekka Abrahamsson,\n  Aleksander Madsen Dahl, Lars Henrik Grytten, Agata Jedryszek, Petter Rostrup,\n  Anh Nguyen-Duc","docs_id":"1809.08656","section":["cs.SE"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Gamifying the Escape from the Engineering Method Prison - An Innovative\n  Board Game to Teach the Essence Theory to Future Project Managers and\n  Software Engineers. Software Engineering is an engineering discipline but lacks a solid theoretical foundation. One effort in remedying this situation has been the SEMAT Essence specification. Essence consists of a language for modeling Software Engineering (SE) practices and methods and a kernel containing what its authors describe as being elements that are present in every software development project. In practice, it is a method agnostic project management tool for SE Projects. Using the language of the specification, Essence can be used to model any software development method or practice. Thus, the specification can potentially be applied to any software development context, making it a powerful tool. However, due to the manual work and the learning process involved in modeling practices with Essence, its initial adoption can be tasking for development teams. Due to the importance of project management in SE projects, new project management tools such as Essence are valuable, and facilitating their adoption is consequently important. To tackle this issue in the case of Essence, we present a game-based approach to teaching the use Essence. In this paper, we gamify the learning process by means of an innovative board game. The game is empirically validated in a study involving students from the IT faculty of University of Jyv\\\"askyl\\\"a (n=61). Based on the results, we report the effectiveness of the game-based approach to teaching both Essence and SE project work."},{"title":"Fair Scheduling Policies Exploiting Multiuser Diversity in Cellular\n  Systems with Device-to-Device Communications","source":"PhuongBang Nguyen and Bhaskar Rao","docs_id":"1503.08485","section":["cs.IT","cs.NI","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Fair Scheduling Policies Exploiting Multiuser Diversity in Cellular\n  Systems with Device-to-Device Communications. We consider the resource allocation problem in cellular networks which support Device-to-Device Communications (D2D). For systems that enable D2D via only orthogonal resource sharing, we propose and analyze two resource allocation policies that guarantee access fairness among all users, while taking advantage of multi-user diversity and local D2D communications, to provide marked improvements over existing cellular-only policies. The first policy, the Cellular Fairness Scheduling (CFS) Policy, provides the simplest D2D extension to existing cellular systems, while the second policy, the D2D Fairness Scheduling (DFS) Policy, harnesses maximal performance from D2D-enabled systems under the orthogonal sharing setting. For even higher spectral efficiency, cellular systems with D2D can schedule the same frequency resource for more than one D2D pairs. Under this non-orthogonal sharing environment, we propose a novel group scheduling policy, the Group Fairness Scheduling (GFS) Policy, that exploits both spatial frequency reuse and multiuser diversity in order to deliver dramatic improvements to system performance with perfect fairness among the users, regardless of whether they are cellular or D2D users."},{"title":"In-ear EEG biometrics for feasible and readily collectable real-world\n  person authentication","source":"Takashi Nakamura, Valentin Goverdovsky, Danilo P. Mandic","docs_id":"1705.03742","section":["cs.CR"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"In-ear EEG biometrics for feasible and readily collectable real-world\n  person authentication. The use of EEG as a biometrics modality has been investigated for about a decade, however its feasibility in real-world applications is not yet conclusively established, mainly due to the issues with collectability and reproducibility. To this end, we propose a readily deployable EEG biometrics system based on a `one-fits-all' viscoelastic generic in-ear EEG sensor (collectability), which does not require skilled assistance or cumbersome preparation. Unlike most existing studies, we consider data recorded over multiple recording days and for multiple subjects (reproducibility) while, for rigour, the training and test segments are not taken from the same recording days. A robust approach is considered based on the resting state with eyes closed paradigm, the use of both parametric (autoregressive model) and non-parametric (spectral) features, and supported by simple and fast cosine distance, linear discriminant analysis and support vector machine classifiers. Both the verification and identification forensics scenarios are considered and the achieved results are on par with the studies based on impractical on-scalp recordings. Comprehensive analysis over a number of subjects, setups, and analysis features demonstrates the feasibility of the proposed ear-EEG biometrics, and its potential in resolving the critical collectability, robustness, and reproducibility issues associated with current EEG biometrics."},{"title":"We Are Humor Beings: Understanding and Predicting Visual Humor","source":"Arjun Chandrasekaran, Ashwin K. Vijayakumar, Stanislaw Antol, Mohit\n  Bansal, Dhruv Batra, C. Lawrence Zitnick and Devi Parikh","docs_id":"1512.04407","section":["cs.CV","cs.CL","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"We Are Humor Beings: Understanding and Predicting Visual Humor. Humor is an integral part of human lives. Despite being tremendously impactful, it is perhaps surprising that we do not have a detailed understanding of humor yet. As interactions between humans and AI systems increase, it is imperative that these systems are taught to understand subtleties of human expressions such as humor. In this work, we are interested in the question - what content in a scene causes it to be funny? As a first step towards understanding visual humor, we analyze the humor manifested in abstract scenes and design computational models for them. We collect two datasets of abstract scenes that facilitate the study of humor at both the scene-level and the object-level. We analyze the funny scenes and explore the different types of humor depicted in them via human studies. We model two tasks that we believe demonstrate an understanding of some aspects of visual humor. The tasks involve predicting the funniness of a scene and altering the funniness of a scene. We show that our models perform well quantitatively, and qualitatively through human studies. Our datasets are publicly available."},{"title":"Detection of Malaria Vector Breeding Habitats using Topographic Models","source":"Aishwarya Jadhav","docs_id":"2011.13714","section":["cs.LG","cs.CV","eess.IV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Detection of Malaria Vector Breeding Habitats using Topographic Models. Treatment of stagnant water bodies that act as a breeding site for malarial vectors is a fundamental step in most malaria elimination campaigns. However, identification of such water bodies over large areas is expensive, labour-intensive and time-consuming and hence, challenging in countries with limited resources. Practical models that can efficiently locate water bodies can target the limited resources by greatly reducing the area that needs to be scanned by the field workers. To this end, we propose a practical topographic model based on easily available, global, high-resolution DEM data to predict locations of potential vector-breeding water sites. We surveyed the Obuasi region of Ghana to assess the impact of various topographic features on different types of water bodies and uncover the features that significantly influence the formation of aquatic habitats. We further evaluate the effectiveness of multiple models. Our best model significantly outperforms earlier attempts that employ topographic variables for detection of small water sites, even the ones that utilize additional satellite imagery data and demonstrates robustness across different settings."},{"title":"Annotators with Attitudes: How Annotator Beliefs And Identities Bias\n  Toxic Language Detection","source":"Maarten Sap, Swabha Swayamdipta, Laura Vianna, Xuhui Zhou, Yejin Choi,\n  Noah A. Smith","docs_id":"2111.07997","section":["cs.CL","cs.HC"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Annotators with Attitudes: How Annotator Beliefs And Identities Bias\n  Toxic Language Detection. The perceived toxicity of language can vary based on someone's identity and beliefs, but this variation is often ignored when collecting toxic language datasets, resulting in dataset and model biases. We seek to understand the who, why, and what behind biases in toxicity annotations. In two online studies with demographically and politically diverse participants, we investigate the effect of annotator identities (who) and beliefs (why), drawing from social psychology research about hate speech, free speech, racist beliefs, political leaning, and more. We disentangle what is annotated as toxic by considering posts with three characteristics: anti-Black language, African American English (AAE) dialect, and vulgarity. Our results show strong associations between annotator identity and beliefs and their ratings of toxicity. Notably, more conservative annotators and those who scored highly on our scale for racist beliefs were less likely to rate anti-Black language as toxic, but more likely to rate AAE as toxic. We additionally present a case study illustrating how a popular toxicity detection system's ratings inherently reflect only specific beliefs and perspectives. Our findings call for contextualizing toxicity labels in social variables, which raises immense implications for toxic language annotation and detection."},{"title":"On the Tasks and Characteristics of Product Owners: A Case Study in the\n  Oil & Gas Industry","source":"Carolin Unger-Windeler, Jil Kluender","docs_id":"1809.00830","section":["cs.SE"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"On the Tasks and Characteristics of Product Owners: A Case Study in the\n  Oil & Gas Industry. Product owners in the Scrum framework - respectively the on-site customer when applying eXtreme Programming - have an important role in the development process. They are responsible for the requirements and backlog deciding about the next steps within the development process. However, many companies face the difficulty of defining the tasks and the responsibilities of a product owner on their way towards an agile work environment. While literature addresses the tailoring of the product owner's role in general, research does not particularly consider the specifics of this role in the context of a systems development as we find for example in the oil and gas industry. Consequently, the question arises whether there are any differences between these two areas. In order to answer this question, we investigated on the current state of characteristics and tasks of product owners at Baker Hughes, a GE company (BHGE). In this position paper, we present initial results based on an online survey with answers of ten active product owners within the technical software department of BHGE. The results indicate that current product owners at BHGE primarily act as a nexus between all ends. While technical tasks are performed scarcely, communication skills seem even more important for product owners in a system development organization. However, to obtain more reliable results additional research in this area is required."},{"title":"Physics-Driven Regularization of Deep Neural Networks for Enhanced\n  Engineering Design and Analysis","source":"Mohammad Amin Nabian, Hadi Meidani","docs_id":"1810.05547","section":["cs.LG","cs.CE","cs.NA","math.AP","math.NA","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Physics-Driven Regularization of Deep Neural Networks for Enhanced\n  Engineering Design and Analysis. In this paper, we introduce a physics-driven regularization method for training of deep neural networks (DNNs) for use in engineering design and analysis problems. In particular, we focus on prediction of a physical system, for which in addition to training data, partial or complete information on a set of governing laws is also available. These laws often appear in the form of differential equations, derived from first principles, empirically-validated laws, or domain expertise, and are usually neglected in data-driven prediction of engineering systems. We propose a training approach that utilizes the known governing laws and regularizes data-driven DNN models by penalizing divergence from those laws. The first two numerical examples are synthetic examples, where we show that in constructing a DNN model that best fits the measurements from a physical system, the use of our proposed regularization results in DNNs that are more interpretable with smaller generalization errors, compared to other common regularization methods. The last two examples concern metamodeling for a random Burgers' system and for aerodynamic analysis of passenger vehicles, where we demonstrate that the proposed regularization provides superior generalization accuracy compared to other common alternatives."},{"title":"Compatible Certificateless and Identity-Based Cryptosystems for\n  Heterogeneous IoT","source":"Rouzbeh Behnia, Attila A. Yavuz, Muslum Ozgur Ozmen, Tsz Hon Yuen","docs_id":"2103.09345","section":["cs.CR"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Compatible Certificateless and Identity-Based Cryptosystems for\n  Heterogeneous IoT. Certificates ensure the authenticity of users' public keys, however their overhead (e.g., certificate chains) might be too costly for some IoT systems like aerial drones. Certificate-free cryptosystems, like identity-based and certificateless systems, lift the burden of certificates and could be a suitable alternative for such IoTs. However, despite their merits, there is a research gap in achieving compatible identity-based and certificateless systems to allow users from different domains (identity-based or certificateless) to communicate seamlessly. Moreover, more efficient constructions can enable their adoption in resource-limited IoTs. In this work, we propose new identity-based and certificateless cryptosystems that provide such compatibility and efficiency. This feature is beneficial for heterogeneous IoT settings (e.g., commercial aerial drones), where different levels of trust\/control is assumed on the trusted third party. Our schemes are more communication efficient than their public key based counterparts, as they do not need certificate processing. Our experimental analysis on both commodity and embedded IoT devices show that, only with the cost of having a larger system public key, our cryptosystems are more computation and communication efficient than their certificate-free counterparts. We prove the security of our schemes (in the random oracle model) and open-source our cryptographic framework for public testing\/adoption."},{"title":"On Rendering Synthetic Images for Training an Object Detector","source":"Artem Rozantsev, Vincent Lepetit, Pascal Fua","docs_id":"1411.7911","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"On Rendering Synthetic Images for Training an Object Detector. We propose a novel approach to synthesizing images that are effective for training object detectors. Starting from a small set of real images, our algorithm estimates the rendering parameters required to synthesize similar images given a coarse 3D model of the target object. These parameters can then be reused to generate an unlimited number of training images of the object of interest in arbitrary 3D poses, which can then be used to increase classification performances. A key insight of our approach is that the synthetically generated images should be similar to real images, not in terms of image quality, but rather in terms of features used during the detector training. We show in the context of drone, plane, and car detection that using such synthetically generated images yields significantly better performances than simply perturbing real images or even synthesizing images in such way that they look very realistic, as is often done when only limited amounts of training data are available."},{"title":"Hunting the Ethereum Smart Contract: Color-inspired Inspection of\n  Potential Attacks","source":"TonTon Hsien-De Huang","docs_id":"1807.01868","section":["cs.CR","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Hunting the Ethereum Smart Contract: Color-inspired Inspection of\n  Potential Attacks. Blockchain and Cryptocurrencies are gaining unprecedented popularity and understanding. Meanwhile, Ethereum is gaining a significant popularity in the blockchain community, mainly due to the fact that it is designed in a way that enables developers to write smart contract and decentralized applications (Dapps). This new paradigm of applications opens the door to many possibilities and opportunities. However, the security of Ethereum smart contracts has not received much attention; several Ethereum smart contracts malfunctioning have recently been reported. Unlike many previous works that have applied static and dynamic analyses to find bugs in smart contracts, we do not attempt to define and extract any features; instead we focus on reducing the expert's labor costs. We first present a new in-depth analysis of potential attacks methodology and then translate the bytecode of solidity into RGB color code. After that, we transform them to a fixed-sized encoded image. Finally, the encoded image is fed to convolutional neural network (CNN) for automatic feature extraction and learning, detecting compiler bugs of Ethereum smart contract."},{"title":"Joint-task Self-supervised Learning for Temporal Correspondence","source":"Xueting Li, Sifei Liu, Shalini De Mello, Xiaolong Wang, Jan Kautz,\n  Ming-Hsuan Yang","docs_id":"1909.11895","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Joint-task Self-supervised Learning for Temporal Correspondence. This paper proposes to learn reliable dense correspondence from videos in a self-supervised manner. Our learning process integrates two highly related tasks: tracking large image regions \\emph{and} establishing fine-grained pixel-level associations between consecutive video frames. We exploit the synergy between both tasks through a shared inter-frame affinity matrix, which simultaneously models transitions between video frames at both the region- and pixel-levels. While region-level localization helps reduce ambiguities in fine-grained matching by narrowing down search regions; fine-grained matching provides bottom-up features to facilitate region-level localization. Our method outperforms the state-of-the-art self-supervised methods on a variety of visual correspondence tasks, including video-object and part-segmentation propagation, keypoint tracking, and object tracking. Our self-supervised method even surpasses the fully-supervised affinity feature representation obtained from a ResNet-18 pre-trained on the ImageNet."},{"title":"Beamforming Design for Intelligent Reflecting Surface-Enhanced Symbiotic\n  Radio Systems","source":"Shaokang Hu, Chang Liu, Zhiqiang Wei, Yuanxin Cai, Derrick Wing Kwan\n  Ng, and Jinhong Yuan","docs_id":"2110.10316","section":["cs.IT","eess.SP","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Beamforming Design for Intelligent Reflecting Surface-Enhanced Symbiotic\n  Radio Systems. This paper investigates multiuser multi-input single-output downlink symbiotic radio communication systems assisted by an intelligent reflecting surface (IRS). Different from existing methods ideally assuming the secondary user (SU) can jointly decode information symbols from both the access point (AP) and the IRS via multiuser detection, we consider a more practical SU that only non-coherent detection is available. To characterize the non-coherent decoding performance, a practical upper bound of the average symbol error rate (SER) is derived. Subsequently, we jointly optimize the beamformer at the AP and the phase shifts at the IRS to maximize the average sum-rate of the primary system taking into account the maximum tolerable SER constraint for the SU. To circumvent the couplings of variables, we exploit the Schur complement that facilitates the design of a suboptimal beamforming algorithm based on successive convex approximation. Our simulation results show that compared with various benchmark algorithms, the proposed scheme significantly improves the average sum-rate of the primary system, while guaranteeing the decoding performance of the secondary system."},{"title":"Multi-view Low-rank Sparse Subspace Clustering","source":"Maria Brbic and Ivica Kopriva","docs_id":"1708.08732","section":["cs.CV","cs.LG","math.OC","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Multi-view Low-rank Sparse Subspace Clustering. Most existing approaches address multi-view subspace clustering problem by constructing the affinity matrix on each view separately and afterwards propose how to extend spectral clustering algorithm to handle multi-view data. This paper presents an approach to multi-view subspace clustering that learns a joint subspace representation by constructing affinity matrix shared among all views. Relying on the importance of both low-rank and sparsity constraints in the construction of the affinity matrix, we introduce the objective that balances between the agreement across different views, while at the same time encourages sparsity and low-rankness of the solution. Related low-rank and sparsity constrained optimization problem is for each view solved using the alternating direction method of multipliers. Furthermore, we extend our approach to cluster data drawn from nonlinear subspaces by solving the corresponding problem in a reproducing kernel Hilbert space. The proposed algorithm outperforms state-of-the-art multi-view subspace clustering algorithms on one synthetic and four real-world datasets."},{"title":"Cube Sampled K-Prototype Clustering for Featured Data","source":"Seemandhar Jain, Aditya A. Shastri, Kapil Ahuja, Yann Busnel, and\n  Navneet Pratap Singh","docs_id":"2108.10262","section":["cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Cube Sampled K-Prototype Clustering for Featured Data. Clustering large amount of data is becoming increasingly important in the current times. Due to the large sizes of data, clustering algorithm often take too much time. Sampling this data before clustering is commonly used to reduce this time. In this work, we propose a probabilistic sampling technique called cube sampling along with K-Prototype clustering. Cube sampling is used because of its accurate sample selection. K-Prototype is most frequently used clustering algorithm when the data is numerical as well as categorical (very common in today's time). The novelty of this work is in obtaining the crucial inclusion probabilities for cube sampling using Principal Component Analysis (PCA). Experiments on multiple datasets from the UCI repository demonstrate that cube sampled K-Prototype algorithm gives the best clustering accuracy among similarly sampled other popular clustering algorithms (K-Means, Hierarchical Clustering (HC), Spectral Clustering (SC)). When compared with unsampled K-Prototype, K-Means, HC and SC, it still has the best accuracy with the added advantage of reduced computational complexity (due to reduced data size)."},{"title":"Channel-coded Collision Resolution by Exploiting Symbol Misalignment","source":"Lu Lu, Soung Chang Liew and Shengli Zhang","docs_id":"1009.4046","section":["cs.IT","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Channel-coded Collision Resolution by Exploiting Symbol Misalignment. In random-access networks, such as the IEEE 802.11 network, different users may transmit their packets simultaneously, resulting in packet collisions. Traditionally, the collided packets are simply discarded. To improve performance, advanced signal processing techniques can be applied to extract the individual packets from the collided signals. Prior work of ours has shown that the symbol misalignment among the collided packets can be exploited to improve the likelihood of successfully extracting the individual packets. However, the failure rate is still unacceptably high. This paper investigates how channel coding can be used to reduce the failure rate. We propose and investigate a decoding scheme that incorporates the exploitation of the aforementioned symbol misalignment into the channel decoding process. This is a fine-grained integration at the symbol level. In particular, collision resolution and channel decoding are applied in an integrated manner. Simulation results indicate that our method outperforms other schemes, including the straightforward method in which collision resolution and channel coding are applied separately."},{"title":"Optimal Unsupervised Domain Translation","source":"Emmanuel de B\\'ezenac, Ibrahim Ayed, Patrick Gallinari","docs_id":"1906.01292","section":["cs.LG","cs.CV","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Optimal Unsupervised Domain Translation. Domain Translation is the problem of finding a meaningful correspondence between two domains. Since in a majority of settings paired supervision is not available, much work focuses on Unsupervised Domain Translation (UDT) where data samples from each domain are unpaired. Following the seminal work of CycleGAN for UDT, many variants and extensions of this model have been proposed. However, there is still little theoretical understanding behind their success. We observe that these methods yield solutions which are approximately minimal w.r.t. a given transportation cost, leading us to reformulate the problem in the Optimal Transport (OT) framework. This viewpoint gives us a new perspective on Unsupervised Domain Translation and allows us to prove the existence and uniqueness of the retrieved mapping, given a large family of transport costs. We then propose a novel framework to efficiently compute optimal mappings in a dynamical setting. We show that it generalizes previous methods and enables a more explicit control over the computed optimal mapping. It also provides smooth interpolations between the two domains. Experiments on toy and real world datasets illustrate the behavior of our method."},{"title":"Heterogeneity-aware Twitter Bot Detection with Relational Graph\n  Transformers","source":"Shangbin Feng, Zhaoxuan Tan, Rui Li, Minnan Luo","docs_id":"2109.02927","section":["cs.SI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Heterogeneity-aware Twitter Bot Detection with Relational Graph\n  Transformers. Twitter bot detection has become an important and challenging task to combat misinformation and protect the integrity of the online discourse. State-of-the-art approaches generally leverage the topological structure of the Twittersphere, while they neglect the heterogeneity of relations and influence among users. In this paper, we propose a novel bot detection framework to alleviate this problem, which leverages the topological structure of user-formed heterogeneous graphs and models varying influence intensity between users. Specifically, we construct a heterogeneous information network with users as nodes and diversified relations as edges. We then propose relational graph transformers to model heterogeneous influence between users and learn node representations. Finally, we use semantic attention networks to aggregate messages across users and relations and conduct heterogeneity-aware Twitter bot detection. Extensive experiments demonstrate that our proposal outperforms state-of-the-art methods on a comprehensive Twitter bot detection benchmark. Additional studies also bear out the effectiveness of our proposed relational graph transformers, semantic attention networks and the graph-based approach in general."},{"title":"Exploring Temporal Information for Improved Video Understanding","source":"Yi Zhu","docs_id":"1905.10654","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Exploring Temporal Information for Improved Video Understanding. In this dissertation, I present my work towards exploring temporal information for better video understanding. Specifically, I have worked on two problems: action recognition and semantic segmentation. For action recognition, I have proposed a framework, termed hidden two-stream networks, to learn an optimal motion representation that does not require the computation of optical flow. My framework alleviates several challenges faced in video classification, such as learning motion representations, real-time inference, multi-framerate handling, generalizability to unseen actions, etc. For semantic segmentation, I have introduced a general framework that uses video prediction models to synthesize new training samples. By scaling up the training dataset, my trained models are more accurate and robust than previous models even without modifications to the network architectures or objective functions. I believe videos have much more potential to be mined, and temporal information is one of the most important cues for machines to perceive the visual world better."},{"title":"Translation of \"Zur Ermittlung eines Objektes aus zwei Perspektiven mit\n  innerer Orientierung\" by Erwin Kruppa (1913)","source":"Guillermo Gallego, Elias Mueggler, Peter Sturm","docs_id":"1801.01454","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Translation of \"Zur Ermittlung eines Objektes aus zwei Perspektiven mit\n  innerer Orientierung\" by Erwin Kruppa (1913). Erwin Kruppa's 1913 paper, Erwin Kruppa, \"Zur Ermittlung eines Objektes aus zwei Perspektiven mit innerer Orientierung\", Sitzungsberichte der Mathematisch-Naturwissenschaftlichen Kaiserlichen Akademie der Wissenschaften, Vol. 122 (1913), pp. 1939-1948, which may be translated as \"To determine a 3D object from two perspective views with known inner orientation\", is a landmark paper in Computer Vision because it provides the first five-point algorithm for relative pose estimation. Kruppa showed that (a finite number of solutions for) the relative pose between two calibrated images of a rigid object can be computed from five point matches between the images. Kruppa's work also gained attention in the topic of camera self-calibration, as presented in (Maybank and Faugeras, 1992). Since the paper is still relevant today (more than a hundred citations within the last ten years) and the paper is not available online, we ordered a copy from the German National Library in Frankfurt and provide an English translation along with the German original. We also adapt the terminology to a modern jargon and provide some clarifications (highlighted in sans-serif font). For a historical review of geometric computer vision, the reader is referred to the recent survey paper (Sturm, 2011)."},{"title":"Graph Generators: State of the Art and Open Challenges","source":"Angela Bonifati, Irena Holubov\\'a, Arnau Prat-P\\'erez, Sherif Sakr","docs_id":"2001.07906","section":["cs.DB","cs.IR","cs.SI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Graph Generators: State of the Art and Open Challenges. The abundance of interconnected data has fueled the design and implementation of graph generators reproducing real-world linking properties, or gauging the effectiveness of graph algorithms, techniques and applications manipulating these data. We consider graph generation across multiple subfields, such as Semantic Web, graph databases, social networks, and community detection, along with general graphs. Despite the disparate requirements of modern graph generators throughout these communities, we analyze them under a common umbrella, reaching out the functionalities, the practical usage, and their supported operations. We argue that this classification is serving the need of providing scientists, researchers and practitioners with the right data generator at hand for their work. This survey provides a comprehensive overview of the state-of-the-art graph generators by focusing on those that are pertinent and suitable for several data-intensive tasks. Finally, we discuss open challenges and missing requirements of current graph generators along with their future extensions to new emerging fields."},{"title":"Building an Aerial-Ground Robotics System for Precision Farming: An\n  Adaptable Solution","source":"Alberto Pretto, St\\'ephanie Aravecchia, Wolfram Burgard, Nived\n  Chebrolu, Christian Dornhege, Tillmann Falck, Freya Fleckenstein, Alessandra\n  Fontenla, Marco Imperoli, Raghav Khanna, Frank Liebisch, Philipp Lottes,\n  Andres Milioto, Daniele Nardi, Sandro Nardi, Johannes Pfeifer, Marija\n  Popovi\\'c, Ciro Potena, C\\'edric Pradalier, Elisa Rothacker-Feder, Inkyu Sa,\n  Alexander Schaefer, Roland Siegwart, Cyrill Stachniss, Achim Walter, Wera\n  Winterhalter, Xiaolong Wu and Juan Nieto","docs_id":"1911.03098","section":["cs.RO"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Building an Aerial-Ground Robotics System for Precision Farming: An\n  Adaptable Solution. The application of autonomous robots in agriculture is gaining increasing popularity thanks to the high impact it may have on food security, sustainability, resource use efficiency, reduction of chemical treatments, and the optimization of human effort and yield. With this vision, the Flourish research project aimed to develop an adaptable robotic solution for precision farming that combines the aerial survey capabilities of small autonomous unmanned aerial vehicles (UAVs) with targeted intervention performed by multi-purpose unmanned ground vehicles (UGVs). This paper presents an overview of the scientific and technological advances and outcomes obtained in the project. We introduce multi-spectral perception algorithms and aerial and ground-based systems developed for monitoring crop density, weed pressure, crop nitrogen nutrition status, and to accurately classify and locate weeds. We then introduce the navigation and mapping systems tailored to our robots in the agricultural environment, as well as the modules for collaborative mapping. We finally present the ground intervention hardware, software solutions, and interfaces we implemented and tested in different field conditions and with different crops. We describe a real use case in which a UAV collaborates with a UGV to monitor the field and to perform selective spraying without human intervention."},{"title":"Motion Capture from Internet Videos","source":"Junting Dong, Qing Shuai, Yuanqing Zhang, Xian Liu, Xiaowei Zhou,\n  Hujun Bao","docs_id":"2008.07931","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Motion Capture from Internet Videos. Recent advances in image-based human pose estimation make it possible to capture 3D human motion from a single RGB video. However, the inherent depth ambiguity and self-occlusion in a single view prohibit the recovery of as high-quality motion as multi-view reconstruction. While multi-view videos are not common, the videos of a celebrity performing a specific action are usually abundant on the Internet. Even if these videos were recorded at different time instances, they would encode the same motion characteristics of the person. Therefore, we propose to capture human motion by jointly analyzing these Internet videos instead of using single videos separately. However, this new task poses many new challenges that cannot be addressed by existing methods, as the videos are unsynchronized, the camera viewpoints are unknown, the background scenes are different, and the human motions are not exactly the same among videos. To address these challenges, we propose a novel optimization-based framework and experimentally demonstrate its ability to recover much more precise and detailed motion from multiple videos, compared against monocular motion capture methods."},{"title":"Verifying Response Times in Networked Automation Systems Using Jitter\n  Bounds","source":"Seshadhri Srinivasan, Furio Buonopane, Srini Ramaswamy, Juri Vain","docs_id":"1507.04300","section":["cs.SY"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Verifying Response Times in Networked Automation Systems Using Jitter\n  Bounds. Networked Automation Systems (NAS) have to meet stringent response time during operation. Verifying response time of automation is an important step during design phase before deployment. Timing discrepancies due to hardware, software and communication components of NAS affect the response time. This investigation uses model templates for verifying the response time in NAS. First, jitter bounds model the timing fluctuations of NAS components. These jitter bounds are the inputs to model templates that are formal models of timing fluctuations. The model templates are atomic action patterns composed of three composition operators- sequential, alternative, and parallel and embedded in time wrapper that specifies clock driven activation conditions. Model templates in conjunction with formal model of technical process offer an easier way to verify the response time. The investigation demonstrates the proposed verification method using an industrial steam boiler with typical NAS components in plant floor."},{"title":"Solid shell prism elements based on hierarchical, heterogeneous, and\n  anisotropic shape functions","source":"Lukasz Kaczmarczyk, Hoang Nguyen, Zahur Ullah, Mebratu Wakeni, Chris\n  Pearce","docs_id":"2010.08799","section":["math.NA","cs.NA"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Solid shell prism elements based on hierarchical, heterogeneous, and\n  anisotropic shape functions. The formulation of a new prism finite element is presented for the nonlinear analysis of solid shells subject to large strains and large displacements. The element is based on hierarchical, heterogeneous, and anisotropic shape functions. As with other solid shell formulations, only displacement degrees of freedom are required to describe the shell kinematics and general three-dimensional material laws can be adopted. However, the novelty of this formulation is the ability to capture complex shell behaviour and avoid locking phenomena, without the need to use reduced integration or adopt additional natural strain or enhanced strain fields. Thus, this element is ideally suited for geometrically and physically nonlinear problems. This is achieved by constructing independent approximation shape functions on both the prism element's triangular faces and through the thickness, where the latter is associated with a local coordinate system that convects with deformation of the shell. The element is extremely efficient, with the hierarchical property lending itself to an efficient and highly scalable multigrid solver, and the heterogeneity property enables local p-adaptivity. The paper demonstrates performance of the element for a number of linear and geometrically nonlinear problems, benchmarked against well established problems in the literature. The formulation has been implemented in the MoFEM."},{"title":"Musical Prosody-Driven Emotion Classification: Interpreting Vocalists\n  Portrayal of Emotions Through Machine Learning","source":"Nicholas Farris, Brian Model, Richard Savery, Gil Weinberg","docs_id":"2106.02556","section":["cs.SD","cs.LG","eess.AS"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Musical Prosody-Driven Emotion Classification: Interpreting Vocalists\n  Portrayal of Emotions Through Machine Learning. The task of classifying emotions within a musical track has received widespread attention within the Music Information Retrieval (MIR) community. Music emotion recognition has traditionally relied on the use of acoustic features, verbal features, and metadata-based filtering. The role of musical prosody remains under-explored despite several studies demonstrating a strong connection between prosody and emotion. In this study, we restrict the input of traditional machine learning algorithms to the features of musical prosody. Furthermore, our proposed approach builds upon the prior by classifying emotions under an expanded emotional taxonomy, using the Geneva Wheel of Emotion. We utilize a methodology for individual data collection from vocalists, and personal ground truth labeling by the artist themselves. We found that traditional machine learning algorithms when limited to the features of musical prosody (1) achieve high accuracies for a single singer, (2) maintain high accuracy when the dataset is expanded to multiple singers, and (3) achieve high accuracies when trained on a reduced subset of the total features."},{"title":"Training Confidence-calibrated Classifiers for Detecting\n  Out-of-Distribution Samples","source":"Kimin Lee, Honglak Lee, Kibok Lee, Jinwoo Shin","docs_id":"1711.09325","section":["stat.ML","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Training Confidence-calibrated Classifiers for Detecting\n  Out-of-Distribution Samples. The problem of detecting whether a test sample is from in-distribution (i.e., training distribution by a classifier) or out-of-distribution sufficiently different from it arises in many real-world machine learning applications. However, the state-of-art deep neural networks are known to be highly overconfident in their predictions, i.e., do not distinguish in- and out-of-distributions. Recently, to handle this issue, several threshold-based detectors have been proposed given pre-trained neural classifiers. However, the performance of prior works highly depends on how to train the classifiers since they only focus on improving inference procedures. In this paper, we develop a novel training method for classifiers so that such inference algorithms can work better. In particular, we suggest two additional terms added to the original loss (e.g., cross entropy). The first one forces samples from out-of-distribution less confident by the classifier and the second one is for (implicitly) generating most effective training samples for the first one. In essence, our method jointly trains both classification and generative neural networks for out-of-distribution. We demonstrate its effectiveness using deep convolutional neural networks on various popular image datasets."},{"title":"Bipartite Network Model for Inferring Hidden Ties in Crime Data","source":"Haruna Isah, Daniel Neagu, Paul Trundle","docs_id":"1510.02343","section":["cs.SI","physics.soc-ph"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Bipartite Network Model for Inferring Hidden Ties in Crime Data. Certain crimes are hardly committed by individuals but carefully organised by group of associates and affiliates loosely connected to each other with a single or small group of individuals coordinating the overall actions. A common starting point in understanding the structural organisation of criminal groups is to identify the criminals and their associates. Situations arise in many criminal datasets where there is no direct connection among the criminals. In this paper, we investigate ties and community structure in crime data in order to understand the operations of both traditional and cyber criminals, as well as to predict the existence of organised criminal networks. Our contributions are twofold: we propose a bipartite network model for inferring hidden ties between actors who initiated an illegal interaction and objects affected by the interaction, we then validate the method in two case studies on pharmaceutical crime and underground forum data using standard network algorithms for structural and community analysis. The vertex level metrics and community analysis results obtained indicate the significance of our work in understanding the operations and structure of organised criminal networks which were not immediately obvious in the data. Identifying these groups and mapping their relationship to one another is essential in making more effective disruption strategies in the future."},{"title":"ROSE: Real One-Stage Effort to Detect the Fingerprint Singular Point\n  Based on Multi-scale Spatial Attention","source":"Liaojun Pang, Jiong Chen, Fei Guo, Zhicheng Cao, and Heng Zhao","docs_id":"2003.03918","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"ROSE: Real One-Stage Effort to Detect the Fingerprint Singular Point\n  Based on Multi-scale Spatial Attention. Detecting the singular point accurately and efficiently is one of the most important tasks for fingerprint recognition. In recent years, deep learning has been gradually used in the fingerprint singular point detection. However, current deep learning-based singular point detection methods are either two-stage or multi-stage, which makes them time-consuming. More importantly, their detection accuracy is yet unsatisfactory, especially in the case of the low-quality fingerprint. In this paper, we make a Real One-Stage Effort to detect fingerprint singular points more accurately and efficiently, and therefore we name the proposed algorithm ROSE for short, in which the multi-scale spatial attention, the Gaussian heatmap and the variant of focal loss are applied together to achieve a higher detection rate. Experimental results on the datasets FVC2002 DB1 and NIST SD4 show that our ROSE outperforms the state-of-art algorithms in terms of detection rate, false alarm rate and detection speed."},{"title":"Lazy TSO Reachability","source":"Ahmed Bouajjani, Georgel Calin, Egor Derevenetc, Roland Meyer","docs_id":"1501.02683","section":["cs.PL"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Lazy TSO Reachability. We address the problem of checking state reachability for programs running under Total Store Order (TSO). The problem has been shown to be decidable but the cost is prohibitive, namely non-primitive recursive. We propose here to give up completeness. Our contribution is a new algorithm for TSO reachability: it uses the standard SC semantics and introduces the TSO semantics lazily and only where needed. At the heart of our algorithm is an iterative refinement of the program of interest. If the program's goal state is SC-reachable, we are done. If the goal state is not SC-reachable, this may be due to the fact that SC under-approximates TSO. We employ a second algorithm that determines TSO computations which are infeasible under SC, and hence likely to lead to new states. We enrich the program to emulate, under SC, these TSO computations. Altogether, this yields an iterative under-approximation that we prove sound and complete for bug hunting, i.e., a semi-decision procedure halting for positive cases of reachability. We have implemented the procedure as an extension to the tool Trencher and compared it to the Memorax and CBMC model checkers."},{"title":"Shared-Control Teleoperation Paradigms on a Soft Growing Robot\n  Manipulator","source":"Fabio Stroppa and Mario Selvaggio and Nathaniel Agharese and MingLuo\n  and Laura H. Blumenschein and Elliot W. Hawkes and Allison M. Okamura","docs_id":"2108.00677","section":["cs.RO"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Shared-Control Teleoperation Paradigms on a Soft Growing Robot\n  Manipulator. Semi-autonomous telerobotic systems allow both humans and robots to exploit their strengths, while enabling personalized execution of a task. However, for new soft robots with degrees of freedom dissimilar to those of human operators, it is unknown how the control of a task should be divided between the human and robot. This work presents a set of interaction paradigms between a human and a soft growing robot manipulator, and demonstrates them in both real and simulated scenarios. The robot can grow and retract by eversion and inversion of its tubular body, a property we exploit to implement interaction paradigms. We implemented and tested six different paradigms of human-robot interaction, beginning with full teleoperation and gradually adding automation to various aspects of the task execution. All paradigms were demonstrated by two expert and two naive operators. Results show that humans and the soft robot manipulator can split control along degrees of freedom while acting simultaneously. In the simple pick-and-place task studied in this work, performance improves as the control is gradually given to the robot, because the robot can correct certain human errors. However, human engagement and enjoyment may be maximized when the task is at least partially shared. Finally, when the human operator is assisted by haptic feedback based on soft robot position errors, we observed that the improvement in performance is highly dependent on the expertise of the human operator."},{"title":"Learning to Automatically Catch Potholes in Worldwide Road Scene Images","source":"J. Javier Yebes, David Montero, Ignacio Arriola","docs_id":"2105.07986","section":["cs.CV","cs.LG","eess.IV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Learning to Automatically Catch Potholes in Worldwide Road Scene Images. Among several road hazards that are present in any paved way in the world, potholes are one of the most annoying and also involving higher maintenance costs. There exists an increasing interest on the automated detection of these hazards enabled by technological and research progress. Our research work tackled the challenge of pothole detection from images of real world road scenes. The main novelty resides on the application of the latest progress in AI to learn the visual appearance of potholes. We built a large dataset of images with pothole annotations. They contained road scenes from different cities in the world, taken with different cameras, vehicles and viewpoints under varied environmental conditions. Then, we fine-tuned four different object detection models based on Faster R-CNN and SSD deep neural networks. We achieved high average precision and the pothole detector was tested on the Nvidia DrivePX2 platform with GPGPU capability, which can be embedded on vehicles. Moreover, it was deployed on a real vehicle to notify the detected potholes to a given IoT platform as part of AUTOPILOT H2020 project."},{"title":"Master's Thesis : Deep Learning for Visual Recognition","source":"R\\'emi Cad\\`ene, Nicolas Thome, Matthieu Cord","docs_id":"1610.05567","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Master's Thesis : Deep Learning for Visual Recognition. The goal of our research is to develop methods advancing automatic visual recognition. In order to predict the unique or multiple labels associated to an image, we study different kind of Deep Neural Networks architectures and methods for supervised features learning. We first draw up a state-of-the-art review of the Convolutional Neural Networks aiming to understand the history behind this family of statistical models, the limit of modern architectures and the novel techniques currently used to train deep CNNs. The originality of our work lies in our approach focusing on tasks with a low amount of data. We introduce different models and techniques to achieve the best accuracy on several kind of datasets, such as a medium dataset of food recipes (100k images) for building a web API, or a small dataset of satellite images (6,000) for the DSG online challenge that we've won. We also draw up the state-of-the-art in Weakly Supervised Learning, introducing different kind of CNNs able to localize regions of interest. Our last contribution is a framework, build on top of Torch7, for training and testing deep models on any visual recognition tasks and on datasets of any scale."},{"title":"Filtering Approaches for Dealing with Noise in Anomaly Detection","source":"Navid Hashemi, Eduardo Verdugo German, Jonatan Pena Ramirez, and\n  Justin Ruths","docs_id":"1909.01477","section":["eess.SY","cs.SY"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Filtering Approaches for Dealing with Noise in Anomaly Detection. The leading workhorse of anomaly (and attack) detection in the literature has been residual-based detectors, where the residual is the discrepancy between the observed output provided by the sensors (inclusive of any tampering along the way) and the estimated output provided by an observer. These techniques calculate some statistic of the residual and apply a threshold to determine whether or not to raise an alarm. To date, these methods have not leveraged the frequency content of the residual signal in making the detection problem easier, specifically dealing with the case of (e.g., measurement) noise. Here we demonstrate some opportunities to combine filtering to enhance the performance of residual-based detectors. We also demonstrate how filtering can provide a compelling alternative to residual-based methods when paired with a robust observer. In this process, we consider the class of attacks that are stealthy, or undetectable, by such filtered detection methods and the impact they can have on the system."},{"title":"Artistic style transfer for videos and spherical images","source":"Manuel Ruder, Alexey Dosovitskiy, Thomas Brox","docs_id":"1708.04538","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Artistic style transfer for videos and spherical images. Manually re-drawing an image in a certain artistic style takes a professional artist a long time. Doing this for a video sequence single-handedly is beyond imagination. We present two computational approaches that transfer the style from one image (for example, a painting) to a whole video sequence. In our first approach, we adapt to videos the original image style transfer technique by Gatys et al. based on energy minimization. We introduce new ways of initialization and new loss functions to generate consistent and stable stylized video sequences even in cases with large motion and strong occlusion. Our second approach formulates video stylization as a learning problem. We propose a deep network architecture and training procedures that allow us to stylize arbitrary-length videos in a consistent and stable way, and nearly in real time. We show that the proposed methods clearly outperform simpler baselines both qualitatively and quantitatively. Finally, we propose a way to adapt these approaches also to 360 degree images and videos as they emerge with recent virtual reality hardware."},{"title":"Locally Differentially Private Minimum Finding","source":"Kazuto Fukuchi, Chia-Mu Yu, Arashi Haishima, Jun Sakuma","docs_id":"1905.11067","section":["math.ST","cs.CR","cs.LG","stat.TH"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Locally Differentially Private Minimum Finding. We investigate a problem of finding the minimum, in which each user has a real value and we want to estimate the minimum of these values under the local differential privacy constraint. We reveal that this problem is fundamentally difficult, and we cannot construct a mechanism that is consistent in the worst case. Instead of considering the worst case, we aim to construct a private mechanism whose error rate is adaptive to the easiness of estimation of the minimum. As a measure of easiness, we introduce a parameter $\\alpha$ that characterizes the fatness of the minimum-side tail of the user data distribution. As a result, we reveal that the mechanism can achieve $O((\\ln^6N\/\\epsilon^2N)^{1\/2\\alpha})$ error without knowledge of $\\alpha$ and the error rate is near-optimal in the sense that any mechanism incurs $\\Omega((1\/\\epsilon^2N)^{1\/2\\alpha})$ error. Furthermore, we demonstrate that our mechanism outperforms a naive mechanism by empirical evaluations on synthetic datasets. Also, we conducted experiments on the MovieLens dataset and a purchase history dataset and demonstrate that our algorithm achieves $\\tilde{O}((1\/N)^{1\/2\\alpha})$ error adaptively to $\\alpha$."},{"title":"Info-computational constructivism in modelling of life as cognition","source":"Gordana Dodig-Crnkovic","docs_id":"1401.4942","section":["cs.AI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Info-computational constructivism in modelling of life as cognition. This paper addresses the open question formulated as: Which levels of abstraction are appropriate in the synthetic modelling of life and cognition? within the framework of info-computational constructivism, treating natural phenomena as computational processes on informational structures. At present we lack the common understanding of the processes of life and cognition in living organisms with the details of co-construction of informational structures and computational processes in embodied, embedded cognizing agents, both living and artifactual ones. Starting with the definition of an agent as an entity capable of acting on its own behalf, as an actor in Hewitt Actor model of computation, even so simple systems as molecules can be modelled as actors exchanging messages (information). We adopt Kauffmans view of a living agent as something that can reproduce and undergoes at least one thermodynamic work cycle. This definition of living agents leads to the Maturana and Varelas identification of life with cognition. Within the info-computational constructive approach to living beings as cognizing agents, from the simplest to the most complex living systems, mechanisms of cognition can be studied in order to construct synthetic model classes of artifactual cognizing agents on different levels of organization."},{"title":"VR Hackathon with Goethe Institute: Lessons Learned from Organizing a\n  Transdisciplinary VR Hackathon","source":"Wies{\\l}aw Kope\\'c, Krzysztof Kalinowski, Monika Kornacka, Kinga\n  Skorupska, Julia Paluch, Anna Jaskulska, Grzegorz Pochwatko, Jakub Mo\\.zaryn,\n  Pawe{\\l} Kobyli\\'nski, Piotr Gago","docs_id":"2104.02100","section":["cs.HC","cs.CY","cs.MM"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"VR Hackathon with Goethe Institute: Lessons Learned from Organizing a\n  Transdisciplinary VR Hackathon. In this article we report a case study of a Language Learning Bauhaus VR hackathon with Goethe Institute. It was organized as an educational and research project to tap into the dynamics of transdisciplinary teams challenged with a specific requirement. In our case, it was to build a Bauhaus-themed German Language Learning VR App. We constructed this experiment to simulate how representatives of different disciplines may work together towards a very specific purpose under time pressure. So, each participating team consisted of members of various expert-fields: software development (Unity or Unreal), design, psychology and linguistics. The results of this study cast light on the recommended cycle of design thinking and customer-centered design in VR. Especially in interdisciplinary rapid prototyping conditions, where stakeholders initially do not share competences. They also showcase educational benefits of working in transdisciplinary environments. This study, combined with our previous work on human factors in rapid software development and co-design, including hackathon dynamics, allowed us to formulate recommendations for organizing content creation VR hackathons for specific purposes. We also provide guidelines on how to prepare the participants to work in rapid prototyping VR environments and benefit from such experiences in the long term."},{"title":"Benchmarking Deep Trackers on Aerial Videos","source":"Abu Md Niamul Taufique, Breton Minnehan, Andreas Savakis","docs_id":"2103.12924","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Benchmarking Deep Trackers on Aerial Videos. In recent years, deep learning-based visual object trackers have achieved state-of-the-art performance on several visual object tracking benchmarks. However, most tracking benchmarks are focused on ground level videos, whereas aerial tracking presents a new set of challenges. In this paper, we compare ten trackers based on deep learning techniques on four aerial datasets. We choose top performing trackers utilizing different approaches, specifically tracking by detection, discriminative correlation filters, Siamese networks and reinforcement learning. In our experiments, we use a subset of OTB2015 dataset with aerial style videos; the UAV123 dataset without synthetic sequences; the UAV20L dataset, which contains 20 long sequences; and DTB70 dataset as our benchmark datasets. We compare the advantages and disadvantages of different trackers in different tracking situations encountered in aerial data. Our findings indicate that the trackers perform significantly worse in aerial datasets compared to standard ground level videos. We attribute this effect to smaller target size, camera motion, significant camera rotation with respect to the target, out of view movement, and clutter in the form of occlusions or similar looking distractors near tracked object."},{"title":"Learning to Assist Agents by Observing Them","source":"Antti Keurulainen (1 and 3), Isak Westerlund (3), Samuel Kaski (1 and\n  2), and Alexander Ilin (1) ((1) Helsinki Institute for Information Technology\n  HIIT, Department of Computer Science, Aalto University, (2) Department of\n  Computer Science, University of Manchester, (3) Bitville Oy, Espoo, Finland)","docs_id":"2110.01311","section":["cs.AI","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Learning to Assist Agents by Observing Them. The ability of an AI agent to assist other agents, such as humans, is an important and challenging goal, which requires the assisting agent to reason about the behavior and infer the goals of the assisted agent. Training such an ability by using reinforcement learning usually requires large amounts of online training, which is difficult and costly. On the other hand, offline data about the behavior of the assisted agent might be available, but is non-trivial to take advantage of by methods such as offline reinforcement learning. We introduce methods where the capability to create a representation of the behavior is first pre-trained with offline data, after which only a small amount of interaction data is needed to learn an assisting policy. We test the setting in a gridworld where the helper agent has the capability to manipulate the environment of the assisted artificial agents, and introduce three different scenarios where the assistance considerably improves the performance of the assisted agents."},{"title":"Isogeometric approach for nonlinear bending and post-buckling analysis\n  of functionally graded plates under thermal environment","source":"Loc V. Tran, Phuc Phung-Van, Jaehong Lee, H. Nguyen-Xuan, M. Abdel\n  Wahab","docs_id":"1511.01380","section":["cs.CE"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Isogeometric approach for nonlinear bending and post-buckling analysis\n  of functionally graded plates under thermal environment. In this paper, equilibrium and stability equations of functionally graded material (FGM) plate under thermal environment are formulated based on isogeometric analysis (IGA) in combination with higher-order shear deformation theory (HSDT). The FGM plate is made by a mixture of two distinct components, for which material properties not only vary continuously through thickness according to a power-law distribution but also are assumed to be a function of temperature. Temperature field is assumed to be constant in any plane and uniform, linear and nonlinear through plate thickness, respectively. The governing equation is in nonlinear form based on von Karman assumption and thermal effect. A NURBS-based isogeometric finite element formulation is utilized to naturally fulfil the rigorous C1-continuity required by the present plate model. Influences of gradient indices, boundary conditions, temperature distributions, material properties, length-to-thickness ratios on the behaviour of FGM plate are discussed in details. Numerical results demonstrate excellent performance of the present approach."},{"title":"Non interactive simulation of correlated distributions is decidable","source":"Anindya De and Elchanan Mossel and Joe Neeman","docs_id":"1701.01485","section":["cs.CC","cs.IT","math.IT","math.PR"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Non interactive simulation of correlated distributions is decidable. A basic problem in information theory is the following: Let $\\mathbf{P} = (\\mathbf{X}, \\mathbf{Y})$ be an arbitrary distribution where the marginals $\\mathbf{X}$ and $\\mathbf{Y}$ are (potentially) correlated. Let Alice and Bob be two players where Alice gets samples $\\{x_i\\}_{i \\ge 1}$ and Bob gets samples $\\{y_i\\}_{i \\ge 1}$ and for all $i$, $(x_i, y_i) \\sim \\mathbf{P}$. What joint distributions $\\mathbf{Q}$ can be simulated by Alice and Bob without any interaction? Classical works in information theory by G{\\'a}cs-K{\\\"o}rner and Wyner answer this question when at least one of $\\mathbf{P}$ or $\\mathbf{Q}$ is the distribution on $\\{0,1\\} \\times \\{0,1\\}$ where each marginal is unbiased and identical. However, other than this special case, the answer to this question is understood in very few cases. Recently, Ghazi, Kamath and Sudan showed that this problem is decidable for $\\mathbf{Q}$ supported on $\\{0,1\\} \\times \\{0,1\\}$. We extend their result to $\\mathbf{Q}$ supported on any finite alphabet. We rely on recent results in Gaussian geometry (by the authors) as well as a new \\emph{smoothing argument} inspired by the method of \\emph{boosting} from learning theory and potential function arguments from complexity theory and additive combinatorics."},{"title":"Deep Multi-Agent Reinforcement Learning for Cost Efficient Distributed\n  Load Frequency Control","source":"Sergio Rozada, Dimitra Apostolopoulou, and Eduardo Alonso","docs_id":"2010.06293","section":["eess.SY","cs.SY"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Deep Multi-Agent Reinforcement Learning for Cost Efficient Distributed\n  Load Frequency Control. The rise of microgrid-based architectures is heavily modifying the energy control landscape in distribution systems making distributed control mechanisms necessary to ensure reliable power system operations. In this paper, we propose the use of Reinforcement Learning techniques to implement load frequency control without requiring a central authority. To this end, we approximate the optimal solution of the primary, secondary, and tertiary control with the use of the Multi- Agent Deep Deterministic Policy Gradient (MADDPG) algorithm. Generation units are characterised as agents that learn how to maximise their long-term performance by acting and interacting with the environment to balance generation and load in a cost efficient way. Network effects are also modelled in our framework for the restoration of frequency to the nominal value. We validate our Reinforcement Learning methodology through numerical results and show that it can be used to implement the load frequency control in a distributed and cost efficient way."},{"title":"Finding a Needle in a Haystack: Tiny Flying Object Detection in 4K\n  Videos using a Joint Detection-and-Tracking Approach","source":"Ryota Yoshihashi, Rei Kawakami, Shaodi You, Tu Tuan Trinh, Makoto\n  Iida, Takeshi Naemura","docs_id":"2105.08253","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Finding a Needle in a Haystack: Tiny Flying Object Detection in 4K\n  Videos using a Joint Detection-and-Tracking Approach. Detecting tiny objects in a high-resolution video is challenging because the visual information is little and unreliable. Specifically, the challenge includes very low resolution of the objects, MPEG artifacts due to compression and a large searching area with many hard negatives. Tracking is equally difficult because of the unreliable appearance, and the unreliable motion estimation. Luckily, we found that by combining this two challenging tasks together, there will be mutual benefits. Following the idea, in this paper, we present a neural network model called the Recurrent Correlational Network, where detection and tracking are jointly performed over a multi-frame representation learned through a single, trainable, and end-to-end network. The framework exploits a convolutional long short-term memory network for learning informative appearance changes for detection, while the learned representation is shared in tracking for enhancing its performance. In experiments with datasets containing images of scenes with small flying objects, such as birds and unmanned aerial vehicles, the proposed method yielded consistent improvements in detection performance over deep single-frame detectors and existing motion-based detectors. Furthermore, our network performs as well as state-of-the-art generic object trackers when it was evaluated as a tracker on a bird image dataset."},{"title":"The Communication of Meaning and the Structuration of Expectations:\n  Giddens' \"structuration theory\" and Luhmann's \"self-organization\"","source":"Loet Leydesdorff","docs_id":"0911.5565","section":["cs.CY","nlin.AO","physics.soc-ph"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"The Communication of Meaning and the Structuration of Expectations:\n  Giddens' \"structuration theory\" and Luhmann's \"self-organization\". The communication of meaning as different from (Shannon-type) information is central to Luhmann's social systems theory and Giddens' structuration theory of action. These theories share an emphasis on reflexivity, but focus on meaning along a divide between inter-human communication and intentful action as two different systems of reference. Recombining these two theories into a theory about the structuration of expectations, interactions, organization, and self-organization of intentional communications can be simulated based on algorithms from the computation of anticipatory systems. The self-organizing and organizing layers remain rooted in the double contingency of the human encounter which provides the variation. Organization and self-organization of communication are reflexive upon and therefore reconstructive of each other. Using mutual information in three dimensions, the imprint of meaning processing in the modeling system on the historical organization of uncertainty in the modeled system can be measured. This is shown empirically in the case of intellectual organization as \"structurating\" structure in the textual domain of scientific articles."},{"title":"R-SARL: Crowd-aware Navigation Based Deep Reinforcement Learning for\n  Nonholonomic Robot in Complex Environments","source":"Yanying Zhou, Shijie Li, Jochen Garcke","docs_id":"2105.13409","section":["cs.RO"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"R-SARL: Crowd-aware Navigation Based Deep Reinforcement Learning for\n  Nonholonomic Robot in Complex Environments. Robot navigation in a safe way for complex and crowded situations is studied in this work. When facing complex environments with both static and dynamic obstacles, in existing works unicycle nonholonomic robots are prone to two extreme behaviors, one is to fall into dead ends formed by obstacles, and the other is to not complete the navigation task in time due to excessive collision avoidance.As a result, we propose the R-SARL framework, which is based on a deep reinforcement learning algorithm and where we augment the reward function to avoid collisions. In particular, we estimate unsafe interactions between the robot and obstacles in a look-ahead distance and penalize accordingly, so that the robot can avoid collisions in advance and reach its destination safely.Furthermore, we penalize frequent excessive detours to reduce the timeout and thus improve the efficiency of navigation.We test our method in various challenging and complex crowd navigation tasks. The results show that our method improves navigation performance and outperforms state-of-the-art methods."},{"title":"Towards Axiomatic Explanations for Neural Ranking Models","source":"Michael V\\\"olske, Alexander Bondarenko, Maik Fr\\\"obe, Matthias Hagen,\n  Benno Stein, Jaspreet Singh, Avishek Anand","docs_id":"2106.08019","section":["cs.IR"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Towards Axiomatic Explanations for Neural Ranking Models. Recently, neural networks have been successfully employed to improve upon state-of-the-art performance in ad-hoc retrieval tasks via machine-learned ranking functions. While neural retrieval models grow in complexity and impact, little is understood about their correspondence with well-studied IR principles. Recent work on interpretability in machine learning has provided tools and techniques to understand neural models in general, yet there has been little progress towards explaining ranking models. We investigate whether one can explain the behavior of neural ranking models in terms of their congruence with well understood principles of document ranking by using established theories from axiomatic IR. Axiomatic analysis of information retrieval models has formalized a set of constraints on ranking decisions that reasonable retrieval models should fulfill. We operationalize this axiomatic thinking to reproduce rankings based on combinations of elementary constraints. This allows us to investigate to what extent the ranking decisions of neural rankers can be explained in terms of retrieval axioms, and which axioms apply in which situations. Our experimental study considers a comprehensive set of axioms over several representative neural rankers. While the existing axioms can already explain the particularly confident ranking decisions rather well, future work should extend the axiom set to also cover the other still \"unexplainable\" neural IR rank decisions."},{"title":"Kernelization Using Structural Parameters on Sparse Graph Classes","source":"Jakub Gajarsk\\'y, Petr Hlin\\v{e}n\\'y, Jan Obdr\\v{z}\\'alek, Sebastian\n  Ordyniak, Felix Reidl, Peter Rossmanith, Fernando S\\'anchez Villaamil,\n  Somnath Sikdar","docs_id":"1302.6863","section":["cs.DS"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Kernelization Using Structural Parameters on Sparse Graph Classes. Meta-theorems for polynomial (linear) kernels have been the subject of intensive research in parameterized complexity. Heretofore, meta-theorems for linear kernels exist on graphs of bounded genus, $H$-minor-free graphs, and $H$-topological-minor-free graphs. To the best of our knowledge, no meta-theorems for polynomial kernels are known for any larger sparse graph classes; e.g., for classes of bounded expansion or for nowhere dense ones. In this paper we prove such meta-theorems for the two latter cases. More specifically, we show that graph problems that have finite integer index (FII) have linear kernels on graphs of bounded expansion when parameterized by the size of a modulator to constant-treedepth graphs. For nowhere dense graph classes, our result yields almost-linear kernels. While our parameter may seem rather strong, we argue that a linear kernelization result on graphs of bounded expansion with a weaker parameter (than treedepth modulator) would fail to include some of the problems covered by our framework. Moreover, we only require the problems to have FII on graphs of constant treedepth. This allows us to prove linear kernels for problems such as Longest Path\/Cycle, Exact $s,t$-Path, Treewidth, and Pathwidth, which do not have FII on general graphs (and the first two not even on bounded treewidth graphs)."},{"title":"Cross-Cultural and Cultural-Specific Production and Perception of Facial\n  Expressions of Emotion in the Wild","source":"Ramprakash Srinivasan, Aleix M. Martinez","docs_id":"1808.04399","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Cross-Cultural and Cultural-Specific Production and Perception of Facial\n  Expressions of Emotion in the Wild. Automatic recognition of emotion from facial expressions is an intense area of research, with a potentially long list of important application. Yet, the study of emotion requires knowing which facial expressions are used within and across cultures in the wild, not in controlled lab conditions; but such studies do not exist. Which and how many cross-cultural and cultural-specific facial expressions do people commonly use? And, what affect variables does each expression communicate to observers? If we are to design technology that understands the emotion of users, we need answers to these two fundamental questions. In this paper, we present the first large-scale study of the production and visual perception of facial expressions of emotion in the wild. We find that of the 16,384 possible facial configurations that people can theoretically produce, only 35 are successfully used to transmit emotive information across cultures, and only 8 within a smaller number of cultures. Crucially, we find that visual analysis of cross-cultural expressions yields consistent perception of emotion categories and valence, but not arousal. In contrast, visual analysis of cultural-specific expressions yields consistent perception of valence and arousal, but not of emotion categories. Additionally, we find that the number of expressions used to communicate each emotion is also different, e.g., 17 expressions transmit happiness, but only 1 is used to convey disgust."},{"title":"Strategic COVID-19 vaccine distribution can simultaneously elevate\n  social utility and equity","source":"Lin Chen, Fengli Xu, Zhenyu Han, Kun Tang, Pan Hui, James Evans, Yong\n  Li","docs_id":"2111.06689","section":["cs.CY"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Strategic COVID-19 vaccine distribution can simultaneously elevate\n  social utility and equity. Balancing social utility and equity in distributing limited vaccines represents a critical policy concern for protecting against the prolonged COVID-19 pandemic. What is the nature of the trade-off between maximizing collective welfare and minimizing disparities between more and less privileged communities? To evaluate vaccination strategies, we propose a novel epidemic model that explicitly accounts for both demographic and mobility differences among communities and their association with heterogeneous COVID-19 risks, then calibrate it with large-scale data. Using this model, we find that social utility and equity can be simultaneously improved when vaccine access is prioritized for the most disadvantaged communities, which holds even when such communities manifest considerable vaccine reluctance. Nevertheless, equity among distinct demographic features are in tension due to their complex correlation in society. We design two behavior-and-demography-aware indices, community risk and societal harm, which capture the risks communities face and those they impose on society from not being vaccinated, to inform the design of comprehensive vaccine distribution strategies. Our study provides a framework for uniting utility and equity-based considerations in vaccine distribution, and sheds light on how to balance multiple ethical values in complex settings for epidemic control."},{"title":"An Analytical Model for CBAP Allocations in IEEE 802.11ad","source":"Chiara Pielli, Tanguy Ropitault, Nada Golmie, Michele Zorzi","docs_id":"1906.07097","section":["cs.NI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"An Analytical Model for CBAP Allocations in IEEE 802.11ad. The IEEE 802.11ad standard extends WiFi operation to the millimeter wave frequencies, and introduces novel features concerning both the physical (PHY) and Medium Access Control (MAC) layers. However, while there are extensive research efforts to develop mechanisms for establishing and maintaining directional links for mmWave communications, fewer works deal with transmission scheduling and the hybrid MAC introduced by the standard. The hybrid MAC layer provides for two different kinds of resource allocations: Contention Based Access Periods (CBAPs) and contention free Service Periods (SPs). In this paper, we propose a Markov Chain model to represent CBAPs, which takes into account operation interruptions due to scheduled SPs and the deafness and hidden node problems that directional communication exacerbates. We also propose a mathematical analysis to assess interference among stations. We derive analytical expressions to assess the impact of various transmission parameters and of the Data Transmission Interval configuration on some key performance metrics such as throughput, delay and packet dropping rate. This information may be used to efficiently design a transmission scheduler that allocates contention-based and contention-free periods based on the application requirements."},{"title":"Software Defined Networks based Smart Grid Communication: A\n  Comprehensive Survey","source":"Mubashir Husain Rehmani, Alan Davy, Brendan Jennings, and Chadi Assi","docs_id":"1801.04613","section":["cs.NI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Software Defined Networks based Smart Grid Communication: A\n  Comprehensive Survey. The current power grid is no longer a feasible solution due to ever-increasing user demand of electricity, old infrastructure, and reliability issues and thus require transformation to a better grid a.k.a., smart grid (SG). The key features that distinguish SG from the conventional electrical power grid are its capability to perform two-way communication, demand side management, and real time pricing. Despite all these advantages that SG will bring, there are certain issues which are specific to SG communication system. For instance, network management of current SG systems is complex, time consuming, and done manually. Moreover, SG communication (SGC) system is built on different vendor specific devices and protocols. Therefore, the current SG systems are not protocol independent, thus leading to interoperability issue. Software defined network (SDN) has been proposed to monitor and manage the communication networks globally. This article serves as a comprehensive survey on SDN-based SGC. In this article, we first discuss taxonomy of advantages of SDNbased SGC.We then discuss SDN-based SGC architectures, along with case studies. Our article provides an in-depth discussion on routing schemes for SDN-based SGC. We also provide detailed survey of security and privacy schemes applied to SDN-based SGC. We furthermore present challenges, open issues, and future research directions related to SDN-based SGC."},{"title":"Driver Hand Localization and Grasp Analysis: A Vision-based Real-time\n  Approach","source":"Siddharth, Akshay Rangesh, Eshed Ohn-Bar, and Mohan M. Trivedi","docs_id":"1802.07854","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Driver Hand Localization and Grasp Analysis: A Vision-based Real-time\n  Approach. Extracting hand regions and their grasp information from images robustly in real-time is critical for occupants' safety and in-vehicular infotainment applications. It must however, be noted that naturalistic driving scenes suffer from rapidly changing illumination and occlusion. This is aggravated by the fact that hands are highly deformable objects, and change in appearance frequently. This work addresses the task of accurately localizing driver hands and classifying the grasp state of each hand. We use a fast ConvNet to first detect likely hand regions. Next, a pixel-based skin classifier that takes into account the global illumination changes is used to refine the hand detections and remove false positives. This step generates a pixel-level mask for each hand. Finally, we study each such masked regions and detect if the driver is grasping the wheel, or in some cases a mobile phone. Through evaluation we demonstrate that our method can outperform state-of-the-art pixel based hand detectors, while running faster (at 35 fps) than other deep ConvNet based frameworks even for grasp analysis. Hand mask cues are shown to be crucial when analyzing a set of driver hand gestures (wheel\/mobile phone grasp and no-grasp) in naturalistic driving settings. The proposed detection and localization pipeline hence can act as a general framework for real-time hand detection and gesture classification."},{"title":"Extend the shallow part of Single Shot MultiBox Detector via\n  Convolutional Neural Network","source":"Liwen Zheng, Canmiao Fu, Yong Zhao","docs_id":"1801.05918","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Extend the shallow part of Single Shot MultiBox Detector via\n  Convolutional Neural Network. Single Shot MultiBox Detector (SSD) is one of the fastest algorithms in the current object detection field, which uses fully convolutional neural network to detect all scaled objects in an image. Deconvolutional Single Shot Detector (DSSD) is an approach which introduces more context information by adding the deconvolution module to SSD. And the mean Average Precision (mAP) of DSSD on PASCAL VOC2007 is improved from SSD's 77.5% to 78.6%. Although DSSD obtains higher mAP than SSD by 1.1%, the frames per second (FPS) decreases from 46 to 11.8. In this paper, we propose a single stage end-to-end image detection model called ESSD to overcome this dilemma. Our solution to this problem is to cleverly extend better context information for the shallow layers of the best single stage (e.g. SSD) detectors. Experimental results show that our model can reach 79.4% mAP, which is higher than DSSD and SSD by 0.8 and 1.9 points respectively. Meanwhile, our testing speed is 25 FPS in Titan X GPU which is more than double the original DSSD."},{"title":"Random Matching under Priorities: Stability and No Envy Concepts","source":"Haris Aziz and Bettina Klaus","docs_id":"1707.01231","section":["cs.GT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Random Matching under Priorities: Stability and No Envy Concepts. We consider stability concepts for random matchings where agents have preferences over objects and objects have priorities for the agents. When matchings are deterministic, the standard stability concept also captures the fairness property of no (justified) envy. When matchings can be random, there are a number of natural stability \/ fairness concepts that coincide with stability \/ no envy whenever matchings are deterministic. We formalize known stability concepts for random matchings for a general setting that allows weak preferences and weak priorities, unacceptability, and an unequal number of agents and objects. We then present a clear taxonomy of the stability concepts and identify logical relations between them. Furthermore, we provide no envy \/ claims interpretations for some of the stability concepts that are based on a consumption process interpretation of random matchings. Finally, we present a transformation from the most general setting to the most restricted setting, and show how almost all our stability concepts are preserved by that transformation."},{"title":"Multi-Task Gaussian Processes and Dilated Convolutional Networks for\n  Reconstruction of Reproductive Hormonal Dynamics","source":"I\\~nigo Urteaga, Tristan Bertin, Theresa M. Hardy, David J. Albers,\n  No\\'emie Elhadad","docs_id":"1908.10226","section":["cs.LG","stat.AP","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Multi-Task Gaussian Processes and Dilated Convolutional Networks for\n  Reconstruction of Reproductive Hormonal Dynamics. We present an end-to-end statistical framework for personalized, accurate, and minimally invasive modeling of female reproductive hormonal patterns. Reconstructing and forecasting the evolution of hormonal dynamics is a challenging task, but a critical one to improve general understanding of the menstrual cycle and personalized detection of potential health issues. Our goal is to infer and forecast individual hormone daily levels over time, while accommodating pragmatic and minimally invasive measurement settings. To that end, our approach combines the power of probabilistic generative models (i.e., multi-task Gaussian processes) with the flexibility of neural networks (i.e., a dilated convolutional architecture) to learn complex temporal mappings. To attain accurate hormone level reconstruction with as little data as possible, we propose a sampling mechanism for optimal reconstruction accuracy with limited sampling budget. Our results show the validity of our proposed hormonal dynamic modeling framework, as it provides accurate predictive performance across different realistic sampling budgets and outperforms baselines methods."},{"title":"Reweighted Proximal Pruning for Large-Scale Language Representation","source":"Fu-Ming Guo, Sijia Liu, Finlay S. Mungall, Xue Lin and Yanzhi Wang","docs_id":"1909.12486","section":["cs.LG","cs.CL","cs.NE","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Reweighted Proximal Pruning for Large-Scale Language Representation. Recently, pre-trained language representation flourishes as the mainstay of the natural language understanding community, e.g., BERT. These pre-trained language representations can create state-of-the-art results on a wide range of downstream tasks. Along with continuous significant performance improvement, the size and complexity of these pre-trained neural models continue to increase rapidly. Is it possible to compress these large-scale language representation models? How will the pruned language representation affect the downstream multi-task transfer learning objectives? In this paper, we propose Reweighted Proximal Pruning (RPP), a new pruning method specifically designed for a large-scale language representation model. Through experiments on SQuAD and the GLUE benchmark suite, we show that proximal pruned BERT keeps high accuracy for both the pre-training task and the downstream multiple fine-tuning tasks at high prune ratio. RPP provides a new perspective to help us analyze what large-scale language representation might learn. Additionally, RPP makes it possible to deploy a large state-of-the-art language representation model such as BERT on a series of distinct devices (e.g., online servers, mobile phones, and edge devices)."},{"title":"Comparison of Global Algorithms in Word Sense Disambiguation","source":"Lo\\\"ic Vial and Andon Tchechmedjiev and Didier Schwab","docs_id":"1704.02293","section":["cs.CL"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Comparison of Global Algorithms in Word Sense Disambiguation. This article compares four probabilistic algorithms (global algorithms) for Word Sense Disambiguation (WSD) in terms of the number of scorer calls (local algo- rithm) and the F1 score as determined by a gold-standard scorer. Two algorithms come from the state of the art, a Simulated Annealing Algorithm (SAA) and a Genetic Algorithm (GA) as well as two algorithms that we first adapt from WSD that are state of the art probabilistic search algorithms, namely a Cuckoo search algorithm (CSA) and a Bat Search algorithm (BS). As WSD requires to evaluate exponentially many word sense combinations (with branching factors of up to 6 or more), probabilistic algorithms allow to find approximate solution in a tractable time by sampling the search space. We find that CSA, GA and SA all eventually converge to similar results (0.98 F1 score), but CSA gets there faster (in fewer scorer calls) and reaches up to 0.95 F1 before SA in fewer scorer calls. In BA a strict convergence criterion prevents it from reaching above 0.89 F1."},{"title":"Good Graph to Optimize: Cost-Effective, Budget-Aware Bundle Adjustment\n  in Visual SLAM","source":"Yipu Zhao, Justin S. Smith, Patricio A. Vela","docs_id":"2008.10123","section":["cs.CV","cs.RO"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Good Graph to Optimize: Cost-Effective, Budget-Aware Bundle Adjustment\n  in Visual SLAM. The cost-efficiency of visual(-inertial) SLAM (VSLAM) is a critical characteristic of resource-limited applications. While hardware and algorithm advances have been significantly improved the cost-efficiency of VSLAM front-ends, the cost-efficiency of VSLAM back-ends remains a bottleneck. This paper describes a novel, rigorous method to improve the cost-efficiency of local BA in a BA-based VSLAM back-end. An efficient algorithm, called Good Graph, is developed to select size-reduced graphs optimized in local BA with condition preservation. To better suit BA-based VSLAM back-ends, the Good Graph predicts future estimation needs, dynamically assigns an appropriate size budget, and selects a condition-maximized subgraph for BA estimation. Evaluations are conducted on two scenarios: 1) VSLAM as standalone process, and 2) VSLAM as part of closed-loop navigation system. Results from the first scenario show Good Graph improves accuracy and robustness of VSLAM estimation, when computational limits exist. Results from the second scenario, indicate that Good Graph benefits the trajectory tracking performance of VSLAM-based closed-loop navigation systems, which is a primary application of VSLAM."},{"title":"Deep Supervised Discrete Hashing","source":"Qi Li, Zhenan Sun, Ran He, Tieniu Tan","docs_id":"1705.10999","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Deep Supervised Discrete Hashing. With the rapid growth of image and video data on the web, hashing has been extensively studied for image or video search in recent years. Benefit from recent advances in deep learning, deep hashing methods have achieved promising results for image retrieval. However, there are some limitations of previous deep hashing methods (e.g., the semantic information is not fully exploited). In this paper, we develop a deep supervised discrete hashing algorithm based on the assumption that the learned binary codes should be ideal for classification. Both the pairwise label information and the classification information are used to learn the hash codes within one stream framework. We constrain the outputs of the last layer to be binary codes directly, which is rarely investigated in deep hashing algorithm. Because of the discrete nature of hash codes, an alternating minimization method is used to optimize the objective function. Experimental results have shown that our method outperforms current state-of-the-art methods on benchmark datasets."},{"title":"Fast kNN mode seeking clustering applied to active learning","source":"Robert P.W. Duin and Sergey Verzakov","docs_id":"1712.07454","section":["stat.ML","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Fast kNN mode seeking clustering applied to active learning. A significantly faster algorithm is presented for the original kNN mode seeking procedure. It has the advantages over the well-known mean shift algorithm that it is feasible in high-dimensional vector spaces and results in uniquely, well defined modes. Moreover, without any additional computational effort it may yield a multi-scale hierarchy of clusterings. The time complexity is just O(n^1.5). resulting computing times range from seconds for 10^4 objects to minutes for 10^5 objects and to less than an hour for 10^6 objects. The space complexity is just O(n). The procedure is well suited for finding large sets of small clusters and is thereby a candidate to analyze thousands of clusters in millions of objects. The kNN mode seeking procedure can be used for active learning by assigning the clusters to the class of the modal objects of the clusters. Its feasibility is shown by some examples with up to 1.5 million handwritten digits. The obtained classification results based on the clusterings are compared with those obtained by the nearest neighbor rule and the support vector classifier based on the same labeled objects for training. It can be concluded that using the clustering structure for classification can be significantly better than using the trained classifiers. A drawback of using the clustering for classification, however, is that no classifier is obtained that may be used for out-of-sample objects."},{"title":"Video Object Segmentation using Space-Time Memory Networks","source":"Seoung Wug Oh, Joon-Young Lee, Ning Xu, Seon Joo Kim","docs_id":"1904.00607","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Video Object Segmentation using Space-Time Memory Networks. We propose a novel solution for semi-supervised video object segmentation. By the nature of the problem, available cues (e.g. video frame(s) with object masks) become richer with the intermediate predictions. However, the existing methods are unable to fully exploit this rich source of information. We resolve the issue by leveraging memory networks and learn to read relevant information from all available sources. In our framework, the past frames with object masks form an external memory, and the current frame as the query is segmented using the mask information in the memory. Specifically, the query and the memory are densely matched in the feature space, covering all the space-time pixel locations in a feed-forward fashion. Contrast to the previous approaches, the abundant use of the guidance information allows us to better handle the challenges such as appearance changes and occlussions. We validate our method on the latest benchmark sets and achieved the state-of-the-art performance (overall score of 79.4 on Youtube-VOS val set, J of 88.7 and 79.2 on DAVIS 2016\/2017 val set respectively) while having a fast runtime (0.16 second\/frame on DAVIS 2016 val set)."},{"title":"Integral Cryptanalysis of the Block Cipher E2","source":"Wentan Yi and Shaozhen Chen","docs_id":"1405.6483","section":["cs.CR"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Integral Cryptanalysis of the Block Cipher E2. Block cipher E2, designed and submitted by Nippon Telegraph and Telephone Corporation, is a first-round Advanced Encryption Standard candidate. It employs a Feistel structure as global structure and two-layer substitution-permutation network structure in round function with initial transformation IT function before the first round and final transformation FT function after the last round. The design principles influences several more recent block ciphers including Camellia, an ISO\/IEC standard cipher. In this paper, we focus on the key-recovery attacks on reduced-round E2-128\/192 taking both IT and FT functions in consideration with integral cryptanalysis. We first improve the relations between zero-correlation linear approximations and integral distinguishers, and then deduce some integral distinguishers from zero-correlation linear approximations over 6 rounds of E2. Furthermore, we apply these integral distinguishers to break 6-round E2-128 with 2^{120} known plaintexts (KPs), 2^{115.4} encryptions and 2^{28} bytes memory. In addition, the attack on 7-round E2-192 requires 2^{120} KPs, 2^{167.2} encryptions and 2^{60} bytes memory."},{"title":"Conditional t-SNE: Complementary t-SNE embeddings through factoring out\n  prior information","source":"Bo Kang, Dar\\'io Garc\\'ia Garc\\'ia, Jefrey Lijffijt, Ra\\'ul\n  Santos-Rodr\\'iguez, Tijl De Bie","docs_id":"1905.10086","section":["cs.LG","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Conditional t-SNE: Complementary t-SNE embeddings through factoring out\n  prior information. Dimensionality reduction and manifold learning methods such as t-Distributed Stochastic Neighbor Embedding (t-SNE) are routinely used to map high-dimensional data into a 2-dimensional space to visualize and explore the data. However, two dimensions are typically insufficient to capture all structure in the data, the salient structure is often already known, and it is not obvious how to extract the remaining information in a similarly effective manner. To fill this gap, we introduce \\emph{conditional t-SNE} (ct-SNE), a generalization of t-SNE that discounts prior information from the embedding in the form of labels. To achieve this, we propose a conditioned version of the t-SNE objective, obtaining a single, integrated, and elegant method. ct-SNE has one extra parameter over t-SNE; we investigate its effects and show how to efficiently optimize the objective. Factoring out prior knowledge allows complementary structure to be captured in the embedding, providing new insights. Qualitative and quantitative empirical results on synthetic and (large) real data show ct-SNE is effective and achieves its goal."},{"title":"Continuous and Discrete-Time Survival Prediction with Neural Networks","source":"H{\\aa}vard Kvamme and {\\O}rnulf Borgan","docs_id":"1910.06724","section":["stat.ML","cs.LG","stat.ME"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Continuous and Discrete-Time Survival Prediction with Neural Networks. Application of discrete-time survival methods for continuous-time survival prediction is considered. For this purpose, a scheme for discretization of continuous-time data is proposed by considering the quantiles of the estimated event-time distribution, and, for smaller data sets, it is found to be preferable over the commonly used equidistant scheme. Furthermore, two interpolation schemes for continuous-time survival estimates are explored, both of which are shown to yield improved performance compared to the discrete-time estimates. The survival methods considered are based on the likelihood for right-censored survival data, and parameterize either the probability mass function (PMF) or the discrete-time hazard rate, both with neural networks. Through simulations and study of real-world data, the hazard rate parametrization is found to perform slightly better than the parametrization of the PMF. Inspired by these investigations, a continuous-time method is proposed by assuming that the continuous-time hazard rate is piecewise constant. The method, named PC-Hazard, is found to be highly competitive with the aforementioned methods in addition to other methods for survival prediction found in the literature."},{"title":"Efficient and Flexible Crowdsourcing of Specialized Tasks with\n  Precedence Constraints","source":"Avhishek Chatterjee, Michael Borokhovich, Lav R. Varshney, Sriram\n  Vishwanath","docs_id":"1601.04094","section":["cs.MA"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Efficient and Flexible Crowdsourcing of Specialized Tasks with\n  Precedence Constraints. Many companies now use crowdsourcing to leverage external (as well as internal) crowds to perform specialized work, and so methods of improving efficiency are critical. Tasks in crowdsourcing systems with specialized work have multiple steps and each step requires multiple skills. Steps may have different flexibilities in terms of obtaining service from one or multiple agents, due to varying levels of dependency among parts of steps. Steps of a task may have precedence constraints among them. Moreover, there are variations in loads of different types of tasks requiring different skill-sets and availabilities of different types of agents with different skill-sets. Considering these constraints together necessitates the design of novel schemes to allocate steps to agents. In addition, large crowdsourcing systems require allocation schemes that are simple, fast, decentralized and offer customers (task requesters) the freedom to choose agents. In this work we study the performance limits of such crowdsourcing systems and propose efficient allocation schemes that provably meet the performance limits under these additional requirements. We demonstrate our algorithms on data from a crowdsourcing platform run by a non-profit company and show significant improvements over current practice."},{"title":"Deep Networks Can Resemble Human Feed-forward Vision in Invariant Object\n  Recognition","source":"Saeed Reza Kheradpisheh, Masoud Ghodrati, Mohammad Ganjtabesh,\n  Timoth\\'ee Masquelier","docs_id":"1508.03929","section":["cs.CV","q-bio.NC"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Deep Networks Can Resemble Human Feed-forward Vision in Invariant Object\n  Recognition. Deep convolutional neural networks (DCNNs) have attracted much attention recently, and have shown to be able to recognize thousands of object categories in natural image databases. Their architecture is somewhat similar to that of the human visual system: both use restricted receptive fields, and a hierarchy of layers which progressively extract more and more abstracted features. Yet it is unknown whether DCNNs match human performance at the task of view-invariant object recognition, whether they make similar errors and use similar representations for this task, and whether the answers depend on the magnitude of the viewpoint variations. To investigate these issues, we benchmarked eight state-of-the-art DCNNs, the HMAX model, and a baseline shallow model and compared their results to those of humans with backward masking. Unlike in all previous DCNN studies, we carefully controlled the magnitude of the viewpoint variations to demonstrate that shallow nets can outperform deep nets and humans when variations are weak. When facing larger variations, however, more layers were needed to match human performance and error distributions, and to have representations that are consistent with human behavior. A very deep net with 18 layers even outperformed humans at the highest variation level, using the most human-like representations."},{"title":"Twitter for Sparking a Movement, Reddit for Sharing the Moment: #metoo\n  through the Lens of Social Media","source":"Lydia Manikonda, Ghazaleh Beigi, Huan Liu, and Subbarao Kambhampati","docs_id":"1803.08022","section":["cs.SI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Twitter for Sparking a Movement, Reddit for Sharing the Moment: #metoo\n  through the Lens of Social Media. Social media platforms are revolutionizing the way users communicate by increasing the exposure to highly stigmatized issues in the society. Sexual abuse is one such issue that recently took over social media via attaching the hashtag #metoo to the shared posts. Individuals with different backgrounds and ethnicities began sharing their unfortunate personal experiences of being assaulted. Through comparative analysis of the tweets via #meToo on Twitter versus the posts shared on the #meToo subreddit, this paper makes an initial attempt to assess public reactions and emotions. Though nearly equal ratios of negative and positive posts are shared on both platforms, Reddit posts are focused on the sexual assaults within families and workplaces while Twitter posts are on showing empathy and encouraging others to continue the #metoo movement. The data collected in this research and preliminary analysis demonstrate that users use various ways to share their experience, exchange ideas and encourage each other, and social media is suitable for groundswells such as #metoo movement."},{"title":"Implications of Ocular Pathologies for Iris Recognition Reliability","source":"Mateusz Trokielewicz and Adam Czajka and Piotr Maciejewicz","docs_id":"1809.00168","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Implications of Ocular Pathologies for Iris Recognition Reliability. This paper presents an analysis of how iris recognition is influenced by eye disease and an appropriate dataset comprising 2996 images of irises taken from 230 distinct eyes (including 184 affected by more than 20 different eye conditions). The images were collected in near infrared and visible light during routine ophthalmological examination. The experimental study carried out utilizing four independent iris recognition algorithms (MIRLIN, VeriEye, OSIRIS and IriCore) renders four valuable results. First, the enrollment process is highly sensitive to those eye conditions that obstruct the iris or cause geometrical distortions. Second, even those conditions that do not produce visible changes to the structure of the iris may increase the dissimilarity between samples of the same eyes. Third, eye conditions affecting the geometry or the tissue structure of the iris or otherwise producing obstructions significantly decrease same-eye similarity and have a lower, yet still statistically significant, influence on impostor comparison scores. Fourth, for unhealthy eyes, the most prominent effect of disease on iris recognition is to cause segmentation errors. To our knowledge this paper describes the largest database of iris images for disease-affected eyes made publicly available to researchers and offers the most comprehensive study of what we can expect when iris recognition is employed for diseased eyes."},{"title":"Network Representation Learning: From Preprocessing, Feature Extraction\n  to Node Embedding","source":"Jingya Zhou, Ling Liu, Wenqi Wei, Jianxi Fan","docs_id":"2110.07582","section":["cs.SI","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Network Representation Learning: From Preprocessing, Feature Extraction\n  to Node Embedding. Network representation learning (NRL) advances the conventional graph mining of social networks, knowledge graphs, and complex biomedical and physics information networks. Over dozens of network representation learning algorithms have been reported in the literature. Most of them focus on learning node embeddings for homogeneous networks, but they differ in the specific encoding schemes and specific types of node semantics captured and used for learning node embedding. This survey paper reviews the design principles and the different node embedding techniques for network representation learning over homogeneous networks. To facilitate the comparison of different node embedding algorithms, we introduce a unified reference framework to divide and generalize the node embedding learning process on a given network into preprocessing steps, node feature extraction steps and node embedding model training for a NRL task such as link prediction and node clustering. With this unifying reference framework, we highlight the representative methods, models, and techniques used at different stages of the node embedding model learning process. This survey not only helps researchers and practitioners to gain an in-depth understanding of different network representation learning techniques but also provides practical guidelines for designing and developing the next generation of network representation learning algorithms and systems."},{"title":"Approximate Frank-Wolfe Algorithms over Graph-structured Support Sets","source":"Baojian Zhou, Yifan Sun","docs_id":"2107.00472","section":["math.OC","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Approximate Frank-Wolfe Algorithms over Graph-structured Support Sets. In this paper, we propose approximate Frank-Wolfe (FW) algorithms to solve convex optimization problems over graph-structured support sets where the \\textit{linear minimization oracle} (LMO) cannot be efficiently obtained in general. We first demonstrate that two popular approximation assumptions (\\textit{additive} and \\textit{multiplicative gap errors)}, are not valid for our problem, in that no cheap gap-approximate LMO oracle exists in general. Instead, a new \\textit{approximate dual maximization oracle} (DMO) is proposed, which approximates the inner product rather than the gap. When the objective is $L$-smooth, we prove that the standard FW method using a $\\delta$-approximate DMO converges as $\\mathcal{O}(L \/ \\delta t + (1-\\delta)(\\delta^{-1} + \\delta^{-2}))$ in general, and as $\\mathcal{O}(L\/(\\delta^2(t+2)))$ over a $\\delta$-relaxation of the constraint set. Additionally, when the objective is $\\mu$-strongly convex and the solution is unique, a variant of FW converges to $\\mathcal{O}(L^2\\log(t)\/(\\mu \\delta^6 t^2))$ with the same per-iteration complexity. Our empirical results suggest that even these improved bounds are pessimistic, with significant improvement in recovering real-world images with graph-structured sparsity."},{"title":"SLAYER: Spike Layer Error Reassignment in Time","source":"Sumit Bam Shrestha and Garrick Orchard","docs_id":"1810.08646","section":["cs.NE","cs.LG","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"SLAYER: Spike Layer Error Reassignment in Time. Configuring deep Spiking Neural Networks (SNNs) is an exciting research avenue for low power spike event based computation. However, the spike generation function is non-differentiable and therefore not directly compatible with the standard error backpropagation algorithm. In this paper, we introduce a new general backpropagation mechanism for learning synaptic weights and axonal delays which overcomes the problem of non-differentiability of the spike function and uses a temporal credit assignment policy for backpropagating error to preceding layers. We describe and release a GPU accelerated software implementation of our method which allows training both fully connected and convolutional neural network (CNN) architectures. Using our software, we compare our method against existing SNN based learning approaches and standard ANN to SNN conversion techniques and show that our method achieves state of the art performance for an SNN on the MNIST, NMNIST, DVS Gesture, and TIDIGITS datasets."},{"title":"PHD-GIFs: Personalized Highlight Detection for Automatic GIF Creation","source":"Ana Garc\\'ia del Molino and Michael Gygli","docs_id":"1804.06604","section":["cs.CV","cs.MM"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"PHD-GIFs: Personalized Highlight Detection for Automatic GIF Creation. Highlight detection models are typically trained to identify cues that make visual content appealing or interesting for the general public, with the objective of reducing a video to such moments. However, the \"interestingness\" of a video segment or image is subjective. Thus, such highlight models provide results of limited relevance for the individual user. On the other hand, training one model per user is inefficient and requires large amounts of personal information which is typically not available. To overcome these limitations, we present a global ranking model which conditions on each particular user's interests. Rather than training one model per user, our model is personalized via its inputs, which allows it to effectively adapt its predictions, given only a few user-specific examples. To train this model, we create a large-scale dataset of users and the GIFs they created, giving us an accurate indication of their interests. Our experiments show that using the user history substantially improves the prediction accuracy. On our test set of 850 videos, our model improves the recall by 8% with respect to generic highlight detectors. Furthermore, our method proves more precise than the user-agnostic baselines even with just one person-specific example."},{"title":"A New Asymptotic Analysis Technique for Diversity Receptions Over\n  Correlated Lognormal Fading Channels","source":"Bingcheng Zhu, Julian Cheng, Jun Yan, Jinyuan Wang, Lenan Wu, Yongjin\n  Wang","docs_id":"1707.08200","section":["cs.IT","math.IT","math.PR"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A New Asymptotic Analysis Technique for Diversity Receptions Over\n  Correlated Lognormal Fading Channels. Prior asymptotic performance analyses are based on the series expansion of the moment-generating function (MGF) or the probability density function (PDF) of channel coefficients. However, these techniques fail for lognormal fading channels because the Taylor series of the PDF of a lognormal random variable is zero at the origin and the MGF does not have an explicit form. Although lognormal fading model has been widely applied in wireless communications and free-space optical communications, few analytical tools are available to provide elegant performance expressions for correlated lognormal channels. In this work, we propose a novel framework to analyze the asymptotic outage probabilities of selection combining (SC), equal-gain combining (EGC) and maximum-ratio combining (MRC) over equally correlated lognormal fading channels. Based on these closed-form results, we reveal the followings: i) the outage probability of EGC or MRC becomes an infinitely small quantity compared to that of SC at large signal-to-noise ratio (SNR); ii) channel correlation can result in an infinite performance loss at large SNR. More importantly, the analyses reveal insights into the long-standing problem of performance analyses over correlated lognormal channels at high SNR, and circumvent the time-consuming Monte Carlo simulation and numerical integration."},{"title":"Development of swarm behavior in artificial learning agents that adapt\n  to different foraging environments","source":"Andrea L\\'opez-Incera, Katja Ried, Thomas M\\\"uller, Hans J. Briegel","docs_id":"2004.00552","section":["q-bio.PE","cs.LG","physics.soc-ph"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Development of swarm behavior in artificial learning agents that adapt\n  to different foraging environments. Collective behavior, and swarm formation in particular, has been studied from several perspectives within a large variety of fields, ranging from biology to physics. In this work, we apply Projective Simulation to model each individual as an artificial learning agent that interacts with its neighbors and surroundings in order to make decisions and learn from them. Within a reinforcement learning framework, we discuss one-dimensional learning scenarios where agents need to get to food resources to be rewarded. We observe how different types of collective motion emerge depending on the distance the agents need to travel to reach the resources. For instance, strongly aligned swarms emerge when the food source is placed far away from the region where agents are situated initially. In addition, we study the properties of the individual trajectories that occur within the different types of emergent collective dynamics. Agents trained to find distant resources exhibit individual trajectories with L\\'evy-like characteristics as a consequence of the collective motion, whereas agents trained to reach nearby resources present Brownian-like trajectories."},{"title":"On the Performance of Bytecode Interpreters in Prolog","source":"Philipp K\\\"orner, David Schneider, Michael Leuschel","docs_id":"2008.12543","section":["cs.PL"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"On the Performance of Bytecode Interpreters in Prolog. The semantics and the recursive execution model of Prolog make it very natural to express language interpreters in form of AST (Abstract Syntax Tree) interpreters where the execution follows the tree representation of a program. An alternative implementation technique is that of bytecode interpreters. These interpreters transform the program into a compact and linear representation before evaluating it and are generally considered to be faster and to make better use of resources. In this paper, we discuss different ways to express the control flow of interpreters in Prolog and present several implementations of AST and bytecode interpreters. On a simple language designed for this purpose, we evaluate whether techniques best known from imperative languages are applicable in Prolog and how well they perform. Our ultimate goal is to assess which interpreter design in Prolog is the most efficient, as we intend to apply these results to a more complex language. However, we believe the analysis in this paper to be of more general interest."},{"title":"Distributed Storage Allocations","source":"Derek Leong, Alexandros G. Dimakis, Tracey Ho","docs_id":"1011.5287","section":["cs.IT","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Distributed Storage Allocations. We examine the problem of allocating a given total storage budget in a distributed storage system for maximum reliability. A source has a single data object that is to be coded and stored over a set of storage nodes; it is allowed to store any amount of coded data in each node, as long as the total amount of storage used does not exceed the given budget. A data collector subsequently attempts to recover the original data object by accessing only the data stored in a random subset of the nodes. By using an appropriate code, successful recovery can be achieved whenever the total amount of data accessed is at least the size of the original data object. The goal is to find an optimal storage allocation that maximizes the probability of successful recovery. This optimization problem is challenging in general because of its combinatorial nature, despite its simple formulation. We study several variations of the problem, assuming different allocation models and access models. The optimal allocation and the optimal symmetric allocation (in which all nonempty nodes store the same amount of data) are determined for a variety of cases. Our results indicate that the optimal allocations often have nonintuitive structure and are difficult to specify. We also show that depending on the circumstances, coding may or may not be beneficial for reliable storage."},{"title":"Phase retrieval with background information","source":"Ziyang Yuan, Hongxia Wang","docs_id":"1802.01256","section":["cs.IT","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Phase retrieval with background information. Phase retrieval problem has been studied in various applications. It is an inverse problem without the standard uniqueness guarantee. To make complete theoretical analyses and devise efficient algorithms to recover the signal is sophisticated. In this paper, we come up with a model called \\textit{phase retrieval with background information} which recovers the signal with the known background information from the intensity of their combinational Fourier transform spectrum. We prove that the uniqueness of phase retrieval can be guaranteed even considering those trivial solutions when the background information is sufficient. Under this condition, we construct a loss function and utilize the projected gradient descent method to search for the ground truth. We prove that the stationary point is the global optimum with probability 1. Numerical simulations demonstrate the projected gradient descent method performs well both for 1-D and 2-D signals. Furthermore, this method is quite robust to the Gaussian noise and the bias of the background information."},{"title":"Pretzel: Email encryption and provider-supplied functions are compatible","source":"Trinabh Gupta, Henrique Fingler, Lorenzo Alvisi, Michael Walfish","docs_id":"1612.04265","section":["cs.CR"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Pretzel: Email encryption and provider-supplied functions are compatible. Emails today are often encrypted, but only between mail servers---the vast majority of emails are exposed in plaintext to the mail servers that handle them. While better than no encryption, this arrangement leaves open the possibility of attacks, privacy violations, and other disclosures. Publicly, email providers have stated that default end-to-end encryption would conflict with essential functions (spam filtering, etc.), because the latter requires analyzing email text. The goal of this paper is to demonstrate that there is no conflict. We do so by designing, implementing, and evaluating Pretzel. Starting from a cryptographic protocol that enables two parties to jointly perform a classification task without revealing their inputs to each other, Pretzel refines and adapts this protocol to the email context. Our experimental evaluation of a prototype demonstrates that email can be encrypted end-to-end \\emph{and} providers can compute over it, at tolerable cost: clients must devote some storage and processing, and provider overhead is roughly 5 times versus the status quo."},{"title":"An agent-based model of interdisciplinary interactions in science","source":"Juste Raimbault","docs_id":"2006.16399","section":["physics.soc-ph","cs.DL","cs.MA"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"An agent-based model of interdisciplinary interactions in science. An increased interdisciplinarity in science projects has been highlighted as crucial to tackle complex real-world challenges, but also as beneficial for the development of disciplines themselves. This paper introduces a parcimonious agent-based model of interdisciplinary relationships in collective entreprises of knowledge discovery, to investigate the impact of scientist-level decisions and preferences on global interdisciplinarity patterns. Under the assumption of simple rules for individual researcher project management, such as trade-offs between invested time overhead and knowledge benefit, model simulations show that individual choices influence the distribution of compromise points between emergent level of disciplinary depth and interdisciplinarity in a non-linear way. Different structures for collaboration networks may also yield various outcomes in terms of global interdisciplinarity. We conclude that independently of the research field, the organization of research, and more particularly the local balancing between vertical and horizontal research, already influences the final positioning of research results and the extent of the knowledge front. This suggests direct applications to research policies with a bottom-up leverage on the interactions between disciplines."},{"title":"The color out of space: learning self-supervised representations for\n  Earth Observation imagery","source":"Stefano Vincenzi, Angelo Porrello, Pietro Buzzega, Marco Cipriano,\n  Pietro Fronte, Roberto Cuccu, Carla Ippoliti, Annamaria Conte, Simone\n  Calderara","docs_id":"2006.12119","section":["cs.CV","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"The color out of space: learning self-supervised representations for\n  Earth Observation imagery. The recent growth in the number of satellite images fosters the development of effective deep-learning techniques for Remote Sensing (RS). However, their full potential is untapped due to the lack of large annotated datasets. Such a problem is usually countered by fine-tuning a feature extractor that is previously trained on the ImageNet dataset. Unfortunately, the domain of natural images differs from the RS one, which hinders the final performance. In this work, we propose to learn meaningful representations from satellite imagery, leveraging its high-dimensionality spectral bands to reconstruct the visible colors. We conduct experiments on land cover classification (BigEarthNet) and West Nile Virus detection, showing that colorization is a solid pretext task for training a feature extractor. Furthermore, we qualitatively observe that guesses based on natural images and colorization rely on different parts of the input. This paves the way to an ensemble model that eventually outperforms both the above-mentioned techniques."},{"title":"CRFace: Confidence Ranker for Model-Agnostic Face Detection Refinement","source":"Noranart Vesdapunt, Baoyuan Wang","docs_id":"2103.07017","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"CRFace: Confidence Ranker for Model-Agnostic Face Detection Refinement. Face detection is a fundamental problem for many downstream face applications, and there is a rising demand for faster, more accurate yet support for higher resolution face detectors. Recent smartphones can record a video in 8K resolution, but many of the existing face detectors still fail due to the anchor size and training data. We analyze the failure cases and observe a large number of correct predicted boxes with incorrect confidences. To calibrate these confidences, we propose a confidence ranking network with a pairwise ranking loss to re-rank the predicted confidences locally within the same image. Our confidence ranker is model-agnostic, so we can augment the data by choosing the pairs from multiple face detectors during the training, and generalize to a wide range of face detectors during the testing. On WiderFace, we achieve the highest AP on the single-scale, and our AP is competitive with the previous multi-scale methods while being significantly faster. On 8K resolution, our method solves the GPU memory issue and allows us to indirectly train on 8K. We collect 8K resolution test set to show the improvement, and we will release our test set as a new benchmark for future research."},{"title":"Reciprocal Learning Networks for Human Trajectory Prediction","source":"Hao Sun, Zhiqun Zhao and Zhihai He","docs_id":"2004.04340","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Reciprocal Learning Networks for Human Trajectory Prediction. We observe that the human trajectory is not only forward predictable, but also backward predictable. Both forward and backward trajectories follow the same social norms and obey the same physical constraints with the only difference in their time directions. Based on this unique property, we develop a new approach, called reciprocal learning, for human trajectory prediction. Two networks, forward and backward prediction networks, are tightly coupled, satisfying the reciprocal constraint, which allows them to be jointly learned. Based on this constraint, we borrow the concept of adversarial attacks of deep neural networks, which iteratively modifies the input of the network to match the given or forced network output, and develop a new method for network prediction, called reciprocal attack for matched prediction. It further improves the prediction accuracy. Our experimental results on benchmark datasets demonstrate that our new method outperforms the state-of-the-art methods for human trajectory prediction."},{"title":"Fundamental Limits of Blind Deconvolution Part I: Ambiguity Kernel","source":"Sunav Choudhary and Urbashi Mitra","docs_id":"1411.3810","section":["cs.IT","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Fundamental Limits of Blind Deconvolution Part I: Ambiguity Kernel. Blind deconvolution is an ubiquitous non-linear inverse problem in applications like wireless communications and image processing. This problem is generally ill-posed, and there have been efforts to use sparse models for regularizing blind deconvolution to promote signal identifiability. Part I of this two-part paper characterizes the ambiguity space of blind deconvolution and shows unidentifiability of this inverse problem for almost every pair of unconstrained input signals. The approach involves lifting the deconvolution problem to a rank one matrix recovery problem and analyzing the rank two null space of the resultant linear operator. A measure theoretically tight (parametric and recursive) representation of the key rank two null space is stated and proved. This representation is a novel foundational result for signal and code design strategies promoting identifiability under convolutive observation models. Part II of this paper analyzes the identifiability of sparsity constrained blind deconvolution and establishes surprisingly strong negative results on scaling laws for the sparsity-ambiguity trade-off."},{"title":"gSketch: On Query Estimation in Graph Streams","source":"Peixiang Zhao, Charu C. Aggarwal, Min Wang","docs_id":"1111.7167","section":["cs.DB"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"gSketch: On Query Estimation in Graph Streams. Many dynamic applications are built upon large network infrastructures, such as social networks, communication networks, biological networks and the Web. Such applications create data that can be naturally modeled as graph streams, in which edges of the underlying graph are received and updated sequentially in a form of a stream. It is often necessary and important to summarize the behavior of graph streams in order to enable effective query processing. However, the sheer size and dynamic nature of graph streams present an enormous challenge to existing graph management techniques. In this paper, we propose a new graph sketch method, gSketch, which combines well studied synopses for traditional data streams with a sketch partitioning technique, to estimate and optimize the responses to basic queries on graph streams. We consider two different scenarios for query estimation: (1) A graph stream sample is available; (2) Both a graph stream sample and a query workload sample are available. Algorithms for different scenarios are designed respectively by partitioning a global sketch to a group of localized sketches in order to optimize the query estimation accuracy. We perform extensive experimental studies on both real and synthetic data sets and demonstrate the power and robustness of gSketch in comparison with the state-of-the-art global sketch method."},{"title":"Iterative Model Predictive Control for Piecewise Systems","source":"Ugo Rosolia and Aaron D. Ames","docs_id":"2104.08267","section":["eess.SY","cs.SY"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Iterative Model Predictive Control for Piecewise Systems. In this paper, we present an iterative Model Predictive Control (MPC) design for piecewise nonlinear systems. We consider finite time control tasks where the goal of the controller is to steer the system from a starting configuration to a goal state while minimizing a cost function. First, we present an algorithm that leverages a feasible trajectory that completes the task to construct a control policy which guarantees that state and input constraints are recursively satisfied and that the closed-loop system reaches the goal state in finite time. Utilizing this construction, we present a policy iteration scheme that iteratively generates safe trajectories which have non-decreasing performance. Finally, we test the proposed strategy on a discretized Spring Loaded Inverted Pendulum (SLIP) model with massless legs. We show that our methodology is robust to changes in initial conditions and disturbances acting on the system. Furthermore, we demonstrate the effectiveness of our policy iteration algorithm in a minimum time control task."},{"title":"Cascaded Structure Tensor Framework for Robust Identification of Heavily\n  Occluded Baggage Items from Multi-Vendor X-ray Scans","source":"Taimur Hassan, Salman H. Khan, Samet Akcay, Mohammed Bennamoun,\n  Naoufel Werghi","docs_id":"1912.04251","section":["cs.CV","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Cascaded Structure Tensor Framework for Robust Identification of Heavily\n  Occluded Baggage Items from Multi-Vendor X-ray Scans. In the last two decades, luggage scanning has globally become one of the prime aviation security concerns. Manual screening of the baggage items is a cumbersome, subjective and inefficient process. Hence, many researchers have developed Xray imagery-based autonomous systems to address these shortcomings. However, to the best of our knowledge, there is no framework, up to now, that can recognize heavily occluded and cluttered baggage items from multi-vendor X-ray scans. This paper presents a cascaded structure tensor framework which can automatically extract and recognize suspicious items irrespective of their position and orientation in the multi-vendor X-ray scans. The proposed framework is unique, as it intelligently extracts each object by iteratively picking contour based transitional information from different orientations and uses only a single feedforward convolutional neural network for the recognition. The proposed framework has been rigorously tested on publicly available GDXray and SIXray datasets containing a total of 1,067,381 X-ray scans where it significantly outperformed the state-of-the-art solutions by achieving the mean average precision score of 0.9343 and 0.9595 for extracting and recognizing suspicious items from GDXray and SIXray scans, respectively. Furthermore, the proposed framework has achieved 15.78% better time"},{"title":"Nonlinear Dipole Inversion (NDI) enables Quantitative Susceptibility\n  Mapping (QSM) without parameter tuning","source":"Daniel Polak, Itthi Chatnuntawech, Jaeyeon Yoon, Siddharth Srinivasan\n  Iyer, Jongho Lee, Peter Bachert, Elfar Adalsteinsson, Kawin Setsompop, Berkin\n  Bilgic","docs_id":"1909.13692","section":["eess.IV","cs.LG","eess.SP","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Nonlinear Dipole Inversion (NDI) enables Quantitative Susceptibility\n  Mapping (QSM) without parameter tuning. We propose Nonlinear Dipole Inversion (NDI) for high-quality Quantitative Susceptibility Mapping (QSM) without regularization tuning, while matching the image quality of state-of-the-art reconstruction techniques. In addition to avoiding over-smoothing that these techniques often suffer from, we also obviate the need for parameter selection. NDI is flexible enough to allow for reconstruction from an arbitrary number of head orientations, and outperforms COSMOS even when using as few as 1-direction data. This is made possible by a nonlinear forward-model that uses the magnitude as an effective prior, for which we derived a simple gradient descent update rule. We synergistically combine this physics-model with a Variational Network (VN) to leverage the power of deep learning in the VaNDI algorithm. This technique adopts the simple gradient descent rule from NDI and learns the network parameters during training, hence requires no additional parameter tuning. Further, we evaluate NDI at 7T using highly accelerated Wave-CAIPI acquisitions at 0.5 mm isotropic resolution and demonstrate high-quality QSM from as few as 2-direction data."},{"title":"Constructions of Binary Optimal Locally Repairable Codes via\n  Intersection Subspaces","source":"Wenqin Zhang, Deng Tang, Yuan Luo","docs_id":"2105.11271","section":["cs.IT","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Constructions of Binary Optimal Locally Repairable Codes via\n  Intersection Subspaces. Locally repairable codes (LRCs), which can recover any symbol of a codeword by reading only a small number of other symbols, have been widely used in real-world distributed storage systems, such as Microsoft Azure Storage and Ceph Storage Cluster. Since the binary linear LRCs can significantly reduce the coding and decoding complexity, the construction of binary LRCs is of particular interest. To date, all the known optimal binary linear LRCs with the locality $2^b$ ($b\\geq 3$) are based on the so-called partial spread which is a collection of the same dimensional subspaces with pairwise trivial, i.e., zero-dimensional intersection. In this paper, we concentrate on binary linear LRCs with disjoint local repair groups. We construct dimensional optimal binary linear LRCs with locality $2^b$ ($b\\geq 3$) and minimum distance $d\\geq 6$ by employing intersection subspaces deduced from the direct sum vs. the traditional partial spread construction. This method will increase the number of possible repair groups of LRCs as many as possible, and thus efficiently enlarge the range of the construction parameters while keeping the largest code rates compared with all known binary linear LRCs with minimum distance $d\\geq 6$ and locality $2^b$ ($b\\geq 3$)."},{"title":"Navigation of a UAV Equipped with a Reconfigurable Intelligent Surface\n  for LoS Wireless Communication with a Ground Vehicle","source":"Mohsen Eskandari, Hailong Huang, Andrey V. Savkin, Wei Ni","docs_id":"2110.09012","section":["eess.SY","cs.SY"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Navigation of a UAV Equipped with a Reconfigurable Intelligent Surface\n  for LoS Wireless Communication with a Ground Vehicle. Unmanned aerial vehicles (UAVs) have been successfully adopted to enhance the flexibility and robustness of wireless communication networks. And recently, the reconfigurable intelligent surface (RIS) technology has been paid increasing attention to improve the throughput of the fifth-generation (5G) millimeter-wave (mmWave) wireless communication. In this work, we propose an RIS-outfitted UAV (RISoUAV) to secure an uninterrupted line-of-sight (LoS) link with a ground moving target (MT). The MT can be an emergency ambulance and need a secure wireless communication link for continuous monitoring and diagnosing the health condition of a patient, which is vital for delivering critical patient care. In this light, real-time communication is required for sending various clinical multimedia data including videos, medical images, and vital signs. This significant target is achievable thanks to the 5G wireless communication assisted with RISoUAV. A two-stage optimization method is proposed to optimize the RISoUAV trajectory limited to UAV motion and LoS constraints. At the first stage, the optimal tube path of the RISoUAV is determined by taking into account the energy consumption, instant LoS link, and UAV speed\/acceleration constraints. At the second stage, an accurate RISoUAV trajectory is obtained by considering the communication channel performance and passive beamforming. Simulation results show the accuracy and effectiveness of the method."},{"title":"Teaching the Old Dog New Tricks: Supervised Learning with Constraints","source":"Fabrizio Detassis, Michele Lombardi, Michela Milano","docs_id":"2002.10766","section":["cs.LG","cs.AI","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Teaching the Old Dog New Tricks: Supervised Learning with Constraints. Adding constraint support in Machine Learning has the potential to address outstanding issues in data-driven AI systems, such as safety and fairness. Existing approaches typically apply constrained optimization techniques to ML training, enforce constraint satisfaction by adjusting the model design, or use constraints to correct the output. Here, we investigate a different, complementary, strategy based on \"teaching\" constraint satisfaction to a supervised ML method via the direct use of a state-of-the-art constraint solver: this enables taking advantage of decades of research on constrained optimization with limited effort. In practice, we use a decomposition scheme alternating master steps (in charge of enforcing the constraints) and learner steps (where any supervised ML model and training algorithm can be employed). The process leads to approximate constraint satisfaction in general, and convergence properties are difficult to establish; despite this fact, we found empirically that even a na\\\"ive setup of our approach performs well on ML tasks with fairness constraints, and on classical datasets with synthetic constraints."},{"title":"FedFog: Network-Aware Optimization of Federated Learning over Wireless\n  Fog-Cloud Systems","source":"Van-Dinh Nguyen, Symeon Chatzinotas, Bjorn Ottersten, and Trung Q.\n  Duong","docs_id":"2107.02755","section":["cs.LG","cs.DC"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"FedFog: Network-Aware Optimization of Federated Learning over Wireless\n  Fog-Cloud Systems. Federated learning (FL) is capable of performing large distributed machine learning tasks across multiple edge users by periodically aggregating trained local parameters. To address key challenges of enabling FL over a wireless fog-cloud system (e.g., non-i.i.d. data, users' heterogeneity), we first propose an efficient FL algorithm based on Federated Averaging (called FedFog) to perform the local aggregation of gradient parameters at fog servers and global training update at the cloud. Next, we employ FedFog in wireless fog-cloud systems by investigating a novel network-aware FL optimization problem that strikes the balance between the global loss and completion time. An iterative algorithm is then developed to obtain a precise measurement of the system performance, which helps design an efficient stopping criteria to output an appropriate number of global rounds. To mitigate the straggler effect, we propose a flexible user aggregation strategy that trains fast users first to obtain a certain level of accuracy before allowing slow users to join the global training updates. Extensive numerical results using several real-world FL tasks are provided to verify the theoretical convergence of FedFog. We also show that the proposed co-design of FL and communication is essential to substantially improve resource utilization while achieving comparable accuracy of the learning model."},{"title":"Automated Crowdturfing Attacks and Defenses in Online Review Systems","source":"Yuanshun Yao, Bimal Viswanath, Jenna Cryan, Haitao Zheng, Ben Y. Zhao","docs_id":"1708.08151","section":["cs.CR","cs.SI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Automated Crowdturfing Attacks and Defenses in Online Review Systems. Malicious crowdsourcing forums are gaining traction as sources of spreading misinformation online, but are limited by the costs of hiring and managing human workers. In this paper, we identify a new class of attacks that leverage deep learning language models (Recurrent Neural Networks or RNNs) to automate the generation of fake online reviews for products and services. Not only are these attacks cheap and therefore more scalable, but they can control rate of content output to eliminate the signature burstiness that makes crowdsourced campaigns easy to detect. Using Yelp reviews as an example platform, we show how a two phased review generation and customization attack can produce reviews that are indistinguishable by state-of-the-art statistical detectors. We conduct a survey-based user study to show these reviews not only evade human detection, but also score high on \"usefulness\" metrics by users. Finally, we develop novel automated defenses against these attacks, by leveraging the lossy transformation introduced by the RNN training and generation cycle. We consider countermeasures against our mechanisms, show that they produce unattractive cost-benefit tradeoffs for attackers, and that they can be further curtailed by simple constraints imposed by online service providers."},{"title":"Video Object Segmentation with Joint Re-identification and\n  Attention-Aware Mask Propagation","source":"Xiaoxiao Li, Chen Change Loy","docs_id":"1803.04242","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Video Object Segmentation with Joint Re-identification and\n  Attention-Aware Mask Propagation. The problem of video object segmentation can become extremely challenging when multiple instances co-exist. While each instance may exhibit large scale and pose variations, the problem is compounded when instances occlude each other causing failures in tracking. In this study, we formulate a deep recurrent network that is capable of segmenting and tracking objects in video simultaneously by their temporal continuity, yet able to re-identify them when they re-appear after a prolonged occlusion. We combine both temporal propagation and re-identification functionalities into a single framework that can be trained end-to-end. In particular, we present a re-identification module with template expansion to retrieve missing objects despite their large appearance changes. In addition, we contribute a new attention-based recurrent mask propagation approach that is robust to distractors not belonging to the target segment. Our approach achieves a new state-of-the-art global mean (Region Jaccard and Boundary F measure) of 68.2 on the challenging DAVIS 2017 benchmark (test-dev set), outperforming the winning solution which achieves a global mean of 66.1 on the same partition."},{"title":"Asynchronous Averaging of Gait Cycles for Classification of Gait and\n  Device Modes","source":"Parinaz Kasebzadeh, Gustaf Hendeby, Fredrik Gustafsson","docs_id":"1907.02329","section":["eess.SP","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Asynchronous Averaging of Gait Cycles for Classification of Gait and\n  Device Modes. An approach for computing unique gait signature using measurements collected from body-worn inertial measurement units (IMUs) is proposed. The gait signature represents one full cycle of the human gait, and is suitable for off-line or on-line classification of the gait mode. The signature can also be used to jointly classify the gait mode and the device mode. The device mode identifies how the IMU-equipped device is being carried by the user. The method is based on precise segmentation and resampling of the measured IMU signal, as an initial step, further tuned by minimizing the variability of the obtained signature within each gait cycle. Finally, a Fourier series expansion of the gait signature is introduced which provides a low-dimensional feature vector well suited for classification purposes. The proposed method is evaluated on a large dataset involving several subjects, each one containing two different gait modes and four different device modes. The gait signatures enable a high classification rate for each step cycle."},{"title":"MNL-Bandit with Knapsacks","source":"Abdellah Aznag, Vineet Goyal and Noemie Perivier","docs_id":"2106.01135","section":["cs.LG","cs.DS"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"MNL-Bandit with Knapsacks. We consider a dynamic assortment selection problem where a seller has a fixed inventory of $N$ substitutable products and faces an unknown demand that arrives sequentially over $T$ periods. In each period, the seller needs to decide on the assortment of products (of cardinality at most $K$) to offer to the customers. The customer's response follows an unknown multinomial logit model (MNL) with parameters $v$. The goal of the seller is to maximize the total expected revenue given the fixed initial inventory of $N$ products. We give a policy that achieves a regret of $\\tilde O\\left(K \\sqrt{K N T}\\left(1 + \\frac{\\sqrt{v_{\\max}}}{q_{\\min}}\\text{OPT}\\right) \\right)$ under a mild assumption on the model parameters. In particular, our policy achieves a near-optimal $\\tilde O(\\sqrt{T})$ regret in the large inventory setting. Our policy builds upon the UCB-based approach for MNL-bandit without inventory constraints in [1] and addresses the inventory constraints through an exponentially sized LP for which we present a tractable approximation while keeping the $\\tilde O(\\sqrt{T})$ regret bound."},{"title":"The Locus Algorithm IV: Performance metrics of a grid computing system\n  used to create catalogues of optimised pointings","source":"Ois\\'in Creaner, John Walsh, Kevin Nolan and Eugene Hickey","docs_id":"2003.04570","section":["astro-ph.IM","cs.PF"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"The Locus Algorithm IV: Performance metrics of a grid computing system\n  used to create catalogues of optimised pointings. This paper discusses the requirements for and performance metrics of the the Grid Computing system used to implement the Locus Algorithm to identify optimum pointings for differential photometry of 61,662,376 stars and 23,779 quasars. Initial operational tests indicated a need for a software system to analyse the data and a High Performance Computing system to run that software in a scalable manner. Practical assessments of the performance of the software in a serial computing environment were used to provide a benchmark against which the performance metrics of the HPC solution could be compared, as well as to indicate any bottlenecks in performance. These performance metrics indicated a distinct split in the performance dictated more by differences in the input data than by differences in the design of the systems used. This indicates a need for experimental analysis of system performance, and suggests that algorithmic complexity analyses may lead to incorrect or naive conclusions, especially in systems with high data I\/O overhead such as grid computing. Further, it implies that systems which reduce or eliminate this bottleneck such as in-memory processing could lead to a substantial increase in performance."},{"title":"Essencery - A Tool for Essentializing Software Engineering Practices","source":"Arthur Evensen, Kai-Kristian Kemell, Xiaofeng Wang, Juhani Risku,\n  Pekka Abrahamsson","docs_id":"1808.02723","section":["cs.SE"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Essencery - A Tool for Essentializing Software Engineering Practices. Software Engineering practitioners work using highly diverse methods and practices, and general theories in software engineering are lacking. One attempt at creating a common ground in the area of software engineering methodologies has been the Essence Theory of Software Engineering, which can be considered a method-agnostic project management tool for software engineering. Essence supports the use of any development practices and provides a framework for building a suitable method for any software engineering context. However, Essence presently suffers from low practitioner adoption that is partially considered to be caused by a lack of proper tooling. In this paper, we present Essencery, a tool for essentializing software engineering methods and practices using the Essence graphical syntax. Essencery aims to facilitate adoption of Essence among potential future users. We present an empirical evaluation of the tool by means of a qualitative, quasi-formal experiment and, based on the experiment, confirm that the tool is easy to use and useful for its intended purpose."},{"title":"Towards Autonomous Robotic Precision Harvesting: Mapping, Localization,\n  Planning and Control for a Legged Tree Harvester","source":"Edo Jelavic and Dominic Jud and Pascal Egli and Marco Hutter","docs_id":"2104.10110","section":["cs.RO","cs.SY","eess.SY"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Towards Autonomous Robotic Precision Harvesting: Mapping, Localization,\n  Planning and Control for a Legged Tree Harvester. This paper presents an integrated system for performing precision harvesting missions using a legged harvester. Our harvester performs a challenging task of autonomous navigation and tree grabbing in a confined, GPS denied forest environment. Strategies for mapping, localization, planning, and control are proposed and integrated into a fully autonomous system. The mission starts with a human mapping the area of interest using a custom-made sensor module. Subsequently, a human expert selects the trees for harvesting. The sensor module is then mounted on the machine and used for localization within the given map. A planning algorithm searches for both an approach pose and a path in a single path planning problem. We design a path following controller leveraging the legged harvester's capabilities for negotiating rough terrain. Upon reaching the approach pose, the machine grabs a tree with a general-purpose gripper. This process repeats for all the trees selected by the operator. Our system has been tested on a testing field with tree trunks and in a natural forest. To the best of our knowledge, this is the first time this level of autonomy has been shown on a full-size hydraulic machine operating in a realistic environment."},{"title":"Topological optimization of hybrid quantum key distribution networks","source":"Ya-Xing Wang, Qiong Li, Hao-Kun Mao, Qi Han, Fu-Rong Huang, Hong-Wei\n  Xu","docs_id":"2003.14100","section":["quant-ph","cs.NI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Topological optimization of hybrid quantum key distribution networks. With the growing complexity of quantum key distribution (QKD) network structures, aforehand topology design is of great significance to support a large-number of nodes over a large-spatial area. However, the exclusivity of quantum channels, the limitation of key generation capabilities, the variety of QKD protocols and the necessity of untrusted-relay selection, make the optimal topology design a very complicated task. In this research, a hybrid QKD network is studied for the first time from the perspective of topology, by analyzing the topological differences of various QKD protocols. In addition, to make full use of hybrid networking, an analytical model for optimal topology calculation is proposed, to reach the goal of best secure communication service by optimizing the deployment of various QKD devices and the selection of untrusted-relays under a given cost limit. Plentiful simulation results show that hybrid networking and untrusted-relay selection can bring great performance advantages, and then the universality and effectiveness of the proposed analytical model are verified."},{"title":"Practical Deep Raw Image Denoising on Mobile Devices","source":"Yuzhi Wang, Haibin Huang, Qin Xu, Jiaming Liu, Yiqun Liu, Jue Wang","docs_id":"2010.06935","section":["eess.IV","cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Practical Deep Raw Image Denoising on Mobile Devices. Deep learning-based image denoising approaches have been extensively studied in recent years, prevailing in many public benchmark datasets. However, the stat-of-the-art networks are computationally too expensive to be directly applied on mobile devices. In this work, we propose a light-weight, efficient neural network-based raw image denoiser that runs smoothly on mainstream mobile devices, and produces high quality denoising results. Our key insights are twofold: (1) by measuring and estimating sensor noise level, a smaller network trained on synthetic sensor-specific data can out-perform larger ones trained on general data; (2) the large noise level variation under different ISO settings can be removed by a novel k-Sigma Transform, allowing a small network to efficiently handle a wide range of noise levels. We conduct extensive experiments to demonstrate the efficiency and accuracy of our approach. Our proposed mobile-friendly denoising model runs at ~70 milliseconds per megapixel on Qualcomm Snapdragon 855 chipset, and it is the basis of the night shot feature of several flagship smartphones released in 2019."},{"title":"Estimating the confidence of speech spoofing countermeasure","source":"Xin Wang, Junichi Yamagishi","docs_id":"2110.04775","section":["eess.AS","cs.CR","cs.SD"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Estimating the confidence of speech spoofing countermeasure. Conventional speech spoofing countermeasures (CMs) are designed to make a binary decision on an input trial. However, a CM trained on a closed-set database is theoretically not guaranteed to perform well on unknown spoofing attacks. In some scenarios, an alternative strategy is to let the CM defer a decision when it is not confident. The question is then how to estimate a CM's confidence regarding an input trial. We investigated a few confidence estimators that can be easily plugged into a CM. On the ASVspoof2019 logical access database, the results demonstrate that an energy-based estimator and a neural-network-based one achieved acceptable performance in identifying unknown attacks in the test set. On a test set with additional unknown attacks and bona fide trials from other databases, the confidence estimators performed moderately well, and the CMs better discriminated bona fide and spoofed trials that had a high confidence score. Additional results also revealed the difficulty in enhancing a confidence estimator by adding unknown attacks to the training set."},{"title":"Combinatorial Optimization based Feature Selection Method: A study on\n  Network Intrusion Detection","source":"Anjum Nazir, Rizwan Ahmed Khan","docs_id":"1906.04494","section":["cs.CR","cs.NI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Combinatorial Optimization based Feature Selection Method: A study on\n  Network Intrusion Detection. Advancements in computer networks and communication technologies like software defined networks (SDN), Internet of things (IoT), microservices architecture, cloud computing and network function virtualization (NFV) have opened new fronts and challenges for security experts to combat against modern cyberattacks. Relying on perimeter defense and signature-based network security solutions like Intrusion Detection and Prevention Systems (IDS\/IPS) have failed to deliver adequate level of security against new attack vectors such as advance persistent threats, zero days, ransomware, botnets and other forms of targeted attacks. Recent developments in machine learning and cognitive computing have shown great potential to detect unknown and new intrusion events where legacy misuse and anomaly based intrusion detection systems usually fail. In this research study we applied state of the art machine learning algorithms on UNSW-NB15 dataset for potential applicability to detect new attacks. We also proposed a novel wrapper based feature selection technique TS-RF using metaheuristic Tabu Search (TS) algorithm and Random Forest (RF) ensemble classifier. Results obtained by applying proposed feature selection technique i.e. TS-RF on UNSW-NB15 dataset show improvement in overall intrusion detection accuracy while it reduces computation complexity as it removes more than 60% features."},{"title":"ZS-BERT: Towards Zero-Shot Relation Extraction with Attribute\n  Representation Learning","source":"Chih-Yao Chen, Cheng-Te Li","docs_id":"2104.04697","section":["cs.CL","cs.IR","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"ZS-BERT: Towards Zero-Shot Relation Extraction with Attribute\n  Representation Learning. While relation extraction is an essential task in knowledge acquisition and representation, and new-generated relations are common in the real world, less effort is made to predict unseen relations that cannot be observed at the training stage. In this paper, we formulate the zero-shot relation extraction problem by incorporating the text description of seen and unseen relations. We propose a novel multi-task learning model, zero-shot BERT (ZS-BERT), to directly predict unseen relations without hand-crafted attribute labeling and multiple pairwise classifications. Given training instances consisting of input sentences and the descriptions of their relations, ZS-BERT learns two functions that project sentences and relation descriptions into an embedding space by jointly minimizing the distances between them and classifying seen relations. By generating the embeddings of unseen relations and new-coming sentences based on such two functions, we use nearest neighbor search to obtain the prediction of unseen relations. Experiments conducted on two well-known datasets exhibit that ZS-BERT can outperform existing methods by at least 13.54\\% improvement on F1 score."},{"title":"Differentiable Signal Processing With Black-Box Audio Effects","source":"Marco A. Mart\\'inez Ram\\'irez, Oliver Wang, Paris Smaragdis, Nicholas\n  J. Bryan","docs_id":"2105.04752","section":["eess.AS","cs.LG","cs.SD","eess.SP"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Differentiable Signal Processing With Black-Box Audio Effects. We present a data-driven approach to automate audio signal processing by incorporating stateful third-party, audio effects as layers within a deep neural network. We then train a deep encoder to analyze input audio and control effect parameters to perform the desired signal manipulation, requiring only input-target paired audio data as supervision. To train our network with non-differentiable black-box effects layers, we use a fast, parallel stochastic gradient approximation scheme within a standard auto differentiation graph, yielding efficient end-to-end backpropagation. We demonstrate the power of our approach with three separate automatic audio production applications: tube amplifier emulation, automatic removal of breaths and pops from voice recordings, and automatic music mastering. We validate our results with a subjective listening test, showing our approach not only can enable new automatic audio effects tasks, but can yield results comparable to a specialized, state-of-the-art commercial solution for music mastering."},{"title":"Learning-based personal speech enhancement for teleconferencing by\n  exploiting spatial-spectral features","source":"Yicheng Hsu, Yonghan Lee, Mingsian R. Bai","docs_id":"2112.05686","section":["eess.AS","cs.SD"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Learning-based personal speech enhancement for teleconferencing by\n  exploiting spatial-spectral features. Teleconferencing is becoming essential during the COVID-19 pandemic. However, in real-world applications, speech quality can deteriorate due to, for example, background interference, noise, or reverberation. To solve this problem, target speech extraction from the mixture signals can be performed with the aid of the user's vocal features. Various features are accounted for in this study's proposed system, including speaker embeddings derived from user enrollment and a novel long-short-term spatial coherence (LSTSC) feature to the target speaker activity. As a learning-based approach, a target speech sifting network was employed to extract the target speech signal. The network trained with LSTSC in the proposed approach is robust to microphone array geometries and the number of microphones. Furthermore, the proposed enhancement system was compared with a baseline system with speaker embeddings and interchannel phase difference. The results demonstrated the superior performance of the proposed system over the baseline in enhancement performance and robustness."},{"title":"Unsupervised Domain Adaptation for Object Detection via Cross-Domain\n  Semi-Supervised Learning","source":"Fuxun Yu, Di Wang, Yinpeng Chen, Nikolaos Karianakis, Tong Shen, Pei\n  Yu, Dimitrios Lymberopoulos, Sidi Lu, Weisong Shi, Xiang Chen","docs_id":"1911.07158","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Unsupervised Domain Adaptation for Object Detection via Cross-Domain\n  Semi-Supervised Learning. Current state-of-the-art object detectors can have significant performance drop when deployed in the wild due to domain gaps with training data. Unsupervised Domain Adaptation (UDA) is a promising approach to adapt models for new domains\/environments without any expensive label cost. However, without ground truth labels, most prior works on UDA for object detection tasks can only perform coarse image-level and\/or feature-level adaptation by using adversarial learning methods. In this work, we show that such adversarial-based methods can only reduce the domain style gap, but cannot address the domain content distribution gap that is shown to be important for object detectors. To overcome this limitation, we propose the Cross-Domain Semi-Supervised Learning (CDSSL) framework by leveraging high-quality pseudo labels to learn better representations from the target domain directly. To enable SSL for cross-domain object detection, we propose fine-grained domain transfer, progressive-confidence-based label sharpening and imbalanced sampling strategy to address two challenges: (i) non-identical distribution between source and target domain data, (ii) error amplification\/accumulation due to noisy pseudo labeling on the target domain. Experiment results show that our proposed approach consistently achieves new state-of-the-art performance (2.2% - 9.5% better than prior best work on mAP) under various domain gap scenarios. The code will be released."},{"title":"Avoiding Implementation Pitfalls of \"Matrix Capsules with EM Routing\" by\n  Hinton et al","source":"Ashley Daniel Gritzman","docs_id":"1907.00652","section":["cs.LG","cs.CV","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Avoiding Implementation Pitfalls of \"Matrix Capsules with EM Routing\" by\n  Hinton et al. The recent progress on capsule networks by Hinton et al. has generated considerable excitement in the machine learning community. The idea behind a capsule is inspired by a cortical minicolumn in the brain, whereby a vertically organised group of around 100 neurons receive common inputs, have common outputs, are interconnected, and may well constitute a fundamental computation unit of the cerebral cortex. However, Hinton's paper on \"Matrix Capsule with EM Routing'\" was unfortunately not accompanied by a release of source code, which left interested researchers attempting to implement the architecture and reproduce the benchmarks on their own. This has certainly slowed the progress of research building on this work. While writing our own implementation, we noticed several common mistakes in other open source implementations that we came across. In this paper we share some of these learnings, specifically focusing on three implementation pitfalls and how to avoid them: (1) parent capsules with only one child; (2) normalising the amount of data assigned to parent capsules; (3) parent capsules at different positions compete for child capsules. While our implementation is a considerable improvement over currently available implementations, it still falls slightly short of the performance reported by Hinton et al. (2018). The source code for this implementation is available on GitHub at the following URL: https:\/\/github.com\/IBM\/matrix-capsules-with-em-routing."},{"title":"Phase reconstruction from amplitude spectrograms based on\n  von-Mises-distribution deep neural network","source":"Shinnosuke Takamichi and Yuki Saito and Norihiro Takamune and Daichi\n  Kitamura and Hiroshi Saruwatari","docs_id":"1807.03474","section":["cs.SD","eess.AS"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Phase reconstruction from amplitude spectrograms based on\n  von-Mises-distribution deep neural network. This paper presents a deep neural network (DNN)-based phase reconstruction from amplitude spectrograms. In audio signal and speech processing, the amplitude spectrogram is often used for processing, and the corresponding phase spectrogram is reconstructed from the amplitude spectrogram on the basis of the Griffin-Lim method. However, the Griffin-Lim method causes unnatural artifacts in synthetic speech. Addressing this problem, we introduce the von-Mises-distribution DNN for phase reconstruction. The DNN is a generative model having the von Mises distribution that can model distributions of a periodic variable such as a phase, and the model parameters of the DNN are estimated on the basis of the maximum likelihood criterion. Furthermore, we propose a group-delay loss for DNN training to make the predicted group delay close to a natural group delay. The experimental results demonstrate that 1) the trained DNN can predict group delay accurately more than phases themselves, and 2) our phase reconstruction methods achieve better speech quality than the conventional Griffin-Lim method."},{"title":"Proving Equivalence Between Complex Expressions Using Graph-to-Sequence\n  Neural Models","source":"Steve Kommrusch, Th\\'eo Barollet and Louis-No\\\"el Pouchet","docs_id":"2106.02452","section":["cs.PL","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Proving Equivalence Between Complex Expressions Using Graph-to-Sequence\n  Neural Models. We target the problem of provably computing the equivalence between two complex expression trees. To this end, we formalize the problem of equivalence between two such programs as finding a set of semantics-preserving rewrite rules from one into the other, such that after the rewrite the two programs are structurally identical, and therefore trivially equivalent.We then develop a graph-to-sequence neural network system for program equivalence, trained to produce such rewrite sequences from a carefully crafted automatic example generation algorithm. We extensively evaluate our system on a rich multi-type linear algebra expression language, using arbitrary combinations of 100+ graph-rewriting axioms of equivalence. Our machine learning system guarantees correctness for all true negatives, and ensures 0 false positive by design. It outputs via inference a valid proof of equivalence for 93% of the 10,000 equivalent expression pairs isolated for testing, using up to 50-term expressions. In all cases, the validity of the sequence produced and therefore the provable assertion of program equivalence is always computable, in negligible time."},{"title":"On the relationships between bibliographic characteristics of scientific\n  documents and citation and Mendeley readership counts: A large-scale analysis\n  of Web of Science publications","source":"Zohreh Zahedi, Stefanie Haustein","docs_id":"1712.08637","section":["cs.DL"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"On the relationships between bibliographic characteristics of scientific\n  documents and citation and Mendeley readership counts: A large-scale analysis\n  of Web of Science publications. In this paper we present a first large-scale analysis of the relationship between Mendeley readership and citation counts with particular documents bibliographic characteristics. A data set of 1.3 million publications from different fields published in journals covered by the Web of Science (WoS) has been analyzed. This work reveals that document types that are often excluded from citation analysis due to their lower citation values, like editorial materials, letters, or news items, are strongly covered and saved in Mendeley, suggesting that Mendeley readership can reliably inform the analysis of these document types. Findings show that collaborative papers are frequently saved in Mendeley, which is similar to what is observed for citations. The relationship between readership and the length of titles and number of pages, however, is weaker than for the same relationship observed for citations. The analysis of different disciplines also points to different patterns in the relationship between several document characteristics, readership, and citation counts. Overall, results highlight that although disciplinary differences exist, readership counts are related to similar bibliographic characteristics as those related to citation counts, reinforcing the idea that Mendeley readership and citations capture a similar concept of impact, although they cannot be considered as equivalent indicators."},{"title":"Ontology-driven Event Type Classification in Images","source":"Eric M\\\"uller-Budack, Matthias Springstein, Sherzod Hakimov, Kevin\n  Mrutzek, Ralph Ewerth","docs_id":"2011.04714","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Ontology-driven Event Type Classification in Images. Event classification can add valuable information for semantic search and the increasingly important topic of fact validation in news. So far, only few approaches address image classification for newsworthy event types such as natural disasters, sports events, or elections. Previous work distinguishes only between a limited number of event types and relies on rather small datasets for training. In this paper, we present a novel ontology-driven approach for the classification of event types in images. We leverage a large number of real-world news events to pursue two objectives: First, we create an ontology based on Wikidata comprising the majority of event types. Second, we introduce a novel large-scale dataset that was acquired through Web crawling. Several baselines are proposed including an ontology-driven learning approach that aims to exploit structured information of a knowledge graph to learn relevant event relations using deep neural networks. Experimental results on existing as well as novel benchmark datasets demonstrate the superiority of the proposed ontology-driven approach."},{"title":"Space-time multilevel Monte Carlo methods and their application to\n  cardiac electrophysiology","source":"Seif Ben Bader, Pietro Benedusi, Alessio Quaglino, Patrick Zulian,\n  Rolf Krause","docs_id":"1911.06066","section":["cs.CE","cs.NA","math.AP","math.NA"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Space-time multilevel Monte Carlo methods and their application to\n  cardiac electrophysiology. We present a novel approach aimed at high-performance uncertainty quantification for time-dependent problems governed by partial differential equations. In particular, we consider input uncertainties described by a Karhunen-Loeeve expansion and compute statistics of high-dimensional quantities-of-interest, such as the cardiac activation potential. Our methodology relies on a close integration of multilevel Monte Carlo methods, parallel iterative solvers, and a space-time discretization. This combination allows for space-time adaptivity, time-changing domains, and to take advantage of past samples to initialize the space-time solution. The resulting sequence of problems is distributed using a multilevel parallelization strategy, allocating batches of samples having different sizes to a different number of processors. We assess the performance of the proposed framework by showing in detail its application to the solution of nonlinear equations arising from cardiac electrophysiology. Specifically, we study the effect of spatially-correlated perturbations of the heart fibers conductivities on the mean and variance of the resulting activation map. As shown by the experiments, the theoretical rates of convergence of multilevel Monte Carlo are achieved. Moreover, the total computational work for a prescribed accuracy is reduced by an order of magnitude with respect to standard Monte Carlo methods."},{"title":"A Combination of Testability and Decodability by Tensor Products","source":"Michael Viderman","docs_id":"1105.5806","section":["cs.CC"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A Combination of Testability and Decodability by Tensor Products. Ben-Sasson and Sudan (RSA 2006) showed that repeated tensor products of linear codes with a very large distance are locally testable. Due to the requirement of a very large distance the associated tensor products could be applied only over sufficiently large fields. Then Meir (SICOMP 2009) used this result (as a black box) to present a combinatorial construction of locally testable codes that match best known parameters. As a consequence, this construction was obtained over sufficiently large fields. In this paper we improve the result of Ben-Sasson and Sudan and show that for \\emph{any} linear codes the associated tensor products are locally testable. Consequently, the construction of Meir can be taken over any field, including the binary field. Moreover, a combination of our result with the result of Spielman (IEEE IT, 1996) implies a construction of linear codes (over any field) that combine the following properties: have constant rate and constant relative distance; have blocklength $n$ and testable with $n^{\\epsilon}$ queries, for any constant $\\epsilon > 0$; linear time encodable and linear-time decodable from a constant fraction of errors. Furthermore, a combination of our result with the result of Guruswami et al. (STOC 2009) implies a similar corollary regarding the list-decodable codes."},{"title":"MoEVC: A Mixture-of-experts Voice Conversion System with Sparse Gating\n  Mechanism for Accelerating Online Computation","source":"Yu-Tao Chang, Yuan-Hong Yang, Yu-Huai Peng, Syu-Siang Wang, Tai-Shih\n  Chi, Yu Tsao, Hsin-Min Wang","docs_id":"1912.11984","section":["cs.SD","eess.AS"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"MoEVC: A Mixture-of-experts Voice Conversion System with Sparse Gating\n  Mechanism for Accelerating Online Computation. With the recent advancements of deep learning technologies, the performance of voice conversion (VC) in terms of quality and similarity has been significantly improved. However, heavy computations are generally required for deep-learning-based VC systems, which can cause notable latency and thus confine their deployments in real-world applications. Therefore, increasing online computation efficiency has become an important task. In this study, we propose a novel mixture-of-experts (MoE) based VC system. The MoE model uses a gating mechanism to specify optimal weights to feature maps to increase VC performance. In addition, assigning sparse constraints on the gating mechanism can accelerate online computation by skipping the convolution process by zeroing out redundant feature maps. Experimental results show that by specifying suitable sparse constraints, we can effectively increase the online computation efficiency with a notable 70% FLOPs (floating-point operations per second) reduction while improving the VC performance in both objective evaluations and human listening tests."},{"title":"Short-term Load Forecasting Based on Hybrid Strategy Using Warm-start\n  Gradient Tree Boosting","source":"Yuexin Zhang, Jiahong Wang","docs_id":"2005.11478","section":["cs.LG","eess.SP","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Short-term Load Forecasting Based on Hybrid Strategy Using Warm-start\n  Gradient Tree Boosting. A deep-learning-based hybrid strategy for short-term load forecasting is presented. The strategy proposes a novel tree-based ensemble method Warm-start Gradient Tree Boosting (WGTB). Current strategies either ensemble submodels of a single type, which fail to take advantage of the statistical strengths of different inference models. Or they simply sum the outputs from completely different inference models, which doesn't maximize the potential of ensemble. Inspired by the bias-variance trade-off, WGTB is proposed and tailored to the great disparity among different inference models on accuracy, volatility and linearity. The complete strategy integrates four different inference models of different capacities. WGTB then ensembles their outputs by a warm-start and a hybrid of bagging and boosting, which lowers bias and variance concurrently. It is validated on two real datasets from State Grid Corporation of China of hourly resolution. The result demonstrates the effectiveness of the proposed strategy that hybridizes the statistical strengths of both low-bias and low-variance inference models."},{"title":"Answer Generation through Unified Memories over Multiple Passages","source":"Makoto Nakatsuji, Sohei Okui","docs_id":"2004.13829","section":["cs.CL","cs.AI","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Answer Generation through Unified Memories over Multiple Passages. Machine reading comprehension methods that generate answers by referring to multiple passages for a question have gained much attention in AI and NLP communities. The current methods, however, do not investigate the relationships among multiple passages in the answer generation process, even though topics correlated among the passages may be answer candidates. Our method, called neural answer Generation through Unified Memories over Multiple Passages (GUM-MP), solves this problem as follows. First, it determines which tokens in the passages are matched to the question. In particular, it investigates matches between tokens in positive passages, which are assigned to the question, and those in negative passages, which are not related to the question. Next, it determines which tokens in the passage are matched to other passages assigned to the same question and at the same time it investigates the topics in which they are matched. Finally, it encodes the token sequences with the above two matching results into unified memories in the passage encoders and learns the answer sequence by using an encoder-decoder with a multiple-pointer-generator mechanism. As a result, GUM-MP can generate answers by pointing to important tokens present across passages. Evaluations indicate that GUM-MP generates much more accurate results than the current models do."},{"title":"Uncovering the Temporal Dynamics of Diffusion Networks","source":"Manuel Gomez Rodriguez, David Balduzzi, Bernhard Sch\\\"olkopf","docs_id":"1105.0697","section":["cs.SI","cs.DS","cs.IR","physics.soc-ph"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Uncovering the Temporal Dynamics of Diffusion Networks. Time plays an essential role in the diffusion of information, influence and disease over networks. In many cases we only observe when a node copies information, makes a decision or becomes infected -- but the connectivity, transmission rates between nodes and transmission sources are unknown. Inferring the underlying dynamics is of outstanding interest since it enables forecasting, influencing and retarding infections, broadly construed. To this end, we model diffusion processes as discrete networks of continuous temporal processes occurring at different rates. Given cascade data -- observed infection times of nodes -- we infer the edges of the global diffusion network and estimate the transmission rates of each edge that best explain the observed data. The optimization problem is convex. The model naturally (without heuristics) imposes sparse solutions and requires no parameter tuning. The problem decouples into a collection of independent smaller problems, thus scaling easily to networks on the order of hundreds of thousands of nodes. Experiments on real and synthetic data show that our algorithm both recovers the edges of diffusion networks and accurately estimates their transmission rates from cascade data."},{"title":"Distributed Detection of Cycles","source":"Pierre Fraigniaud and Dennis Olivetti","docs_id":"1706.03992","section":["cs.DC"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Distributed Detection of Cycles. Distributed property testing in networks has been introduced by Brakerski and Patt-Shamir (2011), with the objective of detecting the presence of large dense sub-networks in a distributed manner. Recently, Censor-Hillel et al. (2016) have shown how to detect 3-cycles in a constant number of rounds by a distributed algorithm. In a follow up work, Fraigniaud et al. (2016) have shown how to detect 4-cycles in a constant number of rounds as well. However, the techniques in these latter works were shown not to generalize to larger cycles $C_k$ with $k\\geq 5$. In this paper, we completely settle the problem of cycle detection, by establishing the following result. For every $k\\geq 3$, there exists a distributed property testing algorithm for $C_k$-freeness, performing in a constant number of rounds. All these results hold in the classical CONGEST model for distributed network computing. Our algorithm is 1-sided error. Its round-complexity is $O(1\/\\epsilon)$ where $\\epsilon\\in(0,1)$ is the property testing parameter measuring the gap between legal and illegal instances."},{"title":"Can Your Face Detector Do Anti-spoofing? Face Presentation Attack\n  Detection with a Multi-Channel Face Detector","source":"Anjith George and Sebastien Marcel","docs_id":"2006.16836","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Can Your Face Detector Do Anti-spoofing? Face Presentation Attack\n  Detection with a Multi-Channel Face Detector. In a typical face recognition pipeline, the task of the face detector is to localize the face region. However, the face detector localizes regions that look like a face, irrespective of the liveliness of the face, which makes the entire system susceptible to presentation attacks. In this work, we try to reformulate the task of the face detector to detect real faces, thus eliminating the threat of presentation attacks. While this task could be challenging with visible spectrum images alone, we leverage the multi-channel information available from off the shelf devices (such as color, depth, and infrared channels) to design a multi-channel face detector. The proposed system can be used as a live-face detector obviating the need for a separate presentation attack detection module, making the system reliable in practice without any additional computational overhead. The main idea is to leverage a single-stage object detection framework, with a joint representation obtained from different channels for the PAD task. We have evaluated our approach in the multi-channel WMCA dataset containing a wide variety of attacks to show the effectiveness of the proposed framework."},{"title":"Sometimes, Convex Separable Optimization Is Much Harder than Linear\n  Optimization, and Other Surprises","source":"Cornelius Brand, Martin Kouteck\\'y, Alexandra Lassota, Sebastian\n  Ordyniak","docs_id":"2111.08048","section":["cs.DM","math.OC"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Sometimes, Convex Separable Optimization Is Much Harder than Linear\n  Optimization, and Other Surprises. An influential 1990 paper of Hochbaum and Shanthikumar made it common wisdom that \"convex separable optimization is not much harder than linear optimization\" [JACM 1990]. We exhibit two fundamental classes of mixed integer (linear) programs that run counter this intuition. Namely those whose constraint matrices have small coefficients and small primal or dual treedepth: While linear optimization is easy [Brand, Kouteck\\'y, Ordyniak, AAAI 2021], we prove that separable convex optimization IS much harder. Moreover, in the pure integer and mixed integer linear cases, these two classes have the same parameterized complexity. We show that they yet behave quite differently in the separable convex mixed integer case. Our approach employs the mixed Graver basis introduced by Hemmecke [Math. Prog. 2003]. We give the first non-trivial lower and upper bounds on the norm of mixed Graver basis elements. In previous works involving the integer Graver basis, such upper bounds have consistently resulted in efficient algorithms for integer programming. Curiously, this does not happen in our case. In fact, we even rule out such an algorithm."},{"title":"Explaining dimensionality reduction results using Shapley values","source":"Wilson Est\\'ecio Marc\\'ilio J\\'unior and Danilo Medeiros Eler","docs_id":"2103.05678","section":["cs.LG","cs.GR"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Explaining dimensionality reduction results using Shapley values. Dimensionality reduction (DR) techniques have been consistently supporting high-dimensional data analysis in various applications. Besides the patterns uncovered by these techniques, the interpretation of DR results based on each feature's contribution to the low-dimensional representation supports new finds through exploratory analysis. Current literature approaches designed to interpret DR techniques do not explain the features' contributions well since they focus only on the low-dimensional representation or do not consider the relationship among features. This paper presents ClusterShapley to address these problems, using Shapley values to generate explanations of dimensionality reduction techniques and interpret these algorithms using a cluster-oriented analysis. ClusterShapley explains the formation of clusters and the meaning of their relationship, which is useful for exploratory data analysis in various domains. We propose novel visualization techniques to guide the interpretation of features' contributions on clustering formation and validate our methodology through case studies of publicly available datasets. The results demonstrate our approach's interpretability and analysis power to generate insights about pathologies and patients in different conditions using DR results."},{"title":"On the minimal ranks of matrix pencils and the existence of a best\n  approximate block-term tensor decomposition","source":"Jos\\'e Henrique de Morais Goulart, Pierre Comon","docs_id":"1712.05742","section":["math.NA","cs.NA","math.AG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"On the minimal ranks of matrix pencils and the existence of a best\n  approximate block-term tensor decomposition. Under the action of the general linear group with tensor structure, the ranks of matrices $A$ and $B$ forming an $m \\times n$ pencil $A + \\lambda B$ can change, but in a restricted manner. Specifically, with every pencil one can associate a pair of minimal ranks, which is unique up to a permutation. This notion can be defined for matrix pencils and, more generally, also for matrix polynomials of arbitrary degree. In this paper, we provide a formal definition of the minimal ranks, discuss its properties and the natural hierarchy it induces in a pencil space. Then, we show how the minimal ranks of a pencil can be determined from its Kronecker canonical form. For illustration, we classify the orbits according to their minimal ranks (under the action of the general linear group) in the case of real pencils with $m, n \\le 4$. Subsequently, we show that real regular $2k \\times 2k$ pencils having only complex-valued eigenvalues, which form an open positive-volume set, do not admit a best approximation (in the norm topology) on the set of real pencils whose minimal ranks are bounded by $2k-1$. Our results can be interpreted from a tensor viewpoint, where the minimal ranks of a degree-$(d-1)$ matrix polynomial characterize the minimal ranks of matrices constituting a block-term decomposition of an $m \\times n \\times d$ tensor into a sum of matrix-vector tensor products."},{"title":"A Dataset and Benchmark for Large-scale Multi-modal Face Anti-spoofing","source":"Shifeng Zhang, Xiaobo Wang, Ajian Liu, Chenxu Zhao, Jun Wan, Sergio\n  Escalera, Hailin Shi, Zezheng Wang, Stan Z. Li","docs_id":"1812.00408","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A Dataset and Benchmark for Large-scale Multi-modal Face Anti-spoofing. Face anti-spoofing is essential to prevent face recognition systems from a security breach. Much of the progresses have been made by the availability of face anti-spoofing benchmark datasets in recent years. However, existing face anti-spoofing benchmarks have limited number of subjects ($\\le\\negmedspace170$) and modalities ($\\leq\\negmedspace2$), which hinder the further development of the academic community. To facilitate face anti-spoofing research, we introduce a large-scale multi-modal dataset, namely CASIA-SURF, which is the largest publicly available dataset for face anti-spoofing in terms of both subjects and visual modalities. Specifically, it consists of $1,000$ subjects with $21,000$ videos and each sample has $3$ modalities (i.e., RGB, Depth and IR). We also provide a measurement set, evaluation protocol and training\/validation\/testing subsets, developing a new benchmark for face anti-spoofing. Moreover, we present a new multi-modal fusion method as baseline, which performs feature re-weighting to select the more informative channel features while suppressing the less useful ones for each modal. Extensive experiments have been conducted on the proposed dataset to verify its significance and generalization capability. The dataset is available at https:\/\/sites.google.com\/qq.com\/chalearnfacespoofingattackdete"},{"title":"EM-GAN: Fast Stress Analysis for Multi-Segment Interconnect Using\n  Generative Adversarial Networks","source":"Wentian Jin, Sheriff Sadiqbatcha, Jinwei Zhang, Sheldon X.-D. Tan","docs_id":"2004.13181","section":["cs.LG","cs.NE","eess.IV","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"EM-GAN: Fast Stress Analysis for Multi-Segment Interconnect Using\n  Generative Adversarial Networks. In this paper, we propose a fast transient hydrostatic stress analysis for electromigration (EM) failure assessment for multi-segment interconnects using generative adversarial networks (GANs). Our work leverages the image synthesis feature of GAN-based generative deep neural networks. The stress evaluation of multi-segment interconnects, modeled by partial differential equations, can be viewed as time-varying 2D-images-to-image problem where the input is the multi-segment interconnects topology with current densities and the output is the EM stress distribution in those wire segments at the given aging time. Based on this observation, we train conditional GAN model using the images of many self-generated multi-segment wires and wire current densities and aging time (as conditions) against the COMSOL simulation results. Different hyperparameters of GAN were studied and compared. The proposed algorithm, called {\\it EM-GAN}, can quickly give accurate stress distribution of a general multi-segment wire tree for a given aging time, which is important for full-chip fast EM failure assessment. Our experimental results show that the EM-GAN shows 6.6\\% averaged error compared to COMSOL simulation results with orders of magnitude speedup. It also delivers 8.3X speedup over state-of-the-art analytic based EM analysis solver."},{"title":"Multi-Agent Adversarial Inverse Reinforcement Learning","source":"Lantao Yu, Jiaming Song, Stefano Ermon","docs_id":"1907.13220","section":["cs.LG","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Multi-Agent Adversarial Inverse Reinforcement Learning. Reinforcement learning agents are prone to undesired behaviors due to reward mis-specification. Finding a set of reward functions to properly guide agent behaviors is particularly challenging in multi-agent scenarios. Inverse reinforcement learning provides a framework to automatically acquire suitable reward functions from expert demonstrations. Its extension to multi-agent settings, however, is difficult due to the more complex notions of rational behaviors. In this paper, we propose MA-AIRL, a new framework for multi-agent inverse reinforcement learning, which is effective and scalable for Markov games with high-dimensional state-action space and unknown dynamics. We derive our algorithm based on a new solution concept and maximum pseudolikelihood estimation within an adversarial reward learning framework. In the experiments, we demonstrate that MA-AIRL can recover reward functions that are highly correlated with ground truth ones, and significantly outperforms prior methods in terms of policy imitation."},{"title":"Unsplittable coverings in the plane","source":"J\\'anos Pach and D\\\"om\\\"ot\\\"or P\\'alv\\\"olgyi","docs_id":"1310.6900","section":["math.MG","cs.DM","math.CO"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Unsplittable coverings in the plane. A system of sets forms an {\\em $m$-fold covering} of a set $X$ if every point of $X$ belongs to at least $m$ of its members. A $1$-fold covering is called a {\\em covering}. The problem of splitting multiple coverings into several coverings was motivated by classical density estimates for {\\em sphere packings} as well as by the {\\em planar sensor cover problem}. It has been the prevailing conjecture for 35 years (settled in many special cases) that for every plane convex body $C$, there exists a constant $m=m(C)$ such that every $m$-fold covering of the plane with translates of $C$ splits into $2$ coverings. In the present paper, it is proved that this conjecture is false for the unit disk. The proof can be generalized to construct, for every $m$, an unsplittable $m$-fold covering of the plane with translates of any open convex body $C$ which has a smooth boundary with everywhere {\\em positive curvature}. Somewhat surprisingly, {\\em unbounded} open convex sets $C$ do not misbehave, they satisfy the conjecture: every $3$-fold covering of any region of the plane by translates of such a set $C$ splits into two coverings. To establish this result, we prove a general coloring theorem for hypergraphs of a special type: {\\em shift-chains}. We also show that there is a constant $c>0$ such that, for any positive integer $m$, every $m$-fold covering of a region with unit disks splits into two coverings, provided that every point is covered by {\\em at most} $c2^{m\/2}$ sets."},{"title":"Avoiding Kernel Fixed Points: Computing with ELU and GELU Infinite\n  Networks","source":"Russell Tsuchida, Tim Pearce, Chris van der Heide, Fred Roosta, Marcus\n  Gallagher","docs_id":"2002.08517","section":["cs.LG","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Avoiding Kernel Fixed Points: Computing with ELU and GELU Infinite\n  Networks. Analysing and computing with Gaussian processes arising from infinitely wide neural networks has recently seen a resurgence in popularity. Despite this, many explicit covariance functions of networks with activation functions used in modern networks remain unknown. Furthermore, while the kernels of deep networks can be computed iteratively, theoretical understanding of deep kernels is lacking, particularly with respect to fixed-point dynamics. Firstly, we derive the covariance functions of multi-layer perceptrons (MLPs) with exponential linear units (ELU) and Gaussian error linear units (GELU) and evaluate the performance of the limiting Gaussian processes on some benchmarks. Secondly, and more generally, we analyse the fixed-point dynamics of iterated kernels corresponding to a broad range of activation functions. We find that unlike some previously studied neural network kernels, these new kernels exhibit non-trivial fixed-point dynamics which are mirrored in finite-width neural networks. The fixed point behaviour present in some networks explains a mechanism for implicit regularisation in overparameterised deep models. Our results relate to both the static iid parameter conjugate kernel and the dynamic neural tangent kernel constructions. Software at github.com\/RussellTsuchida\/ELU_GELU_kernels."},{"title":"Robust Technique for Representative Volume Element Identification in\n  Noisy Microtomography Images of Porous Materials Based on Pores Morphology\n  and Their Spatial Distribution","source":"Maxim Grigoriev, Anvar Khafizov, Vladislav Kokhan, Viktor Asadchikov","docs_id":"2007.03035","section":["physics.comp-ph","cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Robust Technique for Representative Volume Element Identification in\n  Noisy Microtomography Images of Porous Materials Based on Pores Morphology\n  and Their Spatial Distribution. Microtomography is a powerful method of materials investigation. It enables to obtain physical properties of porous media non-destructively that is useful in studies. One of the application ways is a calculation of porosity, pore sizes, surface area, and other parameters of metal-ceramic (cermet) membranes which are widely spread in the filtration industry. The microtomography approach is efficient because all of those parameters are calculated simultaneously in contrast to the conventional techniques. Nevertheless, the calculations on Micro-CT reconstructed images appear to be time-consuming, consequently representative volume element should be chosen to speed them up. This research sheds light on representative elementary volume identification without consideration of any physical parameters such as porosity, etc. Thus, the volume element could be found even in noised and grayscale images. The proposed method is flexible and does not overestimate the volume size in the case of anisotropic samples. The obtained volume element could be used for computations of the domain's physical characteristics if the image is filtered and binarized, or for selections of optimal filtering parameters for denoising procedure."},{"title":"Learning Long-Range Perception Using Self-Supervision from Short-Range\n  Sensors and Odometry","source":"Mirko Nava, Jerome Guzzi, R. Omar Chavez-Garcia, Luca M. Gambardella,\n  Alessandro Giusti","docs_id":"1809.07207","section":["cs.RO"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Learning Long-Range Perception Using Self-Supervision from Short-Range\n  Sensors and Odometry. We introduce a general self-supervised approach to predict the future outputs of a short-range sensor (such as a proximity sensor) given the current outputs of a long-range sensor (such as a camera); we assume that the former is directly related to some piece of information to be perceived (such as the presence of an obstacle in a given position), whereas the latter is information-rich but hard to interpret directly. We instantiate and implement the approach on a small mobile robot to detect obstacles at various distances using the video stream of the robot's forward-pointing camera, by training a convolutional neural network on automatically-acquired datasets. We quantitatively evaluate the quality of the predictions on unseen scenarios, qualitatively evaluate robustness to different operating conditions, and demonstrate usage as the sole input of an obstacle-avoidance controller. We additionally instantiate the approach on a different simulated scenario with complementary characteristics, to exemplify the generality of our contribution."},{"title":"Neural Descriptor Fields: SE(3)-Equivariant Object Representations for\n  Manipulation","source":"Anthony Simeonov, Yilun Du, Andrea Tagliasacchi, Joshua B. Tenenbaum,\n  Alberto Rodriguez, Pulkit Agrawal, Vincent Sitzmann","docs_id":"2112.05124","section":["cs.RO","cs.AI","cs.CV","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Neural Descriptor Fields: SE(3)-Equivariant Object Representations for\n  Manipulation. We present Neural Descriptor Fields (NDFs), an object representation that encodes both points and relative poses between an object and a target (such as a robot gripper or a rack used for hanging) via category-level descriptors. We employ this representation for object manipulation, where given a task demonstration, we want to repeat the same task on a new object instance from the same category. We propose to achieve this objective by searching (via optimization) for the pose whose descriptor matches that observed in the demonstration. NDFs are conveniently trained in a self-supervised fashion via a 3D auto-encoding task that does not rely on expert-labeled keypoints. Further, NDFs are SE(3)-equivariant, guaranteeing performance that generalizes across all possible 3D object translations and rotations. We demonstrate learning of manipulation tasks from few (5-10) demonstrations both in simulation and on a real robot. Our performance generalizes across both object instances and 6-DoF object poses, and significantly outperforms a recent baseline that relies on 2D descriptors. Project website: https:\/\/yilundu.github.io\/ndf\/."},{"title":"Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE,\n  and node2vec","source":"Jiezhong Qiu, Yuxiao Dong, Hao Ma, Jian Li, Kuansan Wang, Jie Tang","docs_id":"1710.02971","section":["cs.SI","cs.LG","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE,\n  and node2vec. Since the invention of word2vec, the skip-gram model has significantly advanced the research of network embedding, such as the recent emergence of the DeepWalk, LINE, PTE, and node2vec approaches. In this work, we show that all of the aforementioned models with negative sampling can be unified into the matrix factorization framework with closed forms. Our analysis and proofs reveal that: (1) DeepWalk empirically produces a low-rank transformation of a network's normalized Laplacian matrix; (2) LINE, in theory, is a special case of DeepWalk when the size of vertices' context is set to one; (3) As an extension of LINE, PTE can be viewed as the joint factorization of multiple networks' Laplacians; (4) node2vec is factorizing a matrix related to the stationary distribution and transition probability tensor of a 2nd-order random walk. We further provide the theoretical connections between skip-gram based network embedding algorithms and the theory of graph Laplacian. Finally, we present the NetMF method as well as its approximation algorithm for computing network embedding. Our method offers significant improvements over DeepWalk and LINE for conventional network mining tasks. This work lays the theoretical foundation for skip-gram based network embedding methods, leading to a better understanding of latent network representation learning."},{"title":"Investigation of Practical Aspects of Single Channel Speech Separation\n  for ASR","source":"Jian Wu, Zhuo Chen, Sanyuan Chen, Yu Wu, Takuya Yoshioka, Naoyuki\n  Kanda, Shujie Liu, Jinyu Li","docs_id":"2107.01922","section":["eess.AS","cs.SD"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Investigation of Practical Aspects of Single Channel Speech Separation\n  for ASR. Speech separation has been successfully applied as a frontend processing module of conversation transcription systems thanks to its ability to handle overlapped speech and its flexibility to combine with downstream tasks such as automatic speech recognition (ASR). However, a speech separation model often introduces target speech distortion, resulting in a sub-optimum word error rate (WER). In this paper, we describe our efforts to improve the performance of a single channel speech separation system. Specifically, we investigate a two-stage training scheme that firstly applies a feature level optimization criterion for pretraining, followed by an ASR-oriented optimization criterion using an end-to-end (E2E) speech recognition model. Meanwhile, to keep the model light-weight, we introduce a modified teacher-student learning technique for model compression. By combining those approaches, we achieve a absolute average WER improvement of 2.70% and 0.77% using models with less than 10M parameters compared with the previous state-of-the-art results on the LibriCSS dataset for utterance-wise evaluation and continuous evaluation, respectively"},{"title":"Towards Affordance Prediction with Vision via Task Oriented Grasp\n  Quality Metrics","source":"Luca Cavalli, Gianpaolo Di Pietro, Matteo Matteucci","docs_id":"1907.04761","section":["cs.RO","cs.CV","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Towards Affordance Prediction with Vision via Task Oriented Grasp\n  Quality Metrics. While many quality metrics exist to evaluate the quality of a grasp by itself, no clear quantification of the quality of a grasp relatively to the task the grasp is used for has been defined yet. In this paper we propose a framework to extend the concept of grasp quality metric to task-oriented grasping by defining affordance functions via basic grasp metrics for an open set of task affordances. We evaluate both the effectivity of the proposed task oriented metrics and their practical applicability by learning to infer them from vision. Indeed, we assess the validity of our novel framework both in the context of perfect information, i.e., known object model, and in the partial information context, i.e., inferring task oriented metrics from vision, underlining advantages and limitations of both situations. In the former, physical metrics of grasp hypotheses on an object are defined and computed in known object model simulation, in the latter deep models are trained to infer such properties from partial information in the form of synthesized range images."},{"title":"Deep Learning Assisted Heuristic Tree Search for the Container\n  Pre-marshalling Problem","source":"Andr\\'e Hottung, Shunji Tanaka, Kevin Tierney","docs_id":"1709.09972","section":["cs.AI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Deep Learning Assisted Heuristic Tree Search for the Container\n  Pre-marshalling Problem. The container pre-marshalling problem (CPMP) is concerned with the re-ordering of containers in container terminals during off-peak times so that containers can be quickly retrieved when the port is busy. The problem has received significant attention in the literature and is addressed by a large number of exact and heuristic methods. Existing methods for the CPMP heavily rely on problem-specific components (e.g., proven lower bounds) that need to be developed by domain experts with knowledge of optimization techniques and a deep understanding of the problem at hand. With the goal to automate the costly and time-intensive design of heuristics for the CPMP, we propose a new method called Deep Learning Heuristic Tree Search (DLTS). It uses deep neural networks to learn solution strategies and lower bounds customized to the CPMP solely through analyzing existing (near-) optimal solutions to CPMP instances. The networks are then integrated into a tree search procedure to decide which branch to choose next and to prune the search tree. DLTS produces the highest quality heuristic solutions to the CPMP to date with gaps to optimality below 2% on real-world sized instances."},{"title":"Feature selection or extraction decision process for clustering using\n  PCA and FRSD","source":"Jean-Sebastien Dessureault, Daniel Massicotte","docs_id":"2111.10492","section":["cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Feature selection or extraction decision process for clustering using\n  PCA and FRSD. This paper concerns the critical decision process of extracting or selecting the features before applying a clustering algorithm. It is not obvious to evaluate the importance of the features since the most popular methods to do it are usually made for a supervised learning technique process. A clustering algorithm is an unsupervised method. It means that there is no known output label to match the input data. This paper proposes a new method to choose the best dimensionality reduction method (selection or extraction) according to the data scientist's parameters, aiming to apply a clustering process at the end. It uses Feature Ranking Process Based on Silhouette Decomposition (FRSD) algorithm, a Principal Component Analysis (PCA) algorithm, and a K-Means algorithm along with its metric, the Silhouette Index (SI). This paper presents 5 use cases based on a smart city dataset. This research also aims to discuss the impacts, the advantages, and the disadvantages of each choice that can be made in this unsupervised learning process."},{"title":"Jiffy: A Lock-free Skip List with Batch Updates and Snapshots","source":"Tadeusz Kobus, Maciej Kokoci\\'nski, Pawe{\\l} T. Wojciechowski","docs_id":"2102.01044","section":["cs.DS"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Jiffy: A Lock-free Skip List with Batch Updates and Snapshots. In this paper we introduce Jiffy, the first lock-free, linearizable ordered key-value index that offers both (1) batch updates, which are put and remove operations that are executed atomically, and (2) consistent snapshots used by, e.g., range scan operations. Jiffy is built as a multiversioned lock-free skip list and relies on CPU's Time Stamp Counter register to generate version numbers at minimal cost. For faster skip list traversals and better utilization of the CPU caches, key-value entries are grouped into immutable objects called revisions. Moreover, by changing the size of revisions and thus modifying the synchronization granularity, our index can adapt to varying contentions levels (smaller revisions are more suited for write-heavy workloads whereas large revisions benefit read-dominated workloads, especially when they feature many range scan operations). Structure modifications to the index, which result in changing the size of revisions, happen through (lock-free) skip list node split and merge operations that are carefully coordinated with the update operations. Despite rich semantics, Jiffy offers highly scalable performance, which is comparable or exceeds the performance of the state-of-the-art lock-free ordered indices that feature linearizable range scan operations. Compared to its (lock-based) rivals that also support batch updates, Jiffy can execute large batch updates up to 7.4x more efficiently."},{"title":"Makespan Minimization via Posted Prices","source":"Michal Feldman, Amos Fiat, Alan Roytman","docs_id":"1705.01965","section":["cs.GT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Makespan Minimization via Posted Prices. We consider job scheduling settings, with multiple machines, where jobs arrive online and choose a machine selfishly so as to minimize their cost. Our objective is the classic makespan minimization objective, which corresponds to the completion time of the last job to complete. The incentives of the selfish jobs may lead to poor performance. To reconcile the differing objectives, we introduce posted machine prices. The selfish job seeks to minimize the sum of its completion time on the machine and the posted price for the machine. Prices may be static (i.e., set once and for all before any arrival) or dynamic (i.e., change over time), but they are determined only by the past, assuming nothing about upcoming events. Obviously, such schemes are inherently truthful. We consider the competitive ratio: the ratio between the makespan achievable by the pricing scheme and that of the optimal algorithm. We give tight bounds on the competitive ratio for both dynamic and static pricing schemes for identical, restricted, related, and unrelated machine settings. Our main result is a dynamic pricing scheme for related machines that gives a constant competitive ratio, essentially matching the competitive ratio of online algorithms for this setting. In contrast, dynamic pricing gives poor performance for unrelated machines. This lower bound also exhibits a gap between what can be achieved by pricing versus what can be achieved by online algorithms."},{"title":"Reproducibility Study: Comparing Rewinding and Fine-tuning in Neural\n  Network Pruning","source":"Szymon Mikler (Uniwersytet Wroc{\\l}awski)","docs_id":"2109.09670","section":["cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Reproducibility Study: Comparing Rewinding and Fine-tuning in Neural\n  Network Pruning. Scope of reproducibility: We are reproducing Comparing Rewinding and Fine-tuning in Neural Networks from arXiv:2003.02389. In this work the authors compare three different approaches to retraining neural networks after pruning: 1) fine-tuning, 2) rewinding weights as in arXiv:1803.03635 and 3) a new, original method involving learning rate rewinding, building upon Lottery Ticket Hypothesis. We reproduce the results of all three approaches, but we focus on verifying their approach, learning rate rewinding, since it is newly proposed and is described as a universal alternative to other methods. We used CIFAR10 for most reproductions along with additional experiments on the larger CIFAR100, which extends the results originally provided by the authors. We have also extended the list of tested network architectures to include Wide ResNets. The new experiments led us to discover the limitations of learning rate rewinding which can worsen pruning results on large architectures. Results: We were able to reproduce the exact results reported by the authors in all originally reported scenarios. However, extended results on larger Wide Residual Networks have demonstrated the limitations of the newly proposed learning rate rewinding -- we observed a previously unreported accuracy degradation for low sparsity ranges. Nevertheless, the general conclusion of the paper still holds and was indeed reproduced."},{"title":"Efficient and Fair Collaborative Mobile Internet Access","source":"George Iosifidis and Lin Gao and Jianwei Huang and Leandros Tassiulas","docs_id":"1612.05129","section":["cs.NI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Efficient and Fair Collaborative Mobile Internet Access. The surging global mobile data traffic challenges the economic viability of cellular networks and calls for innovative solutions to reduce the network congestion and improve user experience. In this context, user-provided networks (UPNs), where mobile users share their Internet access by exploiting their diverse network resources and needs, turn out to be very promising. Heterogeneous users with advanced handheld devices can form connections in a distributed fashion and unleash dormant network resources at the network edge. However, the success of such services heavily depends on users' willingness to contribute their resources, such as network access and device battery energy. In this paper, we introduce a general framework for UPN services and design a bargaining-based distributed incentive mechanism to ensure users participation. The proposed mechanism determines the resources that each user should contribute in order to maximise the aggregate data rate in UPN, and fairly allocate the benefit among the users. The numerical results verify that the service can always improve performance, and such improvement increases with the diversity of the users' resources. Quantitatively, it can reach an average 30% increase of the total served traffic for a typical scenario even with only 6 mobile users."},{"title":"Dense Registration and Mosaicking of Fingerprints by Training an\n  End-to-End Network","source":"Zhe Cui, Jianjiang Feng, Jie Zhou","docs_id":"2004.05972","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Dense Registration and Mosaicking of Fingerprints by Training an\n  End-to-End Network. Dense registration of fingerprints is a challenging task due to elastic skin distortion, low image quality, and self-similarity of ridge pattern. To overcome the limitation of handcraft features, we propose to train an end-to-end network to directly output pixel-wise displacement field between two fingerprints. The proposed network includes a siamese network for feature embedding, and a following encoder-decoder network for regressing displacement field. By applying displacement fields reliably estimated by tracing high quality fingerprint videos to challenging fingerprints, we synthesize a large number of training fingerprint pairs with ground truth displacement fields. In addition, based on the proposed registration algorithm, we propose a fingerprint mosaicking method based on optimal seam selection. Registration and matching experiments on FVC2004 databases, Tsinghua Distorted Fingerprint (TDF) database, and NIST SD27 latent fingerprint database show that our registration method outperforms previous dense registration methods in accuracy and efficiency. Mosaicking experiment on FVC2004 DB1 demonstrates that the proposed algorithm produced higher quality fingerprints than other algorithms which also validates the performance of our registration algorithm."},{"title":"Highly intensive data dissemination in complex networks","source":"Gabriele D'Angelo, Stefano Ferretti","docs_id":"1507.08417","section":["cs.NI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Highly intensive data dissemination in complex networks. This paper presents a study on data dissemination in unstructured Peer-to-Peer (P2P) network overlays. The absence of a structure in unstructured overlays eases the network management, at the cost of non-optimal mechanisms to spread messages in the network. Thus, dissemination schemes must be employed that allow covering a large portion of the network with a high probability (e.g.~gossip based approaches). We identify principal metrics, provide a theoretical model and perform the assessment evaluation using a high performance simulator that is based on a parallel and distributed architecture. A main point of this study is that our simulation model considers implementation technical details, such as the use of caching and Time To Live (TTL) in message dissemination, that are usually neglected in simulations, due to the additional overhead they cause. Outcomes confirm that these technical details have an important influence on the performance of dissemination schemes and that the studied schemes are quite effective to spread information in P2P overlay networks, whatever their topology. Moreover, the practical usage of such dissemination mechanisms requires a fine tuning of many parameters, the choice between different network topologies and the assessment of behaviors such as free riding. All this can be done only using efficient simulation tools to support both the network design phase and, in some cases, at runtime."},{"title":"Epidemiological data challenges: planning for a more robust future\n  through data standards","source":"Geoffrey Fairchild (1), Byron Tasseff (1), Hari Khalsa (1), Nicholas\n  Generous (2), Ashlynn R. Daughton (1), Nileena Velappan (3), Reid Priedhorsky\n  (4), Alina Deshpande (3) ((1) Analytics, Intelligence, and Technology\n  Division, Los Alamos National Laboratory, Los Alamos, New Mexico, USA, (2)\n  Intelligence and Emerging Threats Program Office, Los Alamos National\n  Laboratory, Los Alamos, New Mexico, USA, (3) Bioscience Division, Los Alamos\n  National Laboratory, Los Alamos, New Mexico, USA, (4) High Performance\n  Computing Division, Los Alamos National Laboratory, Los Alamos, New Mexico,\n  USA)","docs_id":"1805.00445","section":["cs.CY","cs.IR"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Epidemiological data challenges: planning for a more robust future\n  through data standards. Accessible epidemiological data are of great value for emergency preparedness and response, understanding disease progression through a population, and building statistical and mechanistic disease models that enable forecasting. The status quo, however, renders acquiring and using such data difficult in practice. In many cases, a primary way of obtaining epidemiological data is through the internet, but the methods by which the data are presented to the public often differ drastically among institutions. As a result, there is a strong need for better data sharing practices. This paper identifies, in detail and with examples, the three key challenges one encounters when attempting to acquire and use epidemiological data: 1) interfaces, 2) data formatting, and 3) reporting. These challenges are used to provide suggestions and guidance for improvement as these systems evolve in the future. If these suggested data and interface recommendations were adhered to, epidemiological and public health analysis, modeling, and informatics work would be significantly streamlined, which can in turn yield better public health decision-making capabilities."},{"title":"WeText: Scene Text Detection under Weak Supervision","source":"Shangxuan Tian, Shijian Lu and Chongshou Li","docs_id":"1710.04826","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"WeText: Scene Text Detection under Weak Supervision. The requiring of large amounts of annotated training data has become a common constraint on various deep learning systems. In this paper, we propose a weakly supervised scene text detection method (WeText) that trains robust and accurate scene text detection models by learning from unannotated or weakly annotated data. With a \"light\" supervised model trained on a small fully annotated dataset, we explore semi-supervised and weakly supervised learning on a large unannotated dataset and a large weakly annotated dataset, respectively. For the unsupervised learning, the light supervised model is applied to the unannotated dataset to search for more character training samples, which are further combined with the small annotated dataset to retrain a superior character detection model. For the weakly supervised learning, the character searching is guided by high-level annotations of words\/text lines that are widely available and also much easier to prepare. In addition, we design an unified scene character detector by adapting regression based deep networks, which greatly relieves the error accumulation issue that widely exists in most traditional approaches. Extensive experiments across different unannotated and weakly annotated datasets show that the scene text detection performance can be clearly boosted under both scenarios, where the weakly supervised learning can achieve the state-of-the-art performance by using only 229 fully annotated scene text images."},{"title":"Optimal Control Theory in Intelligent Transportation Systems Research -\n  A Review","source":"Jimmy SJ. Ren, Wei Wang, Stephen Shaoyi Liao","docs_id":"1304.3778","section":["cs.SY","cs.NI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Optimal Control Theory in Intelligent Transportation Systems Research -\n  A Review. Continuous motorization and urbanization around the globe leads to an expansion of population in major cities. Therefore, ever-growing pressure imposed on the existing mass transit systems calls for a better technology, Intelligent Transportation Systems (ITS), to solve many new and demanding management issues. Many studies in the extant ITS literature attempted to address these issues within which various research methodologies were adopted. However, there is very few paper summarized what does optimal control theory (OCT), one of the sharpest tools to tackle management issues in engineering, do in solving these issues. It{\\textquoteright}s both important and interesting to answer the following two questions. (1) How does OCT contribute to ITS research objectives? (2) What are the research gaps and possible future research directions? We searched 11 top transportation and control journals and reviewed 41 research articles in ITS area in which OCT was used as the main research methodology. We categorized the articles by four different ways to address our research questions. We can conclude from the review that OCT is widely used to address various aspects of management issues in ITS within which a large portion of the studies aimed to reduce traffic congestion. We also critically discussed these studies and pointed out some possible future research directions towards which OCT can be used."},{"title":"New Results for the Complexity of Resilience for Binary Conjunctive\n  Queries with Self-Joins","source":"Cibele Freire, Wolfgang Gatterbauer, Neil Immerman, Alexandra Meliou","docs_id":"1907.01129","section":["cs.DB","cs.CC"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"New Results for the Complexity of Resilience for Binary Conjunctive\n  Queries with Self-Joins. The resilience of a Boolean query is the minimum number of tuples that need to be deleted from the input tables in order to make the query false. A solution to this problem immediately translates into a solution for the more widely known problem of deletion propagation with source-side effects. In this paper, we give several novel results on the hardness of the resilience problem for $\\textit{binary conjunctive queries with self-joins}$ (i.e. conjunctive queries with relations of maximal arity 2) with one repeated relation. Unlike in the self-join free case, the concept of triad is not enough to fully characterize the complexity of resilience. We identify new structural properties, namely chains, confluences and permutations, which lead to various $NP$-hardness results. We also give novel involved reductions to network flow to show certain cases are in $P$. Overall, we give a dichotomy result for the restricted setting when one relation is repeated at most 2 times, and we cover many of the cases for 3. Although restricted, our results provide important insights into the problem of self-joins that we hope can help solve the general case of all conjunctive queries with self-joins in the future."},{"title":"Structure-aware Person Image Generation with Pose Decomposition and\n  Semantic Correlation","source":"Jilin Tang, Yi Yuan, Tianjia Shao, Yong Liu, Mengmeng Wang, Kun Zhou","docs_id":"2102.02972","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Structure-aware Person Image Generation with Pose Decomposition and\n  Semantic Correlation. In this paper we tackle the problem of pose guided person image generation, which aims to transfer a person image from the source pose to a novel target pose while maintaining the source appearance. Given the inefficiency of standard CNNs in handling large spatial transformation, we propose a structure-aware flow based method for high-quality person image generation. Specifically, instead of learning the complex overall pose changes of human body, we decompose the human body into different semantic parts (e.g., head, torso, and legs) and apply different networks to predict the flow fields for these parts separately. Moreover, we carefully design the network modules to effectively capture the local and global semantic correlations of features within and among the human parts respectively. Extensive experimental results show that our method can generate high-quality results under large pose discrepancy and outperforms state-of-the-art methods in both qualitative and quantitative comparisons."},{"title":"Entity-Switched Datasets: An Approach to Auditing the In-Domain\n  Robustness of Named Entity Recognition Models","source":"Oshin Agarwal, Yinfei Yang, Byron C. Wallace, Ani Nenkova","docs_id":"2004.04123","section":["cs.CL"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Entity-Switched Datasets: An Approach to Auditing the In-Domain\n  Robustness of Named Entity Recognition Models. Named entity recognition systems perform well on standard datasets comprising English news. But given the paucity of data, it is difficult to draw conclusions about the robustness of systems with respect to recognizing a diverse set of entities. We propose a method for auditing the in-domain robustness of systems, focusing specifically on differences in performance due to the national origin of entities. We create entity-switched datasets, in which named entities in the original texts are replaced by plausible named entities of the same type but of different national origin. We find that state-of-the-art systems' performance vary widely even in-domain: In the same context, entities from certain origins are more reliably recognized than entities from elsewhere. Systems perform best on American and Indian entities, and worst on Vietnamese and Indonesian entities. This auditing approach can facilitate the development of more robust named entity recognition systems, and will allow research in this area to consider fairness criteria that have received heightened attention in other predictive technology work."},{"title":"A Formal Comparison of Approaches to Datatype-Generic Programming","source":"Jos\\'e Pedro Magalh\\~aes, Andres L\\\"oh","docs_id":"1202.2920","section":["cs.PL"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A Formal Comparison of Approaches to Datatype-Generic Programming. Datatype-generic programming increases program abstraction and reuse by making functions operate uniformly across different types. Many approaches to generic programming have been proposed over the years, most of them for Haskell, but recently also for dependently typed languages such as Agda. Different approaches vary in expressiveness, ease of use, and implementation techniques. Some work has been done in comparing the different approaches informally. However, to our knowledge there have been no attempts to formally prove relations between different approaches. We thus present a formal comparison of generic programming libraries. We show how to formalise different approaches in Agda, including a coinductive representation, and then establish theorems that relate the approaches to each other. We provide constructive proofs of inclusion of one approach in another that can be used to convert between approaches, helping to reduce code duplication across different libraries. Our formalisation also helps in providing a clear picture of the potential of each approach, especially in relating different generic views and their expressiveness."},{"title":"Relightable 3D Head Portraits from a Smartphone Video","source":"Artem Sevastopolsky, Savva Ignatiev, Gonzalo Ferrer, Evgeny Burnaev,\n  Victor Lempitsky","docs_id":"2012.09963","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Relightable 3D Head Portraits from a Smartphone Video. In this work, a system for creating a relightable 3D portrait of a human head is presented. Our neural pipeline operates on a sequence of frames captured by a smartphone camera with the flash blinking (flash-no flash sequence). A coarse point cloud reconstructed via structure-from-motion software and multi-view denoising is then used as a geometric proxy. Afterwards, a deep rendering network is trained to regress dense albedo, normals, and environmental lighting maps for arbitrary new viewpoints. Effectively, the proxy geometry and the rendering network constitute a relightable 3D portrait model, that can be synthesized from an arbitrary viewpoint and under arbitrary lighting, e.g. directional light, point light, or an environment map. The model is fitted to the sequence of frames with human face-specific priors that enforce the plausibility of albedo-lighting decomposition and operates at the interactive frame rate. We evaluate the performance of the method under varying lighting conditions and at the extrapolated viewpoints and compare with existing relighting methods."},{"title":"Eignets for function approximation on manifolds","source":"H. N. Mhaskar","docs_id":"0909.5000","section":["cs.LG","cs.NA","cs.NE"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Eignets for function approximation on manifolds. Let $\\XX$ be a compact, smooth, connected, Riemannian manifold without boundary, $G:\\XX\\times\\XX\\to \\RR$ be a kernel. Analogous to a radial basis function network, an eignet is an expression of the form $\\sum_{j=1}^M a_jG(\\circ,y_j)$, where $a_j\\in\\RR$, $y_j\\in\\XX$, $1\\le j\\le M$. We describe a deterministic, universal algorithm for constructing an eignet for approximating functions in $L^p(\\mu;\\XX)$ for a general class of measures $\\mu$ and kernels $G$. Our algorithm yields linear operators. Using the minimal separation amongst the centers $y_j$ as the cost of approximation, we give modulus of smoothness estimates for the degree of approximation by our eignets, and show by means of a converse theorem that these are the best possible for every \\emph{individual function}. We also give estimates on the coefficients $a_j$ in terms of the norm of the eignet. Finally, we demonstrate that if any sequence of eignets satisfies the optimal estimates for the degree of approximation of a smooth function, measured in terms of the minimal separation, then the derivatives of the eignets also approximate the corresponding derivatives of the target function in an optimal manner."},{"title":"A Hybrid Model for Combining Neural Image Caption and k-Nearest Neighbor\n  Approach for Image Captioning","source":"Kartik Arora, Ajul Raj, Arun Goel, Seba Susan","docs_id":"2105.03826","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A Hybrid Model for Combining Neural Image Caption and k-Nearest Neighbor\n  Approach for Image Captioning. A hybrid model is proposed that integrates two popular image captioning methods to generate a text-based summary describing the contents of the image. The two image captioning models are the Neural Image Caption (NIC) and the k-nearest neighbor approach. These are trained individually on the training set. We extract a set of five features, from the validation set, for evaluating the results of the two models that in turn is used to train a logistic regression classifier. The BLEU-4 scores of the two models are compared for generating the binary-value ground truth for the logistic regression classifier. For the test set, the input images are first passed separately through the two models to generate the individual captions. The five-dimensional feature set extracted from the two models is passed to the logistic regression classifier to take a decision regarding the final caption generated which is the best of two captions generated by the models. Our implementation of the k-nearest neighbor model achieves a BLEU-4 score of 15.95 and the NIC model achieves a BLEU-4 score of 16.01, on the benchmark Flickr8k dataset. The proposed hybrid model is able to achieve a BLEU-4 score of 18.20 proving the validity of our approach."},{"title":"Online Multiobjective Minimax Optimization and Applications","source":"Georgy Noarov, Mallesh Pai, Aaron Roth","docs_id":"2108.03837","section":["cs.LG","cs.DS","cs.GT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Online Multiobjective Minimax Optimization and Applications. We introduce a simple but general online learning framework, in which at every round, an adaptive adversary introduces a new game, consisting of an action space for the learner, an action space for the adversary, and a vector valued objective function that is convex-concave in every coordinate. The learner and the adversary then play in this game. The learner's goal is to play so as to minimize the maximum coordinate of the cumulative vector-valued loss. The resulting one-shot game is not convex-concave, and so the minimax theorem does not apply. Nevertheless, we give a simple algorithm that can compete with the setting in which the adversary must announce their action first, with optimally diminishing regret. We demonstrate the power of our simple framework by using it to derive optimal bounds and algorithms across a variety of domains. This includes no regret learning: we can recover optimal algorithms and bounds for minimizing external regret, internal regret, adaptive regret, multigroup regret, subsequence regret, and a notion of regret in the sleeping experts setting. Next, we use it to derive a variant of Blackwell's Approachability Theorem, which we term \"Fast Polytope Approachability\". Finally, we are able to recover recently derived algorithms and bounds for online adversarial multicalibration and related notions (mean-conditioned moment multicalibration, and prediction interval multivalidity)."},{"title":"The Dynamics of Group Codes: Dual Abelian Group Codes and Systems","source":"G. David Forney Jr. and Mitchell D. Trott","docs_id":"cs\/0408038","section":["cs.IT","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"The Dynamics of Group Codes: Dual Abelian Group Codes and Systems. Fundamental results concerning the dynamics of abelian group codes (behaviors) and their duals are developed. Duals of sequence spaces over locally compact abelian groups may be defined via Pontryagin duality; dual group codes are orthogonal subgroups of dual sequence spaces. The dual of a complete code or system is finite, and the dual of a Laurent code or system is (anti-)Laurent. If C and C^\\perp are dual codes, then the state spaces of C act as the character groups of the state spaces of C^\\perp. The controllability properties of C are the observability properties of C^\\perp. In particular, C is (strongly) controllable if and only if C^\\perp is (strongly) observable, and the controller memory of C is the observer memory of C^\\perp. The controller granules of C act as the character groups of the observer granules of C^\\perp. Examples of minimal observer-form encoder and syndrome-former constructions are given. Finally, every observer granule of C is an \"end-around\" controller granule of C."},{"title":"MRI to FDG-PET: Cross-Modal Synthesis Using 3D U-Net For Multi-Modal\n  Alzheimer's Classification","source":"Apoorva Sikka, Skand Vishwanath Peri, Deepti.R.Bathula","docs_id":"1807.10111","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"MRI to FDG-PET: Cross-Modal Synthesis Using 3D U-Net For Multi-Modal\n  Alzheimer's Classification. Recent studies suggest that combined analysis of Magnetic resonance imaging~(MRI) that measures brain atrophy and positron emission tomography~(PET) that quantifies hypo-metabolism provides improved accuracy in diagnosing Alzheimer's disease. However, such techniques are limited by the availability of corresponding scans of each modality. Current work focuses on a cross-modal approach to estimate FDG-PET scans for the given MR scans using a 3D U-Net architecture. The use of the complete MR image instead of a local patch based approach helps in capturing non-local and non-linear correlations between MRI and PET modalities. The quality of the estimated PET scans is measured using quantitative metrics such as MAE, PSNR and SSIM. The efficacy of the proposed method is evaluated in the context of Alzheimer's disease classification. The accuracy using only MRI is 70.18% while joint classification using synthesized PET and MRI is 74.43% with a p-value of $0.06$. The significant improvement in diagnosis demonstrates the utility of the synthesized PET scans for multi-modal analysis."},{"title":"qDSA: Small and Secure Digital Signatures with Curve-based\n  Diffie--Hellman Key Pairs","source":"Joost Renes, Benjamin Smith (GRACE, LIX)","docs_id":"1709.03358","section":["cs.CR"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"qDSA: Small and Secure Digital Signatures with Curve-based\n  Diffie--Hellman Key Pairs. qDSA is a high-speed, high-security signature scheme that facilitates implementations with a very small memory footprint, a crucial requirement for embedded systems and IoT devices, and that uses the same public keys as modern Diffie--Hellman schemes based on Montgomery curves (such as Curve25519) or Kummer surfaces. qDSA resembles an adaptation of EdDSA to the world of Kummer varieties, which are quotients of algebraic groups by $\\pm$1. Interestingly, qDSA does not require any full group operations or point recovery: all computations, including signature verification, occur on the quotient where there is no group law. We include details on four implementations of qDSA, using Montgomery and fast Kummer surface arithmetic on the 8-bit AVR ATmega and 32-bit ARM Cortex M0 platforms. We find that qDSA significantly outperforms state-of-the-art signature implementations in terms of stack usage and code size. We also include an efficient compression algorithm for points on fast Kummer surfaces, reducing them to the same size as compressed elliptic curve points for the same security level."},{"title":"Predicting the Reproducibility of Social and Behavioral Science Papers\n  Using Supervised Learning Models","source":"Jian Wu, Rajal Nivargi, Sree Sai Teja Lanka, Arjun Manoj Menon, Sai\n  Ajay Modukuri, Nishanth Nakshatri, Xin Wei, Zhuoer Wang, James Caverlee,\n  Sarah M. Rajtmajer, C. Lee Giles","docs_id":"2104.04580","section":["cs.DL","cs.AI","cs.CL","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Predicting the Reproducibility of Social and Behavioral Science Papers\n  Using Supervised Learning Models. In recent years, significant effort has been invested verifying the reproducibility and robustness of research claims in social and behavioral sciences (SBS), much of which has involved resource-intensive replication projects. In this paper, we investigate prediction of the reproducibility of SBS papers using machine learning methods based on a set of features. We propose a framework that extracts five types of features from scholarly work that can be used to support assessments of reproducibility of published research claims. Bibliometric features, venue features, and author features are collected from public APIs or extracted using open source machine learning libraries with customized parsers. Statistical features, such as p-values, are extracted by recognizing patterns in the body text. Semantic features, such as funding information, are obtained from public APIs or are extracted using natural language processing models. We analyze pairwise correlations between individual features and their importance for predicting a set of human-assessed ground truth labels. In doing so, we identify a subset of 9 top features that play relatively more important roles in predicting the reproducibility of SBS papers in our corpus. Results are verified by comparing performances of 10 supervised predictive classifiers trained on different sets of features."},{"title":"On the Complexity of Best Arm Identification in Multi-Armed Bandit\n  Models","source":"Emilie Kaufmann (SEQUEL, LTCI), Olivier Capp\\'e (LTCI), Aur\\'elien\n  Garivier (IMT)","docs_id":"1407.4443","section":["stat.ML","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"On the Complexity of Best Arm Identification in Multi-Armed Bandit\n  Models. The stochastic multi-armed bandit model is a simple abstraction that has proven useful in many different contexts in statistics and machine learning. Whereas the achievable limit in terms of regret minimization is now well known, our aim is to contribute to a better understanding of the performance in terms of identifying the m best arms. We introduce generic notions of complexity for the two dominant frameworks considered in the literature: fixed-budget and fixed-confidence settings. In the fixed-confidence setting, we provide the first known distribution-dependent lower bound on the complexity that involves information-theoretic quantities and holds when m is larger than 1 under general assumptions. In the specific case of two armed-bandits, we derive refined lower bounds in both the fixed-confidence and fixed-budget settings, along with matching algorithms for Gaussian and Bernoulli bandit models. These results show in particular that the complexity of the fixed-budget setting may be smaller than the complexity of the fixed-confidence setting, contradicting the familiar behavior observed when testing fully specified alternatives. In addition, we also provide improved sequential stopping rules that have guaranteed error probabilities and shorter average running times. The proofs rely on two technical results that are of independent interest : a deviation lemma for self-normalized sums (Lemma 19) and a novel change of measure inequality for bandit models (Lemma 1)."},{"title":"On Learning Discrete Graphical Models Using Greedy Methods","source":"Ali Jalali and Chris Johnson and Pradeep Ravikumar","docs_id":"1107.3258","section":["cs.LG","math.ST","stat.ML","stat.TH"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"On Learning Discrete Graphical Models Using Greedy Methods. In this paper, we address the problem of learning the structure of a pairwise graphical model from samples in a high-dimensional setting. Our first main result studies the sparsistency, or consistency in sparsity pattern recovery, properties of a forward-backward greedy algorithm as applied to general statistical models. As a special case, we then apply this algorithm to learn the structure of a discrete graphical model via neighborhood estimation. As a corollary of our general result, we derive sufficient conditions on the number of samples n, the maximum node-degree d and the problem size p, as well as other conditions on the model parameters, so that the algorithm recovers all the edges with high probability. Our result guarantees graph selection for samples scaling as n = Omega(d^2 log(p)), in contrast to existing convex-optimization based algorithms that require a sample complexity of \\Omega(d^3 log(p)). Further, the greedy algorithm only requires a restricted strong convexity condition which is typically milder than irrepresentability assumptions. We corroborate these results using numerical simulations at the end."},{"title":"A Probabilistic Approach to Knowledge Translation","source":"Shangpu Jiang, Daniel Lowd, Dejing Dou","docs_id":"1507.03181","section":["cs.AI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A Probabilistic Approach to Knowledge Translation. In this paper, we focus on a novel knowledge reuse scenario where the knowledge in the source schema needs to be translated to a semantically heterogeneous target schema. We refer to this task as \"knowledge translation\" (KT). Unlike data translation and transfer learning, KT does not require any data from the source or target schema. We adopt a probabilistic approach to KT by representing the knowledge in the source schema, the mapping between the source and target schemas, and the resulting knowledge in the target schema all as probability distributions, specially using Markov random fields and Markov logic networks. Given the source knowledge and mappings, we use standard learning and inference algorithms for probabilistic graphical models to find an explicit probability distribution in the target schema that minimizes the Kullback-Leibler divergence from the implicit distribution. This gives us a compact probabilistic model that represents knowledge from the source schema as well as possible, respecting the uncertainty in both the source knowledge and the mapping. In experiments on both propositional and relational domains, we find that the knowledge obtained by KT is comparable to other approaches that require data, demonstrating that knowledge can be reused without data."},{"title":"Enhancing Visual Fashion Recommendations with Users in the Loop","source":"Anurag Bhardwaj, Vignesh Jagadeesh, Wei Di, Robinson Piramuthu,\n  Elizabeth Churchill","docs_id":"1405.4013","section":["cs.HC"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Enhancing Visual Fashion Recommendations with Users in the Loop. We describe a completely automated large scale visual recommendation system for fashion. Existing approaches have primarily relied on purely computational models to solving this problem that ignore the role of users in the system. In this paper, we propose to overcome this limitation by incorporating a user-centric design of visual fashion recommendations. Specifically, we propose a technique that augments 'user preferences' in models by exploiting elasticity in fashion choices. We further design a user study on these choices and gather results from the 'wisdom of crowd' for deeper analysis. Our key insights learnt through these results suggest that fashion preferences when constrained to a particular class, contain important behavioral signals that are often ignored in recommendation design. Further, presence of such classes also reflect strong correlations to visual perception which can be utilized to provide aesthetically pleasing user experiences. Finally, we illustrate that user approval of visual fashion recommendations can be substantially improved by carefully incorporating these user-centric feedback into the system framework."},{"title":"NLPExplorer: Exploring the Universe of NLP Papers","source":"Monarch Parmar, Naman Jain, Pranjali Jain, P Jayakrishna Sahit, Soham\n  Pachpande, Shruti Singh and Mayank Singh","docs_id":"1910.07351","section":["cs.IR","cs.DL"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"NLPExplorer: Exploring the Universe of NLP Papers. Understanding the current research trends, problems, and their innovative solutions remains a bottleneck due to the ever-increasing volume of scientific articles. In this paper, we propose NLPExplorer, a completely automatic portal for indexing, searching, and visualizing Natural Language Processing (NLP) research volume. NLPExplorer presents interesting insights from papers, authors, venues, and topics. In contrast to previous topic modelling based approaches, we manually curate five course-grained non-exclusive topical categories namely Linguistic Target (Syntax, Discourse, etc.), Tasks (Tagging, Summarization, etc.), Approaches (unsupervised, supervised, etc.), Languages (English, Chinese,etc.) and Dataset types (news, clinical notes, etc.). Some of the novel features include a list of young popular authors, popular URLs, and datasets, a list of topically diverse papers and recent popular papers. Also, it provides temporal statistics such as yearwise popularity of topics, datasets, and seminal papers. To facilitate future research and system development, we make all the processed datasets accessible through API calls. The current system is available at http:\/\/nlpexplorer.org."},{"title":"NVIDIA NeMo Neural Machine Translation Systems for English-German and\n  English-Russian News and Biomedical Tasks at WMT21","source":"Sandeep Subramanian, Oleksii Hrinchuk, Virginia Adams, Oleksii\n  Kuchaiev","docs_id":"2111.08634","section":["cs.CL","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"NVIDIA NeMo Neural Machine Translation Systems for English-German and\n  English-Russian News and Biomedical Tasks at WMT21. This paper provides an overview of NVIDIA NeMo's neural machine translation systems for the constrained data track of the WMT21 News and Biomedical Shared Translation Tasks. Our news task submissions for English-German (En-De) and English-Russian (En-Ru) are built on top of a baseline transformer-based sequence-to-sequence model. Specifically, we use a combination of 1) checkpoint averaging 2) model scaling 3) data augmentation with backtranslation and knowledge distillation from right-to-left factorized models 4) finetuning on test sets from previous years 5) model ensembling 6) shallow fusion decoding with transformer language models and 7) noisy channel re-ranking. Additionally, our biomedical task submission for English-Russian uses a biomedically biased vocabulary and is trained from scratch on news task data, medically relevant text curated from the news task dataset, and biomedical data provided by the shared task. Our news system achieves a sacreBLEU score of 39.5 on the WMT'20 En-De test set outperforming the best submission from last year's task of 38.8. Our biomedical task Ru-En and En-Ru systems reach BLEU scores of 43.8 and 40.3 respectively on the WMT'20 Biomedical Task Test set, outperforming the previous year's best submissions."},{"title":"Two betweenness centrality measures based on Randomized Shortest Paths","source":"Ilkka Kivim\\\"aki, Bertrand Lebichot, Jari Saram\\\"aki, Marco Saerens","docs_id":"1509.03147","section":["cs.SI","cs.DS","physics.soc-ph"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Two betweenness centrality measures based on Randomized Shortest Paths. This paper introduces two new closely related betweenness centrality measures based on the Randomized Shortest Paths (RSP) framework, which fill a gap between traditional network centrality measures based on shortest paths and more recent methods considering random walks or current flows. The framework defines Boltzmann probability distributions over paths of the network which focus on the shortest paths, but also take into account longer paths depending on an inverse temperature parameter. RSP's have previously proven to be useful in defining distance measures on networks. In this work we study their utility in quantifying the importance of the nodes of a network. The proposed RSP betweenness centralities combine, in an optimal way, the ideas of using the shortest and purely random paths for analysing the roles of network nodes, avoiding issues involving these two paradigms. We present the derivations of these measures and how they can be computed in an efficient way. In addition, we show with real world examples the potential of the RSP betweenness centralities in identifying interesting nodes of a network that more traditional methods might fail to notice."},{"title":"Discovering Differential Features: Adversarial Learning for Information\n  Credibility Evaluation","source":"Lianwei Wu, Yuan Rao, Ambreen Nazir, Haolin Jin","docs_id":"1909.07523","section":["cs.CY","cs.CL","cs.LG","cs.SI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Discovering Differential Features: Adversarial Learning for Information\n  Credibility Evaluation. A series of deep learning approaches extract a large number of credibility features to detect fake news on the Internet. However, these extracted features still suffer from many irrelevant and noisy features that restrict severely the performance of the approaches. In this paper, we propose a novel model based on Adversarial Networks and inspirited by the Shared-Private model (ANSP), which aims at reducing common, irrelevant features from the extracted features for information credibility evaluation. Specifically, ANSP involves two tasks: one is to prevent the binary classification of true and false information for capturing common features relying on adversarial networks guided by reinforcement learning. Another extracts credibility features (henceforth, private features) from multiple types of credibility information and compares with the common features through two strategies, i.e., orthogonality constraints and KL-divergence for making the private features more differential. Experiments first on two six-label LIAR and Weibo datasets demonstrate that ANSP achieves the state-of-the-art performance, boosting the accuracy by 2.1%, 3.1%, respectively and then on four-label Twitter16 validate the robustness of the model with 1.8% performance improvements."},{"title":"Complementary-Similarity Learning using Quadruplet Network","source":"Mansi Ranjit Mane, Stephen Guo, Kannan Achan","docs_id":"1908.09928","section":["cs.LG","cs.IR","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Complementary-Similarity Learning using Quadruplet Network. We propose a novel learning framework to answer questions such as \"if a user is purchasing a shirt, what other items will (s)he need with the shirt?\" Our framework learns distributed representations for items from available textual data, with the learned representations representing items in a latent space expressing functional complementarity as well similarity. In particular, our framework places functionally similar items close together in the latent space, while also placing complementary items closer than non-complementary items, but farther away than similar items. In this study, we introduce a new dataset of similar, complementary, and negative items derived from the Amazon co-purchase dataset. For evaluation purposes, we focus our approach on clothing and fashion verticals. As per our knowledge, this is the first attempt to learn similar and complementary relationships simultaneously through just textual title metadata. Our framework is applicable across a broad set of items in the product catalog and can generate quality complementary item recommendations at scale."},{"title":"Pairing heaps: the forward variant","source":"Dani Dorfman, Haim Kaplan, L\\'aszl\\'o Kozma, Uri Zwick","docs_id":"1709.01152","section":["cs.DS"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Pairing heaps: the forward variant. The pairing heap is a classical heap data structure introduced in 1986 by Fredman, Sedgewick, Sleator, and Tarjan. It is remarkable both for its simplicity and for its excellent performance in practice. The \"magic\" of pairing heaps lies in the restructuring that happens after the deletion of the smallest item. The resulting collection of trees is consolidated in two rounds: a left-to-right pairing round, followed by a right-to-left accumulation round. Fredman et al. showed, via an elegant correspondence to splay trees, that in a pairing heap of size $n$ all operations take $O(\\log{n})$ amortized time. They also proposed an arguably more natural variant, where both pairing and accumulation are performed in a combined left-to-right round (called the forward variant of pairing heaps). The analogy to splaying breaks down in this case, and the analysis of the forward variant was left open. In this paper we show that inserting an item and deleting the minimum in a forward-variant pairing heap both take amortized time $O(\\log{n} \\cdot 4^{\\sqrt{\\log{n}}} )$. This is the first improvement over the $O(\\sqrt{n})$ bound showed by Fredman et al. three decades ago. Our analysis relies on a new potential function that tracks parent-child rank-differences in the heap."},{"title":"Sketch-a-Net that Beats Humans","source":"Qian Yu, Yongxin Yang, Yi-Zhe Song, Tao Xiang and Timothy Hospedales","docs_id":"1501.07873","section":["cs.CV","cs.NE"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Sketch-a-Net that Beats Humans. We propose a multi-scale multi-channel deep neural network framework that, for the first time, yields sketch recognition performance surpassing that of humans. Our superior performance is a result of explicitly embedding the unique characteristics of sketches in our model: (i) a network architecture designed for sketch rather than natural photo statistics, (ii) a multi-channel generalisation that encodes sequential ordering in the sketching process, and (iii) a multi-scale network ensemble with joint Bayesian fusion that accounts for the different levels of abstraction exhibited in free-hand sketches. We show that state-of-the-art deep networks specifically engineered for photos of natural objects fail to perform well on sketch recognition, regardless whether they are trained using photo or sketch. Our network on the other hand not only delivers the best performance on the largest human sketch dataset to date, but also is small in size making efficient training possible using just CPUs."},{"title":"Learning Efficient Structured Sparse Models","source":"Alex Bronstein (Tel Aviv University), Pablo Sprechmann (University of\n  Minnesota), Guillermo Sapiro (University of Minnesota)","docs_id":"1206.4649","section":["cs.LG","cs.CV","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Learning Efficient Structured Sparse Models. We present a comprehensive framework for structured sparse coding and modeling extending the recent ideas of using learnable fast regressors to approximate exact sparse codes. For this purpose, we develop a novel block-coordinate proximal splitting method for the iterative solution of hierarchical sparse coding problems, and show an efficient feed forward architecture derived from its iteration. This architecture faithfully approximates the exact structured sparse codes with a fraction of the complexity of the standard optimization methods. We also show that by using different training objective functions, learnable sparse encoders are no longer restricted to be mere approximants of the exact sparse code for a pre-given dictionary, as in earlier formulations, but can be rather used as full-featured sparse encoders or even modelers. A simple implementation shows several orders of magnitude speedup compared to the state-of-the-art at minimal performance degradation, making the proposed framework suitable for real time and large-scale applications."},{"title":"Jointly Pre-training with Supervised, Autoencoder, and Value Losses for\n  Deep Reinforcement Learning","source":"Gabriel V. de la Cruz Jr. and Yunshu Du and Matthew E. Taylor","docs_id":"1904.02206","section":["cs.LG","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Jointly Pre-training with Supervised, Autoencoder, and Value Losses for\n  Deep Reinforcement Learning. Deep Reinforcement Learning (DRL) algorithms are known to be data inefficient. One reason is that a DRL agent learns both the feature and the policy tabula rasa. Integrating prior knowledge into DRL algorithms is one way to improve learning efficiency since it helps to build helpful representations. In this work, we consider incorporating human knowledge to accelerate the asynchronous advantage actor-critic (A3C) algorithm by pre-training a small amount of non-expert human demonstrations. We leverage the supervised autoencoder framework and propose a novel pre-training strategy that jointly trains a weighted supervised classification loss, an unsupervised reconstruction loss, and an expected return loss. The resulting pre-trained model learns more useful features compared to independently training in supervised or unsupervised fashion. Our pre-training method drastically improved the learning performance of the A3C agent in Atari games of Pong and MsPacman, exceeding the performance of the state-of-the-art algorithms at a much smaller number of game interactions. Our method is light-weight and easy to implement in a single machine. For reproducibility, our code is available at github.com\/gabrieledcjr\/DeepRL\/tree\/A3C-ALA2019"},{"title":"Are socially-aware trajectory prediction models really socially-aware?","source":"Saeed Saadatnejad, Mohammadhossein Bahari, Pedram Khorsandi, Mohammad\n  Saneian, Seyed-Mohsen Moosavi-Dezfooli, Alexandre Alahi","docs_id":"2108.10879","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Are socially-aware trajectory prediction models really socially-aware?. Our field has recently witnessed an arms race of neural network-based trajectory predictors. While these predictors are at the core of many applications such as autonomous navigation or pedestrian flow simulations, their adversarial robustness has not been carefully studied. In this paper, we introduce a socially-attended attack to assess the social understanding of prediction models in terms of collision avoidance. An attack is a small yet carefully-crafted perturbations to fail predictors. Technically, we define collision as a failure mode of the output, and propose hard- and soft-attention mechanisms to guide our attack. Thanks to our attack, we shed light on the limitations of the current models in terms of their social understanding. We demonstrate the strengths of our method on the recent trajectory prediction models. Finally, we show that our attack can be employed to increase the social understanding of state-of-the-art models. The code is available online: https:\/\/s-attack.github.io\/"},{"title":"Planning and Operations of Mixed Fleets in Mobility-on-Demand Systems","source":"Kaidi Yang, Matthew W. Tsao, Xin Xu, Marco Pavone","docs_id":"2008.08131","section":["eess.SY","cs.SY"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Planning and Operations of Mixed Fleets in Mobility-on-Demand Systems. Automated vehicles (AVs) are expected to be beneficial for Mobility-on-Demand (MoD), thanks to their ability of being globally coordinated. To facilitate the steady transition towards full autonomy, we consider the transition period of AV deployment, whereby an MoD system operates a mixed fleet of automated vehicles (AVs) and human-driven vehicles (HVs). In such systems, AVs are centrally coordinated by the operator, and the HVs might strategically respond to the coordination of AVs. We devise computationally tractable strategies to coordinate mixed fleets in MoD systems. Specifically, we model an MoD system with a mixed fleet using a Stackelberg framework where the MoD operator serves as the leader and human-driven vehicles serve as the followers. We develop two models: 1) a steady-state model to analyze the properties of the problem and determine the planning variables (e.g., compensations, prices, and the fleet size of AVs), and 2) a time-varying model to design a real-time coordination algorithm for AVs. The proposed models are validated using a case study inspired by real operational data of a MoD service in Singapore. Results show that the proposed algorithms can significantly improve system performance."},{"title":"Optimal Virtualized Inter-Tenant Resource Sharing for Device-to-Device\n  Communications in 5G Networks","source":"Christoforos Vlachos, Vasilis Friderikos, Mischa Dohler","docs_id":"1606.01849","section":["cs.NI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Optimal Virtualized Inter-Tenant Resource Sharing for Device-to-Device\n  Communications in 5G Networks. Device-to-Device (D2D) communication is expected to enable a number of new services and applications in future mobile networks and has attracted significant research interest over the last few years. Remarkably, little attention has been placed on the issue of D2D communication for users belonging to different operators. In this paper, we focus on this aspect for D2D users that belong to different tenants (virtual network operators), assuming virtualized and programmable future 5G wireless networks. Under the assumption of a cross-tenant orchestrator, we show that significant gains can be achieved in terms of network performance by optimizing resource sharing from the different tenants, i.e., slices of the substrate physical network topology. To this end, a sum-rate optimization framework is proposed for optimal sharing of the virtualized resources. Via a wide site of numerical investigations, we prove the efficacy of the proposed solution and the achievable gains compared to legacy approaches."},{"title":"Gait Recognition via Effective Global-Local Feature Representation and\n  Local Temporal Aggregation","source":"Beibei Lin, Shunli Zhang and Xin Yu","docs_id":"2011.01461","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Gait Recognition via Effective Global-Local Feature Representation and\n  Local Temporal Aggregation. Gait recognition is one of the most important biometric technologies and has been applied in many fields. Recent gait recognition frameworks represent each gait frame by descriptors extracted from either global appearances or local regions of humans. However, the representations based on global information often neglect the details of the gait frame, while local region based descriptors cannot capture the relations among neighboring regions, thus reducing their discriminativeness. In this paper, we propose a novel feature extraction and fusion framework to achieve discriminative feature representations for gait recognition. Towards this goal, we take advantage of both global visual information and local region details and develop a Global and Local Feature Extractor (GLFE). Specifically, our GLFE module is composed of our newly designed multiple global and local convolutional layers (GLConv) to ensemble global and local features in a principle manner. Furthermore, we present a novel operation, namely Local Temporal Aggregation (LTA), to further preserve the spatial information by reducing the temporal resolution to obtain higher spatial resolution. With the help of our GLFE and LTA, our method significantly improves the discriminativeness of our visual features, thus improving the gait recognition performance. Extensive experiments demonstrate that our proposed method outperforms state-of-the-art gait recognition methods on two popular datasets."},{"title":"Forensic Similarity for Digital Images","source":"Owen Mayer, Matthew C. Stamm","docs_id":"1902.04684","section":["cs.CV","eess.IV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Forensic Similarity for Digital Images. In this paper we introduce a new digital image forensics approach called forensic similarity, which determines whether two image patches contain the same forensic trace or different forensic traces. One benefit of this approach is that prior knowledge, e.g. training samples, of a forensic trace are not required to make a forensic similarity decision on it in the future. To do this, we propose a two part deep-learning system composed of a CNN-based feature extractor and a three-layer neural network, called the similarity network. This system maps pairs of image patches to a score indicating whether they contain the same or different forensic traces. We evaluated system accuracy of determining whether two image patches were 1) captured by the same or different camera model, 2) manipulated by the same or different editing operation, and 3) manipulated by the same or different manipulation parameter, given a particular editing operation. Experiments demonstrate applicability to a variety of forensic traces, and importantly show efficacy on \"unknown\" forensic traces that were not used to train the system. Experiments also show that the proposed system significantly improves upon prior art, reducing error rates by more than half. Furthermore, we demonstrated the utility of the forensic similarity approach in two practical applications: forgery detection and localization, and database consistency verification."},{"title":"Active Manifolds: A non-linear analogue to Active Subspaces","source":"Robert A. Bridges, Anthony D. Gruber, Christopher Felder, Miki Verma,\n  Chelsey Hoff","docs_id":"1904.13386","section":["stat.ML","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Active Manifolds: A non-linear analogue to Active Subspaces. We present an approach to analyze $C^1(\\mathbb{R}^m)$ functions that addresses limitations present in the Active Subspaces (AS) method of Constantine et al.(2015; 2014). Under appropriate hypotheses, our Active Manifolds (AM) method identifies a 1-D curve in the domain (the active manifold) on which nearly all values of the unknown function are attained, and which can be exploited for approximation or analysis, especially when $m$ is large (high-dimensional input space). We provide theorems justifying our AM technique and an algorithm permitting functional approximation and sensitivity analysis. Using accessible, low-dimensional functions as initial examples, we show AM reduces approximation error by an order of magnitude compared to AS, at the expense of more computation. Following this, we revisit the sensitivity analysis by Glaws et al. (2017), who apply AS to analyze a magnetohydrodynamic power generator model, and compare the performance of AM on the same data. Our analysis provides detailed information not captured by AS, exhibiting the influence of each parameter individually along an active manifold. Overall, AM represents a novel technique for analyzing functional models with benefits including: reducing $m$-dimensional analysis to a 1-D analogue, permitting more accurate regression than AS (at more computational expense), enabling more informative sensitivity analysis, and granting accessible visualizations(2-D plots) of parameter sensitivity along the AM."},{"title":"URSA: A System for Uniform Reduction to SAT","source":"Predrag Janicic (University of Belgrade)","docs_id":"1012.1255","section":["cs.AI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"URSA: A System for Uniform Reduction to SAT. There are a huge number of problems, from various areas, being solved by reducing them to SAT. However, for many applications, translation into SAT is performed by specialized, problem-specific tools. In this paper we describe a new system for uniform solving of a wide class of problems by reducing them to SAT. The system uses a new specification language URSA that combines imperative and declarative programming paradigms. The reduction to SAT is defined precisely by the semantics of the specification language. The domain of the approach is wide (e.g., many NP-complete problems can be simply specified and then solved by the system) and there are problems easily solvable by the proposed system, while they can be hardly solved by using other programming languages or constraint programming systems. So, the system can be seen not only as a tool for solving problems by reducing them to SAT, but also as a general-purpose constraint solving system (for finite domains). In this paper, we also describe an open-source implementation of the described approach. The performed experiments suggest that the system is competitive to state-of-the-art related modelling systems."},{"title":"Generator Pyramid for High-Resolution Image Inpainting","source":"Leilei Cao, Tong Yang, Yixu Wang, Bo Yan, Yandong Guo","docs_id":"2012.02381","section":["cs.CV","eess.IV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Generator Pyramid for High-Resolution Image Inpainting. Inpainting high-resolution images with large holes challenges existing deep learning based image inpainting methods. We present a novel framework -- PyramidFill for high-resolution image inpainting task, which explicitly disentangles content completion and texture synthesis. PyramidFill attempts to complete the content of unknown regions in a lower-resolution image, and synthesis the textures of unknown regions in a higher-resolution image, progressively. Thus, our model consists of a pyramid of fully convolutional GANs, wherein the content GAN is responsible for completing contents in the lowest-resolution masked image, and each texture GAN is responsible for synthesizing textures in a higher-resolution image. Since completing contents and synthesising textures demand different abilities from generators, we customize different architectures for the content GAN and texture GAN. Experiments on multiple datasets including CelebA-HQ, Places2 and a new natural scenery dataset (NSHQ) with different resolutions demonstrate that PyramidFill generates higher-quality inpainting results than the state-of-the-art methods. To better assess high-resolution image inpainting methods, we will release NSHQ, high-quality natural scenery images with high-resolution 1920$\\times$1080."},{"title":"Pretext Tasks selection for multitask self-supervised speech\n  representation learning","source":"Salah Zaiem, Titouan Parcollet and Slim Essid","docs_id":"2107.00594","section":["eess.AS","cs.LG","cs.SD","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Pretext Tasks selection for multitask self-supervised speech\n  representation learning. Through solving pretext tasks, self-supervised learning leverages unlabeled data to extract useful latent representations replacing traditional input features in the downstream task. In audio\/speech signal processing, a wide range of features where engineered through decades of research efforts. As it turns out, learning to predict such features (a.k.a pseudo-labels) has proven to be a particularly relevant pretext task, leading to useful self-supervised representations which prove to be effective for downstream tasks. However, methods and common practices for combining such pretext tasks for better performance on the downstream task have not been explored and understood properly. In fact, the process relies almost exclusively on a computationally heavy experimental procedure, which becomes intractable with the increase of the number of pretext tasks. This paper introduces a method to select a group of pretext tasks among a set of candidates. The method we propose estimates calibrated weights for the partial losses corresponding to the considered pretext tasks during the self-supervised training process. The experiments conducted on automatic speech recognition, speaker and emotion recognition validate our approach, as the groups selected and weighted with our method perform better than classic baselines, thus facilitating the selection and combination of relevant pseudo-labels for self-supervised representation learning."},{"title":"Dynamic Nested Clustering for Parallel PHY-Layer Processing in\n  Cloud-RANs","source":"Congmin Fan, Ying Jun Zhang, Xiaojun Yuan","docs_id":"1408.0876","section":["cs.IT","cs.DC","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Dynamic Nested Clustering for Parallel PHY-Layer Processing in\n  Cloud-RANs. Featured by centralized processing and cloud based infrastructure, Cloud Radio Access Network (C-RAN) is a promising solution to achieve an unprecedented system capacity in future wireless cellular networks. The huge capacity gain mainly comes from the centralized and coordinated signal processing at the cloud server. However, full-scale coordination in a large-scale C-RAN requires the processing of very large channel matrices, leading to high computational complexity and channel estimation overhead. To resolve this challenge, we exploit the near-sparsity of large C-RAN channel matrices, and derive a unified theoretical framework for clustering and parallel processing. Based on the framework, we propose a dynamic nested clustering (DNC) algorithm that not only greatly improves the system scalability in terms of baseband-processing and channel-estimation complexity, but also is amenable to various parallel processing strategies for different data center architectures. With the proposed algorithm, we show that the computation time for the optimal linear detector is greatly reduced from $O(N^3)$ to no higher than $O(N^{\\frac{42}{23}})$, where $N$ is the number of RRHs in C-RAN."},{"title":"Underwater Fish Tracking for Moving Cameras based on Deformable Multiple\n  Kernels","source":"Meng-Che Chuang, Jenq-Neng Hwang, Jian-Hui Ye, Shih-Chia Huang,\n  Kresimir Williams","docs_id":"1603.01695","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Underwater Fish Tracking for Moving Cameras based on Deformable Multiple\n  Kernels. Fishery surveys that call for the use of single or multiple underwater cameras have been an emerging technology as a non-extractive mean to estimate the abundance of fish stocks. Tracking live fish in an open aquatic environment posts challenges that are different from general pedestrian or vehicle tracking in surveillance applications. In many rough habitats fish are monitored by cameras installed on moving platforms, where tracking is even more challenging due to inapplicability of background models. In this paper, a novel tracking algorithm based on the deformable multiple kernels (DMK) is proposed to address these challenges. Inspired by the deformable part model (DPM) technique, a set of kernels is defined to represent the holistic object and several parts that are arranged in a deformable configuration. Color histogram, texture histogram and the histogram of oriented gradients (HOG) are extracted and serve as object features. Kernel motion is efficiently estimated by the mean-shift algorithm on color and texture features to realize tracking. Furthermore, the HOG-feature deformation costs are adopted as soft constraints on kernel positions to maintain the part configuration. Experimental results on practical video set from underwater moving cameras show the reliable performance of the proposed method with much less computational cost comparing with state-of-the-art techniques."},{"title":"Interleaved Polar (I-Polar) Codes","source":"Mao-Ching Chiu","docs_id":"1908.00708","section":["cs.IT","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Interleaved Polar (I-Polar) Codes. By inserting interleavers between intermediate stages of the polar encoder, a new class of polar codes, termed interleaved polar (i-polar) codes, is proposed. By the uniform interleaver assumption, we derive the weight enumerating function (WEF) and input-output weight enumerating function (IOWEF) averaged over the ensemble of i-polar codes. The average WEF can be used to calculate the upper bound on the average block error rate (BLER) of a code selected at random from the ensemble of i-polar codes. Also, we propose a concatenated coding scheme that employs P high rate codes as the outer code and Q i-polar codes as the inner code with an interleaver in between. The average WEF of the concatenated code is derived based on the uniform interleaver assumption. Simulation results show that BLER upper bounds can well predict BLER performance levels of the concatenated codes. The results show that the performance of the proposed concatenated code with P=Q=2 is better than that of the CRC-aided i-polar code with P=Q=1 of the same length and code rate at high signal-to-noise ratios (SNRs). Moreover, the proposed concatenated code allows multiple decoders to operate in parallel, which can reduce the decoding latency and hence is suitable for ultra-reliable low-latency communications (URLLC)."},{"title":"An Integrated First-Order Theory of Points and Intervals over Linear\n  Orders (Part II)","source":"Willem Conradie, Salih Durhan and Guido Sciavicco","docs_id":"1809.04468","section":["cs.LO"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"An Integrated First-Order Theory of Points and Intervals over Linear\n  Orders (Part II). There are two natural and well-studied approaches to temporal ontology and reasoning: point-based and interval-based. Usually, interval-based temporal reasoning deals with points as a particular case of duration-less intervals. A recent result by Balbiani, Goranko, and Sciavicco presented an explicit two-sorted point-interval temporal framework in which time instants (points) and time periods (intervals) are considered on a par, allowing the perspective to shift between these within the formal discourse. We consider here two-sorted first-order languages based on the same principle, and therefore including relations, as first studied by Reich, among others, between points, between intervals, and inter-sort. We give complete classifications of its sub-languages in terms of relative expressive power, thus determining how many, and which, are the intrinsically different extensions of two-sorted first-order logic with one or more such relations. This approach roots out the classical problem of whether or not points should be included in a interval-based semantics. In this Part II, we deal with the cases of all dense and the case of all unbounded linearly ordered sets."},{"title":"DNN-Life: An Energy-Efficient Aging Mitigation Framework for Improving\n  the Lifetime of On-Chip Weight Memories in Deep Neural Network Hardware\n  Architectures","source":"Muhammad Abdullah Hanif, Muhammad Shafique","docs_id":"2101.12351","section":["cs.AR"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"DNN-Life: An Energy-Efficient Aging Mitigation Framework for Improving\n  the Lifetime of On-Chip Weight Memories in Deep Neural Network Hardware\n  Architectures. Negative Biased Temperature Instability (NBTI)-induced aging is one of the critical reliability threats in nano-scale devices. This paper makes the first attempt to study the NBTI aging in the on-chip weight memories of deep neural network (DNN) hardware accelerators, subjected to complex DNN workloads. We propose DNN-Life, a specialized aging analysis and mitigation framework for DNNs, which jointly exploits hardware- and software-level knowledge to improve the lifetime of a DNN weight memory with reduced energy overhead. At the software-level, we analyze the effects of different DNN quantization methods on the distribution of the bits of weight values. Based on the insights gained from this analysis, we propose a micro-architecture that employs low-cost memory-write (and read) transducers to achieve an optimal duty-cycle at run time in the weight memory cells, thereby balancing their aging. As a result, our DNN-Life framework enables efficient aging mitigation of weight memory of the given DNN hardware at minimal energy overhead during the inference process."},{"title":"Precise Null Pointer Analysis Through Global Value Numbering","source":"Ankush Das and Akash Lal","docs_id":"1702.05807","section":["cs.PL"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Precise Null Pointer Analysis Through Global Value Numbering. Precise analysis of pointer information plays an important role in many static analysis techniques and tools today. The precision, however, must be balanced against the scalability of the analysis. This paper focusses on improving the precision of standard context and flow insensitive alias analysis algorithms at a low scalability cost. In particular, we present a semantics-preserving program transformation that drastically improves the precision of existing analyses when deciding if a pointer can alias NULL. Our program transformation is based on Global Value Numbering, a scheme inspired from compiler optimizations literature. It allows even a flow-insensitive analysis to make use of branch conditions such as checking if a pointer is NULL and gain precision. We perform experiments on real-world code to measure the overhead in performing the transformation and the improvement in the precision of the analysis. We show that the precision improves from 86.56% to 98.05%, while the overhead is insignificant."},{"title":"Flexible User Mapping for Radio Resource Assignment in Advanced\n  Satellite Payloads","source":"Tom\\'as Ram\\'irez (1), Carlos Mosquera (1), Nader Alagha (2) ((1)\n  atlanTTic Research Center, Universidade de Vigo, Galicia, Spain, (2) European\n  Space Agency Technical Research Center (ESTEC), Noordwijk, The Netherlands)","docs_id":"2109.09385","section":["cs.IT","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Flexible User Mapping for Radio Resource Assignment in Advanced\n  Satellite Payloads. This work explores the flexible assignment of users to beams in order to match the non-uniform traffic demand in satellite systems, breaking the conventional cell boundaries and serving users potentially by non-dominant beams. The additional degree of freedom from the flexible beam-user mapping brings about a user-centric framework, with a two-step optimization process as a practical strategy. The smart beam-user mapping is jointly explored with adjustable bandwidth allocation per beam, and tested against different techniques for payloads with flexible radio resource allocation. Numerical results are obtained for various non-uniform traffic distributions to evaluate the performance of the solutions. The traffic profile across beams is shaped by the Dirichlet distribution, which can be conveniently parameterized, and makes simulations easily reproducible. Even with ideal conditions for the power allocation, both flexible beam-user mapping and adjustable power allocation similarly enhance the flexible assignment of the bandwidth on average. Results show that a smart pairing of users and beams provides significant advantages in highly asymmetric demand scenarios, with improvements up to 10\\% and 30\\% in terms of the offered and the minimum user rates, respectively, in hot-spot like cases with no impact on the user equipment."},{"title":"HM4: Hidden Markov Model with Memory Management for Visual Place\n  Recognition","source":"Anh-Dzung Doan, Yasir Latif, Tat-Jun Chin, Ian Reid","docs_id":"2011.00450","section":["cs.CV","cs.RO"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"HM4: Hidden Markov Model with Memory Management for Visual Place\n  Recognition. Visual place recognition needs to be robust against appearance variability due to natural and man-made causes. Training data collection should thus be an ongoing process to allow continuous appearance changes to be recorded. However, this creates an unboundedly-growing database that poses time and memory scalability challenges for place recognition methods. To tackle the scalability issue for visual place recognition in autonomous driving, we develop a Hidden Markov Model approach with a two-tiered memory management. Our algorithm, dubbed HM$^4$, exploits temporal look-ahead to transfer promising candidate images between passive storage and active memory when needed. The inference process takes into account both promising images and a coarse representations of the full database. We show that this allows constant time and space inference for a fixed coverage area. The coarse representations can also be updated incrementally to absorb new data. To further reduce the memory requirements, we derive a compact image representation inspired by Locality Sensitive Hashing (LSH). Through experiments on real world data, we demonstrate the excellent scalability and accuracy of the approach under appearance changes and provide comparisons against state-of-the-art techniques."},{"title":"Study of Distributed Conjugate Gradient Strategies for Distributed\n  Estimation Over Sensor Networks","source":"R. C. de Lamare","docs_id":"1601.04102","section":["cs.DC","cs.IT","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Study of Distributed Conjugate Gradient Strategies for Distributed\n  Estimation Over Sensor Networks. This paper presents distributed conjugate gradient algorithms for distributed parameter estimation and spectrum estimation over wireless sensor networks. In particular, distributed conventional conjugate gradient (CCG) and modified conjugate gradient (MCG) are considered, together with incremental and diffusion adaptive solutions. The distributed CCG and MCG algorithms have an improved performance in terms of mean square error as compared with least--mean square (LMS)--based algorithms and a performance that is close to recursive least--squares (RLS) algorithms. In comparison with existing centralized or distributed estimation strategies, key features of the proposed algorithms are: 1) more accurate estimates and faster convergence speed can be obtained; 2) the design of preconditioners for CG algorithms, which have the ability to improve the performance of the proposed CG algorithms is presented and 3) the proposed algorithms are implemented in the area of distributed parameter estimation and spectrum estimation. The performance of the proposed algorithms for distributed estimation is illustrated via simulations and the resulting algorithms are distributed, cooperative and able to respond in real time to change in the environment."},{"title":"Uncertainty Measurement of Basic Probability Assignment Integrity Based\n  on Approximate Entropy in Evidence Theory","source":"Tianxiang Zhan, Yuanpeng He, Hanwen Li, Fuyuan Xiao","docs_id":"2105.07382","section":["cs.AI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Uncertainty Measurement of Basic Probability Assignment Integrity Based\n  on Approximate Entropy in Evidence Theory. Evidence theory is that the extension of probability can better deal with unknowns and inaccurate information. Uncertainty measurement plays a vital role in both evidence theory and probability theory. Approximate Entropy (ApEn) is proposed by Pincus to describe the irregularities of complex systems. The more irregular the time series, the greater the approximate entropy. The ApEn of the network represents the ability of a network to generate new nodes, or the possibility of undiscovered nodes. Through the association of network characteristics and basic probability assignment (BPA) , a measure of the uncertainty of BPA regarding completeness can be obtained. The main contribution of paper is to define the integrity of the basic probability assignment then the approximate entropy of the BPA is proposed to measure the uncertainty of the integrity of the BPA. The proposed method is based on the logical network structure to calculate the uncertainty of BPA in evidence theory. The uncertainty based on the proposed method represents the uncertainty of integrity of BPA and contributes to the identification of the credibility of BPA."},{"title":"Distance Metric Learning for Kernel Machines","source":"Zhixiang Xu, Kilian Q. Weinberger, Olivier Chapelle","docs_id":"1208.3422","section":["stat.ML","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Distance Metric Learning for Kernel Machines. Recent work in metric learning has significantly improved the state-of-the-art in k-nearest neighbor classification. Support vector machines (SVM), particularly with RBF kernels, are amongst the most popular classification algorithms that uses distance metrics to compare examples. This paper provides an empirical analysis of the efficacy of three of the most popular Mahalanobis metric learning algorithms as pre-processing for SVM training. We show that none of these algorithms generate metrics that lead to particularly satisfying improvements for SVM-RBF classification. As a remedy we introduce support vector metric learning (SVML), a novel algorithm that seamlessly combines the learning of a Mahalanobis metric with the training of the RBF-SVM parameters. We demonstrate the capabilities of SVML on nine benchmark data sets of varying sizes and difficulties. In our study, SVML outperforms all alternative state-of-the-art metric learning algorithms in terms of accuracy and establishes itself as a serious alternative to the standard Euclidean metric with model selection by cross validation."},{"title":"Achievable DoF Regions of Three-User MIMO Broadcast Channel with Delayed\n  CSIT","source":"Tong Zhang, and Rui Wang","docs_id":"2001.05134","section":["cs.IT","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Achievable DoF Regions of Three-User MIMO Broadcast Channel with Delayed\n  CSIT. For the two-user multiple-input multiple-output (MIMO) broadcast channel with delayed channel state information at the transmitter (CSIT) and arbitrary antenna configurations, all the degrees-of-freedom (DoF) regions are obtained. However, for the three-user MIMO broadcast channel with delayed CSIT and arbitrary antenna configurations, the DoF region of order-2 messages is still unclear and only a partial achievable DoF region of order-1 messages is obtained, where the order-2 messages and order-1 messages are desired by two receivers and one receiver, respectively. In this paper, for the three-user MIMO broadcast channel with delayed CSIT and arbitrary antenna configurations, we first design transmission schemes for order-2 messages and order-1 messages. Next, we propose to analyze the achievable DoF region of transmission scheme by transformation approach. In particular, we transform the decoding condition of transmission scheme w.r.t. phase duration into the achievable DoF region w.r.t. achievable DoF, through achievable DoF tuple expression connecting phase duration and achievable DoF. As a result, the DoF region of order-2 messages is characterized and an achievable DoF region of order-1 messages is completely expressed. Besides, for order-1 messages, we derive the sufficient condition, under which the proposed achievable DoF region is the DoF region."},{"title":"Trimmed Moebius Inversion and Graphs of Bounded Degree","source":"Andreas Bj\\\"orklund, Thore Husfeldt, Petteri Kaski (HIIT), Mikko\n  Koivisto (HIIT)","docs_id":"0802.2834","section":["cs.DS","math.CO"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Trimmed Moebius Inversion and Graphs of Bounded Degree. We study ways to expedite Yates's algorithm for computing the zeta and Moebius transforms of a function defined on the subset lattice. We develop a trimmed variant of Moebius inversion that proceeds point by point, finishing the calculation at a subset before considering its supersets. For an $n$-element universe $U$ and a family $\\scr F$ of its subsets, trimmed Moebius inversion allows us to compute the number of packings, coverings, and partitions of $U$ with $k$ sets from $\\scr F$ in time within a polynomial factor (in $n$) of the number of supersets of the members of $\\scr F$. Relying on an intersection theorem of Chung et al. (1986) to bound the sizes of set families, we apply these ideas to well-studied combinatorial optimisation problems on graphs of maximum degree $\\Delta$. In particular, we show how to compute the Domatic Number in time within a polynomial factor of $(2^{\\Delta+1-2)^{n\/(\\Delta+1)$ and the Chromatic Number in time within a polynomial factor of $(2^{\\Delta+1-\\Delta-1)^{n\/(\\Delta+1)$. For any constant $\\Delta$, these bounds are $O\\bigl((2-\\epsilon)^n\\bigr)$ for $\\epsilon>0$ independent of the number of vertices $n$."},{"title":"Mathematical foundations of stable RKHSs","source":"Mauro Bisiacco and Gianluigi Pillonetto","docs_id":"2005.02971","section":["cs.LG","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Mathematical foundations of stable RKHSs. Reproducing kernel Hilbert spaces (RKHSs) are key spaces for machine learning that are becoming popular also for linear system identification. In particular, the so-called stable RKHSs can be used to model absolutely summable impulse responses. In combination e.g. with regularized least squares they can then be used to reconstruct dynamic systems from input-output data. In this paper we provide new structural properties of stable RKHSs. The relation between stable kernels and other fundamental classes, like those containing absolutely summable or finite-trace kernels, is elucidated. These insights are then brought into the feature space context. First, it is proved that any stable kernel admits feature maps induced by a basis of orthogonal eigenvectors in l2. The exact connection with classical system identification approaches that exploit such kind of functions to model impulse responses is also provided. Then, the necessary and sufficient stability condition for RKHSs designed by formulating kernel eigenvectors and eigenvalues is obtained. Overall, our new results provide novel mathematical foundations of stable RKHSs with impact on stability tests, impulse responses modeling and computational efficiency of regularized schemes for linear system identification."},{"title":"Competitive Safety Analysis: Robust Decision-Making in Multi-Agent\n  Systems","source":"M. Tennenholtz","docs_id":"1106.4570","section":["cs.GT","cs.AI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Competitive Safety Analysis: Robust Decision-Making in Multi-Agent\n  Systems. Much work in AI deals with the selection of proper actions in a given (known or unknown) environment. However, the way to select a proper action when facing other agents is quite unclear. Most work in AI adopts classical game-theoretic equilibrium analysis to predict agent behavior in such settings. This approach however does not provide us with any guarantee for the agent. In this paper we introduce competitive safety analysis. This approach bridges the gap between the desired normative AI approach, where a strategy should be selected in order to guarantee a desired payoff, and equilibrium analysis. We show that a safety level strategy is able to guarantee the value obtained in a Nash equilibrium, in several classical computer science settings. Then, we discuss the concept of competitive safety strategies, and illustrate its use in a decentralized load balancing setting, typical to network problems. In particular, we show that when we have many agents, it is possible to guarantee an expected payoff which is a factor of 8\/9 of the payoff obtained in a Nash equilibrium. Our discussion of competitive safety analysis for decentralized load balancing is further developed to deal with many communication links and arbitrary speeds. Finally, we discuss the extension of the above concepts to Bayesian games, and illustrate their use in a basic auctions setup."},{"title":"Adversarial Imitation via Variational Inverse Reinforcement Learning","source":"Ahmed H. Qureshi, Byron Boots and Michael C. Yip","docs_id":"1809.06404","section":["cs.LG","cs.AI","cs.RO","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Adversarial Imitation via Variational Inverse Reinforcement Learning. We consider a problem of learning the reward and policy from expert examples under unknown dynamics. Our proposed method builds on the framework of generative adversarial networks and introduces the empowerment-regularized maximum-entropy inverse reinforcement learning to learn near-optimal rewards and policies. Empowerment-based regularization prevents the policy from overfitting to expert demonstrations, which advantageously leads to more generalized behaviors that result in learning near-optimal rewards. Our method simultaneously learns empowerment through variational information maximization along with the reward and policy under the adversarial learning formulation. We evaluate our approach on various high-dimensional complex control tasks. We also test our learned rewards in challenging transfer learning problems where training and testing environments are made to be different from each other in terms of dynamics or structure. The results show that our proposed method not only learns near-optimal rewards and policies that are matching expert behavior but also performs significantly better than state-of-the-art inverse reinforcement learning algorithms."},{"title":"Grant-free Radio Access IoT Networks: Scalability Analysis in\n  Coexistence Scenarios","source":"Meysam Masoudi, Amin Azari, Emre Altug Yavuz, and Cicek Cavdar","docs_id":"1711.00581","section":["cs.IT","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Grant-free Radio Access IoT Networks: Scalability Analysis in\n  Coexistence Scenarios. IoT networks with grant-free radio access, like SigFox and LoRa, offer low-cost durable communications over unlicensed band. These networks are becoming more and more popular due to the ever-increasing need for ultra durable, in terms of battery lifetime, IoT networks. Most studies evaluate the system performance assuming single radio access technology deployment. In this paper, we study the impact of coexisting competing radio access technologies on the system performance. Considering $\\mathpzc K$ technologies, defined by time and frequency activity factors, bandwidth, and power, which share a set of radio resources, we derive closed-form expressions for the successful transmission probability, expected battery lifetime, and experienced delay as a function of distance to the serving access point. Our analytical model, which is validated by simulation results, provides a tool to evaluate the coexistence scenarios and analyze how introduction of a new coexisting technology may degrade the system performance in terms of success probability and battery lifetime. We further investigate solutions in which this destructive effect could be compensated, e.g., by densifying the network to a certain extent and utilizing joint reception."},{"title":"FairVis: Visual Analytics for Discovering Intersectional Bias in Machine\n  Learning","source":"\\'Angel Alexander Cabrera, Will Epperson, Fred Hohman, Minsuk Kahng,\n  Jamie Morgenstern, Duen Horng Chau","docs_id":"1904.05419","section":["cs.LG","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"FairVis: Visual Analytics for Discovering Intersectional Bias in Machine\n  Learning. The growing capability and accessibility of machine learning has led to its application to many real-world domains and data about people. Despite the benefits algorithmic systems may bring, models can reflect, inject, or exacerbate implicit and explicit societal biases into their outputs, disadvantaging certain demographic subgroups. Discovering which biases a machine learning model has introduced is a great challenge, due to the numerous definitions of fairness and the large number of potentially impacted subgroups. We present FairVis, a mixed-initiative visual analytics system that integrates a novel subgroup discovery technique for users to audit the fairness of machine learning models. Through FairVis, users can apply domain knowledge to generate and investigate known subgroups, and explore suggested and similar subgroups. FairVis' coordinated views enable users to explore a high-level overview of subgroup performance and subsequently drill down into detailed investigation of specific subgroups. We show how FairVis helps to discover biases in two real datasets used in predicting income and recidivism. As a visual analytics system devoted to discovering bias in machine learning, FairVis demonstrates how interactive visualization may help data scientists and the general public understand and create more equitable algorithmic systems."},{"title":"Absent Subsequences in Words","source":"Maria Kosche, Tore Ko{\\ss}, Florin Manea, Stefan Siemer","docs_id":"2108.13968","section":["cs.FL","cs.DS"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Absent Subsequences in Words. An absent factor of a string $w$ is a string $u$ which does not occur as a contiguous substring (a.k.a. factor) inside $w$. We extend this well-studied notion and define absent subsequences: a string $u$ is an absent subsequence of a string $w$ if $u$ does not occur as subsequence (a.k.a. scattered factor) inside $w$. Of particular interest to us are minimal absent subsequences, i.e., absent subsequences whose every subsequence is not absent, and shortest absent subsequences, i.e., absent subsequences of minimal length. We show a series of combinatorial and algorithmic results regarding these two notions. For instance: we give combinatorial characterisations of the sets of minimal and, respectively, shortest absent subsequences in a word, as well as compact representations of these sets; we show how we can test efficiently if a string is a shortest or minimal absent subsequence in a word, and we give efficient algorithms computing the lexicographically smallest absent subsequence of each kind; also, we show how a data structure for answering shortest absent subsequence-queries for the factors of a given string can be efficiently computed."},{"title":"How Important is Importance Sampling for Deep Budgeted Training?","source":"Eric Arazo, Diego Ortego, Paul Albert, Noel E. O'Connor, Kevin\n  McGuinness","docs_id":"2110.14283","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"How Important is Importance Sampling for Deep Budgeted Training?. Long iterative training processes for Deep Neural Networks (DNNs) are commonly required to achieve state-of-the-art performance in many computer vision tasks. Importance sampling approaches might play a key role in budgeted training regimes, i.e. when limiting the number of training iterations. These approaches aim at dynamically estimating the importance of each sample to focus on the most relevant and speed up convergence. This work explores this paradigm and how a budget constraint interacts with importance sampling approaches and data augmentation techniques. We show that under budget restrictions, importance sampling approaches do not provide a consistent improvement over uniform sampling. We suggest that, given a specific budget, the best course of action is to disregard the importance and introduce adequate data augmentation; e.g. when reducing the budget to a 30% in CIFAR-10\/100, RICAP data augmentation maintains accuracy, while importance sampling does not. We conclude from our work that DNNs under budget restrictions benefit greatly from variety in the training set and that finding the right samples to train on is not the most effective strategy when balancing high performance with low computational requirements. Source code available at https:\/\/git.io\/JKHa3 ."},{"title":"A new hybrid approach for crude oil price forecasting: Evidence from\n  multi-scale data","source":"Yang Yifan, Guo Ju'e, Sun Shaolong, and Li Yixin","docs_id":"2002.09656","section":["q-fin.ST","cs.LG","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A new hybrid approach for crude oil price forecasting: Evidence from\n  multi-scale data. Faced with the growing research towards crude oil price fluctuations influential factors following the accelerated development of Internet technology, accessible data such as Google search volume index are increasingly quantified and incorporated into forecasting approaches. In this paper, we apply multi-scale data that including both GSVI data and traditional economic data related to crude oil price as independent variables and propose a new hybrid approach for monthly crude oil price forecasting. This hybrid approach, based on divide and conquer strategy, consists of K-means method, kernel principal component analysis and kernel extreme learning machine , where K-means method is adopted to divide input data into certain clusters, KPCA is applied to reduce dimension, and KELM is employed for final crude oil price forecasting. The empirical result can be analyzed from data and method levels. At the data level, GSVI data perform better than economic data in level forecasting accuracy but with opposite performance in directional forecasting accuracy because of Herd Behavior, while hybrid data combined their advantages and obtain best forecasting performance in both level and directional accuracy. At the method level, the approaches with K-means perform better than those without K-means, which demonstrates that divide and conquer strategy can effectively improve the forecasting performance."},{"title":"Face Recognition Algorithms based on Transformed Shape Features","source":"Sambhunath Biswas and Amrita Biswas","docs_id":"1207.2537","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Face Recognition Algorithms based on Transformed Shape Features. Human face recognition is, indeed, a challenging task, especially under the illumination and pose variations. We examine in the present paper effectiveness of two simple algorithms using coiflet packet and Radon transforms to recognize human faces from some databases of still gray level images, under the environment of illumination and pose variations. Both the algorithms convert 2-D gray level training face images into their respective depth maps or physical shape which are subsequently transformed by Coiflet packet and Radon transforms to compute energy for feature extraction. Experiments show that such transformed shape features are robust to illumination and pose variations. With the features extracted, training classes are optimally separated through linear discriminant analysis (LDA), while classification for test face images is made through a k-NN classifier, based on L1 norm and Mahalanobis distance measures. Proposed algorithms are then tested on face images that differ in illumination,expression or pose separately, obtained from three databases,namely, ORL, Yale and Essex-Grimace databases. Results, so obtained, are compared with two different existing algorithms.Performance using Daubechies wavelets is also examined. It is seen that the proposed Coiflet packet and Radon transform based algorithms have significant performance, especially under different illumination conditions and pose variation. Comparison shows the proposed algorithms are superior."},{"title":"When and How Mixup Improves Calibration","source":"Linjun Zhang, Zhun Deng, Kenji Kawaguchi, James Zou","docs_id":"2102.06289","section":["cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"When and How Mixup Improves Calibration. In many machine learning applications, it is important for the model to provide confidence scores that accurately capture its prediction uncertainty. Although modern learning methods have achieved great success in predictive accuracy, generating calibrated confidence scores remains a major challenge. Mixup, a popular yet simple data augmentation technique based on taking convex combinations of pairs of training examples, has been empirically found to significantly improve confidence calibration across diverse applications. However, when and how Mixup helps calibration is still a mystery. In this paper, we theoretically prove that Mixup improves calibration in \\textit{high-dimensional} settings by investigating natural statistical models. Interestingly, the calibration benefit of Mixup increases as the model capacity increases. We support our theories with experiments on common architectures and datasets. In addition, we study how Mixup improves calibration in semi-supervised learning. While incorporating unlabeled data can sometimes make the model less calibrated, adding Mixup training mitigates this issue and provably improves calibration. Our analysis provides new insights and a framework to understand Mixup and calibration."},{"title":"Deep Cropping via Attention Box Prediction and Aesthetics Assessment","source":"Wenguan Wang and Jianbing Shen","docs_id":"1710.08014","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Deep Cropping via Attention Box Prediction and Aesthetics Assessment. We model the photo cropping problem as a cascade of attention box regression and aesthetic quality classification, based on deep learning. A neural network is designed that has two branches for predicting attention bounding box and analyzing aesthetics, respectively. The predicted attention box is treated as an initial crop window where a set of cropping candidates are generated around it, without missing important information. Then, aesthetics assessment is employed to select the final crop as the one with the best aesthetic quality. With our network, cropping candidates share features within full-image convolutional feature maps, thus avoiding repeated feature computation and leading to higher computation efficiency. Via leveraging rich data for attention prediction and aesthetics assessment, the proposed method produces high-quality cropping results, even with the limited availability of training data for photo cropping. The experimental results demonstrate the competitive results and fast processing speed (5 fps with all steps)."},{"title":"Expander Datacenters: From Theory to Practice","source":"Vipul Harsh, Sangeetha Abdu Jyothi, Inderdeep Singh, P. Brighten\n  Godfrey","docs_id":"1811.00212","section":["cs.NI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Expander Datacenters: From Theory to Practice. Recent work has shown that expander-based data center topologies are robust and can yield superior performance over Clos topologies. However, to achieve these benefits, previous proposals use routing and transport schemes that impede quick industry adoption. In this paper, we examine if expanders can be effective for the technology and environments practical in today's data centers, including the use of traditional protocols, at both small and large scale while complying with common practices such as over-subscription. We study bandwidth, latency and burst tolerance of topologies, highlighting pitfalls of previous topology comparisons. We consider several other metrics of interest: packet loss during failures, queue occupancy and topology degradation. Our experiments show that expanders can realize 3x more throughput than an equivalent fat tree, and 1.5x more throughput than an equivalent leaf-spine topology, for a wide range of scenarios, with only traditional protocols. We observe that expanders achieve lower flow completion times, are more resilient to bursty load conditions like incast and outcast and degrade more gracefully with increasing load. Our results are based on extensive simulations and experiments on a hardware testbed with realistic topologies and real traffic patterns."},{"title":"Censorship of Online Encyclopedias: Implications for NLP Models","source":"Eddie Yang, Margaret E. Roberts","docs_id":"2101.09294","section":["cs.CL","cs.AI","cs.CY","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Censorship of Online Encyclopedias: Implications for NLP Models. While artificial intelligence provides the backbone for many tools people use around the world, recent work has brought to attention that the algorithms powering AI are not free of politics, stereotypes, and bias. While most work in this area has focused on the ways in which AI can exacerbate existing inequalities and discrimination, very little work has studied how governments actively shape training data. We describe how censorship has affected the development of Wikipedia corpuses, text data which are regularly used for pre-trained inputs into NLP algorithms. We show that word embeddings trained on Baidu Baike, an online Chinese encyclopedia, have very different associations between adjectives and a range of concepts about democracy, freedom, collective action, equality, and people and historical events in China than its regularly blocked but uncensored counterpart - Chinese language Wikipedia. We examine the implications of these discrepancies by studying their use in downstream AI applications. Our paper shows how government repression, censorship, and self-censorship may impact training data and the applications that draw from them."},{"title":"Multi-Fact Correction in Abstractive Text Summarization","source":"Yue Dong, Shuohang Wang, Zhe Gan, Yu Cheng, Jackie Chi Kit Cheung and\n  Jingjing Liu","docs_id":"2010.02443","section":["cs.CL"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Multi-Fact Correction in Abstractive Text Summarization. Pre-trained neural abstractive summarization systems have dominated extractive strategies on news summarization performance, at least in terms of ROUGE. However, system-generated abstractive summaries often face the pitfall of factual inconsistency: generating incorrect facts with respect to the source text. To address this challenge, we propose Span-Fact, a suite of two factual correction models that leverages knowledge learned from question answering models to make corrections in system-generated summaries via span selection. Our models employ single or multi-masking strategies to either iteratively or auto-regressively replace entities in order to ensure semantic consistency w.r.t. the source text, while retaining the syntactic structure of summaries generated by abstractive summarization models. Experiments show that our models significantly boost the factual consistency of system-generated summaries without sacrificing summary quality in terms of both automatic metrics and human evaluation."},{"title":"Learning Music Helps You Read: Using Transfer to Study Linguistic\n  Structure in Language Models","source":"Isabel Papadimitriou and Dan Jurafsky","docs_id":"2004.14601","section":["cs.CL"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Learning Music Helps You Read: Using Transfer to Study Linguistic\n  Structure in Language Models. We propose transfer learning as a method for analyzing the encoding of grammatical structure in neural language models. We train LSTMs on non-linguistic data and evaluate their performance on natural language to assess which kinds of data induce generalizable structural features that LSTMs can use for natural language. We find that training on non-linguistic data with latent structure (MIDI music or Java code) improves test performance on natural language, despite no overlap in surface form or vocabulary. To pinpoint the kinds of abstract structure that models may be encoding to lead to this improvement, we run similar experiments with two artificial parentheses languages: one which has a hierarchical recursive structure, and a control which has paired tokens but no recursion. Surprisingly, training a model on either of these artificial languages leads to the same substantial gains when testing on natural language. Further experiments on transfer between natural languages controlling for vocabulary overlap show that zero-shot performance on a test language is highly correlated with typological syntactic similarity to the training language, suggesting that representations induced by pre-training correspond to the cross-linguistic syntactic properties. Our results provide insights into the ways that neural models represent abstract syntactic structure, and also about the kind of structural inductive biases which allow for natural language acquisition."},{"title":"Transcoded Video Restoration by Temporal Spatial Auxiliary Network","source":"Li Xu, Gang He, Jinjia Zhou, Jie Lei, Weiying Xie, Yunsong Li, Yu-Wing\n  Tai","docs_id":"2112.07948","section":["cs.CV","eess.IV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Transcoded Video Restoration by Temporal Spatial Auxiliary Network. In most video platforms, such as Youtube, and TikTok, the played videos usually have undergone multiple video encodings such as hardware encoding by recording devices, software encoding by video editing apps, and single\/multiple video transcoding by video application servers. Previous works in compressed video restoration typically assume the compression artifacts are caused by one-time encoding. Thus, the derived solution usually does not work very well in practice. In this paper, we propose a new method, temporal spatial auxiliary network (TSAN), for transcoded video restoration. Our method considers the unique traits between video encoding and transcoding, and we consider the initial shallow encoded videos as the intermediate labels to assist the network to conduct self-supervised attention training. In addition, we employ adjacent multi-frame information and propose the temporal deformable alignment and pyramidal spatial fusion for transcoded video restoration. The experimental results demonstrate that the performance of the proposed method is superior to that of the previous techniques. The code is available at https:\/\/github.com\/icecherylXuli\/TSAN."},{"title":"Hedging predictions in machine learning","source":"Alexander Gammerman and Vladimir Vovk","docs_id":"cs\/0611011","section":["cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Hedging predictions in machine learning. Recent advances in machine learning make it possible to design efficient prediction algorithms for data sets with huge numbers of parameters. This paper describes a new technique for \"hedging\" the predictions output by many such algorithms, including support vector machines, kernel ridge regression, kernel nearest neighbours, and by many other state-of-the-art methods. The hedged predictions for the labels of new objects include quantitative measures of their own accuracy and reliability. These measures are provably valid under the assumption of randomness, traditional in machine learning: the objects and their labels are assumed to be generated independently from the same probability distribution. In particular, it becomes possible to control (up to statistical fluctuations) the number of erroneous predictions by selecting a suitable confidence level. Validity being achieved automatically, the remaining goal of hedged prediction is efficiency: taking full account of the new objects' features and other available information to produce as accurate predictions as possible. This can be done successfully using the powerful machinery of modern machine learning."},{"title":"A performance study of some approximation algorithms for minimum\n  dominating set in a graph","source":"Jonathan S. Li, Rohan Potru, Farhad Shahrokhi","docs_id":"2009.04636","section":["cs.DS"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A performance study of some approximation algorithms for minimum\n  dominating set in a graph. We implement and test the performances of several approximation algorithms for computing the minimum dominating set of a graph. These algorithms are the standard greedy algorithm, the recent LP rounding algorithms and a hybrid algorithm that we design by combining the greedy and LP rounding algorithms. All algorithms perform better than anticipated in their theoretical analysis, and have small performance ratios, measured as the size of output divided by the LP objective lower-bound. However, each may have advantages over the others. For instance, LP rounding algorithm normally outperforms the other algorithms on sparse real-world graphs. On a graph with 400,000+ vertices, LP rounding took less than 15 seconds of CPU time to generate a solution with performance ratio 1.011, while the greedy and hybrid algorithms generated solutions of performance ratio 1.12 in similar time. For synthetic graphs, the hybrid algorithm normally outperforms the others, whereas for hypercubes and k-Queens graphs, greedy outperforms the rest. Another advantage of the hybrid algorithm is to solve very large problems where LP solvers crash, as demonstrated on a real-world graph with 7.7 million+ vertices."},{"title":"Evolutionary Turing in the Context of Evolutionary Machines","source":"Mark Burgin and Eugene Eberbach","docs_id":"1304.3762","section":["cs.AI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Evolutionary Turing in the Context of Evolutionary Machines. One of the roots of evolutionary computation was the idea of Turing about unorganized machines. The goal of this work is the development of foundations for evolutionary computations, connecting Turing's ideas and the contemporary state of art in evolutionary computations. To achieve this goal, we develop a general approach to evolutionary processes in the computational context, building mathematical models of computational systems, functioning of which is based on evolutionary processes, and studying properties of such systems. Operations with evolutionary machines are described and it is explored when definite classes of evolutionary machines are closed with respect to basic operations with these machines. We also study such properties as linguistic and functional equivalence of evolutionary machines and their classes, as well as computational power of evolutionary machines and their classes, comparing of evolutionary machines to conventional automata, such as finite automata or Turing machines."},{"title":"A Data Quality Metric (DQM): How to Estimate The Number of Undetected\n  Errors in Data Sets","source":"Yeounoh Chung, Sanjay Krishnan, Tim Kraska","docs_id":"1611.04878","section":["cs.DB"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A Data Quality Metric (DQM): How to Estimate The Number of Undetected\n  Errors in Data Sets. Data cleaning, whether manual or algorithmic, is rarely perfect leaving a dataset with an unknown number of false positives and false negatives after cleaning. In many scenarios, quantifying the number of remaining errors is challenging because our data integrity rules themselves may be incomplete, or the available gold-standard datasets may be too small to extrapolate. As the use of inherently fallible crowds becomes more prevalent in data cleaning problems, it is important to have estimators to quantify the extent of such errors. We propose novel species estimators to estimate the number of distinct remaining errors in a dataset after it has been cleaned by a set of crowd workers -- essentially, quantifying the utility of hiring additional workers to clean the dataset. This problem requires new estimators that are robust to false positives and false negatives, and we empirically show on three real-world datasets that existing species estimators are unstable for this problem, while our proposed techniques quickly converge."},{"title":"Transferable Representation Learning in Vision-and-Language Navigation","source":"Haoshuo Huang, Vihan Jain, Harsh Mehta, Alexander Ku, Gabriel\n  Magalhaes, Jason Baldridge, Eugene Ie","docs_id":"1908.03409","section":["cs.CV","cs.CL","cs.LG","cs.RO"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Transferable Representation Learning in Vision-and-Language Navigation. Vision-and-Language Navigation (VLN) tasks such as Room-to-Room (R2R) require machine agents to interpret natural language instructions and learn to act in visually realistic environments to achieve navigation goals. The overall task requires competence in several perception problems: successful agents combine spatio-temporal, vision and language understanding to produce appropriate action sequences. Our approach adapts pre-trained vision and language representations to relevant in-domain tasks making them more effective for VLN. Specifically, the representations are adapted to solve both a cross-modal sequence alignment and sequence coherence task. In the sequence alignment task, the model determines whether an instruction corresponds to a sequence of visual frames. In the sequence coherence task, the model determines whether the perceptual sequences are predictive sequentially in the instruction-conditioned latent space. By transferring the domain-adapted representations, we improve competitive agents in R2R as measured by the success rate weighted by path length (SPL) metric."},{"title":"Reinforcement Learning Based Optimal Camera Placement for Depth\n  Observation of Indoor Scenes","source":"Yichuan Chen and Manabu Tsukada and Hiroshi Esaki","docs_id":"2110.11106","section":["cs.CV","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Reinforcement Learning Based Optimal Camera Placement for Depth\n  Observation of Indoor Scenes. Exploring the most task-friendly camera setting -- optimal camera placement (OCP) problem -- in tasks that use multiple cameras is of great importance. However, few existing OCP solutions specialize in depth observation of indoor scenes, and most versatile solutions work offline. To this problem, an OCP online solution to depth observation of indoor scenes based on reinforcement learning is proposed in this paper. The proposed solution comprises a simulation environment that implements scene observation and reward estimation using shadow maps and an agent network containing a soft actor-critic (SAC)-based reinforcement learning backbone and a feature extractor to extract features from the observed point cloud layer-by-layer. Comparative experiments with two state-of-the-art optimization-based offline methods are conducted. The experimental results indicate that the proposed system outperforms seven out of ten test scenes in obtaining lower depth observation error. The total error in all test scenes is also less than 90% of the baseline ones. Therefore, the proposed system is more competent for depth camera placement in scenarios where there is no prior knowledge of the scenes or where a lower depth observation error is the main objective."},{"title":"Reliable Prediction of Channel Assignment Performance in Wireless Mesh\n  Networks","source":"Srikant Manas Kala, Ranadheer Musham, M Pavan Kumar Reddy, and\n  Bheemarjuna Reddy Tamma","docs_id":"1508.03605","section":["cs.NI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Reliable Prediction of Channel Assignment Performance in Wireless Mesh\n  Networks. The advancements in wireless mesh networks (WMN), and the surge in multi-radio multi-channel (MRMC) WMN deployments have spawned a multitude of network performance issues. These issues are intricately linked to the adverse impact of endemic interference. Thus, interference mitigation is a primary design objective in WMNs. Interference alleviation is often effected through efficient channel allocation (CA) schemes which fully utilize the potential of MRMC environment and also restrain the detrimental impact of interference. However, numerous CA schemes have been proposed in research literature and there is a lack of CA performance prediction techniques which could assist in choosing a suitable CA for a given WMN. In this work, we propose a reliable interference estimation and CA performance prediction approach. We demonstrate its efficacy by substantiating the CA performance predictions for a given WMN with experimental data obtained through rigorous simulations on an ns-3 802.11g environment."},{"title":"The KB paradigm and its application to interactive configuration","source":"Pieter Van Hertum, Ingmar Dasseville, Gerda Janssens, Marc Denecker","docs_id":"1605.01846","section":["cs.AI","cs.LO"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"The KB paradigm and its application to interactive configuration. The knowledge base paradigm aims to express domain knowledge in a rich formal language, and to use this domain knowledge as a knowledge base to solve various problems and tasks that arise in the domain by applying multiple forms of inference. As such, the paradigm applies a strict separation of concerns between information and problem solving. In this paper, we analyze the principles and feasibility of the knowledge base paradigm in the context of an important class of applications: interactive configuration problems. In interactive configuration problems, a configuration of interrelated objects under constraints is searched, where the system assists the user in reaching an intended configuration. It is widely recognized in industry that good software solutions for these problems are very difficult to develop. We investigate such problems from the perspective of the KB paradigm. We show that multiple functionalities in this domain can be achieved by applying different forms of logical inferences on a formal specification of the configuration domain. We report on a proof of concept of this approach in a real-life application with a banking company. To appear in Theory and Practice of Logic Programming (TPLP)."},{"title":"Adaptive guaranteed-performance consensus design for high-order\n  multiagent systems","source":"Jianxiang Xi, Jie Yang, Hao Liu, Tang Zheng","docs_id":"1806.09757","section":["cs.MA"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Adaptive guaranteed-performance consensus design for high-order\n  multiagent systems. The current paper addresses the distributed guaranteed-performance consensus design problems for general high-order linear multiagent systems with leaderless and leader-follower structures, respectively. The information about the Laplacian matrix of the interaction topology or its minimum nonzero eigenvalue is usually required in existing works on the guaranteed-performance consensus, which means that their conclusions are not completely distributed. A new translation-adaptive strategy is proposed to realize the completely distributed guaranteed-performance consensus control by using the structure feature of a complete graph in the current paper. For the leaderless case, an adaptive guaranteed-performance consensualization criterion is given in terms of Riccati inequalities and a regulation approach of the consensus control gain is presented by linear matrix inequalities. Extensions to the leader-follower cases are further investigated. Especially, the guaranteed-performance costs for leaderless and leader-follower cases are determined, respectively, which are associated with the intrinsic structure characteristic of the interaction topologies. Finally, two numerical examples are provided to demonstrate theoretical results."},{"title":"RandLA-Net: Efficient Semantic Segmentation of Large-Scale Point Clouds","source":"Qingyong Hu, Bo Yang, Linhai Xie, Stefano Rosa, Yulan Guo, Zhihua\n  Wang, Niki Trigoni, Andrew Markham","docs_id":"1911.11236","section":["cs.CV","cs.LG","eess.IV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"RandLA-Net: Efficient Semantic Segmentation of Large-Scale Point Clouds. We study the problem of efficient semantic segmentation for large-scale 3D point clouds. By relying on expensive sampling techniques or computationally heavy pre\/post-processing steps, most existing approaches are only able to be trained and operate over small-scale point clouds. In this paper, we introduce RandLA-Net, an efficient and lightweight neural architecture to directly infer per-point semantics for large-scale point clouds. The key to our approach is to use random point sampling instead of more complex point selection approaches. Although remarkably computation and memory efficient, random sampling can discard key features by chance. To overcome this, we introduce a novel local feature aggregation module to progressively increase the receptive field for each 3D point, thereby effectively preserving geometric details. Extensive experiments show that our RandLA-Net can process 1 million points in a single pass with up to 200X faster than existing approaches. Moreover, our RandLA-Net clearly surpasses state-of-the-art approaches for semantic segmentation on two large-scale benchmarks Semantic3D and SemanticKITTI."},{"title":"Multiagent Bidirectionally-Coordinated Nets: Emergence of Human-level\n  Coordination in Learning to Play StarCraft Combat Games","source":"Peng Peng, Ying Wen, Yaodong Yang, Quan Yuan, Zhenkun Tang, Haitao\n  Long, Jun Wang","docs_id":"1703.10069","section":["cs.AI","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Multiagent Bidirectionally-Coordinated Nets: Emergence of Human-level\n  Coordination in Learning to Play StarCraft Combat Games. Many artificial intelligence (AI) applications often require multiple intelligent agents to work in a collaborative effort. Efficient learning for intra-agent communication and coordination is an indispensable step towards general AI. In this paper, we take StarCraft combat game as a case study, where the task is to coordinate multiple agents as a team to defeat their enemies. To maintain a scalable yet effective communication protocol, we introduce a Multiagent Bidirectionally-Coordinated Network (BiCNet ['bIknet]) with a vectorised extension of actor-critic formulation. We show that BiCNet can handle different types of combats with arbitrary numbers of AI agents for both sides. Our analysis demonstrates that without any supervisions such as human demonstrations or labelled data, BiCNet could learn various types of advanced coordination strategies that have been commonly used by experienced game players. In our experiments, we evaluate our approach against multiple baselines under different scenarios; it shows state-of-the-art performance, and possesses potential values for large-scale real-world applications."},{"title":"Automating App Review Response Generation","source":"Cuiyun Gao, Jichuan Zeng, Xin Xia, David Lo, Michael R. Lyu, Irwin\n  King","docs_id":"2002.03552","section":["cs.SE","cs.CL"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Automating App Review Response Generation. Previous studies showed that replying to a user review usually has a positive effect on the rating that is given by the user to the app. For example, Hassan et al. found that responding to a review increases the chances of a user updating their given rating by up to six times compared to not responding. To alleviate the labor burden in replying to the bulk of user reviews, developers usually adopt a template-based strategy where the templates can express appreciation for using the app or mention the company email address for users to follow up. However, reading a large number of user reviews every day is not an easy task for developers. Thus, there is a need for more automation to help developers respond to user reviews. Addressing the aforementioned need, in this work we propose a novel approach RRGen that automatically generates review responses by learning knowledge relations between reviews and their responses. RRGen explicitly incorporates review attributes, such as user rating and review length, and learns the relations between reviews and corresponding responses in a supervised way from the available training data. Experiments on 58 apps and 309,246 review-response pairs highlight that RRGen outperforms the baselines by at least 67.4% in terms of BLEU-4 (an accuracy measure that is widely used to evaluate dialogue response generation systems). Qualitative analysis also confirms the effectiveness of RRGen in generating relevant and accurate responses."},{"title":"Towards a more flexible Language of Thought: Bayesian grammar updates\n  after each concept exposure","source":"Pablo Tano, Sergio Romano, Mariano Sigman, Alejo Salles and Santiago\n  Figueira","docs_id":"1805.06924","section":["cs.AI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Towards a more flexible Language of Thought: Bayesian grammar updates\n  after each concept exposure. Recent approaches to human concept learning have successfully combined the power of symbolic, infinitely productive rule systems and statistical learning to explain our ability to learn new concepts from just a few examples. The aim of most of these studies is to reveal the underlying language structuring these representations and providing a general substrate for thought. However, describing a model of thought that is fixed once trained is against the extensive literature that shows how experience shapes concept learning. Here, we ask about the plasticity of these symbolic descriptive languages. We perform a concept learning experiment that demonstrates that humans can change very rapidly the repertoire of symbols they use to identify concepts, by compiling expressions which are frequently used into new symbols of the language. The pattern of concept learning times is accurately described by a Bayesian agent that rationally updates the probability of compiling a new expression according to how useful it has been to compress concepts so far. By portraying the Language of Thought as a flexible system of rules, we also highlight the difficulties to pin it down empirically."},{"title":"Parameterized Model-Checking for Timed-Systems with Conjunctive Guards\n  (Extended Version)","source":"Luca Spalazzi and Francesco Spegni","docs_id":"1407.7305","section":["cs.LO"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Parameterized Model-Checking for Timed-Systems with Conjunctive Guards\n  (Extended Version). In this work we extend the Emerson and Kahlon's cutoff theorems for process skeletons with conjunctive guards to Parameterized Networks of Timed Automata, i.e. systems obtained by an \\emph{apriori} unknown number of Timed Automata instantiated from a finite set $U_1, \\dots, U_n$ of Timed Automata templates. In this way we aim at giving a tool to universally verify software systems where an unknown number of software components (i.e. processes) interact with continuous time temporal constraints. It is often the case, indeed, that distributed algorithms show an heterogeneous nature, combining dynamic aspects with real-time aspects. In the paper we will also show how to model check a protocol that uses special variables storing identifiers of the participating processes (i.e. PIDs) in Timed Automata with conjunctive guards. This is non-trivial, since solutions to the parameterized verification problem often relies on the processes to be symmetric, i.e. indistinguishable. On the other side, many popular distributed algorithms make use of PIDs and thus cannot directly apply those solutions."},{"title":"Coupling of Real-Time and Co-Simulation for the Evaluation of the Large\n  Scale Integration of Electric Vehicles into Intelligent Power Systems","source":"Felix Lehfuss, Georg Lauss, Christian Seitl, Fabian Leimgruber, Martin\n  Noehrer, Thomas I. Strasser","docs_id":"1812.09578","section":["cs.SY"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Coupling of Real-Time and Co-Simulation for the Evaluation of the Large\n  Scale Integration of Electric Vehicles into Intelligent Power Systems. This paper addresses the validation of electric vehicle supply equipment by means of a real-time capable co-simulation approach. This setup implies both pure software and real-time simulation tasks with different sampling rates dependent on the type of the performed experiment. In contrast, controller and power hardware-in-the-loop simulations are methodologies which ask for real-time execution of simulation models with well-defined simulation sampling rates. Software and real-time methods are connected one to each other using an embedded software interface. It is able to process signals with different time step sizes and is called \"LabLink\". Its design implies both common and specific input and output layers (middle layer), as well as a data bus (core). The LabLink enables the application of the co-simulation methodology on the proposed experimental platform targeting the testing of electric vehicle supply equipment. The test setup architecture and representative examples for the implemented co-simulation are presented in this paper. As such, a validation of the usability of this testing platform can be highlighted aiming to support a higher penetration of electric vehicles."},{"title":"Investigations of Process Damping Forces in Metal Cutting","source":"Emily Stone, Suhail Ahmed, Abe Askari and Hong Tat","docs_id":"cs\/0508102","section":["cs.CE"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Investigations of Process Damping Forces in Metal Cutting. Using finite element software developed for metal cutting by Third Wave Systems we investigate the forces involved in chatter, a self-sustained oscillation of the cutting tool. The phenomena is decomposed into a vibrating tool cutting a flat surface work piece, and motionless tool cutting a work piece with a wavy surface. While cutting the wavy surface, the shearplane was seen to oscillate in advance of the oscillation of the depth of cut, as were the cutting, thrust, and shear plane forces. The vibrating tool was used to investigate process damping through the interaction of the relief face of the tool and the workpiece. Crushing forces are isolated and compared to the contact length between the tool and workpiece. We found that the wavelength dependence of the forces depended on the relative size of the wavelength to the length of the relief face of the tool. The results indicate that the damping force from crushing will be proportional to the cutting speed for short tools, and inversely proportional for long tools."},{"title":"Fault-Tolerant Perception for Automated Driving A Lightweight Monitoring\n  Approach","source":"Cornelius Buerkle, Florian Geissler, Michael Paulitsch, Kay-Ulrich\n  Scholl","docs_id":"2111.12360","section":["cs.RO"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Fault-Tolerant Perception for Automated Driving A Lightweight Monitoring\n  Approach. While the most visible part of the safety verification process of automated vehicles concerns the planning and control system, it is often overlooked that safety of the latter crucially depends on the fault-tolerance of the preceding environment perception. Modern perception systems feature complex and often machine-learning-based components with various failure modes that can jeopardize the overall safety. At the same time, a verification by for example redundant execution is not always feasible due to resource constraints. In this paper, we address the need for feasible and efficient perception monitors and propose a lightweight approach that helps to protect the integrity of the perception system while keeping the additional compute overhead minimal. In contrast to existing solutions, the monitor is realized by a well-balanced combination of sensor checks -- here using LiDAR information -- and plausibility checks on the object motion history. It is designed to detect relevant errors in the distance and velocity of objects in the environment of the automated vehicle. In conjunction with an appropriate planning system, such a monitor can help to make safe automated driving feasible."},{"title":"Projected Stochastic Gradient Langevin Algorithms for Constrained\n  Sampling and Non-Convex Learning","source":"Andrew Lamperski","docs_id":"2012.12137","section":["cs.LG","math.OC","math.PR"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Projected Stochastic Gradient Langevin Algorithms for Constrained\n  Sampling and Non-Convex Learning. Langevin algorithms are gradient descent methods with additive noise. They have been used for decades in Markov chain Monte Carlo (MCMC) sampling, optimization, and learning. Their convergence properties for unconstrained non-convex optimization and learning problems have been studied widely in the last few years. Other work has examined projected Langevin algorithms for sampling from log-concave distributions restricted to convex compact sets. For learning and optimization, log-concave distributions correspond to convex losses. In this paper, we analyze the case of non-convex losses with compact convex constraint sets and IID external data variables. We term the resulting method the projected stochastic gradient Langevin algorithm (PSGLA). We show the algorithm achieves a deviation of $O(T^{-1\/4}(\\log T)^{1\/2})$ from its target distribution in 1-Wasserstein distance. For optimization and learning, we show that the algorithm achieves $\\epsilon$-suboptimal solutions, on average, provided that it is run for a time that is polynomial in $\\epsilon^{-1}$ and slightly super-exponential in the problem dimension."},{"title":"Interpreting multi-variate models with setPCA","source":"Nordine Aouni, Luc Linders, David Robinson, Len Vandelaer, Jessica\n  Wiezorek, Geetesh Gupta, Rachel Cavill","docs_id":"2111.09138","section":["q-bio.GN","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Interpreting multi-variate models with setPCA. Principal Component Analysis (PCA) and other multi-variate models are often used in the analysis of \"omics\" data. These models contain much information which is currently neither easily accessible nor interpretable. Here we present an algorithmic method which has been developed to integrate this information with existing databases of background knowledge, stored in the form of known sets (for instance genesets or pathways). To make this accessible we have produced a Graphical User Interface (GUI) in Matlab which allows the overlay of known set information onto the loadings plot and thus improves the interpretability of the multi-variate model. For each known set the optimal convex hull, covering a subset of elements from the known set, is found through a search algorithm and displayed. In this paper we discuss two main topics; the details of the search algorithm for the optimal convex hull for this problem and the GUI interface which is freely available for download for academic use."},{"title":"Efficient Exact Paths For Dyck and semi-Dyck Labeled Path Reachability","source":"Phillip G. Bradford","docs_id":"1802.05239","section":["cs.DS"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Efficient Exact Paths For Dyck and semi-Dyck Labeled Path Reachability. The exact path length problem is to determine if there is a path of a given fixed cost between two vertices. This paper focuses on the exact path problem for costs $-1,0$ or $+1$ between all pairs of vertices in an edge-weighted digraph. The edge weights are from $\\{ -1, +1 \\}$. In this case, this paper gives an $\\widetilde{O}(n^{\\omega})$ exact path solution. Here $\\omega$ is the best exponent for matrix multiplication and $\\widetilde{O}$ is the asymptotic upper-bound mod polylog factors. Variations of this algorithm determine which pairs of digraph nodes have Dyck or semi-Dyck labeled paths between them, assuming two parenthesis. Therefore, determining digraph reachability for Dyck or semi-Dyck labeled paths costs $\\widetilde{O}(n^{\\omega})$. A path label is made by concatenating all symbols along the path's edges. The exact path length problem has many applications. These applications include the labeled path problems given here, which in turn, also have numerous applications."},{"title":"Ethical behavior in humans and machines -- Evaluating training data\n  quality for beneficial machine learning","source":"Thilo Hagendorff","docs_id":"2008.11463","section":["cs.CY","cs.AI","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Ethical behavior in humans and machines -- Evaluating training data\n  quality for beneficial machine learning. Machine behavior that is based on learning algorithms can be significantly influenced by the exposure to data of different qualities. Up to now, those qualities are solely measured in technical terms, but not in ethical ones, despite the significant role of training and annotation data in supervised machine learning. This is the first study to fill this gap by describing new dimensions of data quality for supervised machine learning applications. Based on the rationale that different social and psychological backgrounds of individuals correlate in practice with different modes of human-computer-interaction, the paper describes from an ethical perspective how varying qualities of behavioral data that individuals leave behind while using digital technologies have socially relevant ramification for the development of machine learning applications. The specific objective of this study is to describe how training data can be selected according to ethical assessments of the behavior it originates from, establishing an innovative filter regime to transition from the big data rationale n = all to a more selective way of processing data for training sets in machine learning. The overarching aim of this research is to promote methods for achieving beneficial machine learning applications that could be widely useful for industry as well as academia."},{"title":"Improved Prosodic Clustering for Multispeaker and Speaker-independent\n  Phoneme-level Prosody Control","source":"Myrsini Christidou, Alexandra Vioni, Nikolaos Ellinas, Georgios\n  Vamvoukakis, Konstantinos Markopoulos, Panos Kakoulidis, June Sig Sung,\n  Hyoungmin Park, Aimilios Chalamandaris, Pirros Tsiakoulis","docs_id":"2111.10168","section":["cs.SD","cs.CL","cs.LG","eess.AS"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Improved Prosodic Clustering for Multispeaker and Speaker-independent\n  Phoneme-level Prosody Control. This paper presents a method for phoneme-level prosody control of F0 and duration on a multispeaker text-to-speech setup, which is based on prosodic clustering. An autoregressive attention-based model is used, incorporating multispeaker architecture modules in parallel to a prosody encoder. Several improvements over the basic single-speaker method are proposed that increase the prosodic control range and coverage. More specifically we employ data augmentation, F0 normalization, balanced clustering for duration, and speaker-independent prosodic clustering. These modifications enable fine-grained phoneme-level prosody control for all speakers contained in the training set, while maintaining the speaker identity. The model is also fine-tuned to unseen speakers with limited amounts of data and it is shown to maintain its prosody control capabilities, verifying that the speaker-independent prosodic clustering is effective. Experimental results verify that the model maintains high output speech quality and that the proposed method allows efficient prosody control within each speaker's range despite the variability that a multispeaker setting introduces."},{"title":"On feature selection and evaluation of transportation mode prediction\n  strategies","source":"Mohammad Etemad, Amilcar Soares Junior, Stan Matwin","docs_id":"1808.03096","section":["cs.AI","cs.LG","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"On feature selection and evaluation of transportation mode prediction\n  strategies. Transportation modes prediction is a fundamental task for decision making in smart cities and traffic management systems. Traffic policies designed based on trajectory mining can save money and time for authorities and the public. It may reduce the fuel consumption and commute time and moreover, may provide more pleasant moments for residents and tourists. Since the number of features that may be used to predict a user transportation mode can be substantial, finding a subset of features that maximizes a performance measure is worth investigating. In this work, we explore wrapper and information retrieval methods to find the best subset of trajectory features. After finding the best classifier and the best feature subset, our results were compared with two related papers that applied deep learning methods and the results showed that our framework achieved better performance. Furthermore, two types of cross-validation approaches were investigated, and the performance results show that the random cross-validation method provides optimistic results."},{"title":"Sparse Lifting of Dense Vectors: Unifying Word and Sentence\n  Representations","source":"Wenye Li and Senyue Hao","docs_id":"1911.01625","section":["cs.CL","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Sparse Lifting of Dense Vectors: Unifying Word and Sentence\n  Representations. As the first step in automated natural language processing, representing words and sentences is of central importance and has attracted significant research attention. Different approaches, from the early one-hot and bag-of-words representation to more recent distributional dense and sparse representations, were proposed. Despite the successful results that have been achieved, such vectors tend to consist of uninterpretable components and face nontrivial challenge in both memory and computational requirement in practical applications. In this paper, we designed a novel representation model that projects dense word vectors into a higher dimensional space and favors a highly sparse and binary representation of word vectors with potentially interpretable components, while trying to maintain pairwise inner products between original vectors as much as possible. Computationally, our model is relaxed as a symmetric non-negative matrix factorization problem which admits a fast yet effective solution. In a series of empirical evaluations, the proposed model exhibited consistent improvement and high potential in practical applications."},{"title":"Neural Topic Models with Survival Supervision: Jointly Predicting\n  Time-to-Event Outcomes and Learning How Clinical Features Relate","source":"Linhong Li, Ren Zuo, Amanda Coston, Jeremy C. Weiss, George H. Chen","docs_id":"2007.07796","section":["cs.LG","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Neural Topic Models with Survival Supervision: Jointly Predicting\n  Time-to-Event Outcomes and Learning How Clinical Features Relate. In time-to-event prediction problems, a standard approach to estimating an interpretable model is to use Cox proportional hazards, where features are selected based on lasso regularization or stepwise regression. However, these Cox-based models do not learn how different features relate. As an alternative, we present an interpretable neural network approach to jointly learn a survival model to predict time-to-event outcomes while simultaneously learning how features relate in terms of a topic model. In particular, we model each subject as a distribution over \"topics\", which are learned from clinical features as to help predict a time-to-event outcome. From a technical standpoint, we extend existing neural topic modeling approaches to also minimize a survival analysis loss function. We study the effectiveness of this approach on seven healthcare datasets on predicting time until death as well as hospital ICU length of stay, where we find that neural survival-supervised topic models achieves competitive accuracy with existing approaches while yielding interpretable clinical \"topics\" that explain feature relationships."},{"title":"Learning to Unknot","source":"Sergei Gukov, James Halverson, Fabian Ruehle, Piotr Su{\\l}kowski","docs_id":"2010.16263","section":["math.GT","cs.LG","hep-th"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Learning to Unknot. We introduce natural language processing into the study of knot theory, as made natural by the braid word representation of knots. We study the UNKNOT problem of determining whether or not a given knot is the unknot. After describing an algorithm to randomly generate $N$-crossing braids and their knot closures and discussing the induced prior on the distribution of knots, we apply binary classification to the UNKNOT decision problem. We find that the Reformer and shared-QK Transformer network architectures outperform fully-connected networks, though all perform well. Perhaps surprisingly, we find that accuracy increases with the length of the braid word, and that the networks learn a direct correlation between the confidence of their predictions and the degree of the Jones polynomial. Finally, we utilize reinforcement learning (RL) to find sequences of Markov moves and braid relations that simplify knots and can identify unknots by explicitly giving the sequence of unknotting actions. Trust region policy optimization (TRPO) performs consistently well for a wide range of crossing numbers and thoroughly outperformed other RL algorithms and random walkers. Studying these actions, we find that braid relations are more useful in simplifying to the unknot than one of the Markov moves."},{"title":"Data-Oriented Language Implementation of Lattice-Boltzmann Method for\n  Dense and Sparse Geometries","source":"Tadeusz Tomczak","docs_id":"2108.13241","section":["cs.DC"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Data-Oriented Language Implementation of Lattice-Boltzmann Method for\n  Dense and Sparse Geometries. The performance of lattice-Boltzmann solver implementations usually depends mainly on memory access patterns. Achieving high performance requires then complex code which handles careful data placement and ordering of memory transactions. In this work, we analyse the performance of an implementation based on a new approach called the data-oriented language, which allows the combining of complex memory access patterns with simple source code. As a use case, we present and provide the source code of a solver for D2Q9 lattice and show its performance on GTX Titan Xp GPU for dense and sparse geometries up to 4096 2 nodes. The obtained results are promising, around 1000 lines of code allowed us to achieve performance in the range of 0.6 to 0.7 of maximum theoretical memory bandwidth (over 2.5 and 5.0 GLUPS for double and single precision, respectively) for meshes of size above 1024 2 nodes, which is close to the current state-of-the-art. However, we also observed relatively high and sometimes difficult to predict overheads, especially for sparse data structures. The additional issue was also a rather long compilation, which extended the time of short simulations, and a lack of access to low-level optimisation mechanisms."},{"title":"Dynamics on networks. Case of Heterogeneous Opinion Status Model","source":"Liubov Tupikina","docs_id":"1708.01647","section":["physics.soc-ph","cs.SI","physics.data-an"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Dynamics on networks. Case of Heterogeneous Opinion Status Model. Here we developed a new conceptual, stochastic Heterogeneous Opinion-Status model (HOpS model), which is adaptive network model. The HOpS model admits to identify the main attributes of dynamics on networks and to study analytically the relation between topological network properties and processes taking place on a network. Another key point of the HOpS model is the possibility to study network dynamics via the novel parameter of heterogeneity. We show that not only clear topological network properties, such as node degree, but also, the nodes' status distribution (the factor of network heterogeneity) play an important role in so-called opinion spreading and information diffusion on a network. This model can be potentially used for studying the co-evolution of globally aggregated or averaged key observables of the earth system. These include natural variables such as atmospheric, oceanic and land carbon stocks, as well as socio-economic quantities such as global human population, economic production or wellbeing."},{"title":"GETNET: A General End-to-end Two-dimensional CNN Framework for\n  Hyperspectral Image Change Detection","source":"Qi Wang, Senior Member, IEEE, Zhenghang Yuan, Qian Du, Fellow, IEEE,\n  and Xuelong Li, Fellow, IEEE","docs_id":"1905.01662","section":["cs.CV","eess.IV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"GETNET: A General End-to-end Two-dimensional CNN Framework for\n  Hyperspectral Image Change Detection. Change detection (CD) is an important application of remote sensing, which provides timely change information about large-scale Earth surface. With the emergence of hyperspectral imagery, CD technology has been greatly promoted, as hyperspectral data with the highspectral resolution are capable of detecting finer changes than using the traditional multispectral imagery. Nevertheless, the high dimension of hyperspectral data makes it difficult to implement traditional CD algorithms. Besides, endmember abundance information at subpixel level is often not fully utilized. In order to better handle high dimension problem and explore abundance information, this paper presents a General End-to-end Two-dimensional CNN (GETNET) framework for hyperspectral image change detection (HSI-CD). The main contributions of this work are threefold: 1) Mixed-affinity matrix that integrates subpixel representation is introduced to mine more cross-channel gradient features and fuse multi-source information; 2) 2-D CNN is designed to learn the discriminative features effectively from multi-source data at a higher level and enhance the generalization ability of the proposed CD algorithm; 3) A new HSI-CD data set is designed for the objective comparison of different methods. Experimental results on real hyperspectral data sets demonstrate the proposed method outperforms most of the state-of-the-arts."},{"title":"Tree-Structured Policy based Progressive Reinforcement Learning for\n  Temporally Language Grounding in Video","source":"Jie Wu, Guanbin Li, Si Liu, Liang Lin","docs_id":"2001.06680","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Tree-Structured Policy based Progressive Reinforcement Learning for\n  Temporally Language Grounding in Video. Temporally language grounding in untrimmed videos is a newly-raised task in video understanding. Most of the existing methods suffer from inferior efficiency, lacking interpretability, and deviating from the human perception mechanism. Inspired by human's coarse-to-fine decision-making paradigm, we formulate a novel Tree-Structured Policy based Progressive Reinforcement Learning (TSP-PRL) framework to sequentially regulate the temporal boundary by an iterative refinement process. The semantic concepts are explicitly represented as the branches in the policy, which contributes to efficiently decomposing complex policies into an interpretable primitive action. Progressive reinforcement learning provides correct credit assignment via two task-oriented rewards that encourage mutual promotion within the tree-structured policy. We extensively evaluate TSP-PRL on the Charades-STA and ActivityNet datasets, and experimental results show that TSP-PRL achieves competitive performance over existing state-of-the-art methods."},{"title":"Automatic 4D Facial Expression Recognition via Collaborative\n  Cross-domain Dynamic Image Network","source":"Muzammil Behzad, Nhat Vo, Xiaobai Li, Guoying Zhao","docs_id":"1905.02319","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Automatic 4D Facial Expression Recognition via Collaborative\n  Cross-domain Dynamic Image Network. This paper proposes a novel 4D Facial Expression Recognition (FER) method using Collaborative Cross-domain Dynamic Image Network (CCDN). Given a 4D data of face scans, we first compute its geometrical images, and then combine their correlated information in the proposed cross-domain image representations. The acquired set is then used to generate cross-domain dynamic images (CDI) via rank pooling that encapsulates facial deformations over time in terms of a single image. For the training phase, these CDIs are fed into an end-to-end deep learning model, and the resultant predictions collaborate over multi-views for performance gain in expression classification. Furthermore, we propose a 4D augmentation scheme that not only expands the training data scale but also introduces significant facial muscle movement patterns to improve the FER performance. Results from extensive experiments on the commonly used BU-4DFE dataset under widely adopted settings show that our proposed method outperforms the state-of-the-art 4D FER methods by achieving an accuracy of 96.5% indicating its effectiveness."},{"title":"Automatic Speech Recognition in Sanskrit: A New Speech Corpus and\n  Modelling Insights","source":"Devaraja Adiga, Rishabh Kumar, Amrith Krishna, Preethi Jyothi, Ganesh\n  Ramakrishnan, Pawan Goyal","docs_id":"2106.05852","section":["eess.AS","cs.CL","cs.LG","cs.SD"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Automatic Speech Recognition in Sanskrit: A New Speech Corpus and\n  Modelling Insights. Automatic speech recognition (ASR) in Sanskrit is interesting, owing to the various linguistic peculiarities present in the language. The Sanskrit language is lexically productive, undergoes euphonic assimilation of phones at the word boundaries and exhibits variations in spelling conventions and in pronunciations. In this work, we propose the first large scale study of automatic speech recognition (ASR) in Sanskrit, with an emphasis on the impact of unit selection in Sanskrit ASR. In this work, we release a 78 hour ASR dataset for Sanskrit, which faithfully captures several of the linguistic characteristics expressed by the language. We investigate the role of different acoustic model and language model units in ASR systems for Sanskrit. We also propose a new modelling unit, inspired by the syllable level unit selection, that captures character sequences from one vowel in the word to the next vowel. We also highlight the importance of choosing graphemic representations for Sanskrit and show the impact of this choice on word error rates (WER). Finally, we extend these insights from Sanskrit ASR for building ASR systems in two other Indic languages, Gujarati and Telugu. For both these languages, our experimental results show that the use of phonetic based graphemic representations in ASR results in performance improvements as compared to ASR systems that use native scripts."},{"title":"GLocal-K: Global and Local Kernels for Recommender Systems","source":"Soyeon Caren Han, Taejun Lim, Siqu Long, Bernd Burgstaller, Josiah\n  Poon","docs_id":"2108.12184","section":["cs.IR","cs.AI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"GLocal-K: Global and Local Kernels for Recommender Systems. Recommender systems typically operate on high-dimensional sparse user-item matrices. Matrix completion is a very challenging task to predict one's interest based on millions of other users having each seen a small subset of thousands of items. We propose a Global-Local Kernel-based matrix completion framework, named GLocal-K, that aims to generalise and represent a high-dimensional sparse user-item matrix entry into a low dimensional space with a small number of important features. Our GLocal-K can be divided into two major stages. First, we pre-train an auto encoder with the local kernelised weight matrix, which transforms the data from one space into the feature space by using a 2d-RBF kernel. Then, the pre-trained auto encoder is fine-tuned with the rating matrix, produced by a convolution-based global kernel, which captures the characteristics of each item. We apply our GLocal-K model under the extreme low-resource setting, which includes only a user-item rating matrix, with no side information. Our model outperforms the state-of-the-art baselines on three collaborative filtering benchmarks: ML-100K, ML-1M, and Douban."},{"title":"All-in-one: Certifiable Optimal Distributed Kalman Filter under Unknown\n  Correlations","source":"Eduardo Sebasti\\'an and Eduardo Montijano and Carlos Sag\\\"u\\'es","docs_id":"2105.15061","section":["eess.SY","cs.SY"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"All-in-one: Certifiable Optimal Distributed Kalman Filter under Unknown\n  Correlations. The optimal fusion of estimates in a Distributed Kalman Filter (DKF) requires tracking of the complete network error covariance, problematic in terms of memory and communication. A scalable alternative is to fuse estimates under unknown correlations, doing the update by solving an optimisation problem. Unfortunately, this problem is NP-hard, forcing relaxations that lose optimality guarantees. Motivated by this, we present the first Certifiable Optimal DKF (CO-DKF). Using only information from one-hop neighbours, CO-DKF solves the optimal fusion of estimates under unknown correlations by a particular tight Semidefinite Programming (SDP) relaxation which allows to certify, locally and in real time, if the relaxed solution is the actual optimum. In that case, we prove optimality in the Mean Square Error (MSE) sense. Additionally, we demonstrate the global asymptotic stability of the estimator. CO-DKF outperforms other state-of-the-art DKF algorithms, specially in sparse, highly noisy setups."},{"title":"Physical Layer Authentication for Mission Critical Machine Type\n  Communication using Gaussian Mixture Model based Clustering","source":"Andreas Weinand, Michael Karrenbauer, Ji Lianghai, Hans D. Schotten","docs_id":"1711.06101","section":["cs.NI","eess.SP"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Physical Layer Authentication for Mission Critical Machine Type\n  Communication using Gaussian Mixture Model based Clustering. The application of Mission Critical Machine Type Communication (MC-MTC) in wireless systems is currently a hot research topic. Wireless systems are considered to provide numerous advantages over wired systems in e.g. industrial applications such as closed loop control. However, due to the broadcast nature of the wireless channel, such systems are prone to a wide range of cyber attacks. These range from passive eavesdropping attacks to active attacks like data manipulation or masquerade attacks. Therefore it is necessary to provide reliable and efficient security mechanisms. Some of the most important security issues in such a system are to ensure integrity as well as authenticity of exchanged messages over the air between communicating devices. In the present work, an approach on how to achieve this goal in MC-MTC systems based on Physical Layer Security (PHYSEC) is presented. A new method that clusters channel estimates of different transmitters based on a Gaussian Mixture Model is applied for that purpose. Further, an experimental proof-of-concept evaluation is given and we compare the performance of our approach with a mean square error based detection method."},{"title":"Parichayana: An Eclipse Plugin for Detecting Exception Handling\n  Anti-Patterns and Code Smells in Java Programs","source":"Ashish Sureka","docs_id":"1701.00108","section":["cs.SE"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Parichayana: An Eclipse Plugin for Detecting Exception Handling\n  Anti-Patterns and Code Smells in Java Programs. Anti-patterns and code-smells are signs in the source code which are not defects (does not prevent the program from functioning and does not cause compile errors) and are rather indicators of deeper and bigger problems. Exception handling is a programming construct de- signed to handle the occurrence of anomalous or exceptional conditions (that changes the normal flow of program execution). In this paper, we present an Eclipse plug-in (called as Parichayana) for detecting exception handling anti-patterns and code smells in Java programs. Parichayana is capable of automatically detecting several commonly occurring excep- tion handling programming mistakes. We extend the Eclipse IDE and create new menu entries and associated action via the Parichayana plug- in (free and open-source hosted on GitHub). We compare and contrast Parichayana with several code smell detection tools and demonstrate that our tool provides unique capabilities in context to existing tools. We have created an update site and developers can use the Eclipse up- date manager to install Parichayana from our site. We used Parichyana on several large open-source Java based projects and detected presence of exception handling anti-patterns"},{"title":"VoIPLoc: Passive VoIP call provenance via acoustic side-channels","source":"Shishir Nagaraja, Ryan Shah","docs_id":"1909.01904","section":["cs.CR","cs.SD","eess.AS"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"VoIPLoc: Passive VoIP call provenance via acoustic side-channels. We propose VoIPLoc, a novel location fingerprinting technique and apply it to the VoIP call provenance problem. It exploits echo-location information embedded within VoIP audio to support fine-grained location inference. We found consistent statistical features induced by the echo-reflection characteristics of the location into recorded speech. These features are discernible within traces received at the VoIP destination, enabling location inference. We evaluated VoIPLoc by developing a dataset of audio traces received through VoIP channels over the Tor network. We show that recording locations can be fingerprinted and detected remotely with a low false-positive rate, even when a majority of the audio samples are unlabelled. Finally, we note that the technique is fully passive and thus undetectable, unlike prior art. VoIPLoc is robust to the impact of environmental noise and background sounds, as well as the impact of compressive codecs and network jitter. The technique is also highly scalable and offers several degrees of freedom terms of the fingerprintable space."},{"title":"Machine Learning on Volatile Instances","source":"Xiaoxi Zhang, Jianyu Wang, Gauri Joshi, and Carlee Joe-Wong","docs_id":"2003.05649","section":["cs.LG","cs.DC","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Machine Learning on Volatile Instances. Due to the massive size of the neural network models and training datasets used in machine learning today, it is imperative to distribute stochastic gradient descent (SGD) by splitting up tasks such as gradient evaluation across multiple worker nodes. However, running distributed SGD can be prohibitively expensive because it may require specialized computing resources such as GPUs for extended periods of time. We propose cost-effective strategies to exploit volatile cloud instances that are cheaper than standard instances, but may be interrupted by higher priority workloads. To the best of our knowledge, this work is the first to quantify how variations in the number of active worker nodes (as a result of preemption) affects SGD convergence and the time to train the model. By understanding these trade-offs between preemption probability of the instances, accuracy, and training time, we are able to derive practical strategies for configuring distributed SGD jobs on volatile instances such as Amazon EC2 spot instances and other preemptible cloud instances. Experimental results show that our strategies achieve good training performance at substantially lower cost."},{"title":"DeepSEED: 3D Squeeze-and-Excitation Encoder-Decoder Convolutional Neural\n  Networks for Pulmonary Nodule Detection","source":"Yuemeng Li, Yong Fan","docs_id":"1904.03501","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"DeepSEED: 3D Squeeze-and-Excitation Encoder-Decoder Convolutional Neural\n  Networks for Pulmonary Nodule Detection. Pulmonary nodule detection plays an important role in lung cancer screening with low-dose computed tomography (CT) scans. It remains challenging to build nodule detection deep learning models with good generalization performance due to unbalanced positive and negative samples. In order to overcome this problem and further improve state-of-the-art nodule detection methods, we develop a novel deep 3D convolutional neural network with an Encoder-Decoder structure in conjunction with a region proposal network. Particularly, we utilize a dynamically scaled cross entropy loss to reduce the false positive rate and combat the sample imbalance problem associated with nodule detection. We adopt the squeeze-and-excitation structure to learn effective image features and utilize inter-dependency information of different feature maps. We have validated our method based on publicly available CT scans with manually labelled ground-truth obtained from LIDC\/IDRI dataset and its subset LUNA16 with thinner slices. Ablation studies and experimental results have demonstrated that our method could outperform state-of-the-art nodule detection methods by a large margin."},{"title":"Bias-Reduced Hindsight Experience Replay with Virtual Goal\n  Prioritization","source":"Binyamin Manela, Armin Biess","docs_id":"1905.05498","section":["cs.LG","cs.AI","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Bias-Reduced Hindsight Experience Replay with Virtual Goal\n  Prioritization. Hindsight Experience Replay (HER) is a multi-goal reinforcement learning algorithm for sparse reward functions. The algorithm treats every failure as a success for an alternative (virtual) goal that has been achieved in the episode. Virtual goals are randomly selected, irrespective of which are most instructive for the agent. In this paper, we present two improvements over the existing HER algorithm. First, we prioritize virtual goals from which the agent will learn more valuable information. We call this property the instructiveness of the virtual goal and define it by a heuristic measure, which expresses how well the agent will be able to generalize from that virtual goal to actual goals. Secondly, we reduce existing bias in HER by the removal of misleading samples. To test our algorithms, we built two challenging environments with sparse reward functions. Our empirical results in both environments show vast improvement in the final success rate and sample efficiency when compared to the original HER algorithm. A video showing experimental results is available at https:\/\/youtu.be\/3cZwfK8Nfps ."},{"title":"An expressive dissimilarity measure for relational clustering using\n  neighbourhood trees","source":"Sebastijan Dumancic and Hendrik Blockeel","docs_id":"1604.08934","section":["stat.ML","cs.AI","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"An expressive dissimilarity measure for relational clustering using\n  neighbourhood trees. Clustering is an underspecified task: there are no universal criteria for what makes a good clustering. This is especially true for relational data, where similarity can be based on the features of individuals, the relationships between them, or a mix of both. Existing methods for relational clustering have strong and often implicit biases in this respect. In this paper, we introduce a novel similarity measure for relational data. It is the first measure to incorporate a wide variety of types of similarity, including similarity of attributes, similarity of relational context, and proximity in a hypergraph. We experimentally evaluate how using this similarity affects the quality of clustering on very different types of datasets. The experiments demonstrate that (a) using this similarity in standard clustering methods consistently gives good results, whereas other measures work well only on datasets that match their bias; and (b) on most datasets, the novel similarity outperforms even the best among the existing ones."},{"title":"A secure key transfer protocol for group communication","source":"R. Velumadhava Rao, K. Selvamani, R. Elakkiya","docs_id":"1212.2720","section":["cs.CR","cs.DC"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A secure key transfer protocol for group communication. Providing security for messages in group communication is more essential and critical nowadays. In group oriented applications such as Video conferencing and entertainment applications, it is necessary to secure the confidential data in such a way that intruders are not able to modify or transmit the data. Key transfer protocols fully rely on trusted Key Generation Center (KGC) to compute group key and to transport the group keys to all communication parties in a secured and secret manner. In this paper, an efficient key generation and key transfer protocol has been proposed where KGC can broadcast group key information to all group members in a secure way. Hence, only authorized group members will be able to retrieve the secret key and unauthorized members cannot retrieve the secret key. Hence, inorder to maintain the forward and backward secrecy, the group keys are updated whenever a new member joins or leaves the communication group. The proposed algorithm is more efficient and relies on NP class. In addition, the keys are distributed to the group users in a safe and secure way. Moreover, the key generated is also very strong since it uses cryptographic techniques which provide efficient computation."},{"title":"Privacy Preserving Point-of-interest Recommendation Using Decentralized\n  Matrix Factorization","source":"Chaochao Chen, Ziqi Liu, Peilin Zhao, Jun Zhou, Xiaolong Li","docs_id":"2003.05610","section":["cs.LG","cs.AI","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Privacy Preserving Point-of-interest Recommendation Using Decentralized\n  Matrix Factorization. Points of interest (POI) recommendation has been drawn much attention recently due to the increasing popularity of location-based networks, e.g., Foursquare and Yelp. Among the existing approaches to POI recommendation, Matrix Factorization (MF) based techniques have proven to be effective. However, existing MF approaches suffer from two major problems: (1) Expensive computations and storages due to the centralized model training mechanism: the centralized learners have to maintain the whole user-item rating matrix, and potentially huge low rank matrices. (2) Privacy issues: the users' preferences are at risk of leaking to malicious attackers via the centralized learner. To solve these, we present a Decentralized MF (DMF) framework for POI recommendation. Specifically, instead of maintaining all the low rank matrices and sensitive rating data for training, we propose a random walk based decentralized training technique to train MF models on each user's end, e.g., cell phone and Pad. By doing so, the ratings of each user are still kept on one's own hand, and moreover, decentralized learning can be taken as distributed learning with multi-learners (users), and thus alleviates the computation and storage issue. Experimental results on two real-world datasets demonstrate that, comparing with the classic and state-of-the-art latent factor models, DMF significantly improvements the recommendation performance in terms of precision and recall."},{"title":"SOLIS -- The MLOps journey from data acquisition to actionable insights","source":"Razvan Ciobanu, Alexandru Purdila, Laurentiu Piciu and Andrei Damian","docs_id":"2112.11925","section":["cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"SOLIS -- The MLOps journey from data acquisition to actionable insights. Machine Learning operations is unarguably a very important and also one of the hottest topics in Artificial Intelligence lately. Being able to define very clear hypotheses for actual real-life problems that can be addressed by machine learning models, collecting and curating large amounts of data for model training and validation followed by model architecture search and actual optimization and finally presenting the results fits very well the scenario of Data Science experiments. This approach however does not supply the needed procedures and pipelines for the actual deployment of machine learning capabilities in real production grade systems. Automating live configuration mechanisms, on the fly adapting to live or offline data capture and consumption, serving multiple models in parallel either on edge or cloud architectures, addressing specific limitations of GPU memory or compute power, post-processing inference or prediction results and serving those either as APIs or with IoT based communication stacks in the same end-to-end pipeline are the real challenges that we try to address in this particular paper. In this paper we present a unified deployment pipeline and freedom-to-operate approach that supports all above requirements while using basic cross-platform tensor framework and script language engines."},{"title":"Grey-Box Learning of Register Automata","source":"Bharat Garhewal, Frits Vaandrager, Falk Howar, Timo Schrijvers, Toon\n  Lenaerts, Rob Smits","docs_id":"2009.09975","section":["cs.FL"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Grey-Box Learning of Register Automata. Model learning (a.k.a. active automata learning) is a highly effective technique for obtaining black-box finite state models of software components. Thus far, generalisation to infinite state systems with inputs\/outputs that carry data parameters has been challenging. Existing model learning tools for infinite state systems face scalability problems and can only be applied to restricted classes of systems (register automata with equality\/inequality). In this article, we show how we can boost the performance of model learning techniques by extracting the constraints on input and output parameters from a run, and making this grey-box information available to the learner. More specifically, we provide new implementations of the tree oracle and equivalence oracle from RALib, which use the derived constraints. We extract the constraints from runs of Python programs using an existing tainting library for Python, and compare our grey-box version of RALib with the existing black-box version on several benchmarks, including some data structures from Python's standard library. Our proof-of-principle implementation results in almost two orders of magnitude improvement in terms of numbers of inputs sent to the software system. Our approach, which can be generalised to richer model classes, also enables RALib to learn models that are out of reach of black-box techniques, such as combination locks."},{"title":"MReD: A Meta-Review Dataset for Controllable Text Generation","source":"Chenhui Shen, Liying Cheng, Ran Zhou, Lidong Bing, Yang You, Luo Si","docs_id":"2110.07474","section":["cs.CL"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"MReD: A Meta-Review Dataset for Controllable Text Generation. When directly using existing text generation datasets for controllable generation, we are facing the problem of not having the domain knowledge and thus the aspects that could be controlled are limited.A typical example is when using CNN\/Daily Mail dataset for controllable text summarization, there is no guided information on the emphasis of summary sentences. A more useful text generator should leverage both the input text and control variables to guide the generation, which can only be built with deep understanding of the domain knowledge. Motivated by this vi-sion, our paper introduces a new text generation dataset, named MReD. Our new dataset consists of 7,089 meta-reviews and all its 45k meta-review sentences are manually annotated as one of the carefully defined 9 categories, including abstract, strength, decision, etc. We present experimental results on start-of-the-art summarization models, and propose methods for controlled generation on both extractive and abstractive models using our annotated data. By exploring various settings and analaysing the model behavior with respect to the control inputs, we demonstrate the challenges and values of our dataset. MReD allows us to have a better understanding of the meta-review corpora and enlarge the research room for controllable text generation."},{"title":"On the Volatility of Optimal Control Policies and the Capacity of a\n  Class of Linear Quadratic Regulators","source":"Avinash Mohan, Shie Mannor and Arman Kizilkale","docs_id":"2002.06808","section":["eess.SY","cs.SY"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"On the Volatility of Optimal Control Policies and the Capacity of a\n  Class of Linear Quadratic Regulators. It is well known that highly volatile control laws, while theoretically optimal for certain systems, are undesirable from an engineering perspective, being generally deleterious to the controlled system. In this article we are concerned with the temporal volatility of the control process of the regulator in discrete time Linear Quadratic Regulators (LQRs). Our investigation in this paper unearths a surprising connection between the cost functional which an LQR is tasked with minimizing and the temporal variations of its control laws. We first show that optimally controlling the system always implies high levels of control volatility, i.e., it is impossible to reduce volatility in the optimal control process without sacrificing cost. We also show that, akin to communication systems, every LQR has a $Capacity~Region$ associated with it, that dictates and quantifies how much cost is achievable at a given level of control volatility. This additionally establishes the fact that no admissible control policy can simultaneously achieve low volatility and low cost. We then employ this analysis to explain the phenomenon of temporal price volatility frequently observed in deregulated electricity markets."},{"title":"Inspiration through Observation: Demonstrating the Influence of\n  Automatically Generated Text on Creative Writing","source":"Melissa Roemmele","docs_id":"2107.04007","section":["cs.CL","cs.AI","cs.HC"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Inspiration through Observation: Demonstrating the Influence of\n  Automatically Generated Text on Creative Writing. Getting machines to generate text perceived as creative is a long-pursued goal. A growing body of research directs this goal towards augmenting the creative writing abilities of human authors. In this paper, we pursue this objective by analyzing how observing examples of automatically generated text influences writing. In particular, we examine a task referred to as sentence infilling, which involves transforming a list of words into a complete sentence. We emphasize \"storiability\" as a desirable feature of the resulting sentences, where \"storiable\" sentences are those that suggest a story a reader would be curious to hear about. Both humans and an automated system (based on a neural language model) performed this sentence infilling task. In one setting, people wrote sentences on their own; in a different setting, people observed the sentences produced by the model while writing their own sentences. Readers then assigned storiability preferences to the resulting sentences in a subsequent evaluation. We find that human-authored sentences were judged as more storiable when authors observed the generated examples, and that storiability increased as authors derived more semantic content from the examples. This result gives evidence of an \"inspiration through observation\" paradigm for human-computer collaborative writing, through which human writing can be enhanced by text generation models without directly copying their output."},{"title":"Automatic Aggregation by Joint Modeling of Aspects and Values","source":"Christina Sauper, Regina Barzilay","docs_id":"1401.6422","section":["cs.CL"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Automatic Aggregation by Joint Modeling of Aspects and Values. We present a model for aggregation of product review snippets by joint aspect identification and sentiment analysis. Our model simultaneously identifies an underlying set of ratable aspects presented in the reviews of a product (e.g., sushi and miso for a Japanese restaurant) and determines the corresponding sentiment of each aspect. This approach directly enables discovery of highly-rated or inconsistent aspects of a product. Our generative model admits an efficient variational mean-field inference algorithm. It is also easily extensible, and we describe several modifications and their effects on model structure and inference. We test our model on two tasks, joint aspect identification and sentiment analysis on a set of Yelp reviews and aspect identification alone on a set of medical summaries. We evaluate the performance of the model on aspect identification, sentiment analysis, and per-word labeling accuracy. We demonstrate that our model outperforms applicable baselines by a considerable margin, yielding up to 32% relative error reduction on aspect identification and up to 20% relative error reduction on sentiment analysis."},{"title":"Progressive Deep Video Dehazing without Explicit Alignment Estimation","source":"Runde Li","docs_id":"2107.07837","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Progressive Deep Video Dehazing without Explicit Alignment Estimation. To solve the issue of video dehazing, there are two main tasks to attain: how to align adjacent frames to the reference frame; how to restore the reference frame. Some papers adopt explicit approaches (e.g., the Markov random field, optical flow, deformable convolution, 3D convolution) to align neighboring frames with the reference frame in feature space or image space, they then use various restoration methods to achieve the final dehazing results. In this paper, we propose a progressive alignment and restoration method for video dehazing. The alignment process aligns consecutive neighboring frames stage by stage without using the optical flow estimation. The restoration process is not only implemented under the alignment process but also uses a refinement network to improve the dehazing performance of the whole network. The proposed networks include four fusion networks and one refinement network. To decrease the parameters of networks, three fusion networks in the first fusion stage share the same parameters. Extensive experiments demonstrate that the proposed video dehazing method achieves outstanding performance against the-state-of-art methods."},{"title":"Puzzle Mix: Exploiting Saliency and Local Statistics for Optimal Mixup","source":"Jang-Hyun Kim, Wonho Choo, Hyun Oh Song","docs_id":"2009.06962","section":["cs.LG","cs.AI","cs.CV","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Puzzle Mix: Exploiting Saliency and Local Statistics for Optimal Mixup. While deep neural networks achieve great performance on fitting the training distribution, the learned networks are prone to overfitting and are susceptible to adversarial attacks. In this regard, a number of mixup based augmentation methods have been recently proposed. However, these approaches mainly focus on creating previously unseen virtual examples and can sometimes provide misleading supervisory signal to the network. To this end, we propose Puzzle Mix, a mixup method for explicitly utilizing the saliency information and the underlying statistics of the natural examples. This leads to an interesting optimization problem alternating between the multi-label objective for optimal mixing mask and saliency discounted optimal transport objective. Our experiments show Puzzle Mix achieves the state of the art generalization and the adversarial robustness results compared to other mixup methods on CIFAR-100, Tiny-ImageNet, and ImageNet datasets. The source code is available at https:\/\/github.com\/snu-mllab\/PuzzleMix."},{"title":"Safe Testing","source":"Peter Gr\\\"unwald, Rianne de Heide, and Wouter Koolen","docs_id":"1906.07801","section":["math.ST","cs.IT","cs.LG","math.IT","stat.ME","stat.TH"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Safe Testing. We develop the theory of hypothesis testing based on the E-value, a notion of evidence that, unlike the p-value, allows for effortlessly combining results from several studies in the common scenario where the decision to perform a new study may depend on previous outcomes. Tests based on E-values are safe, i.e. they preserve Type-I error guarantees, under such optional continuation. We define growth-rate optimality (GRO) as an analogue of power in an optional continuation context, and we show how to construct GRO E-variables for general testing problems with composite null and alternative, emphasizing models with nuisance parameters. GRO E-values take the form of Bayes factors with special priors. We illustrate the theory using several classic examples including a one-sample safe t-test (in which the right Haar prior turns out to be GRO) and the 2x2 contingency table (in which the GRO prior is different from standard priors). Sharing Fisherian, Neymanian and Jeffreys-Bayesian interpretations, E-values and the corresponding tests may provide a methodology acceptable to adherents of all three schools."},{"title":"AB-Mapper: Attention and BicNet Based Multi-agent Path Finding for\n  Dynamic Crowded Environment","source":"Huifeng Guan, Yuan Gao, Min Zhao, Yong Yang, Fuqin Deng, Tin Lun Lam","docs_id":"2110.00760","section":["cs.RO","cs.MA"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"AB-Mapper: Attention and BicNet Based Multi-agent Path Finding for\n  Dynamic Crowded Environment. Multi-agent path finding in dynamic crowded environments is of great academic and practical value for multi-robot systems in the real world. To improve the effectiveness and efficiency of communication and learning process during path planning in dynamic crowded environments, we introduce an algorithm called Attention and BicNet based Multi-agent path planning with effective reinforcement (AB-Mapper)under the actor-critic reinforcement learning framework. In this framework, on the one hand, we utilize the BicNet with communication function in the actor-network to achieve intra team coordination. On the other hand, we propose a centralized critic network that can selectively allocate attention weights to surrounding agents. This attention mechanism allows an individual agent to automatically learn a better evaluation of actions by also considering the behaviours of its surrounding agents. Compared with the state-of-the-art method Mapper,our AB-Mapper is more effective (85.86% vs. 81.56% in terms of success rate) in solving the general path finding problems with dynamic obstacles. In addition, in crowded scenarios, our method outperforms the Mapper method by a large margin,reaching a stunning gap of more than 40% for each experiment."},{"title":"Epipolar Geometry Based On Line Similarity","source":"Gil Ben-Artzi, Tavi Halperin, Michael Werman, Shmuel Peleg","docs_id":"1604.04848","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Epipolar Geometry Based On Line Similarity. It is known that epipolar geometry can be computed from three epipolar line correspondences but this computation is rarely used in practice since there are no simple methods to find corresponding lines. Instead, methods for finding corresponding points are widely used. This paper proposes a similarity measure between lines that indicates whether two lines are corresponding epipolar lines and enables finding epipolar line correspondences as needed for the computation of epipolar geometry. A similarity measure between two lines, suitable for video sequences of a dynamic scene, has been previously described. This paper suggests a stereo matching similarity measure suitable for images. It is based on the quality of stereo matching between the two lines, as corresponding epipolar lines yield a good stereo correspondence. Instead of an exhaustive search over all possible pairs of lines, the search space is substantially reduced when two corresponding point pairs are given. We validate the proposed method using real-world images and compare it to state-of-the-art methods. We found this method to be more accurate by a factor of five compared to the standard method using seven corresponding points and comparable to the 8-points algorithm."},{"title":"Efficient LTE Access with Collision Resolution for Massive M2M\n  Communications","source":"Germ\\'an Corrales Madue\\~no, \\v{C}edomir Stefanovi\\'c, Petar Popovski","docs_id":"1410.6628","section":["cs.IT","cs.NI","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Efficient LTE Access with Collision Resolution for Massive M2M\n  Communications. LTE random access procedure performs satisfactorily in case of asynchronous, uncorrelated traffic arrivals. However, when the arrivals are correlated and arrive synchronously, the performance of the random access channel (RACH) is drastically reduced, causing a large number of devices to experience outage. In this work we propose a LTE RACH scheme tailored for delay-sensitive M2M services with synchronous traffic arrivals. The key idea is, upon detection of a RACH overload, to apply a collision resolution algorithm based on splitting trees. The solution is implemented on top of the existing LTE RACH mechanism, requiring only minor modifications of the protocol operation and not incurring any changes to the physical layer. The results are very promising, outperforming the related solutions by a wide margin. As an illustration, the proposed scheme can resolve 30k devices with an average of 5 preamble transmissions and delay of 1.2 seconds, under a realistic probability of transmissions error both in the downlink and in the uplink."},{"title":"A Family of Simplified Geometric Distortion Models for Camera\n  Calibration","source":"Lili Ma, YangQuan Chen, and Kevin L. Moore","docs_id":"cs\/0308003","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A Family of Simplified Geometric Distortion Models for Camera\n  Calibration. The commonly used radial distortion model for camera calibration is in fact an assumption or a restriction. In practice, camera distortion could happen in a general geometrical manner that is not limited to the radial sense. This paper proposes a simplified geometrical distortion modeling method by using two different radial distortion functions in the two image axes. A family of simplified geometric distortion models is proposed, which are either simple polynomials or the rational functions of polynomials. Analytical geometric undistortion is possible using two of the distortion functions discussed in this paper and their performance can be improved by applying a piecewise fitting idea. Our experimental results show that the geometrical distortion models always perform better than their radial distortion counterparts. Furthermore, the proposed geometric modeling method is more appropriate for cameras whose distortion is not perfectly radially symmetric around the center of distortion."},{"title":"Nonadaptive Mastermind Algorithms for String and Vector Databases, with\n  Case Studies","source":"Arthur U. Asuncion and Michael T. Goodrich (Department of Computer\n  Science, University of California, Irvine)","docs_id":"1012.2509","section":["cs.CR"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Nonadaptive Mastermind Algorithms for String and Vector Databases, with\n  Case Studies. In this paper, we study sparsity-exploiting Mastermind algorithms for attacking the privacy of an entire database of character strings or vectors, such as DNA strings, movie ratings, or social network friendship data. Based on reductions to nonadaptive group testing, our methods are able to take advantage of minimal amounts of privacy leakage, such as contained in a single bit that indicates if two people in a medical database have any common genetic mutations, or if two people have any common friends in an online social network. We analyze our Mastermind attack algorithms using theoretical characterizations that provide sublinear bounds on the number of queries needed to clone the database, as well as experimental tests on genomic information, collaborative filtering data, and online social networks. By taking advantage of the generally sparse nature of these real-world databases and modulating a parameter that controls query sparsity, we demonstrate that relatively few nonadaptive queries are needed to recover a large majority of each database."},{"title":"Kernel Aggregated Fast Multipole Method: Efficient summation of Laplace\n  and Stokes kernel functions","source":"Wen Yan and Robert Blackwell","docs_id":"2010.15155","section":["math.NA","cs.NA","physics.comp-ph","physics.flu-dyn"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Kernel Aggregated Fast Multipole Method: Efficient summation of Laplace\n  and Stokes kernel functions. Many different simulation methods for Stokes flow problems involve a common computationally intense task -- the summation of a kernel function over $O(N^2)$ pairs of points. One popular technique is the Kernel Independent Fast Multipole Method (KIFMM), which constructs a spatial adaptive octree for all points and places a small number of equivalent multipole and local equivalent points around each octree box, and completes the kernel sum with $O(N)$ cost, using these equivalent points. Simpler kernels can be used between these equivalent points to improve the efficiency of KIFMM. Here we present further extensions and applications to this idea, to enable efficient summations and flexible boundary conditions for various kernels. We call our method the Kernel Aggregated Fast Multipole Method (KAFMM), because it uses different kernel functions at different stages of octree traversal. We have implemented our method as an open-source software library STKFMM based on the high performance library PVFMM, with support for Laplace kernels, the Stokeslet, regularized Stokeslet, Rotne-Prager-Yamakawa (RPY) tensor, and the Stokes double-layer and traction operators. Open and periodic boundary conditions are supported for all kernels, and the no-slip wall boundary condition is supported for the Stokeslet and RPY tensor. The package is designed to be ready-to-use as well as being readily extensible to additional kernels."},{"title":"An Exact Algorithm for Semi-supervised Minimum Sum-of-Squares Clustering","source":"Veronica Piccialli, Anna Russo Russo, Antonio M. Sudoso","docs_id":"2111.15571","section":["math.OC","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"An Exact Algorithm for Semi-supervised Minimum Sum-of-Squares Clustering. The minimum sum-of-squares clustering (MSSC), or k-means type clustering, is traditionally considered an unsupervised learning task. In recent years, the use of background knowledge to improve the cluster quality and promote interpretability of the clustering process has become a hot research topic at the intersection of mathematical optimization and machine learning research. The problem of taking advantage of background information in data clustering is called semi-supervised or constrained clustering. In this paper, we present a new branch-and-bound algorithm for semi-supervised MSSC, where background knowledge is incorporated as pairwise must-link and cannot-link constraints. For the lower bound procedure, we solve the semidefinite programming relaxation of the MSSC discrete optimization model, and we use a cutting-plane procedure for strengthening the bound. For the upper bound, instead, by using integer programming tools, we propose an adaptation of the k-means algorithm to the constrained case. For the first time, the proposed global optimization algorithm efficiently manages to solve real-world instances up to 800 data points with different combinations of must-link and cannot-link constraints and with a generic number of features. This problem size is about four times larger than the one of the instances solved by state-of-the-art exact algorithms."},{"title":"A Survey on Deep Domain Adaptation for LiDAR Perception","source":"Larissa T. Triess and Mariella Dreissig and Christoph B. Rist and J.\n  Marius Z\\\"ollner","docs_id":"2106.02377","section":["cs.CV","cs.AI","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A Survey on Deep Domain Adaptation for LiDAR Perception. Scalable systems for automated driving have to reliably cope with an open-world setting. This means, the perception systems are exposed to drastic domain shifts, like changes in weather conditions, time-dependent aspects, or geographic regions. Covering all domains with annotated data is impossible because of the endless variations of domains and the time-consuming and expensive annotation process. Furthermore, fast development cycles of the system additionally introduce hardware changes, such as sensor types and vehicle setups, and the required knowledge transfer from simulation. To enable scalable automated driving, it is therefore crucial to address these domain shifts in a robust and efficient manner. Over the last years, a vast amount of different domain adaptation techniques evolved. There already exists a number of survey papers for domain adaptation on camera images, however, a survey for LiDAR perception is absent. Nevertheless, LiDAR is a vital sensor for automated driving that provides detailed 3D scans of the vehicle's surroundings. To stimulate future research, this paper presents a comprehensive review of recent progress in domain adaptation methods and formulates interesting research questions specifically targeted towards LiDAR perception."},{"title":"BS-NAS: Broadening-and-Shrinking One-Shot NAS with Searchable Numbers of\n  Channels","source":"Zan Shen, Jiang Qian, Bojin Zhuang, Shaojun Wang, Jing Xiao","docs_id":"2003.09821","section":["cs.LG","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"BS-NAS: Broadening-and-Shrinking One-Shot NAS with Searchable Numbers of\n  Channels. One-Shot methods have evolved into one of the most popular methods in Neural Architecture Search (NAS) due to weight sharing and single training of a supernet. However, existing methods generally suffer from two issues: predetermined number of channels in each layer which is suboptimal; and model averaging effects and poor ranking correlation caused by weight coupling and continuously expanding search space. To explicitly address these issues, in this paper, a Broadening-and-Shrinking One-Shot NAS (BS-NAS) framework is proposed, in which `broadening' refers to broadening the search space with a spring block enabling search for numbers of channels during training of the supernet; while `shrinking' refers to a novel shrinking strategy gradually turning off those underperforming operations. The above innovations broaden the search space for wider representation and then shrink it by gradually removing underperforming operations, followed by an evolutionary algorithm to efficiently search for the optimal architecture. Extensive experiments on ImageNet illustrate the effectiveness of the proposed BS-NAS as well as the state-of-the-art performance."},{"title":"iPiano: Inertial Proximal Algorithm for Non-Convex Optimization","source":"Peter Ochs and Yunjin Chen and Thomas Brox and Thomas Pock","docs_id":"1404.4805","section":["cs.CV","math.OC"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"iPiano: Inertial Proximal Algorithm for Non-Convex Optimization. In this paper we study an algorithm for solving a minimization problem composed of a differentiable (possibly non-convex) and a convex (possibly non-differentiable) function. The algorithm iPiano combines forward-backward splitting with an inertial force. It can be seen as a non-smooth split version of the Heavy-ball method from Polyak. A rigorous analysis of the algorithm for the proposed class of problems yields global convergence of the function values and the arguments. This makes the algorithm robust for usage on non-convex problems. The convergence result is obtained based on the \\KL inequality. This is a very weak restriction, which was used to prove convergence for several other gradient methods. First, an abstract convergence theorem for a generic algorithm is proved, and, then iPiano is shown to satisfy the requirements of this theorem. Furthermore, a convergence rate is established for the general problem class. We demonstrate iPiano on computer vision problems: image denoising with learned priors and diffusion based image compression."},{"title":"An Interactive Control Approach to 3D Shape Reconstruction","source":"Bipul Islam, Ji Liu, Anthony Yezzi, Romeil Sandhu","docs_id":"1910.02738","section":["cs.CV","cs.RO","cs.SY","eess.SY"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"An Interactive Control Approach to 3D Shape Reconstruction. The ability to accurately reconstruct the 3D facets of a scene is one of the key problems in robotic vision. However, even with recent advances with machine learning, there is no high-fidelity universal 3D reconstruction method for this optimization problem as schemes often cater to specific image modalities and are often biased by scene abnormalities. Simply put, there always remains an information gap due to the dynamic nature of real-world scenarios. To this end, we demonstrate a feedback control framework which invokes operator inputs (also prone to errors) in order to augment existing reconstruction schemes. For proof-of-concept, we choose a classical region-based stereoscopic reconstruction approach and show how an ill-posed model can be augmented with operator input to be much more robust to scene artifacts. We provide necessary conditions for stability via Lyapunov analysis and perhaps more importantly, we show that the stability depends on a notion of absolute curvature. Mathematically, this aligns with previous work that has shown Ricci curvature as proxy for functional robustness of dynamical networked systems. We conclude with results that show how our method can improve standalone reconstruction schemes."},{"title":"Beyond the rich-club: Properties of networks related to the better\n  connected nodes","source":"Raul J Mondragon","docs_id":"1810.12328","section":["physics.soc-ph","cs.SI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Beyond the rich-club: Properties of networks related to the better\n  connected nodes. Many of the structural characteristics of a network depend on the connectivity with and within the hubs. These dependencies can be related to the degree of a node and the number of links that a node shares with nodes of higher degree. In here we revise and present new results showing how to construct network ensembles which give a good approximation to the degree-degree correlations, and hence to the projections of this correlation like the assortativity coefficient or the average neighbours degree. We present a new bound for the structural cut--off degree based on the connectivity within the hubs. Also we show that the connections with and within the hubs can be used to define different networks cores. Two of these cores are related to the spectral properties and walks of length one and two which contain at least on hub node, and they are related to the eigenvector centrality. We introduce a new centrality measured based on the connectivity with the hubs. In addition, as the ensembles and cores are related by the connectivity of the hubs, we show several examples how changes in the hubs linkage effects the degree--degree correlations and core properties."},{"title":"C-3PO: Click-sequence-aware DeeP Neural Network (DNN)-based Pop-uPs\n  RecOmmendation","source":"TonTon Hsien-De Huang, and Hung-Yu Kao","docs_id":"1803.00458","section":["cs.CY","cs.HC","cs.IR"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"C-3PO: Click-sequence-aware DeeP Neural Network (DNN)-based Pop-uPs\n  RecOmmendation. With the emergence of mobile and wearable devices, push notification becomes a powerful tool to connect and maintain the relationship with App users, but sending inappropriate or too many messages at the wrong time may result in the App being removed by the users. In order to maintain the retention rate and the delivery rate of advertisement, we adopt Deep Neural Network (DNN) to develop a pop-up recommendation system \"Click sequence-aware deeP neural network (DNN)-based Pop-uPs recOmmendation (C-3PO)\" enabled by collaborative filtering-based hybrid user behavioral analysis. We further verified the system with real data collected from the product Security Master, Clean Master and CM Browser, supported by Leopard Mobile Inc. (Cheetah Mobile Taiwan Agency). In this way, we can know precisely about users' preference and frequency to click on the push notification\/pop-ups, decrease the troublesome to users efficiently, and meanwhile increase the click through rate of push notifications\/pop-ups."},{"title":"Energy-Efficient Precoding for Multiple-Antenna Terminals","source":"E. V. Belmega and S. Lasaulce","docs_id":"1011.4597","section":["cs.IT","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Energy-Efficient Precoding for Multiple-Antenna Terminals. The problem of energy-efficient precoding is investigated when the terminals in the system are equipped with multiple antennas. Considering static and fast-fading multiple-input multiple-output (MIMO) channels, the energy-efficiency is defined as the transmission rate to power ratio and shown to be maximized at low transmit power. The most interesting case is the one of slow fading MIMO channels. For this type of channels, the optimal precoding scheme is generally not trivial. Furthermore, using all the available transmit power is not always optimal in the sense of energy-efficiency (which, in this case, corresponds to the communication-theoretic definition of the goodput-to-power (GPR) ratio). Finding the optimal precoding matrices is shown to be a new open problem and is solved in several special cases: 1. when there is only one receive antenna; 2. in the low or high signal-to-noise ratio regime; 3. when uniform power allocation and the regime of large numbers of antennas are assumed. A complete numerical analysis is provided to illustrate the derived results and stated conjectures. In particular, the impact of the number of antennas on the energy-efficiency is assessed and shown to be significant."},{"title":"Robotic Playing for Hierarchical Complex Skill Learning","source":"Simon Hangl, Emre Ugur, Sandor Szedmak and Justus Piater","docs_id":"1603.00794","section":["cs.RO"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Robotic Playing for Hierarchical Complex Skill Learning. In complex manipulation scenarios (e.g. tasks requiring complex interaction of two hands or in-hand manipulation), generalization is a hard problem. Current methods still either require a substantial amount of (supervised) training data and \/ or strong assumptions on both the environment and the task. In this paradigm, controllers solving these tasks tend to be complex. We propose a paradigm of maintaining simpler controllers solving the task in a small number of specific situations. In order to generalize to novel situations, the robot transforms the environment from novel situations into a situation where the solution of the task is already known. Our solution to this problem is to play with objects and use previously trained skills (basis skills). These skills can either be used for estimating or for changing the current state of the environment and are organized in skill hierarchies. The approach is evaluated in complex pick-and-place scenarios that involve complex manipulation. We further show that these skills can be learned by autonomous playing."},{"title":"Benchmarking Scientific Image Forgery Detectors","source":"Jo\\~ao P. Cardenuto, Anderson Rocha","docs_id":"2105.12872","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Benchmarking Scientific Image Forgery Detectors. The scientific image integrity area presents a challenging research bottleneck, the lack of available datasets to design and evaluate forensic techniques. Its data sensitivity creates a legal hurdle that prevents one to rely on real tampered cases to build any sort of accessible forensic benchmark. To mitigate this bottleneck, we present an extendable open-source library that reproduces the most common image forgery operations reported by the research integrity community: duplication, retouching, and cleaning. Using this library and realistic scientific images, we create a large scientific forgery image benchmark (39,423 images) with an enriched ground-truth. In addition, concerned about the high number of retracted papers due to image duplication, this work evaluates the state-of-the-art copy-move detection methods in the proposed dataset, using a new metric that asserts consistent match detection between the source and the copied region. The dataset and source-code will be freely available upon acceptance of the paper."},{"title":"Multi-Relay Selection Design and Analysis for Multi-Stream Cooperative\n  Communications","source":"Shunqing Zhang and Vincent K. N. Lau","docs_id":"1101.1643","section":["cs.IT","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Multi-Relay Selection Design and Analysis for Multi-Stream Cooperative\n  Communications. In this paper, we consider the problem of multi-relay selection for multi-stream cooperative MIMO systems with $M$ relay nodes. Traditionally, relay selection approaches are primarily focused on selecting one relay node to improve the transmission reliability given a single-antenna destination node. As such, in the cooperative phase whereby both the source and the selected relay nodes transmit to the destination node, it is only feasible to exploit cooperative spatial diversity (for example by means of distributed space time coding). For wireless systems with a multi-antenna destination node, in the cooperative phase it is possible to opportunistically transmit multiple data streams to the destination node by utilizing multiple relay nodes. Therefore, we propose a low overhead multi-relay selection protocol to support multi-stream cooperative communications. In addition, we derive the asymptotic performance results at high SNR for the proposed scheme and discuss the diversity-multiplexing tradeoff as well as the throughput-reliability tradeoff. From these results, we show that the proposed multi-stream cooperative communication scheme achieves lower outage probability compared to existing baseline schemes."},{"title":"Time-Aware Language Models as Temporal Knowledge Bases","source":"Bhuwan Dhingra, Jeremy R. Cole, Julian Martin Eisenschlos, Daniel\n  Gillick, Jacob Eisenstein, William W. Cohen","docs_id":"2106.15110","section":["cs.CL"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Time-Aware Language Models as Temporal Knowledge Bases. Many facts come with an expiration date, from the name of the President to the basketball team Lebron James plays for. But language models (LMs) are trained on snapshots of data collected at a specific moment in time, and this can limit their utility, especially in the closed-book setting where the pretraining corpus must contain the facts the model should memorize. We introduce a diagnostic dataset aimed at probing LMs for factual knowledge that changes over time and highlight problems with LMs at either end of the spectrum -- those trained on specific slices of temporal data, as well as those trained on a wide range of temporal data. To mitigate these problems, we propose a simple technique for jointly modeling text with its timestamp. This improves memorization of seen facts from the training time period, as well as calibration on predictions about unseen facts from future time periods. We also show that models trained with temporal context can be efficiently ``refreshed'' as new data arrives, without the need for retraining from scratch."},{"title":"Semi-Supervised Recurrent Neural Network for Adverse Drug Reaction\n  Mention Extraction","source":"Shashank Gupta, Sachin Pawar, Nitin Ramrakhiyani, Girish Palshikar and\n  Vasudeva Varma","docs_id":"1709.01687","section":["cs.IR","cs.CL"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Semi-Supervised Recurrent Neural Network for Adverse Drug Reaction\n  Mention Extraction. Social media is an useful platform to share health-related information due to its vast reach. This makes it a good candidate for public-health monitoring tasks, specifically for pharmacovigilance. We study the problem of extraction of Adverse-Drug-Reaction (ADR) mentions from social media, particularly from twitter. Medical information extraction from social media is challenging, mainly due to short and highly information nature of text, as compared to more technical and formal medical reports. Current methods in ADR mention extraction relies on supervised learning methods, which suffers from labeled data scarcity problem. The State-of-the-art method uses deep neural networks, specifically a class of Recurrent Neural Network (RNN) which are Long-Short-Term-Memory networks (LSTMs) \\cite{hochreiter1997long}. Deep neural networks, due to their large number of free parameters relies heavily on large annotated corpora for learning the end task. But in real-world, it is hard to get large labeled data, mainly due to heavy cost associated with manual annotation. Towards this end, we propose a novel semi-supervised learning based RNN model, which can leverage unlabeled data also present in abundance on social media. Through experiments we demonstrate the effectiveness of our method, achieving state-of-the-art performance in ADR mention extraction."},{"title":"Recognising and evaluating the effectiveness of extortion in the\n  Iterated Prisoner's Dilemma","source":"Vincent A. Knight and Marc Harper and Nikoleta E. Glynatsi and\n  Jonathan Gillard","docs_id":"1904.00973","section":["cs.GT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Recognising and evaluating the effectiveness of extortion in the\n  Iterated Prisoner's Dilemma. Since the introduction of zero-determinant strategies, extortionate strategies have received considerable interest. While an interesting class of strategies, the definitions of extortionate strategies are algebraically rigid, apply only to memory-one strategies, and require complete knowledge of a strategy (memory-one cooperation probabilities). We describe a method to detect extortionate behaviour from the history of play of a strategy. When applied to a corpus of 204 strategies this method detects extortionate behaviour in well-known extortionate strategies as well others that do not fit the algebraic definition. The highest performing strategies in this corpus are able to exhibit selectively extortionate behavior, cooperating with strong strategies while exploiting weaker strategies, which no memory-one strategy can do. These strategies emerged from an evolutionary selection process and their existence contradicts widely-repeated folklore in the evolutionary game theory literature: complex strategies can be extraordinarily effective, zero-determinant strategies can be outperformed by non-zero determinant strategies, and longer memory strategies are able to outperform short memory strategies. Moreover, while resistance to extortion is critical for the evolution of cooperation, the extortion of weak opponents need not prevent cooperation between stronger opponents, and this adaptability may be crucial to maintaining cooperation in the long run."},{"title":"Pseudospectral roaming contour integral methods for convection-diffusion\n  equations","source":"Nicola Guglielmi, Maria L\\'opez-Fern\\'andez, Mattia Manucci","docs_id":"2012.07085","section":["math.NA","cs.NA"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Pseudospectral roaming contour integral methods for convection-diffusion\n  equations. We generalize ideas in the recent literature and develop new ones in order to propose a general class of contour integral methods for linear convection-diffusion PDEs and in particular for those arising in finance. These methods aim to provide a numerical approximation of the solution by computing its inverse Laplace transform. The choice of the integration contour is determined by the computation of a few suitably weighted pseudo-spectral level sets of the leading operator of the equation. Parabolic and hyperbolic profiles proposed in the literature are investigated and compared to the elliptic contour originally proposed by Guglielmi, L\\'opez-Fern\\'andez and Nino. In summary, the article (i) provides a comparison among three different integration profiles; (ii) proposes a new fast pseudospectral roaming method; (iii) optimizes the selection of time windows on which one may arbitrarily approximate the solution by no extra computational cost with respect to the case of a fixed time instant; (iv) focuses extensively on computational aspects and it is the reference of the MATLAB code https:\/\/github.com\/MattiaManucci\/Contour_Integral_Methods.git, where all algorithms described here are implemented."},{"title":"Real-Time Decentralized knowledge Transfer at the Edge","source":"Orpaz Goldstein, Mohammad Kachuee, Derek Shiell, Majid Sarrafzadeh","docs_id":"2011.05961","section":["cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Real-Time Decentralized knowledge Transfer at the Edge. The proliferation of edge networks creates islands of learning agents working on local streams of data. Transferring knowledge between these agents in real-time without exposing private data allows for collaboration to decrease learning time and increase model confidence. Incorporating knowledge from data that a local model did not see creates an ability to debias a local model or add to classification abilities on data never before seen. Transferring knowledge in a selective decentralized approach enables models to retain their local insights, allowing for local flavors of a machine learning model. This approach suits the decentralized architecture of edge networks, as a local edge node will serve a community of learning agents that will likely encounter similar data. We propose a method based on knowledge distillation for pairwise knowledge transfer pipelines from models trained on non-i.i.d. data and compare it to other popular knowledge transfer methods. Additionally, we test different scenarios of knowledge transfer network construction and show the practicality of our approach. Our experiments show knowledge transfer using our model outperforms standard methods in a real-time transfer scenario."},{"title":"Dynamic Actuator Selection and Robust State-Feedback Control of\n  Networked Soft Actuators","source":"Nafiseh Ebrahimi and Sebastian Nugroho and Ahmad F. Taha and Nikolaos\n  Gatsis and Wei Gao and Amir Jafari","docs_id":"1804.01615","section":["cs.SY"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Dynamic Actuator Selection and Robust State-Feedback Control of\n  Networked Soft Actuators. The design of robots that are light, soft, powerful is a grand challenge. Since they can easily adapt to dynamic environments, soft robotic systems have the potential of changing the status-quo of bulky robotics. A crucial component of soft robotics is a soft actuator that is activated by external stimuli to generate desired motions. Unfortunately, there is a lack of powerful soft actuators that operate through lightweight power sources. To that end, we recently designed a highly scalable, flexible, biocompatible Electromagnetic Soft Actuator (ESA). With ESAs, artificial muscles can be designed by integrating a network of ESAs. The main research gap addressed in this work is in the absence of system-theoretic understanding of the impact of the realtime control and actuator selection algorithms on the performance of networked soft-body actuators and ESAs. The objective of this paper is to establish a framework that guides the analysis and robust control of networked ESAs. A novel ESA is described, and a configuration of soft actuator matrix to resemble artificial muscle fiber is presented. A mathematical model which depicts the physical network is derived, considering the disturbances due to external forces and linearization errors as an integral part of this model. Then, a robust control and minimal actuator selection problem with logistic constraints and control input bounds is formulated, and tractable computational routines are proposed with numerical case studies."},{"title":"Analyzing Linear Communication Networks using the Ribosome Flow Model","source":"Yoram Zarai and Oz Mendel and Michael Margaliot","docs_id":"1508.06038","section":["cs.NI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Analyzing Linear Communication Networks using the Ribosome Flow Model. The Ribosome Flow Model (RFM) describes the unidirectional movement of interacting particles along a one-dimensional chain of sites. As a site becomes fuller, the effective entry rate into this site decreases. The RFM has been used to model and analyze mRNA translation, a biological process in which ribosomes (the particles) move along the mRNA molecule (the chain), and decode the genetic information into proteins. Here we propose the RFM as an analytical framework for modeling and analyzing linear communication networks. In this context, the moving particles are data-packets, the chain of sites is a one dimensional set of ordered buffers, and the decreasing entry rate to a fuller buffer represents a kind of decentralized backpressure flow control. For an RFM with homogeneous link capacities, we provide closed-form expressions for important network metrics including the throughput and end-to-end delay. We use these results to analyze the hop length and the transmission probability (in a contention access mode) that minimize the end-to-end delay in a multihop linear network, and provide closed-form expressions for the optimal parameter values."},{"title":"Proximity results and faster algorithms for Integer Programming using\n  the Steinitz Lemma","source":"Friedrich Eisenbrand, Robert Weismantel","docs_id":"1707.00481","section":["cs.DM"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Proximity results and faster algorithms for Integer Programming using\n  the Steinitz Lemma. We consider integer programming problems in standard form $\\max \\{c^Tx : Ax = b, \\, x\\geq 0, \\, x \\in Z^n\\}$ where $A \\in Z^{m \\times n}$, $b \\in Z^m$ and $c \\in Z^n$. We show that such an integer program can be solved in time $(m \\Delta)^{O(m)} \\cdot \\|b\\|_\\infty^2$, where $\\Delta$ is an upper bound on each absolute value of an entry in $A$. This improves upon the longstanding best bound of Papadimitriou (1981) of $(m\\cdot \\Delta)^{O(m^2)}$, where in addition, the absolute values of the entries of $b$ also need to be bounded by $\\Delta$. Our result relies on a lemma of Steinitz that states that a set of vectors in $R^m$ that is contained in the unit ball of a norm and that sum up to zero can be ordered such that all partial sums are of norm bounded by $m$. We also use the Steinitz lemma to show that the $\\ell_1$-distance of an optimal integer and fractional solution, also under the presence of upper bounds on the variables, is bounded by $m \\cdot (2\\,m \\cdot \\Delta+1)^m$. Here $\\Delta$ is again an upper bound on the absolute values of the entries of $A$. The novel strength of our bound is that it is independent of $n$. We provide evidence for the significance of our bound by applying it to general knapsack problems where we obtain structural and algorithmic results that improve upon the recent literature."},{"title":"Real-Time Monocular Human Depth Estimation and Segmentation on Embedded\n  Systems","source":"Shan An, Fangru Zhou, Mei Yang, Haogang Zhu, Changhong Fu, and\n  Konstantinos A. Tsintotas","docs_id":"2108.10506","section":["cs.CV","cs.RO"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Real-Time Monocular Human Depth Estimation and Segmentation on Embedded\n  Systems. Estimating a scene's depth to achieve collision avoidance against moving pedestrians is a crucial and fundamental problem in the robotic field. This paper proposes a novel, low complexity network architecture for fast and accurate human depth estimation and segmentation in indoor environments, aiming to applications for resource-constrained platforms (including battery-powered aerial, micro-aerial, and ground vehicles) with a monocular camera being the primary perception module. Following the encoder-decoder structure, the proposed framework consists of two branches, one for depth prediction and another for semantic segmentation. Moreover, network structure optimization is employed to improve its forward inference speed. Exhaustive experiments on three self-generated datasets prove our pipeline's capability to execute in real-time, achieving higher frame rates than contemporary state-of-the-art frameworks (114.6 frames per second on an NVIDIA Jetson Nano GPU with TensorRT) while maintaining comparable accuracy."},{"title":"Seizure Detection using Least EEG Channels by Deep Convolutional Neural\n  Network","source":"Mustafa Talha Avcu, Zhuo Zhang, Derrick Wei Shih Chan","docs_id":"1901.05305","section":["eess.SP","cs.CV","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Seizure Detection using Least EEG Channels by Deep Convolutional Neural\n  Network. This work aims to develop an end-to-end solution for seizure onset detection. We design the SeizNet, a Convolutional Neural Network for seizure detection. To compare SeizNet with traditional machine learning approach, a baseline classifier is implemented using spectrum band power features with Support Vector Machines (BPsvm). We explore the possibility to use the least number of channels for accurate seizure detection by evaluating SeizNet and BPsvm approaches using all channels and two channels settings respectively. EEG Data is acquired from 29 pediatric patients admitted to KK Woman's and Children's Hospital who were diagnosed as typical absence seizures. We conduct leave-one-out cross validation for all subjects. Using full channel data, BPsvm yields a sensitivity of 86.6\\% and 0.84 false alarm (per hour) while SeizNet yields overall sensitivity of 95.8 \\% with 0.17 false alarm. More interestingly, two channels seizNet outperforms full channel BPsvm with a sensitivity of 93.3\\% and 0.58 false alarm. We further investigate interpretability of SeizNet by decoding the filters learned along convolutional layers. Seizure-like characteristics can be clearly observed in the filters from third and forth convolutional layers."},{"title":"Few-Example Object Detection with Model Communication","source":"Xuanyi Dong, Liang Zheng, Fan Ma, Yi Yang, Deyu Meng","docs_id":"1706.08249","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Few-Example Object Detection with Model Communication. In this paper, we study object detection using a large pool of unlabeled images and only a few labeled images per category, named \"few-example object detection\". The key challenge consists in generating trustworthy training samples as many as possible from the pool. Using few training examples as seeds, our method iterates between model training and high-confidence sample selection. In training, easy samples are generated first and, then the poorly initialized model undergoes improvement. As the model becomes more discriminative, challenging but reliable samples are selected. After that, another round of model improvement takes place. To further improve the precision and recall of the generated training samples, we embed multiple detection models in our framework, which has proven to outperform the single model baseline and the model ensemble method. Experiments on PASCAL VOC'07, MS COCO'14, and ILSVRC'13 indicate that by using as few as three or four samples selected for each category, our method produces very competitive results when compared to the state-of-the-art weakly-supervised approaches using a large number of image-level labels."},{"title":"M3: Semantic API Migrations","source":"Bruce Collie, Philip Ginsbach, Jackson Woodruff, Ajitha Rajan, Michael\n  O'Boyle","docs_id":"2008.12118","section":["cs.SE"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"M3: Semantic API Migrations. Library migration is a challenging problem, where most existing approaches rely on prior knowledge. This can be, for example, information derived from changelogs or statistical models of API usage. This paper addresses a different API migration scenario where there is no prior knowledge of the target library. We have no historical changelogs and no access to its internal representation. To tackle this problem, this paper proposes a novel approach (M$^3$), where probabilistic program synthesis is used to semantically model the behavior of library functions. Then, we use an SMT-based code search engine to discover similar code in user applications. These discovered instances provide potential locations for API migrations. We evaluate our approach against 7 well-known libraries from varied application domains, learning correct implementations for 94 functions. Our approach is integrated with standard compiler tooling, and we use this integration to evaluate migration opportunities in 9 existing C\/C++ applications with over 1MLoC. We discover over 7,000 instances of these functions, of which more than 2,000 represent migration opportunities."},{"title":"PolyScientist: Automatic Loop Transformations Combined with Microkernels\n  for Optimization of Deep Learning Primitives","source":"Sanket Tavarageri, Alexander Heinecke, Sasikanth Avancha, Gagandeep\n  Goyal, Ramakrishna Upadrasta, Bharat Kaul","docs_id":"2002.02145","section":["cs.PL","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"PolyScientist: Automatic Loop Transformations Combined with Microkernels\n  for Optimization of Deep Learning Primitives. At the heart of deep learning training and inferencing are computationally intensive primitives such as convolutions which form the building blocks of deep neural networks. Researchers have taken two distinct approaches to creating high performance implementations of deep learning kernels, namely, 1) library development exemplified by Intel MKL-DNN for CPUs, 2) automatic compilation represented by the TensorFlow XLA compiler. The two approaches have their drawbacks: even though a custom built library can deliver very good performance, the cost and time of development of the library can be high. Automatic compilation of kernels is attractive but in practice, till date, automatically generated implementations lag expert coded kernels in performance by orders of magnitude. In this paper, we develop a hybrid solution to the development of deep learning kernels that achieves the best of both worlds: the expert coded microkernels are utilized for the innermost loops of kernels and we use the advanced polyhedral technology to automatically tune the outer loops for performance. We design a novel polyhedral model based data reuse algorithm to optimize the outer loops of the kernel. Through experimental evaluation on an important class of deep learning primitives namely convolutions, we demonstrate that the approach we develop attains the same levels of performance as Intel MKL-DNN, a hand coded deep learning library."},{"title":"Towards Training Recurrent Neural Networks for Lifelong Learning","source":"Shagun Sodhani, Sarath Chandar, Yoshua Bengio","docs_id":"1811.07017","section":["cs.LG","cs.AI","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Towards Training Recurrent Neural Networks for Lifelong Learning. Catastrophic forgetting and capacity saturation are the central challenges of any parametric lifelong learning system. In this work, we study these challenges in the context of sequential supervised learning with an emphasis on recurrent neural networks. To evaluate the models in the lifelong learning setting, we propose a curriculum-based, simple, and intuitive benchmark where the models are trained on tasks with increasing levels of difficulty. To measure the impact of catastrophic forgetting, the model is tested on all the previous tasks as it completes any task. As a step towards developing true lifelong learning systems, we unify Gradient Episodic Memory (a catastrophic forgetting alleviation approach) and Net2Net(a capacity expansion approach). Both these models are proposed in the context of feedforward networks and we evaluate the feasibility of using them for recurrent networks. Evaluation on the proposed benchmark shows that the unified model is more suitable than the constituent models for lifelong learning setting."},{"title":"ThirdEye: Triplet Based Iris Recognition without Normalization","source":"Sohaib Ahmad, Benjamin Fuller","docs_id":"1907.06147","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"ThirdEye: Triplet Based Iris Recognition without Normalization. Most iris recognition pipelines involve three stages: segmenting into iris\/non-iris pixels, normalization the iris region to a fixed area, and extracting relevant features for comparison. Given recent advances in deep learning it is prudent to ask which stages are required for accurate iris recognition. Lojez et al. (IWBF 2019) recently concluded that the segmentation stage is still crucial for good accuracy.We ask if normalization is beneficial? Towards answering this question, we develop a new iris recognition system called ThirdEye based on triplet convolutional neural networks (Schroff et al., ICCV 2015). ThirdEye directly uses segmented images without normalization. We observe equal error rates of 1.32%, 9.20%, and 0.59% on the ND-0405, UbirisV2, and IITD datasets respectively. For IITD, the most constrained dataset, this improves on the best prior work. However, for ND-0405 and UbirisV2,our equal error rate is slightly worse than prior systems. Our concluding hypothesis is that normalization is more important for less constrained environments."},{"title":"A Simple 1-1\/e Approximation for Oblivious Bipartite Matching","source":"Zhihao Gavin Tang, Xiaowei Wu, Yuhao Zhang","docs_id":"2002.06037","section":["cs.DS"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A Simple 1-1\/e Approximation for Oblivious Bipartite Matching. We study the oblivious matching problem, which aims at finding a maximum matching on a graph with unknown edge set. Any algorithm for the problem specifies an ordering of the vertex pairs. The matching is then produced by probing the pairs following the ordering, and including a pair if both of them are unmatched and there exists an edge between them. The unweighted (Chan et al. (SICOMP 2018)) and the vertex-weighted (Chan et al. (TALG 2018)) versions of the problem are well studied. In this paper, we consider the edge-weighted oblivious matching problem on bipartite graphs, which generalizes the stochastic bipartite matching problem. Very recently, Gamlath et al. (SODA 2019) studied the stochastic bipartite matching problem, and proposed an (1-1\/e)-approximate algorithm. We give a very simple algorithm adapted from the Ranking algorithm by Karp et al. (STOC 1990), and show that it achieves the same (1-1\/e) approximation ratio for the oblivious matching problem on bipartite graph."},{"title":"Non-parametric Differentially Private Confidence Intervals for the\n  Median","source":"Joerg Drechsler, Ira Globus-Harris, Audra McMillan, Jayshree Sarathy,\n  and Adam Smith","docs_id":"2106.10333","section":["cs.CR","cs.LG","stat.ME","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Non-parametric Differentially Private Confidence Intervals for the\n  Median. Differential privacy is a restriction on data processing algorithms that provides strong confidentiality guarantees for individual records in the data. However, research on proper statistical inference, that is, research on properly quantifying the uncertainty of the (noisy) sample estimate regarding the true value in the population, is currently still limited. This paper proposes and evaluates several strategies to compute valid differentially private confidence intervals for the median. Instead of computing a differentially private point estimate and deriving its uncertainty, we directly estimate the interval bounds and discuss why this approach is superior if ensuring privacy is important. We also illustrate that addressing both sources of uncertainty--the error from sampling and the error from protecting the output--simultaneously should be preferred over simpler approaches that incorporate the uncertainty in a sequential fashion. We evaluate the performance of the different algorithms under various parameter settings in extensive simulation studies and demonstrate how the findings could be applied in practical settings using data from the 1940 Decennial Census."},{"title":"Power-Constrained Trajectory Optimization for Wireless UAV Relays with\n  Random Requests","source":"Matthew Bliss and Nicol\\`o Michelusi","docs_id":"2002.09617","section":["cs.IT","eess.SP","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Power-Constrained Trajectory Optimization for Wireless UAV Relays with\n  Random Requests. This paper studies the adaptive trajectory design of a rotary-wing UAV serving as a relay between ground nodes dispersed in a circular cell and generating uplink data transmissions randomly according to a Poisson process, and a central base station. We seek to minimize the expected average communication delay to service the data transmission requests, subject to an average power constraint on the mobility of the UAV. The problem is cast as a semi-Markov decision process, and it is shown that the policy exhibits a two-scale structure, which can be efficiently optimized: in the outer decision, upon starting a communication phase, and given its current radius, the UAV selects a target end radius position so as to optimally balance a trade-off between average long-term communication delay and power consumption; in the inner decision, the UAV selects its trajectory between the start radius and the selected end radius, so as to greedily minimize the delay and energy consumption to serve the current request. Numerical evaluations show that, during waiting phases, the UAV circles at some optimal radius at the most energy efficient speed, until a new request is received. Lastly, the expected average communication delay and power consumption of the optimal policy is compared to that of several heuristics, demonstrating a reduction in latency by over 50% and 20%, respectively, compared to static and mobile heuristic schemes."},{"title":"A Sparse Linear Model and Significance Test for Individual Consumption\n  Prediction","source":"Pan Li, Baosen Zhang, Yang Weng, Ram Rajagopal","docs_id":"1511.01853","section":["stat.ML","cs.SY","math.OC"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A Sparse Linear Model and Significance Test for Individual Consumption\n  Prediction. Accurate prediction of user consumption is a key part not only in understanding consumer flexibility and behavior patterns, but in the design of robust and efficient energy saving programs as well. Existing prediction methods usually have high relative errors that can be larger than 30% and have difficulties accounting for heterogeneity between individual users. In this paper, we propose a method to improve prediction accuracy of individual users by adaptively exploring sparsity in historical data and leveraging predictive relationship between different users. Sparsity is captured by popular least absolute shrinkage and selection estimator, while user selection is formulated as an optimal hypothesis testing problem and solved via a covariance test. Using real world data from PG&E, we provide extensive simulation validation of the proposed method against well-known techniques such as support vector machine, principle component analysis combined with linear regression, and random forest. The results demonstrate that our proposed methods are operationally efficient because of linear nature, and achieve optimal prediction performance."},{"title":"DiNTS: Differentiable Neural Network Topology Search for 3D Medical\n  Image Segmentation","source":"Yufan He, Dong Yang, Holger Roth, Can Zhao, Daguang Xu","docs_id":"2103.15954","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"DiNTS: Differentiable Neural Network Topology Search for 3D Medical\n  Image Segmentation. Recently, neural architecture search (NAS) has been applied to automatically search high-performance networks for medical image segmentation. The NAS search space usually contains a network topology level (controlling connections among cells with different spatial scales) and a cell level (operations within each cell). Existing methods either require long searching time for large-scale 3D image datasets, or are limited to pre-defined topologies (such as U-shaped or single-path). In this work, we focus on three important aspects of NAS in 3D medical image segmentation: flexible multi-path network topology, high search efficiency, and budgeted GPU memory usage. A novel differentiable search framework is proposed to support fast gradient-based search within a highly flexible network topology search space. The discretization of the searched optimal continuous model in differentiable scheme may produce a sub-optimal final discrete model (discretization gap). Therefore, we propose a topology loss to alleviate this problem. In addition, the GPU memory usage for the searched 3D model is limited with budget constraints during search. Our Differentiable Network Topology Search scheme (DiNTS) is evaluated on the Medical Segmentation Decathlon (MSD) challenge, which contains ten challenging segmentation tasks. Our method achieves the state-of-the-art performance and the top ranking on the MSD challenge leaderboard."},{"title":"Lexis: An Optimization Framework for Discovering the Hierarchical\n  Structure of Sequential Data","source":"Payam Siyari, Bistra Dilkina, Constantine Dovrolis","docs_id":"1602.05561","section":["cs.AI","cs.DS"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Lexis: An Optimization Framework for Discovering the Hierarchical\n  Structure of Sequential Data. Data represented as strings abounds in biology, linguistics, document mining, web search and many other fields. Such data often have a hierarchical structure, either because they were artificially designed and composed in a hierarchical manner or because there is an underlying evolutionary process that creates repeatedly more complex strings from simpler substrings. We propose a framework, referred to as \"Lexis\", that produces an optimized hierarchical representation of a given set of \"target\" strings. The resulting hierarchy, \"Lexis-DAG\", shows how to construct each target through the concatenation of intermediate substrings, minimizing the total number of such concatenations or DAG edges. The Lexis optimization problem is related to the smallest grammar problem. After we prove its NP-Hardness for two cost formulations, we propose an efficient greedy algorithm for the construction of Lexis-DAGs. We also consider the problem of identifying the set of intermediate nodes (substrings) that collectively form the \"core\" of a Lexis-DAG, which is important in the analysis of Lexis-DAGs. We show that the Lexis framework can be applied in diverse applications such as optimized synthesis of DNA fragments in genomic libraries, hierarchical structure discovery in protein sequences, dictionary-based text compression, and feature extraction from a set of documents."},{"title":"TrackingNet: A Large-Scale Dataset and Benchmark for Object Tracking in\n  the Wild","source":"Matthias M\\\"uller, Adel Bibi, Silvio Giancola, Salman Al-Subaihi,\n  Bernard Ghanem","docs_id":"1803.10794","section":["cs.CV","cs.RO"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"TrackingNet: A Large-Scale Dataset and Benchmark for Object Tracking in\n  the Wild. Despite the numerous developments in object tracking, further development of current tracking algorithms is limited by small and mostly saturated datasets. As a matter of fact, data-hungry trackers based on deep-learning currently rely on object detection datasets due to the scarcity of dedicated large-scale tracking datasets. In this work, we present TrackingNet, the first large-scale dataset and benchmark for object tracking in the wild. We provide more than 30K videos with more than 14 million dense bounding box annotations. Our dataset covers a wide selection of object classes in broad and diverse context. By releasing such a large-scale dataset, we expect deep trackers to further improve and generalize. In addition, we introduce a new benchmark composed of 500 novel videos, modeled with a distribution similar to our training dataset. By sequestering the annotation of the test set and providing an online evaluation server, we provide a fair benchmark for future development of object trackers. Deep trackers fine-tuned on a fraction of our dataset improve their performance by up to 1.6% on OTB100 and up to 1.7% on TrackingNet Test. We provide an extensive benchmark on TrackingNet by evaluating more than 20 trackers. Our results suggest that object tracking in the wild is far from being solved."},{"title":"Visual Reference Resolution using Attention Memory for Visual Dialog","source":"Paul Hongsuck Seo, Andreas Lehrmann, Bohyung Han, Leonid Sigal","docs_id":"1709.07992","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Visual Reference Resolution using Attention Memory for Visual Dialog. Visual dialog is a task of answering a series of inter-dependent questions given an input image, and often requires to resolve visual references among the questions. This problem is different from visual question answering (VQA), which relies on spatial attention (a.k.a. visual grounding) estimated from an image and question pair. We propose a novel attention mechanism that exploits visual attentions in the past to resolve the current reference in the visual dialog scenario. The proposed model is equipped with an associative attention memory storing a sequence of previous (attention, key) pairs. From this memory, the model retrieves the previous attention, taking into account recency, which is most relevant for the current question, in order to resolve potentially ambiguous references. The model then merges the retrieved attention with a tentative one to obtain the final attention for the current question; specifically, we use dynamic parameter prediction to combine the two attentions conditioned on the question. Through extensive experiments on a new synthetic visual dialog dataset, we show that our model significantly outperforms the state-of-the-art (by ~16 % points) in situations, where visual reference resolution plays an important role. Moreover, the proposed model achieves superior performance (~ 2 % points improvement) in the Visual Dialog dataset, despite having significantly fewer parameters than the baselines."},{"title":"Social Graph Restoration via Random Walk Sampling","source":"Kazuki Nakajima, Kazuyuki Shudo","docs_id":"2111.11966","section":["cs.SI","physics.soc-ph"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Social Graph Restoration via Random Walk Sampling. Analyzing social graphs with limited data access is challenging for third-party researchers. To address this challenge, a number of algorithms that estimate the structural properties via a random walk have been developed. However, most existing algorithms are limited to the estimation of local structural properties. Here we propose a method for restoring the original social graph from the small sample obtained by a random walk. The proposed method generates a graph that preserves the estimates of local structural properties and the structure of the subgraph sampled by a random walk. We compare the proposed method with subgraph sampling using a crawling method and the existing method for generating a graph that structurally resembles the original graph via a random walk. Our experimental results show that the proposed method more accurately reproduces local and global structural properties on average and provides a better visual representation of the original graph than the compared methods. We expect that our method will lead to exhaustive analyses of social graphs with limited data access."},{"title":"Persistence of centrality in random growing trees","source":"Varun Jog and Po-Ling Loh","docs_id":"1511.01975","section":["math.PR","cs.DM","cs.SI","math.ST","stat.TH"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Persistence of centrality in random growing trees. We investigate properties of node centrality in random growing tree models. We focus on a measure of centrality that computes the maximum subtree size of the tree rooted at each node, with the most central node being the tree centroid. For random trees grown according to a preferential attachment model, a uniform attachment model, or a diffusion processes over a regular tree, we prove that a single node persists as the tree centroid after a finite number of steps, with probability 1. Furthermore, this persistence property generalizes to the top $K \\ge 1$ nodes with respect to the same centrality measure. We also establish necessary and sufficient conditions for the size of an initial seed graph required to ensure persistence of a particular node with probability $1-\\epsilon$, as a function of $\\epsilon$: In the case of preferential and uniform attachment models, we derive bounds for the size of an initial hub constructed around the special node. In the case of a diffusion process over a regular tree, we derive bounds for the radius of an initial ball centered around the special node. Our necessary and sufficient conditions match up to constant factors for preferential attachment and diffusion tree models."},{"title":"Radon-Nikodym approximation in application to image analysis","source":"Vladislav Gennadievich Malyshkin","docs_id":"1511.01887","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Radon-Nikodym approximation in application to image analysis. For an image pixel information can be converted to the moments of some basis $Q_k$, e.g. Fourier-Mellin, Zernike, monomials, etc. Given sufficient number of moments pixel information can be completely recovered, for insufficient number of moments only partial information can be recovered and the image reconstruction is, at best, of interpolatory type. Standard approach is to present interpolated value as a linear combination of basis functions, what is equivalent to least squares expansion. However, recent progress in numerical stability of moments estimation allows image information to be recovered from moments in a completely different manner, applying Radon-Nikodym type of expansion, what gives the result as a ratio of two quadratic forms of basis functions. In contrast with least squares the Radon-Nikodym approach has oscillation near the boundaries very much suppressed and does not diverge outside of basis support. While least squares theory operate with vectors $<fQ_k>$, Radon-Nikodym theory operates with matrices $<fQ_jQ_k>$, what make the approach much more suitable to image transforms and statistical property estimation."},{"title":"Decidability Results for Multi-objective Stochastic Games","source":"Romain Brenguier and Vojt\\v{e}ch Forejt","docs_id":"1605.03811","section":["cs.GT","cs.LO"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Decidability Results for Multi-objective Stochastic Games. We study stochastic two-player turn-based games in which the objective of one player is to ensure several infinite-horizon total reward objectives, while the other player attempts to spoil at least one of the objectives. The games have previously been shown not to be determined, and an approximation algorithm for computing a Pareto curve has been given. The major drawback of the existing algorithm is that it needs to compute Pareto curves for finite horizon objectives (for increasing length of the horizon), and the size of these Pareto curves can grow unboundedly, even when the infinite-horizon Pareto curve is small. By adapting existing results, we first give an algorithm that computes the Pareto curve for determined games. Then, as the main result of the paper, we show that for the natural class of stopping games and when there are two reward objectives, the problem of deciding whether a player can ensure satisfaction of the objectives with given thresholds is decidable. The result relies on intricate and novel proof which shows that the Pareto curves contain only finitely many points. As a consequence, we get that the two-objective discounted-reward problem for unrestricted class of stochastic games is decidable."},{"title":"The VOISE Algorithm: a Versatile Tool for Automatic Segmentation of\n  Astronomical Images","source":"P. Guio and N. Achilleos","docs_id":"0906.1905","section":["astro-ph.IM","astro-ph.EP","cs.CV","physics.data-an","stat.AP"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"The VOISE Algorithm: a Versatile Tool for Automatic Segmentation of\n  Astronomical Images. The auroras on Jupiter and Saturn can be studied with a high sensitivity and resolution by the Hubble Space Telescope (HST) ultraviolet (UV) and far-ultraviolet (FUV) Space Telescope spectrograph (STIS) and Advanced Camera for Surveys (ACS) instruments. We present results of automatic detection and segmentation of Jupiter's auroral emissions as observed by HST ACS instrument with VOronoi Image SEgmentation (VOISE). VOISE is a dynamic algorithm for partitioning the underlying pixel grid of an image into regions according to a prescribed homogeneity criterion. The algorithm consists of an iterative procedure that dynamically constructs a tessellation of the image plane based on a Voronoi Diagram, until the intensity of the underlying image within each region is classified as homogeneous. The computed tessellations allow the extraction of quantitative information about the auroral features such as mean intensity, latitudinal and longitudinal extents and length scales. These outputs thus represent a more automated and objective method of characterising auroral emissions than manual inspection."},{"title":"A Variational Inequality Approach to Bayesian Regression Games","source":"Wenshuo Guo, Michael I. Jordan, Tianyi Lin","docs_id":"2103.13509","section":["cs.LG","cs.GT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A Variational Inequality Approach to Bayesian Regression Games. Bayesian regression games are a special class of two-player general-sum Bayesian games in which the learner is partially informed about the adversary's objective through a Bayesian prior. This formulation captures the uncertainty in regard to the adversary, and is useful in problems where the learner and adversary may have conflicting, but not necessarily perfectly antagonistic objectives. Although the Bayesian approach is a more general alternative to the standard minimax formulation, the applications of Bayesian regression games have been limited due to computational difficulties, and the existence and uniqueness of a Bayesian equilibrium are only known for quadratic cost functions. First, we prove the existence and uniqueness of a Bayesian equilibrium for a class of convex and smooth Bayesian games by regarding it as a solution of an infinite-dimensional variational inequality (VI) in Hilbert space. We consider two special cases in which the infinite-dimensional VI reduces to a high-dimensional VI or a nonconvex stochastic optimization, and provide two simple algorithms of solving them with strong convergence guarantees. Numerical results on real datasets demonstrate the promise of this approach."},{"title":"Characterizing and Demystifying the Implicit Convolution Algorithm on\n  Commercial Matrix-Multiplication Accelerators","source":"Yangjie Zhou, Mengtian Yang, Cong Guo, Jingwen Leng, Yun Liang, Quan\n  Chen, Minyi Guo, Yuhao Zhu","docs_id":"2110.03901","section":["cs.DC"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Characterizing and Demystifying the Implicit Convolution Algorithm on\n  Commercial Matrix-Multiplication Accelerators. Many of today's deep neural network accelerators, e.g., Google's TPU and NVIDIA's tensor core, are built around accelerating the general matrix multiplication (i.e., GEMM). However, supporting convolution on GEMM-based accelerators is not trivial. The naive method explicitly lowers the convolution to GEMM, commonly known as im2col, which introduces significant performance and memory overhead. Existing implicit im2col algorithms require unscalable hardware and are inefficient in supporting important convolution variants such as strided convolution. In this paper, we propose a memory-efficient and hardware-friendly implicit im2col algorithm used by Google's TPU, which dynamically converts a convolution into a GEMM with practically zero performance and memory overhead, fully unleashing the power of GEMM engines. Through comprehensive experimental results, we quantitatively argue that this algorithm has been adopted in commercial closed-source platforms, and we are the first to describe its high-level idea and implementation details. Finally, we show that our algorithm can also be generally applied to Nvidia's Tensor Cores (TC), matching and out-performing the measured performance on TCs."},{"title":"Unsupervised Learning by Competing Hidden Units","source":"Dmitry Krotov, John Hopfield","docs_id":"1806.10181","section":["cs.LG","cs.CV","cs.NE","q-bio.NC","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Unsupervised Learning by Competing Hidden Units. It is widely believed that the backpropagation algorithm is essential for learning good feature detectors in early layers of artificial neural networks, so that these detectors are useful for the task performed by the higher layers of that neural network. At the same time, the traditional form of backpropagation is biologically implausible. In the present paper we propose an unusual learning rule, which has a degree of biological plausibility, and which is motivated by Hebb's idea that change of the synapse strength should be local - i.e. should depend only on the activities of the pre and post synaptic neurons. We design a learning algorithm that utilizes global inhibition in the hidden layer, and is capable of learning early feature detectors in a completely unsupervised way. These learned lower layer feature detectors can be used to train higher layer weights in a usual supervised way so that the performance of the full network is comparable to the performance of standard feedforward networks trained end-to-end with a backpropagation algorithm."},{"title":"Progressive Adversarial Learning for Bootstrapping: A Case Study on\n  Entity Set Expansion","source":"Lingyong Yan, Xianpei Han, Le Sun","docs_id":"2109.12082","section":["cs.CL"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Progressive Adversarial Learning for Bootstrapping: A Case Study on\n  Entity Set Expansion. Bootstrapping has become the mainstream method for entity set expansion. Conventional bootstrapping methods mostly define the expansion boundary using seed-based distance metrics, which heavily depend on the quality of selected seeds and are hard to be adjusted due to the extremely sparse supervision. In this paper, we propose BootstrapGAN, a new learning method for bootstrapping which jointly models the bootstrapping process and the boundary learning process in a GAN framework. Specifically, the expansion boundaries of different bootstrapping iterations are learned via different discriminator networks; the bootstrapping network is the generator to generate new positive entities, and the discriminator networks identify the expansion boundaries by trying to distinguish the generated entities from known positive entities. By iteratively performing the above adversarial learning, the generator and the discriminators can reinforce each other and be progressively refined along the whole bootstrapping process. Experiments show that BootstrapGAN achieves the new state-of-the-art entity set expansion performance."},{"title":"MBNet: MOS Prediction for Synthesized Speech with Mean-Bias Network","source":"Yichong Leng, Xu Tan, Sheng Zhao, Frank Soong, Xiang-Yang Li, Tao Qin","docs_id":"2103.00110","section":["cs.SD","eess.AS"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"MBNet: MOS Prediction for Synthesized Speech with Mean-Bias Network. Mean opinion score (MOS) is a popular subjective metric to assess the quality of synthesized speech, and usually involves multiple human judges to evaluate each speech utterance. To reduce the labor cost in MOS test, multiple methods have been proposed to automatically predict MOS scores. To our knowledge, for a speech utterance, all previous works only used the average of multiple scores from different judges as the training target and discarded the score of each individual judge, which did not well exploit the precious MOS training data. In this paper, we propose MBNet, a MOS predictor with a mean subnet and a bias subnet to better utilize every judge score in MOS datasets, where the mean subnet is used to predict the mean score of each utterance similar to that in previous works, and the bias subnet to predict the bias score (the difference between the mean score and each individual judge score) and capture the personal preference of individual judges. Experiments show that compared with MOSNet baseline that only leverages mean score for training, MBNet improves the system-level spearmans rank correlation co-efficient (SRCC) by 2.9% on VCC 2018 dataset and 6.7% on VCC 2016 dataset."},{"title":"Accurate Temporal Action Proposal Generation with Relation-Aware Pyramid\n  Network","source":"Jialin Gao, Zhixiang Shi, Jiani Li, Guanshuo Wang, Yufeng Yuan,\n  Shiming Ge, and Xi Zhou","docs_id":"2003.04145","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Accurate Temporal Action Proposal Generation with Relation-Aware Pyramid\n  Network. Accurate temporal action proposals play an important role in detecting actions from untrimmed videos. The existing approaches have difficulties in capturing global contextual information and simultaneously localizing actions with different durations. To this end, we propose a Relation-aware pyramid Network (RapNet) to generate highly accurate temporal action proposals. In RapNet, a novel relation-aware module is introduced to exploit bi-directional long-range relations between local features for context distilling. This embedded module enhances the RapNet in terms of its multi-granularity temporal proposal generation ability, given predefined anchor boxes. We further introduce a two-stage adjustment scheme to refine the proposal boundaries and measure their confidence in containing an action with snippet-level actionness. Extensive experiments on the challenging ActivityNet and THUMOS14 benchmarks demonstrate our RapNet generates superior accurate proposals over the existing state-of-the-art methods."},{"title":"Fast Intrinsic Mode Decomposition and Filtering of Time Series Data","source":"Louis Yu Lu","docs_id":"0808.2827","section":["cs.NA"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Fast Intrinsic Mode Decomposition and Filtering of Time Series Data. The intrinsic mode function (IMF) provides adaptive function bases for nonlinear and non-stationary time series data. A fast convergent iterative method is introduced in this paper to find the IMF components of the data, the method is faster and more predictable than the Empirical Mode Decomposition method devised by the author of Hilbert Huang Transform. The approach is to iteratively adjust the control points on the data function corresponding to the extrema of the refining IMF, the control points of the residue function are calculated as the median of the straight line segments passing through the data control points, the residue function is then constructed as the cubic spline function of the median points. The initial residue function is simply constructed as the straight line segments passing through the extrema of the first derivative of the data function. The refining IMF is the difference between the data function and the improved residue function. The IMF found reveals all the riding waves in the whole data set. A new data filtering method on frequency and amplitude of IMF is also presented with the similar approach of finding the residue on the part to be filtered out. The program to demonstrate the method is distributed under BSD open source license."},{"title":"Diffraction-Aware Sound Localization for a Non-Line-of-Sight Source","source":"Inkyu An, Doheon Lee, Jung-woo Choi, Dinesh Manocha, and Sung-eui Yoon","docs_id":"1809.07524","section":["cs.RO","cs.SD","eess.AS"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Diffraction-Aware Sound Localization for a Non-Line-of-Sight Source. We present a novel sound localization algorithm for a non-line-of-sight (NLOS) sound source in indoor environments. Our approach exploits the diffraction properties of sound waves as they bend around a barrier or an obstacle in the scene. We combine a ray tracing based sound propagation algorithm with a Uniform Theory of Diffraction (UTD) model, which simulate bending effects by placing a virtual sound source on a wedge in the environment. We precompute the wedges of a reconstructed mesh of an indoor scene and use them to generate diffraction acoustic rays to localize the 3D position of the source. Our method identifies the convergence region of those generated acoustic rays as the estimated source position based on a particle filter. We have evaluated our algorithm in multiple scenarios consisting of a static and dynamic NLOS sound source. In our tested cases, our approach can localize a source position with an average accuracy error, 0.7m, measured by the L2 distance between estimated and actual source locations in a 7m*7m*3m room. Furthermore, we observe 37% to 130% improvement in accuracy over a state-of-the-art localization method that does not model diffraction effects, especially when a sound source is not visible to the robot."},{"title":"Need for Critical Cyber Defence, Security Strategy and Privacy Policy in\n  Bangladesh - Hype or Reality?","source":"AKM Bahalul Haque","docs_id":"1906.01285","section":["cs.SI","cs.CR"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Need for Critical Cyber Defence, Security Strategy and Privacy Policy in\n  Bangladesh - Hype or Reality?. Cyber security is one of the burning issues in modern world. Increased IT infrastructure has given rise to enormous chances of security breach. Bangladesh being a relatively new member of cyber security arena has its own demand and appeal. Digitalization is happening in Bangladesh for last few years at an appreciable rate. People are being connected to the worldwide web community with their smart devices. These devices have their own vulnerability issues as well as the data shared over the internet has a very good chances of getting breached. Common vulnerability issues like infecting the device with malware, Trojan, virus are on the rise. Moreover, a lack of proper cyber security policy and strategy might make the existing situation at the vulnerable edge of tipping point. Hence the upcoming new infrastructures will be at a greater risk if the issues are not dealt with at an early age. In this paper common vulnerability issues including their recent attacks on cyber space of Bangladesh, cyber security strategy and need for data privacy policy is discussed and analysed briefly."},{"title":"Optimal and Approximate Q-value Functions for Decentralized POMDPs","source":"Frans A. Oliehoek, Matthijs T. J. Spaan, Nikos Vlassis","docs_id":"1111.0062","section":["cs.AI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Optimal and Approximate Q-value Functions for Decentralized POMDPs. Decision-theoretic planning is a popular approach to sequential decision making problems, because it treats uncertainty in sensing and acting in a principled way. In single-agent frameworks like MDPs and POMDPs, planning can be carried out by resorting to Q-value functions: an optimal Q-value function Q* is computed in a recursive manner by dynamic programming, and then an optimal policy is extracted from Q*. In this paper we study whether similar Q-value functions can be defined for decentralized POMDP models (Dec-POMDPs), and how policies can be extracted from such value functions. We define two forms of the optimal Q-value function for Dec-POMDPs: one that gives a normative description as the Q-value function of an optimal pure joint policy and another one that is sequentially rational and thus gives a recipe for computation. This computation, however, is infeasible for all but the smallest problems. Therefore, we analyze various approximate Q-value functions that allow for efficient computation. We describe how they relate, and we prove that they all provide an upper bound to the optimal Q-value function Q*. Finally, unifying some previous approaches for solving Dec-POMDPs, we describe a family of algorithms for extracting policies from such Q-value functions, and perform an experimental evaluation on existing test problems, including a new firefighting benchmark problem."},{"title":"Versatile Robust Clustering of Ad Hoc Cognitive Radio Network","source":"Di Li, Erwin Fang, James Gross","docs_id":"1704.04828","section":["cs.GT","cs.NI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Versatile Robust Clustering of Ad Hoc Cognitive Radio Network. Cluster structure in cognitive radio networks facilitates cooperative spectrum sensing, routing and other functionalities. The unlicensed channels, which are available for every member of a group of cognitive radio users, consolidate the group into a cluster, and the availability of unlicensed channels decides the robustness of that cluster against the licensed users' influence. This paper analyses the problem that how to form robust clusters in cognitive radio network, so that more cognitive radio users can get benefits from cluster structure even when the primary users' operation are intense. We provide a formal description of robust clustering problem, prove it to be NP-hard and propose a centralized solution, besides, a distributed solution is proposed to suit the dynamics in the ad hoc cognitive radio network. Congestion game model is adopted to analyze the process of cluster formation, which not only contributes designing the distributed clustering scheme directly, but also provides the guarantee of convergence into Nash Equilibrium and convergence speed. Our proposed clustering solution is versatile to fulfill some other requirements such as faster convergence and cluster size control. The proposed distributed clustering scheme outperforms the related work in terms of cluster robustness, convergence speed and overhead. The extensive simulation supports our claims."},{"title":"Snowboot: Bootstrap Methods for Network Inference","source":"Yuzhou Chen, Yulia R. Gel, Vyacheslav Lyubchich, and Kusha Nezafati","docs_id":"1902.09029","section":["stat.CO","cs.SI","physics.soc-ph"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Snowboot: Bootstrap Methods for Network Inference. Complex networks are used to describe a broad range of disparate social systems and natural phenomena, from power grids to customer segmentation to human brain connectome. Challenges of parametric model specification and validation inspire a search for more data-driven and flexible nonparametric approaches for inference of complex networks. In this paper we discuss methodology and R implementation of two bootstrap procedures on random networks, that is, patchwork bootstrap of Thompson et al. (2016) and Gel et al. (2017) and vertex bootstrap of Snijders and Borgatti (1999). To our knowledge, the new R package snowboot is the first implementation of the vertex and patchwork bootstrap inference on networks in R. Our new package is accompanied with a detailed user's manual, and is compatible with the popular R package on network studies igraph. We evaluate the patchwork bootstrap and vertex bootstrap with extensive simulation studies and illustrate their utility in application to analysis of real world networks."},{"title":"Pretraining Federated Text Models for Next Word Prediction","source":"Joel Stremmel and Arjun Singh","docs_id":"2005.04828","section":["cs.LG","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Pretraining Federated Text Models for Next Word Prediction. Federated learning is a decentralized approach for training models on distributed devices, by summarizing local changes and sending aggregate parameters from local models to the cloud rather than the data itself. In this research we employ the idea of transfer learning to federated training for next word prediction (NWP) and conduct a number of experiments demonstrating enhancements to current baselines for which federated NWP models have been successful. Specifically, we compare federated training baselines from randomly initialized models to various combinations of pretraining approaches including pretrained word embeddings and whole model pretraining followed by federated fine tuning for NWP on a dataset of Stack Overflow posts. We realize lift in performance using pretrained embeddings without exacerbating the number of required training rounds or memory footprint. We also observe notable differences using centrally pretrained networks, especially depending on the datasets used. Our research offers effective, yet inexpensive, improvements to federated NWP and paves the way for more rigorous experimentation of transfer learning techniques for federated learning."},{"title":"Syntactically Look-Ahead Attention Network for Sentence Compression","source":"Hidetaka Kamigaito, Manabu Okumura","docs_id":"2002.01145","section":["cs.CL"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Syntactically Look-Ahead Attention Network for Sentence Compression. Sentence compression is the task of compressing a long sentence into a short one by deleting redundant words. In sequence-to-sequence (Seq2Seq) based models, the decoder unidirectionally decides to retain or delete words. Thus, it cannot usually explicitly capture the relationships between decoded words and unseen words that will be decoded in the future time steps. Therefore, to avoid generating ungrammatical sentences, the decoder sometimes drops important words in compressing sentences. To solve this problem, we propose a novel Seq2Seq model, syntactically look-ahead attention network (SLAHAN), that can generate informative summaries by explicitly tracking both dependency parent and child words during decoding and capturing important words that will be decoded in the future. The results of the automatic evaluation on the Google sentence compression dataset showed that SLAHAN achieved the best kept-token-based-F1, ROUGE-1, ROUGE-2 and ROUGE-L scores of 85.5, 79.3, 71.3 and 79.1, respectively. SLAHAN also improved the summarization performance on longer sentences. Furthermore, in the human evaluation, SLAHAN improved informativeness without losing readability."},{"title":"Adaptive Multi-grained Graph Neural Networks","source":"Zhiqiang Zhong, Cheng-Te Li and Jun Pang","docs_id":"2010.00238","section":["cs.AI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Adaptive Multi-grained Graph Neural Networks. Graph Neural Networks (GNNs) have been increasingly deployed in a multitude of different applications that involve node-wise and graph-level tasks. The existing literature usually studies these questions independently while they are inherently correlated. We propose in this work a unified model, Adaptive Multi-grained GNN (AdamGNN), to learn node and graph level representation interactively. Compared with the existing GNN models and pooling methods, AdamGNN enhances node representation with multi-grained semantics and avoids node feature and graph structure information loss during pooling. More specifically, a differentiable pooling operator in AdamGNN is used to obtain a multi-grained structure that involves node-wise and meso\/macro level semantic information. The unpooling and flyback aggregators in AdamGNN is to leverage the multi-grained semantics to enhance node representation. The updated node representation can further enrich the generated graph representation in the next iteration. Experimental results on twelve real-world graphs demonstrate the effectiveness of AdamGNN on multiple tasks, compared with several competing methods. In addition, the ablation and empirical studies confirm the effectiveness of different components in AdamGNN."},{"title":"Practical and Fast Momentum-Based Power Methods","source":"Tahseen Rabbani, Apollo Jain, Arjun Rajkumar, Furong Huang","docs_id":"2108.09264","section":["cs.LG","cs.NA","math.NA"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Practical and Fast Momentum-Based Power Methods. The power method is a classical algorithm with broad applications in machine learning tasks, including streaming PCA, spectral clustering, and low-rank matrix approximation. The distilled purpose of the vanilla power method is to determine the largest eigenvalue (in absolute modulus) and its eigenvector of a matrix. A momentum-based scheme can be used to accelerate the power method, but achieving an optimal convergence rate with existing algorithms critically relies on additional spectral information that is unavailable at run-time, and sub-optimal initializations can result in divergence. In this paper, we provide a pair of novel momentum-based power methods, which we call the delayed momentum power method (DMPower) and a streaming variant, the delayed momentum streaming method (DMStream). Our methods leverage inexact deflation and are capable of achieving near-optimal convergence with far less restrictive hyperparameter requirements. We provide convergence analyses for both algorithms through the lens of perturbation theory. Further, we experimentally demonstrate that DMPower routinely outperforms the vanilla power method and that both algorithms match the convergence speed of an oracle running existing accelerated methods with perfect spectral knowledge."},{"title":"Growing 3D Artefacts and Functional Machines with Neural Cellular\n  Automata","source":"Shyam Sudhakaran, Djordje Grbic, Siyan Li, Adam Katona, Elias Najarro,\n  Claire Glanois, Sebastian Risi","docs_id":"2103.08737","section":["cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Growing 3D Artefacts and Functional Machines with Neural Cellular\n  Automata. Neural Cellular Automata (NCAs) have been proven effective in simulating morphogenetic processes, the continuous construction of complex structures from very few starting cells. Recent developments in NCAs lie in the 2D domain, namely reconstructing target images from a single pixel or infinitely growing 2D textures. In this work, we propose an extension of NCAs to 3D, utilizing 3D convolutions in the proposed neural network architecture. Minecraft is selected as the environment for our automaton since it allows the generation of both static structures and moving machines. We show that despite their simplicity, NCAs are capable of growing complex entities such as castles, apartment blocks, and trees, some of which are composed of over 3,000 blocks. Additionally, when trained for regeneration, the system is able to regrow parts of simple functional machines, significantly expanding the capabilities of simulated morphogenetic systems. The code for the experiment in this paper can be found at: https:\/\/github.com\/real-itu\/3d-artefacts-nca."},{"title":"Fault diagnosis for linear heterodirectional hyperbolic ODE-PDE systems\n  using backstepping-based trajectory planning","source":"Ferdinand Fischer and Joachim Deutscher","docs_id":"2010.11526","section":["eess.SY","cs.SY","eess.SP"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Fault diagnosis for linear heterodirectional hyperbolic ODE-PDE systems\n  using backstepping-based trajectory planning. This paper is concerned with the fault diagnosis problem for general linear heterodirectional hyperbolic ODE-PDE systems. A systematic solution is presented for additive time-varying actuator, process and sensor faults in the presence of disturbances. The faults and disturbances are represented by the solutions of finite-dimensional signal models, which allow to take a large class of signals into account. For disturbances, that are only bounded, a threshold for secured fault diagnosis is derived. By applying integral transformations to the system an algebraic fault detection equation to detect faults in finite time is obtained. The corresponding integral kernels result from the realization of a finite-time transition between a non-equilibrium initial state and a vanishing final state of a hyperbolic ODE-PDE system. For this new challenging problem, a systematic trajectory planning approach is presented. In particular, this problem is facilitated by mapping the kernel equations into backstepping coordinates and tracing the solution of the transition problem back to a simple trajectory planning. The fault diagnosis for a $4\\times 4$ heterodirectional hyperbolic system coupled with a second order ODE demonstrates the results of the paper."},{"title":"Decision Problems for Additive Regular Functions","source":"Rajeev Alur, Mukund Raghothaman","docs_id":"1304.7029","section":["cs.FL"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Decision Problems for Additive Regular Functions. Additive Cost Register Automata (ACRA) map strings to integers using a finite set of registers that are updated using assignments of the form \"x := y + c\" at every step. The corresponding class of additive regular functions has multiple equivalent characterizations, appealing closure properties, and a decidable equivalence problem. In this paper, we solve two decision problems for this model. First, we define the register complexity of an additive regular function to be the minimum number of registers that an ACRA needs to compute it. We characterize the register complexity by a necessary and sufficient condition regarding the largest subset of registers whose values can be made far apart from one another. We then use this condition to design a PSPACE algorithm to compute the register complexity of a given ACRA, and establish a matching lower bound. Our results also lead to a machine-independent characterization of the register complexity of additive regular functions. Second, we consider two-player games over ACRAs, where the objective of one of the players is to reach a target set while minimizing the cost. We show the corresponding decision problem to be EXPTIME-complete when costs are non-negative integers, but undecidable when costs are integers."},{"title":"Quantized VCG Mechanisms for Polymatroid Environments","source":"Hao Ge, Randall Berry","docs_id":"1904.11663","section":["cs.GT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Quantized VCG Mechanisms for Polymatroid Environments. Many network resource allocation problems can be viewed as allocating a divisible resource, where the allocations are constrained to lie in a polymatroid. We consider market-based mechanisms for such problems. Though the Vickrey-Clarke-Groves (VCG) mechanism can provide the efficient allocation with strong incentive properties (namely dominant strategy incentive compatibility), its well-known high communication requirements can prevent it from being used. There have been a number of approaches for reducing the communication costs of VCG by weakening its incentive properties. Here, instead we take a different approach of reducing communication costs via quantization while maintaining VCG's dominant strategy incentive properties. The cost for this approach is a loss in efficiency which we characterize. We first consider quantizing the resource allocations so that agents need only submit a finite number of bids instead of full utility function. We subsequently consider quantizing the agent's bids."},{"title":"Variational Noise-Contrastive Estimation","source":"Benjamin Rhodes, Michael Gutmann","docs_id":"1810.08010","section":["stat.ML","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Variational Noise-Contrastive Estimation. Unnormalised latent variable models are a broad and flexible class of statistical models. However, learning their parameters from data is intractable, and few estimation techniques are currently available for such models. To increase the number of techniques in our arsenal, we propose variational noise-contrastive estimation (VNCE), building on NCE which is a method that only applies to unnormalised models. The core idea is to use a variational lower bound to the NCE objective function, which can be optimised in the same fashion as the evidence lower bound (ELBO) in standard variational inference (VI). We prove that VNCE can be used for both parameter estimation of unnormalised models and posterior inference of latent variables. The developed theory shows that VNCE has the same level of generality as standard VI, meaning that advances made there can be directly imported to the unnormalised setting. We validate VNCE on toy models and apply it to a realistic problem of estimating an undirected graphical model from incomplete data."},{"title":"Image Captioning with Deep Bidirectional LSTMs","source":"Cheng Wang, Haojin Yang, Christian Bartz, Christoph Meinel","docs_id":"1604.00790","section":["cs.CV","cs.CL","cs.MM"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Image Captioning with Deep Bidirectional LSTMs. This work presents an end-to-end trainable deep bidirectional LSTM (Long-Short Term Memory) model for image captioning. Our model builds on a deep convolutional neural network (CNN) and two separate LSTM networks. It is capable of learning long term visual-language interactions by making use of history and future context information at high level semantic space. Two novel deep bidirectional variant models, in which we increase the depth of nonlinearity transition in different way, are proposed to learn hierarchical visual-language embeddings. Data augmentation techniques such as multi-crop, multi-scale and vertical mirror are proposed to prevent overfitting in training deep models. We visualize the evolution of bidirectional LSTM internal states over time and qualitatively analyze how our models \"translate\" image to sentence. Our proposed models are evaluated on caption generation and image-sentence retrieval tasks with three benchmark datasets: Flickr8K, Flickr30K and MSCOCO datasets. We demonstrate that bidirectional LSTM models achieve highly competitive performance to the state-of-the-art results on caption generation even without integrating additional mechanism (e.g. object detection, attention model etc.) and significantly outperform recent methods on retrieval task."},{"title":"Motion Basis Learning for Unsupervised Deep Homography Estimation with\n  Subspace Projection","source":"Nianjin Ye, Chuan Wang, Haoqiang Fan, Shuaicheng Liu","docs_id":"2103.15346","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Motion Basis Learning for Unsupervised Deep Homography Estimation with\n  Subspace Projection. In this paper, we introduce a new framework for unsupervised deep homography estimation. Our contributions are 3 folds. First, unlike previous methods that regress 4 offsets for a homography, we propose a homography flow representation, which can be estimated by a weighted sum of 8 pre-defined homography flow bases. Second, considering a homography contains 8 Degree-of-Freedoms (DOFs) that is much less than the rank of the network features, we propose a Low Rank Representation (LRR) block that reduces the feature rank, so that features corresponding to the dominant motions are retained while others are rejected. Last, we propose a Feature Identity Loss (FIL) to enforce the learned image feature warp-equivariant, meaning that the result should be identical if the order of warp operation and feature extraction is swapped. With this constraint, the unsupervised optimization is achieved more effectively and more stable features are learned. Extensive experiments are conducted to demonstrate the effectiveness of all the newly proposed components, and results show that our approach outperforms the state-of-the-art on the homography benchmark datasets both qualitatively and quantitatively. Code is available at https:\/\/github.com\/megvii-research\/BasesHomo."},{"title":"Nesterov Accelerated Gradient and Scale Invariance for Adversarial\n  Attacks","source":"Jiadong Lin, Chuanbiao Song, Kun He, Liwei Wang, John E. Hopcroft","docs_id":"1908.06281","section":["cs.LG","cs.CR","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Nesterov Accelerated Gradient and Scale Invariance for Adversarial\n  Attacks. Deep learning models are vulnerable to adversarial examples crafted by applying human-imperceptible perturbations on benign inputs. However, under the black-box setting, most existing adversaries often have a poor transferability to attack other defense models. In this work, from the perspective of regarding the adversarial example generation as an optimization process, we propose two new methods to improve the transferability of adversarial examples, namely Nesterov Iterative Fast Gradient Sign Method (NI-FGSM) and Scale-Invariant attack Method (SIM). NI-FGSM aims to adapt Nesterov accelerated gradient into the iterative attacks so as to effectively look ahead and improve the transferability of adversarial examples. While SIM is based on our discovery on the scale-invariant property of deep learning models, for which we leverage to optimize the adversarial perturbations over the scale copies of the input images so as to avoid \"overfitting\" on the white-box model being attacked and generate more transferable adversarial examples. NI-FGSM and SIM can be naturally integrated to build a robust gradient-based attack to generate more transferable adversarial examples against the defense models. Empirical results on ImageNet dataset demonstrate that our attack methods exhibit higher transferability and achieve higher attack success rates than state-of-the-art gradient-based attacks."},{"title":"Evaluation, Modeling and Optimization of Coverage Enhancement Methods of\n  NB-IoT","source":"Sahithya Ravi, Pouria Zand, Mohieddine El Soussi, and Majid Nabi","docs_id":"1902.09455","section":["cs.NI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Evaluation, Modeling and Optimization of Coverage Enhancement Methods of\n  NB-IoT. Narrowband Internet of Things (NB-IoT) is a new Low Power Wide Area Network (LPWAN) technology released by 3GPP. The primary goals of NB-IoT are improved coverage, massive capacity, low cost, and long battery life. In order to improve coverage, NB-IoT has promising solutions, such as increasing transmission repetitions, decreasing bandwidth, and adapting the Modulation and Coding Scheme (MCS). In this paper, we present an implementation of coverage enhancement features of NB-IoT in NS-3, an end-to-end network simulator. The resource allocation and link adaptation in NS-3 are modified to comply with the new features of NB-IoT. Using the developed simulation framework, the influence of the new features on network reliability and latency is evaluated. Furthermore, an optimal hybrid link adaptation strategy based on all three features is proposed. To achieve this, we formulate an optimization problem that has an objective function based on latency, and constraint based on the Signal to Noise Ratio (SNR). Then, we propose several algorithms to minimize latency and compare them with respect to accuracy and speed. The best hybrid solution is chosen and implemented in the NS-3 simulator by which the latency formulation is verified. The numerical results show that the proposed optimization algorithm for hybrid link adaptation is eight times faster than the exhaustive search approach and yields similar latency."},{"title":"A Skeleton-Driven Neural Occupancy Representation for Articulated Hands","source":"Korrawe Karunratanakul, Adrian Spurr, Zicong Fan, Otmar Hilliges, Siyu\n  Tang","docs_id":"2109.11399","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A Skeleton-Driven Neural Occupancy Representation for Articulated Hands. We present Hand ArticuLated Occupancy (HALO), a novel representation of articulated hands that bridges the advantages of 3D keypoints and neural implicit surfaces and can be used in end-to-end trainable architectures. Unlike existing statistical parametric hand models (e.g.~MANO), HALO directly leverages 3D joint skeleton as input and produces a neural occupancy volume representing the posed hand surface. The key benefits of HALO are (1) it is driven by 3D key points, which have benefits in terms of accuracy and are easier to learn for neural networks than the latent hand-model parameters; (2) it provides a differentiable volumetric occupancy representation of the posed hand; (3) it can be trained end-to-end, allowing the formulation of losses on the hand surface that benefit the learning of 3D keypoints. We demonstrate the applicability of HALO to the task of conditional generation of hands that grasp 3D objects. The differentiable nature of HALO is shown to improve the quality of the synthesized hands both in terms of physical plausibility and user preference."},{"title":"A General Rate Duality of the MIMO Multiple Access Channel and the MIMO\n  Broadcast Channel","source":"Raphael Hunger, Michael Joham","docs_id":"0803.2427","section":["cs.IT","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A General Rate Duality of the MIMO Multiple Access Channel and the MIMO\n  Broadcast Channel. We present a general rate duality between the multiple access channel (MAC) and the broadcast channel (BC) which is applicable to systems with and without nonlinear interference cancellation. Different to the state-of-the-art rate duality with interference subtraction from Vishwanath et al., the proposed duality is filter-based instead of covariance-based and exploits the arising unitary degree of freedom to decorrelate every point-to-point link. Therefore, it allows for noncooperative stream-wise decoding which reduces complexity and latency. Moreover, the conversion from one domain to the other does not exhibit any dependencies during its computation making it accessible to a parallel implementation instead of a serial one. We additionally derive a rate duality for systems with multi-antenna terminals when linear filtering without interference (pre-)subtraction is applied and the different streams of a single user are not treated as self-interference. Both dualities are based on a framework already applied to a mean-square-error duality between the MAC and the BC. Thanks to this novel rate duality, any rate-based optimization with linear filtering in the BC can now be handled in the dual MAC where the arising expressions lead to more efficient algorithmic solutions than in the BC due to the alignment of the channel and precoder indices."},{"title":"A Novel Inspection System For Variable Data Printing Using Deep Learning","source":"Oren Haik, Oded Perry, Eli Chen, Peter Klammer","docs_id":"2001.04325","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A Novel Inspection System For Variable Data Printing Using Deep Learning. We present a novel approach for inspecting variable data prints (VDP) with an ultra-low false alarm rate (0.005%) and potential applicability to other real-world problems. The system is based on a comparison between two images: a reference image and an image captured by low-cost scanners. The comparison task is challenging as low-cost imaging systems create artifacts that may erroneously be classified as true (genuine) defects. To address this challenge we introduce two new fusion methods, for change detection applications, which are both fast and efficient. The first is an early fusion method that combines the two input images into a single pseudo-color image. The second, called Change-Detection Single Shot Detector (CD-SSD) leverages the SSD by fusing features in the middle of the network. We demonstrate the effectiveness of the proposed deep learning-based approach with a large dataset from real-world printing scenarios. Finally, we evaluate our models on a different domain of aerial imagery change detection (AICD). Our best method clearly outperforms the state-of-the-art baseline on this dataset."},{"title":"Multi-channel Multi-frame ADL-MVDR for Target Speech Separation","source":"Zhuohuang Zhang, Yong Xu, Meng Yu, Shi-Xiong Zhang, Lianwu Chen,\n  Donald S. Williamson, Dong Yu","docs_id":"2012.13442","section":["eess.AS","cs.SD"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Multi-channel Multi-frame ADL-MVDR for Target Speech Separation. Many purely neural network based speech separation approaches have been proposed to improve objective assessment scores, but they often introduce nonlinear distortions that are harmful to modern automatic speech recognition (ASR) systems. Minimum variance distortionless response (MVDR) filters are often adopted to remove nonlinear distortions, however, conventional neural mask-based MVDR systems still result in relatively high levels of residual noise. Moreover, the matrix inverse involved in the MVDR solution is sometimes numerically unstable during joint training with neural networks. In this study, we propose a multi-channel multi-frame (MCMF) all deep learning (ADL)-MVDR approach for target speech separation, which extends our preliminary multi-channel ADL-MVDR approach. The proposed MCMF ADL-MVDR system addresses linear and nonlinear distortions. Spatio-temporal cross correlations are also fully utilized in the proposed approach. The proposed systems are evaluated using a Mandarin audio-visual corpus and are compared with several state-of-the-art approaches. Experimental results demonstrate the superiority of our proposed systems under different scenarios and across several objective evaluation metrics, including ASR performance."},{"title":"Incorporating Knowledge into Structural Equation Models using Auxiliary\n  Variables","source":"Bryant Chen, Judea Pearl, Elias Bareinboim","docs_id":"1511.02995","section":["stat.ME","cs.AI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Incorporating Knowledge into Structural Equation Models using Auxiliary\n  Variables. In this paper, we extend graph-based identification methods by allowing background knowledge in the form of non-zero parameter values. Such information could be obtained, for example, from a previously conducted randomized experiment, from substantive understanding of the domain, or even an identification technique. To incorporate such information systematically, we propose the addition of auxiliary variables to the model, which are constructed so that certain paths will be conveniently cancelled. This cancellation allows the auxiliary variables to help conventional methods of identification (e.g., single-door criterion, instrumental variables, half-trek criterion), as well as model testing (e.g., d-separation, over-identification). Moreover, by iteratively alternating steps of identification and adding auxiliary variables, we can improve the power of existing identification methods via a bootstrapping approach that does not require external knowledge. We operationalize this method for simple instrumental sets (a generalization of instrumental variables) and show that the resulting method is able to identify at least as many models as the most general identification method for linear systems known to date. We further discuss the application of auxiliary variables to the tasks of model testing and z-identification."},{"title":"A Random Search Framework for Convergence Analysis of Distributed\n  Beamforming with Feedback","source":"C. Lin, V. V. Veeravalli, and S. Meyn","docs_id":"0806.3023","section":["cs.DC","cs.IT","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A Random Search Framework for Convergence Analysis of Distributed\n  Beamforming with Feedback. The focus of this work is on the analysis of transmit beamforming schemes with a low-rate feedback link in wireless sensor\/relay networks, where nodes in the network need to implement beamforming in a distributed manner. Specifically, the problem of distributed phase alignment is considered, where neither the transmitters nor the receiver has perfect channel state information, but there is a low-rate feedback link from the receiver to the transmitters. In this setting, a framework is proposed for systematically analyzing the performance of distributed beamforming schemes. To illustrate the advantage of this framework, a simple adaptive distributed beamforming scheme that was recently proposed by Mudambai et al. is studied. Two important properties for the received signal magnitude function are derived. Using these properties and the systematic framework, it is shown that the adaptive distributed beamforming scheme converges both in probability and in mean. Furthermore, it is established that the time required for the adaptive scheme to converge in mean scales linearly with respect to the number of sensor\/relay nodes."},{"title":"Improving Scalability of Contrast Pattern Mining for Network Traffic\n  Using Closed Patterns","source":"Elaheh AlipourChavary, Sarah M. Erfani, Christopher Leckie","docs_id":"2011.14830","section":["cs.NI","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Improving Scalability of Contrast Pattern Mining for Network Traffic\n  Using Closed Patterns. Contrast pattern mining (CPM) aims to discover patterns whose support increases significantly from a background dataset compared to a target dataset. CPM is particularly useful for characterising changes in evolving systems, e.g., in network traffic analysis to detect unusual activity. While most existing techniques focus on extracting either the whole set of contrast patterns (CPs) or minimal sets, the problem of efficiently finding a relevant subset of CPs, especially in high dimensional datasets, is an open challenge. In this paper, we focus on extracting the most specific set of CPs to discover significant changes between two datasets. Our approach to this problem uses closed patterns to substantially reduce redundant patterns. Our experimental results on several real and emulated network traffic datasets demonstrate that our proposed unsupervised algorithm is up to 100 times faster than an existing approach for CPM on network traffic data [2]. In addition, as an application of CPs, we demonstrate that CPM is a highly effective method for detection of meaningful changes in network traffic."},{"title":"Towards Better Models of Externalities in Sponsored Search Auctions","source":"Nicola Gatti, Marco Rocco, Paolo Serafino, Carmine Ventre","docs_id":"1604.04095","section":["cs.GT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Towards Better Models of Externalities in Sponsored Search Auctions. Sponsored Search Auctions (SSAs) arguably represent the problem at the intersection of computer science and economics with the deepest applications in real life. Within the realm of SSAs, the study of the effects that showing one ad has on the other ads, a.k.a. externalities in economics, is of utmost importance and has so far attracted the attention of much research. However, even the basic question of modeling the problem has so far escaped a definitive answer. The popular cascade model is arguably too idealized to really describe the phenomenon yet it allows a good comprehension of the problem. Other models, instead, describe the setting more adequately but are too complex to permit a satisfactory theoretical analysis. In this work, we attempt to get the best of both approaches: firstly, we define a number of general mathematical formulations for the problem in the attempt to have a rich description of externalities in SSAs and, secondly, prove a host of results drawing a nearly complete picture about the computational complexity of the problem. We complement these approximability results with some considerations about mechanism design in our context."},{"title":"Refining Manually-Designed Symbol Grounding and High-Level Planning by\n  Policy Gradients","source":"Takuya Hiraoka, Takashi Onishi, Takahisa Imagawa, Yoshimasa Tsuruoka","docs_id":"1810.00177","section":["cs.AI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Refining Manually-Designed Symbol Grounding and High-Level Planning by\n  Policy Gradients. Hierarchical planners that produce interpretable and appropriate plans are desired, especially in its application to supporting human decision making. In the typical development of the hierarchical planners, higher-level planners and symbol grounding functions are manually created, and this manual creation requires much human effort. In this paper, we propose a framework that can automatically refine symbol grounding functions and a high-level planner to reduce human effort for designing these modules. In our framework, symbol grounding and high-level planning, which are based on manually-designed knowledge bases, are modeled with semi-Markov decision processes. A policy gradient method is then applied to refine the modules, in which two terms for updating the modules are considered. The first term, called a reinforcement term, contributes to updating the modules to improve the overall performance of a hierarchical planner to produce appropriate plans. The second term, called a penalty term, contributes to keeping refined modules consistent with the manually-designed original modules. Namely, it keeps the planner, which uses the refined modules, producing interpretable plans. We perform preliminary experiments to solve the Mountain car problem, and its results show that a manually-designed high-level planner and symbol grounding function were successfully refined by our framework."},{"title":"Contrastive Attention for Automatic Chest X-ray Report Generation","source":"Fenglin Liu, Changchang Yin, Xian Wu, Shen Ge, Ping Zhang, Xu Sun","docs_id":"2106.06965","section":["cs.CV","cs.CL"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Contrastive Attention for Automatic Chest X-ray Report Generation. Recently, chest X-ray report generation, which aims to automatically generate descriptions of given chest X-ray images, has received growing research interests. The key challenge of chest X-ray report generation is to accurately capture and describe the abnormal regions. In most cases, the normal regions dominate the entire chest X-ray image, and the corresponding descriptions of these normal regions dominate the final report. Due to such data bias, learning-based models may fail to attend to abnormal regions. In this work, to effectively capture and describe abnormal regions, we propose the Contrastive Attention (CA) model. Instead of solely focusing on the current input image, the CA model compares the current input image with normal images to distill the contrastive information. The acquired contrastive information can better represent the visual features of abnormal regions. According to the experiments on the public IU-X-ray and MIMIC-CXR datasets, incorporating our CA into several existing models can boost their performance across most metrics. In addition, according to the analysis, the CA model can help existing models better attend to the abnormal regions and provide more accurate descriptions which are crucial for an interpretable diagnosis. Specifically, we achieve the state-of-the-art results on the two public datasets."},{"title":"Improving Efficiency in Convolutional Neural Network with Multilinear\n  Filters","source":"Dat Thanh Tran, Alexandros Iosifidis, Moncef Gabbouj","docs_id":"1709.09902","section":["cs.CV","cs.AI","cs.NE"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Improving Efficiency in Convolutional Neural Network with Multilinear\n  Filters. The excellent performance of deep neural networks has enabled us to solve several automatization problems, opening an era of autonomous devices. However, current deep net architectures are heavy with millions of parameters and require billions of floating point operations. Several works have been developed to compress a pre-trained deep network to reduce memory footprint and, possibly, computation. Instead of compressing a pre-trained network, in this work, we propose a generic neural network layer structure employing multilinear projection as the primary feature extractor. The proposed architecture requires several times less memory as compared to the traditional Convolutional Neural Networks (CNN), while inherits the similar design principles of a CNN. In addition, the proposed architecture is equipped with two computation schemes that enable computation reduction or scalability. Experimental results show the effectiveness of our compact projection that outperforms traditional CNN, while requiring far fewer parameters."},{"title":"Overload Control in SIP Networks: A Heuristic Approach Based on\n  Mathematical Optimization","source":"Ahmadreza Montazerolghaem, Mohammad Hossein Yaghmaee Moghaddam, Farzad\n  Tashtarian","docs_id":"1710.00817","section":["cs.NI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Overload Control in SIP Networks: A Heuristic Approach Based on\n  Mathematical Optimization. The Session Initiation Protocol (SIP) is an application-layer control protocol for creating, modifying and terminating multimedia sessions. An open issue is the control of overload that occurs when a SIP server lacks sufficient CPU and memory resources to process all messages. We prove that the problem of overload control in SIP network with a set of n servers and limited resources is in the form of NP-hard. This paper proposes a Load-Balanced Call Admission Controller (LB-CAC), based on a heuristic mathematical model to determine an optimal resource allocation in such a way that maximizes call admission rates regarding the limited resources of the SIP servers. LB-CAC determines the optimal \"call admission rates\" and \"signaling paths\" for admitted calls along optimal allocation of CPU and memory resources of the SIP servers through a new linear programming model. This happens by acquiring some critical information of SIP servers. An assessment of the numerical and experimental results demonstrates the efficiency of the proposed method."},{"title":"Fast Incremental SVDD Learning Algorithm with the Gaussian Kernel","source":"Hansi Jiang, Haoyu Wang, Wenhao Hu, Deovrat Kakde and Arin Chaudhuri","docs_id":"1709.00139","section":["stat.ML","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Fast Incremental SVDD Learning Algorithm with the Gaussian Kernel. Support vector data description (SVDD) is a machine learning technique that is used for single-class classification and outlier detection. The idea of SVDD is to find a set of support vectors that defines a boundary around data. When dealing with online or large data, existing batch SVDD methods have to be rerun in each iteration. We propose an incremental learning algorithm for SVDD that uses the Gaussian kernel. This algorithm builds on the observation that all support vectors on the boundary have the same distance to the center of sphere in a higher-dimensional feature space as mapped by the Gaussian kernel function. Each iteration involves only the existing support vectors and the new data point. Moreover, the algorithm is based solely on matrix manipulations; the support vectors and their corresponding Lagrange multiplier $\\alpha_i$'s are automatically selected and determined in each iteration. It can be seen that the complexity of our algorithm in each iteration is only $O(k^2)$, where $k$ is the number of support vectors. Experimental results on some real data sets indicate that FISVDD demonstrates significant gains in efficiency with almost no loss in either outlier detection accuracy or objective function value."},{"title":"Graph partitions and cluster synchronization in networks of oscillators","source":"Michael T. Schaub, Neave O'Clery, Yazan N. Billeh, Jean-Charles\n  Delvenne, Renaud Lambiotte and Mauricio Barahona","docs_id":"1608.04283","section":["physics.soc-ph","cs.SI","cs.SY","nlin.CD"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Graph partitions and cluster synchronization in networks of oscillators. Synchronization over networks depends strongly on the structure of the coupling between the oscillators. When the coupling presents certain regularities, the dynamics can be coarse-grained into clusters by means of External Equitable Partitions of the network graph and their associated quotient graphs. We exploit this graph-theoretical concept to study the phenomenon of cluster synchronization, in which different groups of nodes converge to distinct behaviors. We derive conditions and properties of networks in which such clustered behavior emerges, and show that the ensuing dynamics is the result of the localization of the eigenvectors of the associated graph Laplacians linked to the existence of invariant subspaces. The framework is applied to both linear and non-linear models, first for the standard case of networks with positive edges, before being generalized to the case of signed networks with both positive and negative interactions. We illustrate our results with examples of both signed and unsigned graphs for consensus dynamics and for partial synchronization of oscillator networks under the master stability function as well as Kuramoto oscillators."},{"title":"On the Secrecy Rate of Spatial Modulation Based Indoor Visible Light\n  Communications","source":"Jin-Yuan Wang, Hong Ge, Min Lin, Jun-Bo Wang, Jianxin Dai, and\n  Mohamed-Slim Alouini","docs_id":"1906.09512","section":["cs.IT","cs.PF","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"On the Secrecy Rate of Spatial Modulation Based Indoor Visible Light\n  Communications. In this paper, we investigate the physical-layer security for a spatial modulation (SM) based indoor visible light communication (VLC) system, which includes multiple transmitters, a legitimate receiver, and a passive eavesdropper (Eve). At the transmitters, the SM scheme is employed, i.e., only one transmitter is active at each time instant. To choose the active transmitter, a uniform selection (US) scheme is utilized. Two scenarios are considered: one is with non-negativity and average optical intensity constraints, the other is with non-negativity, average optical intensity and peak optical intensity constraints. Then, lower and upper bounds on the secrecy rate are derived for these two scenarios. Besides, the asymptotic behaviors for the derived secrecy rate bounds at high signal-to-noise ratio (SNR) are analyzed. To further improve the secrecy performance, a channel adaptive selection (CAS) scheme and a greedy selection (GS) scheme are proposed to select the active transmitter. Numerical results show that the lower and upper bounds of the secrecy rate are tight. At high SNR, small asymptotic performance gaps exist between the derived lower and upper bounds. Moreover, the proposed GS scheme has the best performance, followed by the CAS scheme and the US scheme."},{"title":"Efficient Multi-objective Neural Architecture Search via Lamarckian\n  Evolution","source":"Thomas Elsken, Jan Hendrik Metzen, Frank Hutter","docs_id":"1804.09081","section":["stat.ML","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Efficient Multi-objective Neural Architecture Search via Lamarckian\n  Evolution. Neural Architecture Search aims at automatically finding neural architectures that are competitive with architectures designed by human experts. While recent approaches have achieved state-of-the-art predictive performance for image recognition, they are problematic under resource constraints for two reasons: (1)the neural architectures found are solely optimized for high predictive performance, without penalizing excessive resource consumption, (2) most architecture search methods require vast computational resources. We address the first shortcoming by proposing LEMONADE, an evolutionary algorithm for multi-objective architecture search that allows approximating the entire Pareto-front of architectures under multiple objectives, such as predictive performance and number of parameters, in a single run of the method. We address the second shortcoming by proposing a Lamarckian inheritance mechanism for LEMONADE which generates children networks that are warmstarted with the predictive performance of their trained parents. This is accomplished by using (approximate) network morphism operators for generating children. The combination of these two contributions allows finding models that are on par or even outperform both hand-crafted as well as automatically-designed networks."},{"title":"Topological robotics: motion planning in projective spaces","source":"Michael Farber, Serge Tabachnikov and Sergey Yuzvinsky","docs_id":"math\/0210018","section":["math.AT","cs.RO","math.DG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Topological robotics: motion planning in projective spaces. We study an elementary problem of topological robotics: rotation of a line, which is fixed by a revolving joint at a base point: one wants to bring the line from its initial position to a final position by a continuous motion in the space. The final goal is to construct an algorithm which will perform this task once the initial and final positions are given. Any such motion planning algorithm will have instabilities, which are caused by topological reasons. A general approach to study instabilities of robot motion was suggested recently by the first named author. With any path-connected topological space X one associates a number TC(X), called the topological complexity of X. This number is of fundamental importance for the motion planning problem: TC(X) determines character of instabilities which have all motion planning algorithms in X. In the present paper we study the topological complexity of real projective spaces. In particular we compute TC(RP^n) for all n<24. Our main result is that (for n distinct from 1, 3, 7) the problem of calculating of TC(RP^n) is equivalent to finding the smallest k such that RP^n can be immersed into the Euclidean space R^{k-1}."},{"title":"Robust Wireless Fingerprinting via Complex-Valued Neural Networks","source":"Soorya Gopalakrishnan, Metehan Cekic, Upamanyu Madhow","docs_id":"1905.09388","section":["eess.SP","cs.LG","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Robust Wireless Fingerprinting via Complex-Valued Neural Networks. A \"wireless fingerprint\" which exploits hardware imperfections unique to each device is a potentially powerful tool for wireless security. Such a fingerprint should be able to distinguish between devices sending the same message, and should be robust against standard spoofing techniques. Since the information in wireless signals resides in complex baseband, in this paper, we explore the use of neural networks with complex-valued weights to learn fingerprints using supervised learning. We demonstrate that, while there are potential benefits to using sections of the signal beyond just the preamble to learn fingerprints, the network cheats when it can, using information such as transmitter ID (which can be easily spoofed) to artificially inflate performance. We also show that noise augmentation by inserting additional white Gaussian noise can lead to significant performance gains, which indicates that this counter-intuitive strategy helps in learning more robust fingerprints. We provide results for two different wireless protocols, WiFi and ADS-B, demonstrating the effectiveness of the proposed method."},{"title":"UV-Net: Learning from Boundary Representations","source":"Pradeep Kumar Jayaraman, Aditya Sanghi, Joseph G. Lambourne, Karl D.D.\n  Willis, Thomas Davies, Hooman Shayani, Nigel Morris","docs_id":"2006.10211","section":["cs.CV","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"UV-Net: Learning from Boundary Representations. We introduce UV-Net, a novel neural network architecture and representation designed to operate directly on Boundary representation (B-rep) data from 3D CAD models. The B-rep format is widely used in the design, simulation and manufacturing industries to enable sophisticated and precise CAD modeling operations. However, B-rep data presents some unique challenges when used with modern machine learning due to the complexity of the data structure and its support for both continuous non-Euclidean geometric entities and discrete topological entities. In this paper, we propose a unified representation for B-rep data that exploits the U and V parameter domain of curves and surfaces to model geometry, and an adjacency graph to explicitly model topology. This leads to a unique and efficient network architecture, UV-Net, that couples image and graph convolutional neural networks in a compute and memory-efficient manner. To aid in future research we present a synthetic labelled B-rep dataset, SolidLetters, derived from human designed fonts with variations in both geometry and topology. Finally we demonstrate that UV-Net can generalize to supervised and unsupervised tasks on five datasets, while outperforming alternate 3D shape representations such as point clouds, voxels, and meshes."},{"title":"Snel: SQL Native Execution for LLVM","source":"Marcelo Mottalli, Alejo Sanchez, Gustavo Ajzenman, Carlos Sarraute","docs_id":"2002.09449","section":["cs.CY","cs.DB"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Snel: SQL Native Execution for LLVM. Snel is a relational database engine featuring Just-In-Time (JIT) compilation of queries and columnar data representation. Snel is designed for fast on-line analytics by leveraging the LLVM compiler infrastructure. It also has custom special methods like resolving histograms as extensions to the SQL language. \"Snel\" means \"SQL Native Execution for LLVM\". Unlike traditional database engines, it does not provide a client-server interface. Instead, it exposes its interface as an extension to SQLite, for a simple interactive usage from command line and for embedding in applications. Since Snel tables are read-only, it does not provide features like transactions or updates. This allows queries to be very fast since they don't have the overhead of table locking or ensuring consistency. At its core, Snel is simply a dynamic library that can be used by client applications. It has an SQLite extension for seamless integration with a traditional SQL environment and simple interactive usage from command line."},{"title":"Learning Recursive Segments for Discourse Parsing","source":"Stergos Afantenos, Pascal Denis, Philippe Muller, Laurence Danlos","docs_id":"1003.5372","section":["cs.CL"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Learning Recursive Segments for Discourse Parsing. Automatically detecting discourse segments is an important preliminary step towards full discourse parsing. Previous research on discourse segmentation have relied on the assumption that elementary discourse units (EDUs) in a document always form a linear sequence (i.e., they can never be nested). Unfortunately, this assumption turns out to be too strong, for some theories of discourse like SDRT allows for nested discourse units. In this paper, we present a simple approach to discourse segmentation that is able to produce nested EDUs. Our approach builds on standard multi-class classification techniques combined with a simple repairing heuristic that enforces global coherence. Our system was developed and evaluated on the first round of annotations provided by the French Annodis project (an ongoing effort to create a discourse bank for French). Cross-validated on only 47 documents (1,445 EDUs), our system achieves encouraging performance results with an F-score of 73% for finding EDUs."},{"title":"Working Locally Thinking Globally - Part I: Theoretical Guarantees for\n  Convolutional Sparse Coding","source":"Vardan Papyan, Jeremias Sulam and Michael Elad","docs_id":"1607.02005","section":["cs.IT","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Working Locally Thinking Globally - Part I: Theoretical Guarantees for\n  Convolutional Sparse Coding. The celebrated sparse representation model has led to remarkable results in various signal processing tasks in the last decade. However, despite its initial purpose of serving as a global prior for entire signals, it has been commonly used for modeling low dimensional patches due to the computational constraints it entails when deployed with learned dictionaries. A way around this problem has been proposed recently, adopting a convolutional sparse representation model. This approach assumes that the global dictionary is a concatenation of banded Circulant matrices. Although several works have presented algorithmic solutions to the global pursuit problem under this new model, very few truly-effective guarantees are known for the success of such methods. In the first of this two-part work, we address the theoretical aspects of the sparse convolutional model, providing the first meaningful answers to corresponding questions of uniqueness of solutions and success of pursuit algorithms. To this end, we generalize mathematical quantities, such as the $\\ell_0$ norm, the mutual coherence and the Spark, to their counterparts in the convolutional setting, which intrinsically capture local measures of the global model. In a companion paper, we extend the analysis to a noisy regime, addressing the stability of the sparsest solutions and pursuit algorithms, and demonstrate practical approaches for solving the global pursuit problem via simple local processing."},{"title":"Mathematical Analysis of the BIBEE Approximation for Molecular\n  Solvation: Exact Results for Spherical Inclusions","source":"Jaydeep P. Bardhan, Matthew G. Knepley","docs_id":"1109.0651","section":["cs.CE","physics.chem-ph","physics.comp-ph"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Mathematical Analysis of the BIBEE Approximation for Molecular\n  Solvation: Exact Results for Spherical Inclusions. We analyze the mathematically rigorous BIBEE (boundary-integral based electrostatics estimation) approximation of the mixed-dielectric continuum model of molecular electrostatics, using the analytically solvable case of a spherical solute containing an arbitrary charge distribution. Our analysis, which builds on Kirkwood's solution using spherical harmonics, clarifies important aspects of the approximation and its relationship to Generalized Born models. First, our results suggest a new perspective for analyzing fast electrostatic models: the separation of variables between material properties (the dielectric constants) and geometry (the solute dielectric boundary and charge distribution). Second, we find that the eigenfunctions of the reaction-potential operator are exactly preserved in the BIBEE model for the sphere, which supports the use of this approximation for analyzing charge-charge interactions in molecular binding. Third, a comparison of BIBEE to the recent GB$\\epsilon$ theory suggests a modified BIBEE model capable of predicting electrostatic solvation free energies to within 4% of a full numerical Poisson calculation. This modified model leads to a projection-framework understanding of BIBEE and suggests opportunities for future improvements."},{"title":"Using Chaotic Stream Cipher to Enhance Data Hiding in Digital Images","source":"Sana Haimour, Mohammad Rasmi AL-Mousa, Rashiq R. Marie","docs_id":"2101.00897","section":["cs.CR"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Using Chaotic Stream Cipher to Enhance Data Hiding in Digital Images. The growing potential of modern communications needs the use of secure means to protect information from unauthorized access and use during transmission. In general, encryption a message using cryptography techniques and then hidden a message with a steganography methods provides an additional layer of protection. Furthermore, using these combination reduces the chance of finding the hidden message. This paper proposed a system which combines schemes of cryptography with steganography for hiding secret messages and to add more complexity for steganography. The proposed system secret message encoded with chaotic stream cipher and afterwards the encoded data is hidden behind an RGB or Gray cover image by modifying the kth least significant bits (k-LSB) of cover image pixels. The resultant stego-image less distorters. After which can be used by the recipient to extract that bit-plane of the image. In fact, the schemes of encryption\/decryption and embedding\/ extracting in the proposed system depends upon two shred secret keys between the sender and the receiver. An experiment shows that using an unauthorized secret keys between the sender and the receiver have totally different messages from the original ones which improve the confidentiality of the images."},{"title":"Dual Attention-in-Attention Model for Joint Rain Streak and Raindrop\n  Removal","source":"Kaihao Zhang, Dongxu Li, Wenhan Luo, Wenqi Ren","docs_id":"2103.07051","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Dual Attention-in-Attention Model for Joint Rain Streak and Raindrop\n  Removal. Rain streaks and rain drops are two natural phenomena, which degrade image capture in different ways. Currently, most existing deep deraining networks take them as two distinct problems and individually address one, and thus cannot deal adequately with both simultaneously. To address this, we propose a Dual Attention-in-Attention Model (DAiAM) which includes two DAMs for removing both rain streaks and raindrops. Inside the DAM, there are two attentive maps - each of which attends to the heavy and light rainy regions, respectively, to guide the deraining process differently for applicable regions. In addition, to further refine the result, a Differential-driven Dual Attention-in-Attention Model (D-DAiAM) is proposed with a \"heavy-to-light\" scheme to remove rain via addressing the unsatisfying deraining regions. Extensive experiments on one public raindrop dataset, one public rain streak and our synthesized joint rain streak and raindrop (JRSRD) dataset have demonstrated that the proposed method not only is capable of removing rain streaks and raindrops simultaneously, but also achieves the state-of-the-art performance on both tasks."},{"title":"Context-based Barrier Notification Service Toward Outdoor Support for\n  the Elderly","source":"Keisuke Umezu, Takahiro Kawamura, and Akihiko Ohsuga","docs_id":"1307.3013","section":["cs.CY","cs.HC"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Context-based Barrier Notification Service Toward Outdoor Support for\n  the Elderly. Aging society has been becoming a global problem not only in advanced countries. Under such circumstances, it is said that participation of elderly people in social activities is highly desirable from various perspectives including decrease of social welfare costs. Thus, we propose a mobile service that notifies barrier information nearby users outside to lowers the anxiety of elderly people and promote their social activities. There are barrier free maps in some areas, but those are static and updated annually at the earliest. However, there exist temporary barriers like road repairing and parked bicycles, and also every barrier is not for every elder person. That is, the elder people are under several conditions and wills to go out, so that a barrier for an elder person is not necessarily the one for the other. Therefore, we first collect the barrier information in the user participatory manner and select the ones the user need to know, then timely provide them via a mobile phone equipped with GPS. This paper shows the public experiment that we conducted in Tokyo, and confirms the usability and the accuracy of the information filtering."},{"title":"Time-Scale-Chirp_rate Operator for Recovery of Non-stationary Signal\n  Components with Crossover Instantaneous Frequency Curves","source":"Charles K. Chui, Qingtang Jiang, Lin Li and Jian Lu","docs_id":"2012.14010","section":["math.NA","cs.NA","eess.SP"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Time-Scale-Chirp_rate Operator for Recovery of Non-stationary Signal\n  Components with Crossover Instantaneous Frequency Curves. The objective of this paper is to introduce an innovative approach for the recovery of non-stationary signal components with possibly cross-over instantaneous frequency (IF) curves from a multi-component blind-source signal. The main idea is to incorporate a chirp rate parameter with the time-scale continuous wavelet-like transformation, by considering the quadratic phase representation of the signal components. Hence-forth, even if two IF curves cross, the two corresponding signal components can still be separated and recovered, provided that their chirp rates are different. In other words, signal components with the same IF value at any time instant could still be recovered. To facilitate our presentation, we introduce the notion of time-scale-chirp_rate (TSC-R) recovery transform or TSC-R recovery operator to develop a TSC-R theory for the 3-dimensional space of time, scale, chirp rate. Our theoretical development is based on the approximation of the non-stationary signal components with linear chirps and applying the proposed adaptive TSC-R transform to the multi-component blind-source signal to obtain fairly accurate error bounds of IF estimations and signal components recovery. Several numerical experimental results are presented to demonstrate the out-performance of the proposed method over all existing time-frequency and time-scale approaches in the published literature, particularly for non-stationary source signals with crossover IFs."},{"title":"Unified Performance Analysis of Mixed Radio Frequency\/Free-Space Optical\n  Dual-Hop Transmission Systems","source":"Jiayi Zhang, Linglong Dai, Yu Zhang, Zhaocheng Wang","docs_id":"1507.04240","section":["cs.IT","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Unified Performance Analysis of Mixed Radio Frequency\/Free-Space Optical\n  Dual-Hop Transmission Systems. The mixed radio frequency (RF)\/free-space optical (FSO) relaying is a promising technology for coverage improvement, while there lacks unified expressions to describe its performance. In this paper, a unified performance analysis framework of a dual-hop relay system over asymmetric RF\/FSO links is presented. More specifically, we consider the RF link follows generalized $\\kappa$-$\\mu$ or $\\eta$-$\\mu$ distributions, while the FSO link experiences the gamma-gamma distribution, respectively. Novel analytical expressions of the probability density function and cumulative distribution function are derived. We then capitalize on these results to provide new exact analytical expressions of the outage probability and bit error rate (BER). Furthermore, the outage probability for high signal-to-noise ratios and the BER for different modulation schemes are deduced to provide useful insights into the impact of system and channel parameters of the overall system performance. These accurate expressions are general, since they correspond to generalized fading in the RF link and account for pointing errors, atmospheric turbulence and different modulation schemes in the FSO link. The links between derived results and previous results are presented. Finally, numerical and Monte-Carlo simulation results are provided to demonstrate the validity of the proposed unified expressions."},{"title":"Very Sparse Stable Random Projections, Estimators and Tail Bounds for\n  Stable Random Projections","source":"Ping Li","docs_id":"cs\/0611114","section":["cs.DS","cs.IT","cs.LG","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Very Sparse Stable Random Projections, Estimators and Tail Bounds for\n  Stable Random Projections. This paper will focus on three different aspects in improving the current practice of stable random projections. Firstly, we propose {\\em very sparse stable random projections} to significantly reduce the processing and storage cost, by replacing the $\\alpha$-stable distribution with a mixture of a symmetric $\\alpha$-Pareto distribution (with probability $\\beta$, $0<\\beta\\leq1$) and a point mass at the origin (with a probability $1-\\beta$). This leads to a significant $\\frac{1}{\\beta}$-fold speedup for small $\\beta$. Secondly, we provide an improved estimator for recovering the original $l_\\alpha$ norms from the projected data. The standard estimator is based on the (absolute) sample median, while we suggest using the geometric mean. The geometric mean estimator we propose is strictly unbiased and is easier to study. Moreover, the geometric mean estimator is more accurate, especially non-asymptotically. Thirdly, we provide an adequate answer to the basic question of how many projections (samples) are needed for achieving some pre-specified level of accuracy. \\cite{Proc:Indyk_FOCS00,Article:Indyk_TKDE03} did not provide a criterion that can be used in practice. The geometric mean estimator we propose allows us to derive sharp tail bounds which can be expressed in exponential forms with constants explicitly given."},{"title":"Model-based Convolutional De-Aliasing Network Learning for Parallel MR\n  Imaging","source":"Yanxia Chen, Taohui Xiao, Cheng Li, Qiegen Liu and Shanshan Wang","docs_id":"1908.02054","section":["eess.IV","cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Model-based Convolutional De-Aliasing Network Learning for Parallel MR\n  Imaging. Parallel imaging has been an essential technique to accelerate MR imaging. Nevertheless, the acceleration rate is still limited due to the ill-condition and challenges associated with the undersampled reconstruction. In this paper, we propose a model-based convolutional de-aliasing network with adaptive parameter learning to achieve accurate reconstruction from multi-coil undersampled k-space data. Three main contributions have been made: a de-aliasing reconstruction model was proposed to accelerate parallel MR imaging with deep learning exploring both spatial redundancy and multi-coil correlations; a split Bregman iteration algorithm was developed to solve the model efficiently; and unlike most existing parallel imaging methods which rely on the accuracy of the estimated multi-coil sensitivity, the proposed method can perform parallel reconstruction from undersampled data without explicit sensitivity calculation. Evaluations were conducted on \\emph{in vivo} brain dataset with a variety of undersampling patterns and different acceleration factors. Our results demonstrated that this method could achieve superior performance in both quantitative and qualitative analysis, compared to three state-of-the-art methods."},{"title":"Approximate Denial Constraints","source":"Ester Livshits, Alireza Heidari, Ihab F. Ilyas, and Benny Kimelfeld","docs_id":"2005.08540","section":["cs.DB"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Approximate Denial Constraints. The problem of mining integrity constraints from data has been extensively studied over the past two decades for commonly used types of constraints including the classic Functional Dependencies (FDs) and the more general Denial Constraints (DCs). In this paper, we investigate the problem of mining approximate DCs (i.e., DCs that are \"almost\" satisfied) from data. Considering approximate constraints allows us to discover more accurate constraints in inconsistent databases, detect rules that are generally correct but may have a few exceptions, as well as avoid overfitting and obtain more general and less contrived constraints. We introduce the algorithm ADCMiner for mining approximate DCs. An important feature of this algorithm is that it does not assume any specific definition of an approximate DC, but takes the semantics as input. Since there is more than one way to define an approximate DC and different definitions may produce very different results, we do not focus on one definition, but rather on a general family of approximation functions that satisfies some natural axioms defined in this paper and captures commonly used definitions of approximate constraints. We also show how our algorithm can be combined with sampling to return results with high accuracy while significantly reducing the running time."},{"title":"Lower bounds on the Probability of Error for Classical and\n  Classical-Quantum Channels","source":"Marco Dalai","docs_id":"1201.5411","section":["cs.IT","math.IT","quant-ph"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Lower bounds on the Probability of Error for Classical and\n  Classical-Quantum Channels. In this paper, lower bounds on error probability in coding for discrete classical and classical-quantum channels are studied. The contribution of the paper goes in two main directions: i) extending classical bounds of Shannon, Gallager and Berlekamp to classical-quantum channels, and ii) proposing a new framework for lower bounding the probability of error of channels with a zero-error capacity in the low rate region. The relation between these two problems is revealed by showing that Lov\\'asz' bound on zero-error capacity emerges as a natural consequence of the sphere packing bound once we move to the more general context of classical-quantum channels. A variation of Lov\\'asz' bound is then derived to lower bound the probability of error in the low rate region by means of auxiliary channels. As a result of this study, connections between the Lov\\'asz theta function, the expurgated bound of Gallager, the cutoff rate of a classical channel and the sphere packing bound for classical-quantum channels are established."},{"title":"A deep learning approach to real-time parking occupancy prediction in\n  spatio-temporal networks incorporating multiple spatio-temporal data sources","source":"Shuguan Yang, Wei Ma, Xidong Pi, Sean Qian","docs_id":"1901.06758","section":["cs.LG","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A deep learning approach to real-time parking occupancy prediction in\n  spatio-temporal networks incorporating multiple spatio-temporal data sources. A deep learning model is applied for predicting block-level parking occupancy in real time. The model leverages Graph-Convolutional Neural Networks (GCNN) to extract the spatial relations of traffic flow in large-scale networks, and utilizes Recurrent Neural Networks (RNN) with Long-Short Term Memory (LSTM) to capture the temporal features. In addition, the model is capable of taking multiple heterogeneously structured traffic data sources as input, such as parking meter transactions, traffic speed, and weather conditions. The model performance is evaluated through a case study in Pittsburgh downtown area. The proposed model outperforms other baseline methods including multi-layer LSTM and Lasso with an average testing MAPE of 10.6\\% when predicting block-level parking occupancies 30 minutes in advance. The case study also shows that, in generally, the prediction model works better for business areas than for recreational locations. We found that incorporating traffic speed and weather information can significantly improve the prediction performance. Weather data is particularly useful for improving predicting accuracy in recreational areas."},{"title":"Actor-Critic Method for High Dimensional Static\n  Hamilton--Jacobi--Bellman Partial Differential Equations based on Neural\n  Networks","source":"Mo Zhou, Jiequn Han and Jianfeng Lu","docs_id":"2102.11379","section":["math.OC","cs.NA","math.NA"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Actor-Critic Method for High Dimensional Static\n  Hamilton--Jacobi--Bellman Partial Differential Equations based on Neural\n  Networks. We propose a novel numerical method for high dimensional Hamilton--Jacobi--Bellman (HJB) type elliptic partial differential equations (PDEs). The HJB PDEs, reformulated as optimal control problems, are tackled by the actor-critic framework inspired by reinforcement learning, based on neural network parametrization of the value and control functions. Within the actor-critic framework, we employ a policy gradient approach to improve the control, while for the value function, we derive a variance reduced least-squares temporal difference method using stochastic calculus. To numerically discretize the stochastic control problem, we employ an adaptive step size scheme to improve the accuracy near the domain boundary. Numerical examples up to $20$ spatial dimensions including the linear quadratic regulators, the stochastic Van der Pol oscillators, the diffusive Eikonal equations, and fully nonlinear elliptic PDEs derived from a regulator problem are presented to validate the effectiveness of our proposed method."},{"title":"Finite-time influence systems and the Wisdom of Crowd effect","source":"Francesco Bullo, Fabio Fagnani, Barbara Franci","docs_id":"1902.03827","section":["cs.SY"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Finite-time influence systems and the Wisdom of Crowd effect. Recent contributions have studied how an influence system may affect the wisdom of crowd phenomenon. In the so-called naive learning setting, a crowd of individuals holds opinions that are statistically independent estimates of an unknown parameter; the crowd is wise when the average opinion converges to the true parameter in the limit of infinitely many individuals. Unfortunately, even starting from wise initial opinions, a crowd subject to certain influence systems may lose its wisdom. It is of great interest to characterize when an influence system preserves the crowd wisdom effect. In this paper we introduce and characterize numerous wisdom preservation properties of the basic French-DeGroot influence system model. Instead of requiring complete convergence to consensus as in the previous naive learning model by Golub and Jackson, we study finite-time executions of the French-DeGroot influence process and establish in this novel context the notion of prominent families (as a group of individuals with outsize influence). Surprisingly, finite-time wisdom preservation of the influence system is strictly distinct from its infinite-time version. We provide a comprehensive treatment of various finite-time wisdom preservation notions, counterexamples to meaningful conjectures, and a complete characterization of equal-neighbor influence systems."},{"title":"Boosting Unconstrained Face Recognition with Auxiliary Unlabeled Data","source":"Yichun Shi, Anil K. Jain","docs_id":"2003.07936","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Boosting Unconstrained Face Recognition with Auxiliary Unlabeled Data. In recent years, significant progress has been made in face recognition, which can be partially attributed to the availability of large-scale labeled face datasets. However, since the faces in these datasets usually contain limited degree and types of variation, the resulting trained models generalize poorly to more realistic unconstrained face datasets. While collecting labeled faces with larger variations could be helpful, it is practically infeasible due to privacy and labor cost. In comparison, it is easier to acquire a large number of unlabeled faces from different domains, which could be used to regularize the learning of face representations. We present an approach to use such unlabeled faces to learn generalizable face representations, where we assume neither the access to identity labels nor domain labels for unlabeled images. Experimental results on unconstrained datasets show that a small amount of unlabeled data with sufficient diversity can (i) lead to an appreciable gain in recognition performance and (ii) outperform the supervised baseline when combined with less than half of the labeled data. Compared with the state-of-the-art face recognition methods, our method further improves their performance on challenging benchmarks, such as IJB-B, IJB-C and IJB-S."},{"title":"DeepProposal: Hunting Objects by Cascading Deep Convolutional Layers","source":"Amir Ghodrati, Ali Diba, Marco Pedersoli, Tinne Tuytelaars, Luc Van\n  Gool","docs_id":"1510.04445","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"DeepProposal: Hunting Objects by Cascading Deep Convolutional Layers. In this paper we evaluate the quality of the activation layers of a convolutional neural network (CNN) for the gen- eration of object proposals. We generate hypotheses in a sliding-window fashion over different activation layers and show that the final convolutional layers can find the object of interest with high recall but poor localization due to the coarseness of the feature maps. Instead, the first layers of the network can better localize the object of interest but with a reduced recall. Based on this observation we design a method for proposing object locations that is based on CNN features and that combines the best of both worlds. We build an inverse cascade that, going from the final to the initial convolutional layers of the CNN, selects the most promising object locations and refines their boxes in a coarse-to-fine manner. The method is efficient, because i) it uses the same features extracted for detection, ii) it aggregates features using integral images, and iii) it avoids a dense evaluation of the proposals due to the inverse coarse-to-fine cascade. The method is also accurate; it outperforms most of the previously proposed object proposals approaches and when plugged into a CNN-based detector produces state-of-the- art detection performance."},{"title":"Application of Machine Learning in Rock Facies Classification with\n  Physics-Motivated Feature Augmentation","source":"Jie Chen, Yu Zeng (Corresponding author)","docs_id":"1808.09856","section":["stat.ML","cs.LG","physics.geo-ph"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Application of Machine Learning in Rock Facies Classification with\n  Physics-Motivated Feature Augmentation. With recent progress in algorithms and the availability of massive amounts of computation power, application of machine learning techniques is becoming a hot topic in the oil and gas industry. One of the most promising aspects to apply machine learning to the upstream field is the rock facies classification in reservoir characterization, which is crucial in determining the net pay thickness of reservoirs, thus a definitive factor in drilling decision making process. For complex machine learning tasks like facies classification, feature engineering is often critical. This paper shows the inclusion of physics-motivated feature interaction in feature augmentation can further improve the capability of machine learning in rock facies classification. We demonstrate this approach with the SEG 2016 machine learning contest dataset and the top winning algorithms. The improvement is roboust and can be $\\sim5\\%$ better than current existing best F-1 score, where F-1 is an evaluation metric used to quantify average prediction accuracy."},{"title":"Distributed adaptive stabilization","source":"Zhiyong Sun, Anders Rantzer, Zhongkui Li, Anders Robertsson","docs_id":"2105.14004","section":["eess.SY","cs.DC","cs.MA","cs.SY","math.OC","nlin.AO"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Distributed adaptive stabilization. In this paper we consider distributed adaptive stabilization for uncertain multivariable linear systems with a time-varying diagonal matrix gain. We show that uncertain multivariable linear systems are stabilizable by diagonal matrix high gains if the system matrix is an H-matrix with positive diagonal entries. Based on matrix measure and stability theory for diagonally dominant systems, we consider two classes of uncertain linear systems, and derive a threshold condition to ensure their exponential stability by a monotonically increasing diagonal gain matrix. When each individual gain function in the matrix gain is updated by state-dependent functions using only local state information, the boundedness and convergence of both system states and adaptive matrix gains are guaranteed. We apply the adaptive distributed stabilization approach to adaptive synchronization control for large-scale complex networks consisting of nonlinear node dynamics and time-varying coupling weights. A unified framework for adaptive synchronization is proposed that includes several general design approaches for adaptive coupling weights to guarantee network synchronization."},{"title":"Distributed Power Control Schemes for In-Band Full-Duplex Energy\n  Harvesting Wireless Networks","source":"Rojin Aslani, Mehdi Rasti","docs_id":"1807.07622","section":["eess.SP","cs.NI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Distributed Power Control Schemes for In-Band Full-Duplex Energy\n  Harvesting Wireless Networks. This paper studies two power control problems in energy harvesting wireless networks where one hybrid base station (HBS) and all user equipments (UEs) are operating in in-band full-duplex mode. We consider minimizing the aggregate power subject to the quality of service requirement constraint, and maximizing the aggregate throughput. We address these two problems by proposing two distributed power control schemes for controlling the uplink transmit power by the UEs and the downlink energy harvesting signal power by the HBS. In our proposed schemes, the HBS updates the downlink transmit power level of the energy-harvesting signal so that each UE is enabled to harvest its required energy for powering the operating circuit and transmitting its uplink information signal with the power level determined by the proposed schemes. We show that our proposed power control schemes converge to their corresponding unique fixed points starting from any arbitrary initial transmit power. We will show that our proposed schemes well address the stated problems, which is also demonstrated by our extensive simulation results."},{"title":"A Long-Term Analysis of Polarization on Twitter","source":"Kiran Garimella, Ingmar Weber","docs_id":"1703.02769","section":["cs.SI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A Long-Term Analysis of Polarization on Twitter. Social media has played an important role in shaping political discourse over the last decade. At the same time, it is often perceived to have increased political polarization, thanks to the scale of discussions and their public nature. In this paper, we try to answer the question of whether political polarization in the US on Twitter has increased over the last eight years. We analyze a large longitudinal Twitter dataset of 679,000 users and look at signs of polarization in their (i) network - how people follow political and media accounts, (ii) tweeting behavior - whether they retweet content from both sides, and (iii) content - how partisan the hashtags they use are. Our analysis shows that online polarization has indeed increased over the past eight years and that, depending on the measure, the relative change is 10%-20%. Our study is one of very few with such a long-term perspective, encompassing two US presidential elections and two mid-term elections, providing a rare longitudinal analysis."},{"title":"Transparency's Influence on Human-Collective Interactions","source":"Karina A. Roundtree and Jason R. Cody and Jennifer Leaf and H. Onan\n  Demirel and Julie A. Adams","docs_id":"2009.09859","section":["cs.HC"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Transparency's Influence on Human-Collective Interactions. Collective robotic systems are biologically inspired and advantageous due to their apparent global intelligence and emergent behaviors. Many applications can benefit from the incorporation of collectives, including environmental monitoring, disaster response missions, and infrastructure support. Transparency research has primarily focused on how the design of the models, visualizations, and control mechanisms influence human-collective interactions. Traditionally most evaluations have focused only on one particular system design element, evaluating its respective transparency. This manuscript analyzed two models and visualizations to understand how the system design elements impacted human-collective interactions, to quantify which model and visualization combination provided the best transparency, and provide design guidance, based on remote supervision of collectives. The consensus decision-making and baseline models, as well as an individual agent and abstract visualizations, were analyzed for sequential best-of-n decision-making tasks involving four collectives, composed of 200 entities each. Both models and visualizations provided transparency and influenced human-collective interactions differently. No single combination provided the best transparency."},{"title":"Enabling Incremental Training with Forward Pass for Edge Devices","source":"Dana AbdulQader, Shoba Krishnan, Claudionor N. Coelho Jr","docs_id":"2103.14007","section":["cs.LG","cs.AR"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Enabling Incremental Training with Forward Pass for Edge Devices. Deep Neural Networks (DNNs) are commonly deployed on end devices that exist in constantly changing environments. In order for the system to maintain it's accuracy, it is critical that it is able to adapt to changes and recover by retraining parts of the network. However, end devices have limited resources making it challenging to train on the same device. Moreover, training deep neural networks is both memory and compute intensive due to the backpropagation algorithm. In this paper we introduce a method using evolutionary strategy (ES) that can partially retrain the network enabling it to adapt to changes and recover after an error has occurred. This technique enables training on an inference-only hardware without the need to use backpropagation and with minimal resource overhead. We demonstrate the ability of our technique to retrain a quantized MNIST neural network after injecting noise to the input. Furthermore, we present the micro-architecture required to enable training on HLS4ML (an inference hardware architecture) and implement it in Verilog. We synthesize our implementation for a Xilinx Kintex Ultrascale Field Programmable Gate Array (FPGA) resulting in less than 1% resource utilization required to implement the incremental training."},{"title":"Orientation-aware Semantic Segmentation on Icosahedron Spheres","source":"Chao Zhang, Stephan Liwicki, William Smith, Roberto Cipolla","docs_id":"1907.12849","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Orientation-aware Semantic Segmentation on Icosahedron Spheres. We address semantic segmentation on omnidirectional images, to leverage a holistic understanding of the surrounding scene for applications like autonomous driving systems. For the spherical domain, several methods recently adopt an icosahedron mesh, but systems are typically rotation invariant or require significant memory and parameters, thus enabling execution only at very low resolutions. In our work, we propose an orientation-aware CNN framework for the icosahedron mesh. Our representation allows for fast network operations, as our design simplifies to standard network operations of classical CNNs, but under consideration of north-aligned kernel convolutions for features on the sphere. We implement our representation and demonstrate its memory efficiency up-to a level-8 resolution mesh (equivalent to 640 x 1024 equirectangular images). Finally, since our kernels operate on the tangent of the sphere, standard feature weights, pretrained on perspective data, can be directly transferred with only small need for weight refinement. In our evaluation our orientation-aware CNN becomes a new state of the art for the recent 2D3DS dataset, and our Omni-SYNTHIA version of SYNTHIA. Rotation invariant classification and segmentation tasks are additionally presented for comparison to prior art."},{"title":"Neural Collision Clearance Estimator for Batched Motion Planning","source":"J. Chase Kew, Brian Ichter, Maryam Bandari, Tsang-Wei Edward Lee,\n  Aleksandra Faust","docs_id":"1910.05917","section":["cs.RO","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Neural Collision Clearance Estimator for Batched Motion Planning. We present a neural network collision checking heuristic, ClearanceNet, and a planning algorithm, CN-RRT. ClearanceNet learns to predict separation distance (minimum distance between robot and workspace) with respect to a workspace. CN-RRT then efficiently computes a motion plan by leveraging three key features of ClearanceNet. First, CN-RRT explores the space by expanding multiple nodes at the same time, processing batches of thousands of collision checks. Second, CN-RRT adaptively relaxes its clearance requirements for more difficult problems. Third, to repair errors, CN-RRT shifts its nodes in the direction of ClearanceNet's gradient and repairs any residual errors with a traditional RRT, thus maintaining theoretical probabilistic completeness guarantees. In configuration spaces with up to 30 degrees of freedom, ClearanceNet achieves 845x speedup over traditional collision detection methods, while CN-RRT accelerates motion planning by up to 42% over a baseline and finds paths up to 36% more efficient. Experiments on an 11 degree of freedom robot in a cluttered environment confirm the method's feasibility on real robots."},{"title":"Using content features to enhance performance of user-based\n  collaborative filtering performance of user-based collaborative filtering","source":"Niloofar Rastin and Mansoor Zolghadri Jahromi","docs_id":"1402.2145","section":["cs.IR"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Using content features to enhance performance of user-based\n  collaborative filtering performance of user-based collaborative filtering. Content-based and collaborative filtering methods are the most successful solutions in recommender systems. Content based method is based on items attributes. This method checks the features of users favourite items and then proposes the items which have the most similar characteristics with those items. Collaborative filtering method is based on the determination of similar items or similar users, which are called item-based and user-based collaborative filtering, respectively.In this paper we propose a hybrid method that integrates collaborative filtering and content-based methods. The proposed method can be viewed as user-based Collaborative filtering technique. However to find users with similar taste with active user, we used content features of the item under investigation to put more emphasis on users rating for similar items. In other words two users are similar if their ratings are similar on items that have similar context. This is achieved by assigning a weight to each rating when calculating the similarity of two users.We used movielens data set to access the performance of the proposed method in comparison with basic user-based collaborative filtering and other popular methods."},{"title":"Arbitrary Pattern Formation by Opaque Fat Robots with Lights","source":"Kaustav Bose, Ranendu Adhikary, Manash Kumar Kundu, Buddhadeb Sau","docs_id":"1910.02706","section":["cs.DC"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Arbitrary Pattern Formation by Opaque Fat Robots with Lights. Arbitrary Pattern Formation is a widely studied problem in autonomous robot systems. The problem asks to design a distributed algorithm that moves a team of autonomous, anonymous and identical mobile robots to form any arbitrary pattern given as input. The majority of the existing literature investigates this problem for robots with unobstructed visibility. In a few recent works, the problem has been studied in the obstructed visibility model, where the view of a robot can be obstructed by the presence of other robots. However, in these works, the robots have been modelled as dimensionless points in the plane. In this paper, we have considered the problem in the more realistic setting where the robots have a physical extent. In particular, the robots are modelled as opaque disks. Furthermore, the robots operate under a fully asynchronous scheduler. They do not have access to any global coordinate system, but agree on the direction and orientation of one coordinate axis. Each robot is equipped with an externally visible light which can assume a constant number of predefined colors. In this setting, we have given a complete characterization of initial configurations from where any arbitrary pattern can be formed by a deterministic distributed algorithm."},{"title":"Improving the Integrality Gap for Multiway Cut","source":"Krist\\'of B\\'erczi, Karthekeyan Chandrasekaran, Tam\\'as Kir\\'aly,\n  Vivek Madan","docs_id":"1807.09735","section":["cs.DS","math.OC"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Improving the Integrality Gap for Multiway Cut. In the multiway cut problem, we are given an undirected graph with non-negative edge weights and a collection of $k$ terminal nodes, and the goal is to partition the node set of the graph into $k$ non-empty parts each containing exactly one terminal so that the total weight of the edges crossing the partition is minimized. The multiway cut problem for $k\\ge 3$ is APX-hard. For arbitrary $k$, the best-known approximation factor is $1.2965$ due to [Sharma and Vondr\\'{a}k, 2014] while the best known inapproximability factor is $1.2$ due to [Angelidakis, Makarychev and Manurangsi, 2017]. In this work, we improve on the lower bound to $1.20016$ by constructing an integrality gap instance for the CKR relaxation. A technical challenge in improving the gap has been the lack of geometric tools to understand higher-dimensional simplices. Our instance is a non-trivial $3$-dimensional instance that overcomes this technical challenge. We analyze the gap of the instance by viewing it as a convex combination of $2$-dimensional instances and a uniform 3-dimensional instance. We believe that this technique could be exploited further to construct instances with larger integrality gap. One of the ingredients of our proof technique is a generalization of a result on \\emph{Sperner admissible labelings} due to [Mirzakhani and Vondr\\'{a}k, 2015] that might be of independent combinatorial interest."},{"title":"Value of peripheral nodes in controlling multilayer networks","source":"Yan Zhang, Antonios Garas, Frank Schweitzer","docs_id":"1506.02963","section":["physics.soc-ph","cs.SI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Value of peripheral nodes in controlling multilayer networks. We analyze the controllability of a two-layer network, where driver nodes can be chosen randomly only from one layer. Each layer contains a scale-free network with directed links and the node dynamics depends on the incoming links from other nodes. We combine the in-degree and out-degree values to assign an importance value $w$ to each node, and distinguish between peripheral nodes with low $w$ and central nodes with high $w$. Based on numerical simulations, we find that the controllable part of the network is larger when choosing low $w$ nodes to connect the two layers. The control is as efficient when peripheral nodes are driver nodes as it is for the case of more central nodes. However, if we assume a cost to utilize nodes that is proportional to their overall degree, utilizing peripheral nodes to connect the two layers or to act as driver nodes is not only the most cost-efficient solution, it is also the one that performs best in controlling the two-layer network among the different interconnecting strategies we have tested."},{"title":"Thinging-Based Conceptual Modeling: Case Study of a Tendering System","source":"Sabah Al-Fedaghi and Esraa Haidar","docs_id":"2007.00168","section":["cs.SE"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Thinging-Based Conceptual Modeling: Case Study of a Tendering System. In computer science, models are made explicit to provide formality and a precise understanding of small, contingent universes (e.g., an organization), as constructed from stakeholder requirements. Conceptual modeling is a fundamental discipline in this context whose main concerns are identifying, analyzing and describing the critical concepts of a universe of discourse. In the information systems field, one of the reasons why projects fail is an inability to capture requirements in a way that can be technically used to configure a system. This problem of requirements specification is considered to have deficiencies in theory. We apply a recently developed model called the Thinging Machine (TM) model which uniformly integrates static and dynamic modeling features to this problem of requirements specification. The object-Oriented (OO) approach to modeling, as applied in Unified Modeling Language, is by far the most applied and accepted standard in software engineering; nevertheless, new notions in the field may enhance and facilitate a supplementary understanding of the OO model itself. We aim to contribute to the field of conceptual modeling by introducing the TM model s philosophical foundation of requirements analysis. The TM model has only five generic processes of things (e.g., objects), in which genericity indicates generality, as in the generic Aristotelian concepts based on abstraction. We show the TM model s viability by applying it to a real business system."},{"title":"An Accelerated Decentralized Stochastic Proximal Algorithm for Finite\n  Sums","source":"Hadrien Hendrikx, Francis Bach and Laurent Massoulie","docs_id":"1905.11394","section":["math.OC","cs.DC"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"An Accelerated Decentralized Stochastic Proximal Algorithm for Finite\n  Sums. Modern large-scale finite-sum optimization relies on two key aspects: distribution and stochastic updates. For smooth and strongly convex problems, existing decentralized algorithms are slower than modern accelerated variance-reduced stochastic algorithms when run on a single machine, and are therefore not efficient. Centralized algorithms are fast, but their scaling is limited by global aggregation steps that result in communication bottlenecks. In this work, we propose an efficient \\textbf{A}ccelerated \\textbf{D}ecentralized stochastic algorithm for \\textbf{F}inite \\textbf{S}ums named ADFS, which uses local stochastic proximal updates and randomized pairwise communications between nodes. On $n$ machines, ADFS learns from $nm$ samples in the same time it takes optimal algorithms to learn from $m$ samples on one machine. This scaling holds until a critical network size is reached, which depends on communication delays, on the number of samples $m$, and on the network topology. We provide a theoretical analysis based on a novel augmented graph approach combined with a precise evaluation of synchronization times and an extension of the accelerated proximal coordinate gradient algorithm to arbitrary sampling. We illustrate the improvement of ADFS over state-of-the-art decentralized approaches with experiments."},{"title":"Adaptive Energy-aware Scheduling of Dynamic Event Analytics across Edge\n  and Cloud Resources","source":"Rajrup Ghosh, Siva Prakash Reddy Komma and Yogesh Simmhan","docs_id":"1801.01087","section":["cs.DC"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Adaptive Energy-aware Scheduling of Dynamic Event Analytics across Edge\n  and Cloud Resources. The growing deployment of sensors as part of Internet of Things (IoT) is generating thousands of event streams. Complex Event Processing (CEP) queries offer a useful paradigm for rapid decision-making over such data sources. While often centralized in the Cloud, the deployment of capable edge devices on the field motivates the need for cooperative event analytics that span Edge and Cloud computing. Here, we identify a novel problem of query placement on edge and Cloud resources for dynamically arriving and departing analytic dataflows. We define this as an optimization problem to minimize the total makespan for all event analytics, while meeting energy and compute constraints of the resources. We propose 4 adaptive heuristics and 3 rebalancing strategies for such dynamic dataflows, and validate them using detailed simulations for 100 - 1000 edge devices and VMs. The results show that our heuristics offer O(seconds) planning time, give a valid and high quality solution in all cases, and reduce the number of query migrations. Furthermore, rebalance strategies when applied in these heuristics have significantly reduced the makespan by around 20 - 25%."},{"title":"Stochastic Convolutional Sparse Coding","source":"Jinhui Xiong, Peter Richt\\'arik, Wolfgang Heidrich","docs_id":"1909.00145","section":["eess.IV","cs.LG","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Stochastic Convolutional Sparse Coding. State-of-the-art methods for Convolutional Sparse Coding usually employ Fourier-domain solvers in order to speed up the convolution operators. However, this approach is not without shortcomings. For example, Fourier-domain representations implicitly assume circular boundary conditions and make it hard to fully exploit the sparsity of the problem as well as the small spatial support of the filters. In this work, we propose a novel stochastic spatial-domain solver, in which a randomized subsampling strategy is introduced during the learning sparse codes. Afterwards, we extend the proposed strategy in conjunction with online learning, scaling the CSC model up to very large sample sizes. In both cases, we show experimentally that the proposed subsampling strategy, with a reasonable selection of the subsampling rate, outperforms the state-of-the-art frequency-domain solvers in terms of execution time without losing the learning quality. Finally, we evaluate the effectiveness of the over-complete dictionary learned from large-scale datasets, which demonstrates an improved sparse representation of the natural images on account of more abundant learned image features."},{"title":"Living on the Edge: The Role of Proactive Caching in 5G Wireless\n  Networks","source":"Ejder Ba\\c{s}tu\\u{g}, Mehdi Bennis, M\\'erouane Debbah","docs_id":"1405.5974","section":["cs.NI","cs.IT","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Living on the Edge: The Role of Proactive Caching in 5G Wireless\n  Networks. This article explores one of the key enablers of beyond $4$G wireless networks leveraging small cell network deployments, namely proactive caching. Endowed with predictive capabilities and harnessing recent developments in storage, context-awareness and social networks, peak traffic demands can be substantially reduced by proactively serving predictable user demands, via caching at base stations and users' devices. In order to show the effectiveness of proactive caching, we examine two case studies which exploit the spatial and social structure of the network, where proactive caching plays a crucial role. Firstly, in order to alleviate backhaul congestion, we propose a mechanism whereby files are proactively cached during off-peak demands based on file popularity and correlations among users and files patterns. Secondly, leveraging social networks and device-to-device (D2D) communications, we propose a procedure that exploits the social structure of the network by predicting the set of influential users to (proactively) cache strategic contents and disseminate them to their social ties via D2D communications. Exploiting this proactive caching paradigm, numerical results show that important gains can be obtained for each case study, with backhaul savings and a higher ratio of satisfied users of up to $22\\%$ and $26\\%$, respectively. Higher gains can be further obtained by increasing the storage capability at the network edge."},{"title":"A Process to Facilitate Automated Automotive Cybersecurity Testing","source":"Stefan Marksteiner, Nadja Marko, Andre Smulders, Stelios Karagiannis,\n  Florian Stahl, Hayk Hamazaryan, Rupert Schlick, Stefan Kraxberger, Alexandr\n  Vasenev","docs_id":"2101.10048","section":["cs.CR"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A Process to Facilitate Automated Automotive Cybersecurity Testing. Modern vehicles become increasingly digitalized with advanced information technology-based solutions like advanced driving assistance systems and vehicle-to-x communications. These systems are complex and interconnected. Rising complexity and increasing outside exposure has created a steadily rising demand for more cyber-secure systems. Thus, also standardization bodies and regulators issued standards and regulations to prescribe more secure development processes. This security, however, also has to be validated and verified. In order to keep pace with the need for more thorough, quicker and comparable testing, today's generally manual testing processes have to be structured and optimized. Based on existing and emerging standards for cybersecurity engineering, this paper therefore outlines a structured testing process for verifying and validating automotive cybersecurity, for which there is no standardized method so far. Despite presenting a commonly structured framework, the process is flexible in order to allow implementers to utilize their own, accustomed toolsets."},{"title":"Keeping Your Eye on the Ball: Trajectory Attention in Video Transformers","source":"Mandela Patrick, Dylan Campbell, Yuki M. Asano, Ishan Misra, Florian\n  Metze, Christoph Feichtenhofer, Andrea Vedaldi, Jo\\~ao F. Henriques","docs_id":"2106.05392","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Keeping Your Eye on the Ball: Trajectory Attention in Video Transformers. In video transformers, the time dimension is often treated in the same way as the two spatial dimensions. However, in a scene where objects or the camera may move, a physical point imaged at one location in frame $t$ may be entirely unrelated to what is found at that location in frame $t+k$. These temporal correspondences should be modeled to facilitate learning about dynamic scenes. To this end, we propose a new drop-in block for video transformers -- trajectory attention -- that aggregates information along implicitly determined motion paths. We additionally propose a new method to address the quadratic dependence of computation and memory on the input size, which is particularly important for high resolution or long videos. While these ideas are useful in a range of settings, we apply them to the specific task of video action recognition with a transformer model and obtain state-of-the-art results on the Kinetics, Something--Something V2, and Epic-Kitchens datasets. Code and models are available at: https:\/\/github.com\/facebookresearch\/Motionformer"},{"title":"EyeTAP: A Novel Technique using Voice Inputs to Address the Midas Touch\n  Problem for Gaze-based Interactions","source":"Mohsen Parisay, Charalambos Poullis, Marta Kersten","docs_id":"2002.08455","section":["cs.HC"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"EyeTAP: A Novel Technique using Voice Inputs to Address the Midas Touch\n  Problem for Gaze-based Interactions. One of the main challenges of gaze-based interactions is the ability to distinguish normal eye function from a deliberate interaction with the computer system, commonly referred to as 'Midas touch'. In this paper we propose, EyeTAP (Eye tracking point-and-select by Targeted Acoustic Pulse) a hands-free interaction method for point-and-select tasks. We evaluated the prototype in two separate user studies, each containing two experiments with 33 participants and found that EyeTAP is robust even in presence of ambient noise in the audio input signal with tolerance of up to 70 dB, results in a faster movement time, and faster task completion time, and has a lower cognitive workload than voice recognition. In addition, EyeTAP has a lower error rate than the dwell-time method in a ribbon-shaped experiment. These characteristics make it applicable for users for whom physical movements are restricted or not possible due to a disability. Furthermore, EyeTAP has no specific requirements in terms of user interface design and therefore it can be easily integrated into existing systems with minimal modifications. EyeTAP can be regarded as an acceptable alternative to address the Midas touch."},{"title":"An Intuitionistic Formula Hierarchy Based on High-School Identities","source":"Taus Brock-Nannestad and Danko Ilik","docs_id":"1601.04876","section":["math.LO","cs.LO"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"An Intuitionistic Formula Hierarchy Based on High-School Identities. We revisit the notion of intuitionistic equivalence and formal proof representations by adopting the view of formulas as exponential polynomials. After observing that most of the invertible proof rules of intuitionistic (minimal) propositional sequent calculi are formula (i.e. sequent) isomorphisms corresponding to the high-school identities, we show that one can obtain a more compact variant of a proof system, consisting of non-invertible proof rules only, and where the invertible proof rules have been replaced by a formula normalisation procedure. Moreover, for certain proof systems such as the G4ip sequent calculus of Vorob'ev, Hudelmaier, and Dyckhoff, it is even possible to see all of the non-invertible proof rules as strict inequalities between exponential polynomials; a careful combinatorial treatment is given in order to establish this fact. Finally, we extend the exponential polynomial analogy to the first-order quantifiers, showing that it gives rise to an intuitionistic hierarchy of formulas, resembling the classical arithmetical hierarchy, and the first one that classifies formulas while preserving isomorphism."},{"title":"Documenting Large Webtext Corpora: A Case Study on the Colossal Clean\n  Crawled Corpus","source":"Jesse Dodge, Maarten Sap, Ana Marasovi\\'c, William Agnew, Gabriel\n  Ilharco, Dirk Groeneveld, Margaret Mitchell, Matt Gardner","docs_id":"2104.08758","section":["cs.CL","cs.AI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Documenting Large Webtext Corpora: A Case Study on the Colossal Clean\n  Crawled Corpus. Large language models have led to remarkable progress on many NLP tasks, and researchers are turning to ever-larger text corpora to train them. Some of the largest corpora available are made by scraping significant portions of the internet, and are frequently introduced with only minimal documentation. In this work we provide some of the first documentation for the Colossal Clean Crawled Corpus (C4; Raffel et al., 2020), a dataset created by applying a set of filters to a single snapshot of Common Crawl. We begin by investigating where the data came from, and find a significant amount of text from unexpected sources like patents and US military websites. Then we explore the content of the text itself, and find machine-generated text (e.g., from machine translation systems) and evaluation examples from other benchmark NLP datasets. To understand the impact of the filters applied to create this dataset, we evaluate the text that was removed, and show that blocklist filtering disproportionately removes text from and about minority individuals. Finally, we conclude with some recommendations for how to created and document web-scale datasets from a scrape of the internet."},{"title":"Fundamentals of the Backoff Process in 802.11: Dichotomy of the\n  Aggregation","source":"Jeong-woo Cho, Yuming Jiang","docs_id":"0904.4155","section":["cs.NI","cs.PF"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Fundamentals of the Backoff Process in 802.11: Dichotomy of the\n  Aggregation. This paper discovers fundamental principles of the backoff process that governs the performance of IEEE 802.11. A simplistic principle founded upon regular variation theory is that the backoff time has a truncated Pareto-type tail distribution with an exponent of $(\\log \\gamma)\/\\log m$ ($m$ is the multiplicative factor and $\\gamma$ is the collision probability). This reveals that the per-node backoff process is heavy-tailed in the strict sense for $\\gamma>1\/m^2$, and paves the way for the following unifying result. The state-of-the-art theory on the superposition of the heavy-tailed processes is applied to establish a dichotomy exhibited by the aggregate backoff process, putting emphasis on the importance of time-scale on which we view the backoff processes. While the aggregation on normal time-scales leads to a Poisson process, it is approximated by a new limiting process possessing long-range dependence (LRD) on coarse time-scales. This dichotomy turns out to be instrumental in formulating short-term fairness, extending existing formulas to arbitrary population, and to elucidate the absence of LRD in practical situations. A refined wavelet analysis is conducted to strengthen this argument."},{"title":"A Generalization of Permanent Inequalities and Applications in Counting\n  and Optimization","source":"Nima Anari and Shayan Oveis Gharan","docs_id":"1702.02937","section":["cs.DS","cs.DM","cs.IT","math.CO","math.IT","math.PR"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A Generalization of Permanent Inequalities and Applications in Counting\n  and Optimization. A polynomial $p\\in\\mathbb{R}[z_1,\\dots,z_n]$ is real stable if it has no roots in the upper-half complex plane. Gurvits's permanent inequality gives a lower bound on the coefficient of the $z_1z_2\\dots z_n$ monomial of a real stable polynomial $p$ with nonnegative coefficients. This fundamental inequality has been used to attack several counting and optimization problems. Here, we study a more general question: Given a stable multilinear polynomial $p$ with nonnegative coefficients and a set of monomials $S$, we show that if the polynomial obtained by summing up all monomials in $S$ is real stable, then we can lowerbound the sum of coefficients of monomials of $p$ that are in $S$. We also prove generalizations of this theorem to (real stable) polynomials that are not multilinear. We use our theorem to give a new proof of Schrijver's inequality on the number of perfect matchings of a regular bipartite graph, generalize a recent result of Nikolov and Singh, and give deterministic polynomial time approximation algorithms for several counting problems."},{"title":"RECIST-Net: Lesion detection via grouping keypoints on RECIST-based\n  annotation","source":"Cong Xie, Shilei Cao, Dong Wei, Hongyu Zhou, Kai Ma, Xianli Zhang,\n  Buyue Qian, Liansheng Wang, Yefeng Zheng","docs_id":"2107.08715","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"RECIST-Net: Lesion detection via grouping keypoints on RECIST-based\n  annotation. Universal lesion detection in computed tomography (CT) images is an important yet challenging task due to the large variations in lesion type, size, shape, and appearance. Considering that data in clinical routine (such as the DeepLesion dataset) are usually annotated with a long and a short diameter according to the standard of Response Evaluation Criteria in Solid Tumors (RECIST) diameters, we propose RECIST-Net, a new approach to lesion detection in which the four extreme points and center point of the RECIST diameters are detected. By detecting a lesion as keypoints, we provide a more conceptually straightforward formulation for detection, and overcome several drawbacks (e.g., requiring extensive effort in designing data-appropriate anchors and losing shape information) of existing bounding-box-based methods while exploring a single-task, one-stage approach compared to other RECIST-based approaches. Experiments show that RECIST-Net achieves a sensitivity of 92.49% at four false positives per image, outperforming other recent methods including those using multi-task learning."},{"title":"Ranking Catamorphisms and Unranking Anamorphisms on Hereditarily Finite\n  Datatypes","source":"Paul Tarau","docs_id":"0808.0753","section":["cs.SC","cs.DM","cs.DS"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Ranking Catamorphisms and Unranking Anamorphisms on Hereditarily Finite\n  Datatypes. Using specializations of unfold and fold on a generic tree data type we derive unranking and ranking functions providing natural number encodings for various Hereditarily Finite datatypes. In this context, we interpret unranking operations as instances of a generic anamorphism and ranking operations as instances of the corresponding catamorphism. Starting with Ackerman's Encoding from Hereditarily Finite Sets to Natural Numbers we define pairings and tuple encodings that provide building blocks for a theory of Hereditarily Finite Functions. The more difficult problem of ranking and unranking Hereditarily Finite Permutations is then tackled using Lehmer codes and factoradics. The self-contained source code of the paper, as generated from a literate Haskell program, is available at \\url{http:\/\/logic.csci.unt.edu\/tarau\/research\/2008\/fFUN.zip}. Keywords: ranking\/unranking, pairing\/tupling functions, Ackermann encoding, hereditarily finite sets, hereditarily finite functions, permutations and factoradics, computational mathematics, Haskell data representations"},{"title":"Attention Based Real Image Restoration","source":"Saeed Anwar, Nick Barnes, and Lars Petersson","docs_id":"2004.13524","section":["cs.CV","cs.LG","eess.IV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Attention Based Real Image Restoration. Deep convolutional neural networks perform better on images containing spatially invariant degradations, also known as synthetic degradations; however, their performance is limited on real-degraded photographs and requires multiple-stage network modeling. To advance the practicability of restoration algorithms, this paper proposes a novel single-stage blind real image restoration network (R$^2$Net) by employing a modular architecture. We use a residual on the residual structure to ease the flow of low-frequency information and apply feature attention to exploit the channel dependencies. Furthermore, the evaluation in terms of quantitative metrics and visual quality for four restoration tasks i.e. Denoising, Super-resolution, Raindrop Removal, and JPEG Compression on 11 real degraded datasets against more than 30 state-of-the-art algorithms demonstrate the superiority of our R$^2$Net. We also present the comparison on three synthetically generated degraded datasets for denoising to showcase the capability of our method on synthetics denoising. The codes, trained models, and results are available on https:\/\/github.com\/saeed-anwar\/R2Net."},{"title":"Nonparametric Inference for Auto-Encoding Variational Bayes","source":"Erik Bodin, Iman Malik, Carl Henrik Ek, Neill D. F. Campbell","docs_id":"1712.06536","section":["stat.ML","cs.AI","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Nonparametric Inference for Auto-Encoding Variational Bayes. We would like to learn latent representations that are low-dimensional and highly interpretable. A model that has these characteristics is the Gaussian Process Latent Variable Model. The benefits and negative of the GP-LVM are complementary to the Variational Autoencoder, the former provides interpretable low-dimensional latent representations while the latter is able to handle large amounts of data and can use non-Gaussian likelihoods. Our inspiration for this paper is to marry these two approaches and reap the benefits of both. In order to do so we will introduce a novel approximate inference scheme inspired by the GP-LVM and the VAE. We show experimentally that the approximation allows the capacity of the generative bottle-neck (Z) of the VAE to be arbitrarily large without losing a highly interpretable representation, allowing reconstruction quality to be unlimited by Z at the same time as a low-dimensional space can be used to perform ancestral sampling from as well as a means to reason about the embedded data."},{"title":"AdvFoolGen: Creating Persistent Troubles for Deep Classifiers","source":"Yuzhen Ding, Nupur Thakur, Baoxin Li","docs_id":"2007.10485","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"AdvFoolGen: Creating Persistent Troubles for Deep Classifiers. Researches have shown that deep neural networks are vulnerable to malicious attacks, where adversarial images are created to trick a network into misclassification even if the images may give rise to totally different labels by human eyes. To make deep networks more robust to such attacks, many defense mechanisms have been proposed in the literature, some of which are quite effective for guarding against typical attacks. In this paper, we present a new black-box attack termed AdvFoolGen, which can generate attacking images from the same feature space as that of the natural images, so as to keep baffling the network even though state-of-the-art defense mechanisms have been applied. We systematically evaluate our model by comparing with well-established attack algorithms. Through experiments, we demonstrate the effectiveness and robustness of our attack in the face of state-of-the-art defense techniques and unveil the potential reasons for its effectiveness through principled analysis. As such, AdvFoolGen contributes to understanding the vulnerability of deep networks from a new perspective and may, in turn, help in developing and evaluating new defense mechanisms."},{"title":"Rethinking Bottleneck Structure for Efficient Mobile Network Design","source":"Zhou Daquan, Qibin Hou, Yunpeng Chen, Jiashi Feng, Shuicheng Yan","docs_id":"2007.02269","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Rethinking Bottleneck Structure for Efficient Mobile Network Design. The inverted residual block is dominating architecture design for mobile networks recently. It changes the classic residual bottleneck by introducing two design rules: learning inverted residuals and using linear bottlenecks. In this paper, we rethink the necessity of such design changes and find it may bring risks of information loss and gradient confusion. We thus propose to flip the structure and present a novel bottleneck design, called the sandglass block, that performs identity mapping and spatial transformation at higher dimensions and thus alleviates information loss and gradient confusion effectively. Extensive experiments demonstrate that, different from the common belief, such bottleneck structure is more beneficial than the inverted ones for mobile networks. In ImageNet classification, by simply replacing the inverted residual block with our sandglass block without increasing parameters and computation, the classification accuracy can be improved by more than 1.7% over MobileNetV2. On Pascal VOC 2007 test set, we observe that there is also 0.9% mAP improvement in object detection. We further verify the effectiveness of the sandglass block by adding it into the search space of neural architecture search method DARTS. With 25% parameter reduction, the classification accuracy is improved by 0.13% over previous DARTS models. Code can be found at: https:\/\/github.com\/zhoudaquan\/rethinking_bottleneck_design."},{"title":"SIMD Vectorization for the Lennard-Jones Potential with AVX2 and AVX-512\n  instructions","source":"Hiroshi Watanabe and Koh M. Nakagawa","docs_id":"1806.05713","section":["cs.MS","cs.CE"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"SIMD Vectorization for the Lennard-Jones Potential with AVX2 and AVX-512\n  instructions. This work describes the SIMD vectorization of the force calculation of the Lennard-Jones potential with Intel AVX2 and AVX-512 instruction sets. Since the force-calculation kernel of the molecular dynamics method involves indirect access to memory, the data layout is one of the most important factors in vectorization. We find that the Array of Structures (AoS) with padding exhibits better performance than Structure of Arrays (SoA) with appropriate vectorization and optimizations. In particular, AoS with 512-bit width exhibits the best performance among the architectures. While the difference in performance between AoS and SoA is significant for the vectorization with AVX2, that with AVX-512 is minor. The effect of other optimization techniques, such as software pipelining together with vectorization, is also discussed. We present results for benchmarks on three CPU architectures: Intel Haswell (HSW), Knights Landing (KNL), and Skylake (SKL). The performance gains by vectorization are about 42\\% on HSW compared with the code optimized without vectorization. On KNL, the hand-vectorized codes exhibit 34\\% better performance than the codes vectorized automatically by the Intel compiler. On SKL, the code vectorized with AVX2 exhibits slightly better performance than that with vectorized AVX-512."},{"title":"Classification of breast cancer histology images using transfer learning","source":"Sulaiman Vesal, Nishant Ravikumar, AmirAbbas Davari, Stephan Ellmann,\n  Andreas Maier","docs_id":"1802.09424","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Classification of breast cancer histology images using transfer learning. Breast cancer is one of the leading causes of mortality in women. Early detection and treatment are imperative for improving survival rates, which have steadily increased in recent years as a result of more sophisticated computer-aided-diagnosis (CAD) systems. A critical component of breast cancer diagnosis relies on histopathology, a laborious and highly subjective process. Consequently, CAD systems are essential to reduce inter-rater variability and supplement the analyses conducted by specialists. In this paper, a transfer-learning based approach is proposed, for the task of breast histology image classification into four tissue sub-types, namely, normal, benign, \\textit{in situ} carcinoma and invasive carcinoma. The histology images, provided as part of the BACH 2018 grand challenge, were first normalized to correct for color variations resulting from inconsistencies during slide preparation. Subsequently, image patches were extracted and used to fine-tune Google`s Inception-V3 and ResNet50 convolutional neural networks (CNNs), both pre-trained on the ImageNet database, enabling them to learn domain-specific features, necessary to classify the histology images. The ResNet50 network (based on residual learning) achieved a test classification accuracy of 97.50% for four classes, outperforming the Inception-V3 network which achieved an accuracy of 91.25%."},{"title":"Risk-Based Optimization of Virtual Reality over Terahertz Reconfigurable\n  Intelligent Surfaces","source":"Christina Chaccour, Mehdi Naderi Soorki, Walid Saad, Mehdi Bennis,\n  Petar Popovski","docs_id":"2002.09052","section":["cs.IT","eess.SP","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Risk-Based Optimization of Virtual Reality over Terahertz Reconfigurable\n  Intelligent Surfaces. In this paper, the problem of associating reconfigurable intelligent surfaces (RISs) to virtual reality (VR) users is studied for a wireless VR network. In particular, this problem is considered within a cellular network that employs terahertz (THz) operated RISs acting as base stations. To provide a seamless VR experience, high data rates and reliable low latency need to be continuously guaranteed. To address these challenges, a novel risk-based framework based on the entropic value-at-risk is proposed for rate optimization and reliability performance. Furthermore, a Lyapunov optimization technique is used to reformulate the problem as a linear weighted function, while ensuring that higher order statistics of the queue length are maintained under a threshold. To address this problem, given the stochastic nature of the channel, a policy-based reinforcement learning (RL) algorithm is proposed. Since the state space is extremely large, the policy is learned through a deep-RL algorithm. In particular, a recurrent neural network (RNN) RL framework is proposed to capture the dynamic channel behavior and improve the speed of conventional RL policy-search algorithms. Simulation results demonstrate that the maximal queue length resulting from the proposed approach is only within 1% of the optimal solution. The results show a high accuracy and fast convergence for the RNN with a validation accuracy of 91.92%."},{"title":"Wind Field Reconstruction with Adaptive Random Fourier Features","source":"Jonas Kiessling, Emanuel Str\\\"om and Ra\\'ul Tempone","docs_id":"2102.02365","section":["math.NA","cs.NA","stat.AP","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Wind Field Reconstruction with Adaptive Random Fourier Features. We investigate the use of spatial interpolation methods for reconstructing the horizontal near-surface wind field given a sparse set of measurements. In particular, random Fourier features is compared to a set of benchmark methods including Kriging and Inverse distance weighting. Random Fourier features is a linear model $\\beta(\\pmb x) = \\sum_{k=1}^K \\beta_k e^{i\\omega_k \\pmb x}$ approximating the velocity field, with frequencies $\\omega_k$ randomly sampled and amplitudes $\\beta_k$ trained to minimize a loss function. We include a physically motivated divergence penalty term $|\\nabla \\cdot \\beta(\\pmb x)|^2$, as well as a penalty on the Sobolev norm. We derive a bound on the generalization error and derive a sampling density that minimizes the bound. Following (arXiv:2007.10683 [math.NA]), we devise an adaptive Metropolis-Hastings algorithm for sampling the frequencies of the optimal distribution. In our experiments, our random Fourier features model outperforms the benchmark models."},{"title":"PubSub implementation in Haskell with formal verification in Coq","source":"Boro Sitnikovski, Biljana Stojcevska, Lidija Goracinova-Ilieva, Irena\n  Stojmenovska","docs_id":"2005.09452","section":["cs.PL","cs.LO","cs.SE"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"PubSub implementation in Haskell with formal verification in Coq. In the cloud, the technology is used on-demand without the need to install anything on the desktop. Software as a Service is one of the many cloud architectures. The PubSub messaging pattern is a cloud-based Software as a Service solution used in complex systems, especially in the notifications part where there is a need to send a message from one unit to another single unit or multiple units. Haskell is a generic typed programming language which has pioneered several advanced programming language features. Based on the lambda calculus system, it belongs to the family of functional programming languages. Coq, also based on a stricter version of lambda calculus, is a programming language that has a more advanced type system than Haskell and is mainly used for theorem proving i.e. proving software correctness. This paper aims to show how PubSub can be used in conjunction with cloud computing (Software as a Service), as well as to present an example implementation in Haskell and proof of correctness in Coq."},{"title":"Models we Can Trust: Toward a Systematic Discipline of (Agent-Based)\n  Model Interpretation and Validation","source":"Gabriel Istrate","docs_id":"2102.11615","section":["cs.MA","cs.AI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Models we Can Trust: Toward a Systematic Discipline of (Agent-Based)\n  Model Interpretation and Validation. We advocate the development of a discipline of interacting with and extracting information from models, both mathematical (e.g. game-theoretic ones) and computational (e.g. agent-based models). We outline some directions for the development of a such a discipline: - the development of logical frameworks for the systematic formal specification of stylized facts and social mechanisms in (mathematical and computational) social science. Such frameworks would bring to attention new issues, such as phase transitions, i.e. dramatical changes in the validity of the stylized facts beyond some critical values in parameter space. We argue that such statements are useful for those logical frameworks describing properties of ABM. - the adaptation of tools from the theory of reactive systems (such as bisimulation) to obtain practically relevant notions of two systems \"having the same behavior\". - the systematic development of an adversarial theory of model perturbations, that investigates the robustness of conclusions derived from models of social behavior to variations in several features of the social dynamics. These may include: activation order, the underlying social network, individual agent behavior."},{"title":"Adopting E-commerce to User's Needs","source":"Mohammad Alshehri, Hamza Aldabbas, James Sawle and Mai Abu Baqar","docs_id":"1203.3688","section":["cs.CY"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Adopting E-commerce to User's Needs. The objectives of this paper are to identify and analyse the extent to which the site is fulfilling all the user's requirements and needs. The related works comprise the history of interactive design and the benefits of user-centered development, which is the methodology followed in this survey. Moreover, there is a brief comparison between Waterfall and User-centered methodology in terms of addressing the issues of time saving and addressing fulfilment of users' needs. The data required to conduct this study was acquired using two research methods; the questionnaire and direct user observation, in order to address all the performance related attributes in the usability stage of the evaluation. An evaluation of the website, based on statements of usability goals and criteria, was undertaken in relation to the implementation and testing of the new design. JARIR bookstore website was chosen as a case study in this paper to investigate the usability and interactivity of the website design. The analysis section includes needs, users and tasks and data analysis, whereas the design phase covers the user interface and database design. At the end of this paper, some recommendations are presented regarding JARIR website that can be taken into account when developing the website in the future."},{"title":"An Integrated Dynamic Method for Allocating Roles and Planning Tasks for\n  Mixed Human-Robot Teams","source":"Fabio Fusaro (1 and 2), Edoardo Lamon (1), Elena De Momi (2), Arash\n  Ajoudani (1) ((1) Human-Robot Interfaces and physical Interaction, Istituto\n  Italiano di Tecnologia, Genoa, Italy, (2) Department of Electronics,\n  Information and Bioengineering, Politecnico di Milano Politecnico di Milano,\n  Milan, Italy)","docs_id":"2105.12031","section":["cs.RO","cs.AI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"An Integrated Dynamic Method for Allocating Roles and Planning Tasks for\n  Mixed Human-Robot Teams. This paper proposes a novel integrated dynamic method based on Behavior Trees for planning and allocating tasks in mixed human robot teams, suitable for manufacturing environments. The Behavior Tree formulation allows encoding a single job as a compound of different tasks with temporal and logic constraints. In this way, instead of the well-studied offline centralized optimization problem, the role allocation problem is solved with multiple simplified online optimization sub-problem, without complex and cross-schedule task dependencies. These sub-problems are defined as Mixed-Integer Linear Programs, that, according to the worker-actions related costs and the workers' availability, allocate the yet-to-execute tasks among the available workers. To characterize the behavior of the developed method, we opted to perform different simulation experiments in which the results of the action-worker allocation and computational complexity are evaluated. The obtained results, due to the nature of the algorithm and to the possibility of simulating the agents' behavior, should describe well also how the algorithm performs in real experiments."},{"title":"Defining Homomorphisms and Other Generalized Morphisms of Fuzzy\n  Relations in Monoidal Fuzzy Logics by Means of BK-Products","source":"Ladislav J. Kohout","docs_id":"math\/0310175","section":["math.LO","cs.LO","math-ph","math.MP","math.QA"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Defining Homomorphisms and Other Generalized Morphisms of Fuzzy\n  Relations in Monoidal Fuzzy Logics by Means of BK-Products. The present paper extends generalized morphisms of relations into the realm of Monoidal Fuzzy Logics by first proving and then using relational inequalities over pseudo-associative BK-products (compositions) of relations in these logics. In 1977 Bandler and Kohout introduced generalized homomorphism, proteromorphism, amphimorphism, forward and backward compatibility of relations, and non-associative and pseudo-associative products (compositions) of relations into crisp (non-fuzzy Boolean) theory of relations. This was generalized later by Kohout to relations based on fuzzy Basic Logic systems (BL) of H\\'ajek and also for relational systems based on left-continuous t-norms. The present paper is based on monoidal logics, hence it subsumes as special cases the theories of generalized morphisms (etc.) based on the following systems of logics: BL systems (which include the well known Goedel, product logic systems; Lukasiewicz logic and its extension to MV-algebras related to quantum logics), intuitionistic logics and linear logics."},{"title":"Weighted Sum-Throughput Maximization for MIMO Broadcast Channel: Energy\n  Harvesting Under System Imperfection","source":"Zhi Chen, Pingyi Fan, Dapeng Oliver Wu and Khaled Ben Letaief","docs_id":"1511.01953","section":["cs.IT","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Weighted Sum-Throughput Maximization for MIMO Broadcast Channel: Energy\n  Harvesting Under System Imperfection. In this work, a MIMO broadcast channel under the energy harvesting (EH) constraint and the peak power constraint is investigated. The transmitter is equipped with a hybrid energy storage system consisting of a perfect super capacitor (SC) and an inefficient battery, where both elements have limited energy storage capacities. In addition, the effect of data processing circuit power consumption is also addressed. To be specific, two extreme cases are studied here, where the first assumes ideal\/zero circuit power consumption and the second considers a positive constant circuit power consumption where the circuit is always operating at its highest power level. The performance of these two extreme cases hence serve as the upper bound and the lower bound of the system performance in practice, respectively. In this setting, the offline scheduling with ideal and maximum circuit power consumptions are investigated. The associated optimization problems are formulated and solved in terms of weighted throughput optimization. Further, we extend to a general circuit power consumption model. To complement this work, some intuitive online policies are presented for all cases. Interestingly, for the case with maximum circuit power consumption, a close-to-optimal online policy is presented and its performance is shown to be comparable to its offline counterpart in the numerical results."},{"title":"Boundary IoU: Improving Object-Centric Image Segmentation Evaluation","source":"Bowen Cheng and Ross Girshick and Piotr Doll\\'ar and Alexander C. Berg\n  and Alexander Kirillov","docs_id":"2103.16562","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Boundary IoU: Improving Object-Centric Image Segmentation Evaluation. We present Boundary IoU (Intersection-over-Union), a new segmentation evaluation measure focused on boundary quality. We perform an extensive analysis across different error types and object sizes and show that Boundary IoU is significantly more sensitive than the standard Mask IoU measure to boundary errors for large objects and does not over-penalize errors on smaller objects. The new quality measure displays several desirable characteristics like symmetry w.r.t. prediction\/ground truth pairs and balanced responsiveness across scales, which makes it more suitable for segmentation evaluation than other boundary-focused measures like Trimap IoU and F-measure. Based on Boundary IoU, we update the standard evaluation protocols for instance and panoptic segmentation tasks by proposing the Boundary AP (Average Precision) and Boundary PQ (Panoptic Quality) metrics, respectively. Our experiments show that the new evaluation metrics track boundary quality improvements that are generally overlooked by current Mask IoU-based evaluation metrics. We hope that the adoption of the new boundary-sensitive evaluation metrics will lead to rapid progress in segmentation methods that improve boundary quality."},{"title":"Network Coded Gossip with Correlated Data","source":"Bernhard Haeupler, Asaf Cohen, Chen Avin, Muriel M\\'edard","docs_id":"1202.1801","section":["cs.IT","cs.DC","cs.DS","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Network Coded Gossip with Correlated Data. We design and analyze gossip algorithms for networks with correlated data. In these networks, either the data to be distributed, the data already available at the nodes, or both, are correlated. This model is applicable for a variety of modern networks, such as sensor, peer-to-peer and content distribution networks. Although coding schemes for correlated data have been studied extensively, the focus has been on characterizing the rate region in static memory-free networks. In a gossip-based scheme, however, nodes communicate among each other by continuously exchanging packets according to some underlying communication model. The main figure of merit in this setting is the stopping time -- the time required until nodes can successfully decode. While Gossip schemes are practical, distributed and scalable, they have only been studied for uncorrelated data. We wish to close this gap by providing techniques to analyze network coded gossip in (dynamic) networks with correlated data. We give a clean framework for oblivious network models that applies to a multitude of network and communication scenarios, specify a general setting for distributed correlated data, and give tight bounds on the stopping times of network coded protocols in this wide range of scenarios."},{"title":"Multimodal Approach for Metadata Extraction from German Scientific\n  Publications","source":"Azeddine Bouabdallah, Jorge Gavilan, Jennifer Gerbl and Prayuth\n  Patumcharoenpol","docs_id":"2111.05736","section":["cs.IR","cs.CV","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Multimodal Approach for Metadata Extraction from German Scientific\n  Publications. Nowadays, metadata information is often given by the authors themselves upon submission. However, a significant part of already existing research papers have missing or incomplete metadata information. German scientific papers come in a large variety of layouts which makes the extraction of metadata a non-trivial task that requires a precise way to classify the metadata extracted from the documents. In this paper, we propose a multimodal deep learning approach for metadata extraction from scientific papers in the German language. We consider multiple types of input data by combining natural language processing and image vision processing. This model aims to increase the overall accuracy of metadata extraction compared to other state-of-the-art approaches. It enables the utilization of both spatial and contextual features in order to achieve a more reliable extraction. Our model for this approach was trained on a dataset consisting of around 8800 documents and is able to obtain an overall F1-score of 0.923."},{"title":"Tradeoffs Between Information and Ordinal Approximation for Bipartite\n  Matching","source":"Elliot Anshelevich and Wennan Zhu","docs_id":"1707.01608","section":["cs.GT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Tradeoffs Between Information and Ordinal Approximation for Bipartite\n  Matching. We study ordinal approximation algorithms for maximum-weight bipartite matchings. Such algorithms only know the ordinal preferences of the agents\/nodes in the graph for their preferred matches, but must compete with fully omniscient algorithms which know the true numerical edge weights (utilities). %instead of only their relative orderings. Ordinal approximation is all about being able to produce good results with only limited information. Because of this, one important question is how much better the algorithms can be as the amount of information increases. To address this question for forming high-utility matchings between agents in $\\mathcal{X}$ and $\\mathcal{Y}$, we consider three ordinal information types: when we know the preference order of only nodes in $\\mathcal{X}$ for nodes in $\\mathcal{Y}$, when we know the preferences of both $\\mathcal{X}$ and $\\mathcal{Y}$, and when we know the total order of the edge weights in the entire graph, although not the weights themselves. We also consider settings where only the top preferences of the agents are known to us, instead of their full preference orderings. We design new ordinal approximation algorithms for each of these settings, and quantify how well such algorithms perform as the amount of information given to them increases."},{"title":"Disentangling 3D Pose in A Dendritic CNN for Unconstrained 2D Face\n  Alignment","source":"Amit Kumar, Rama Chellappa","docs_id":"1802.06713","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Disentangling 3D Pose in A Dendritic CNN for Unconstrained 2D Face\n  Alignment. Heatmap regression has been used for landmark localization for quite a while now. Most of the methods use a very deep stack of bottleneck modules for heatmap classification stage, followed by heatmap regression to extract the keypoints. In this paper, we present a single dendritic CNN, termed as Pose Conditioned Dendritic Convolution Neural Network (PCD-CNN), where a classification network is followed by a second and modular classification network, trained in an end to end fashion to obtain accurate landmark points. Following a Bayesian formulation, we disentangle the 3D pose of a face image explicitly by conditioning the landmark estimation on pose, making it different from multi-tasking approaches. Extensive experimentation shows that conditioning on pose reduces the localization error by making it agnostic to face pose. The proposed model can be extended to yield variable number of landmark points and hence broadening its applicability to other datasets. Instead of increasing depth or width of the network, we train the CNN efficiently with Mask-Softmax Loss and hard sample mining to achieve upto $15\\%$ reduction in error compared to state-of-the-art methods for extreme and medium pose face images from challenging datasets including AFLW, AFW, COFW and IBUG."},{"title":"Perturbation Analysis of Learning Algorithms: A Unifying Perspective on\n  Generation of Adversarial Examples","source":"Emilio Rafael Balda, Arash Behboodi, Rudolf Mathar","docs_id":"1812.07385","section":["cs.LG","cs.AI","cs.IT","math.IT","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Perturbation Analysis of Learning Algorithms: A Unifying Perspective on\n  Generation of Adversarial Examples. Despite the tremendous success of deep neural networks in various learning problems, it has been observed that adding an intentionally designed adversarial perturbation to inputs of these architectures leads to erroneous classification with high confidence in the prediction. In this work, we propose a general framework based on the perturbation analysis of learning algorithms which consists of convex programming and is able to recover many current adversarial attacks as special cases. The framework can be used to propose novel attacks against learning algorithms for classification and regression tasks under various new constraints with closed form solutions in many instances. In particular we derive new attacks against classification algorithms which are shown to achieve comparable performances to notable existing attacks. The framework is then used to generate adversarial perturbations for regression tasks which include single pixel and single subset attacks. By applying this method to autoencoding and image colorization tasks, it is shown that adversarial perturbations can effectively perturb the output of regression tasks as well."},{"title":"Caffeinated FPGAs: FPGA Framework For Convolutional Neural Networks","source":"Roberto DiCecco, Griffin Lacey, Jasmina Vasiljevic, Paul Chow, Graham\n  Taylor and Shawki Areibi","docs_id":"1609.09671","section":["cs.CV","cs.DC"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Caffeinated FPGAs: FPGA Framework For Convolutional Neural Networks. Convolutional Neural Networks (CNNs) have gained significant traction in the field of machine learning, particularly due to their high accuracy in visual recognition. Recent works have pushed the performance of GPU implementations of CNNs to significantly improve their classification and training times. With these improvements, many frameworks have become available for implementing CNNs on both CPUs and GPUs, with no support for FPGA implementations. In this work we present a modified version of the popular CNN framework Caffe, with FPGA support. This allows for classification using CNN models and specialized FPGA implementations with the flexibility of reprogramming the device when necessary, seamless memory transactions between host and device, simple-to-use test benches, and the ability to create pipelined layer implementations. To validate the framework, we use the Xilinx SDAccel environment to implement an FPGA-based Winograd convolution engine and show that the FPGA layer can be used alongside other layers running on a host processor to run several popular CNNs (AlexNet, GoogleNet, VGG A, Overfeat). The results show that our framework achieves 50 GFLOPS across 3x3 convolutions in the benchmarks. This is achieved within a practical framework, which will aid in future development of FPGA-based CNNs."},{"title":"The Effectiveness of Supervised Machine Learning Algorithms in\n  Predicting Software Refactoring","source":"Maur\\'icio Aniche, Erick Maziero, Rafael Durelli, Vinicius Durelli","docs_id":"2001.03338","section":["cs.SE"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"The Effectiveness of Supervised Machine Learning Algorithms in\n  Predicting Software Refactoring. Refactoring is the process of changing the internal structure of software to improve its quality without modifying its external behavior. Empirical studies have repeatedly shown that refactoring has a positive impact on the understandability and maintainability of software systems. However, before carrying out refactoring activities, developers need to identify refactoring opportunities. Currently, refactoring opportunity identification heavily relies on developers' expertise and intuition. In this paper, we investigate the effectiveness of machine learning algorithms in predicting software refactorings. More specifically, we train six different machine learning algorithms (i.e., Logistic Regression, Naive Bayes, Support Vector Machine, Decision Trees, Random Forest, and Neural Network) with a dataset comprising over two million refactorings from 11,149 real-world projects from the Apache, F-Droid, and GitHub ecosystems. The resulting models predict 20 different refactorings at class, method, and variable-levels with an accuracy often higher than 90%. Our results show that (i) Random Forests are the best models for predicting software refactoring, (ii) process and ownership metrics seem to play a crucial role in the creation of better models, and (iii) models generalize well in different contexts."},{"title":"Disentangling Identifiable Features from Noisy Data with Structured\n  Nonlinear ICA","source":"Hermanni H\\\"alv\\\"a, Sylvain Le Corff, Luc Leh\\'ericy, Jonathan So,\n  Yongjie Zhu, Elisabeth Gassiat, Aapo Hyvarinen","docs_id":"2106.09620","section":["stat.ML","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Disentangling Identifiable Features from Noisy Data with Structured\n  Nonlinear ICA. We introduce a new general identifiable framework for principled disentanglement referred to as Structured Nonlinear Independent Component Analysis (SNICA). Our contribution is to extend the identifiability theory of deep generative models for a very broad class of structured models. While previous works have shown identifiability for specific classes of time-series models, our theorems extend this to more general temporal structures as well as to models with more complex structures such as spatial dependencies. In particular, we establish the major result that identifiability for this framework holds even in the presence of noise of unknown distribution. Finally, as an example of our framework's flexibility, we introduce the first nonlinear ICA model for time-series that combines the following very useful properties: it accounts for both nonstationarity and autocorrelation in a fully unsupervised setting; performs dimensionality reduction; models hidden states; and enables principled estimation and inference by variational maximum-likelihood."},{"title":"Measuring the Utilization of Public Open Spaces by Deep Learning: a\n  Benchmark Study at the Detroit Riverfront","source":"Peng Sun, Rui Hou, Jerome Lynch","docs_id":"2002.01461","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Measuring the Utilization of Public Open Spaces by Deep Learning: a\n  Benchmark Study at the Detroit Riverfront. Physical activities and social interactions are essential activities that ensure a healthy lifestyle. Public open spaces (POS), such as parks, plazas and greenways, are key environments that encourage those activities. To evaluate a POS, there is a need to study how humans use the facilities within it. However, traditional approaches to studying use of POS are manual and therefore time and labor intensive. They also may only provide qualitative insights. It is appealing to make use of surveillance cameras and to extract user-related information through computer vision. This paper proposes a proof-of-concept deep learning computer vision framework for measuring human activities quantitatively in POS and demonstrates a case study of the proposed framework using the Detroit Riverfront Conservancy (DRFC) surveillance camera network. A custom image dataset is presented to train the framework; the dataset includes 7826 fully annotated images collected from 18 cameras across the DRFC park space under various illumination conditions. Dataset analysis is also provided as well as a baseline model for one-step user localization and activity recognition. The mAP results are 77.5\\% for {\\it pedestrian} detection and 81.6\\% for {\\it cyclist} detection. Behavioral maps are autonomously generated by the framework to locate different POS users and the average error for behavioral localization is within 10 cm."},{"title":"Scalable solvers for complex electromagnetics problems","source":"Santiago Badia, Alberto F. Mart\\'in, Marc Olm","docs_id":"1901.08783","section":["cs.CE","cs.NA"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Scalable solvers for complex electromagnetics problems. In this work, we present scalable balancing domain decomposition by constraints methods for linear systems arising from arbitrary order edge finite element discretizations of multi-material and heterogeneous 3D problems. In order to enforce the continuity across subdomains of the method, we use a partition of the interface objects (edges and faces) into sub-objects determined by the variation of the physical coefficients of the problem. For multi-material problems, a constant coefficient condition is enough to define this sub-partition of the objects. For arbitrarily heterogeneous problems, a relaxed version of the method is defined, where we only require that the maximal contrast of the physical coefficient in each object is smaller than a predefined threshold. Besides, the addition of perturbation terms to the preconditioner is empirically shown to be effective in order to deal with the case where the two coefficients of the model problem jump simultaneously across the interface. The new method, in contrast to existing approaches for problems in curl-conforming spaces does not require spectral information whilst providing robustness with regard to coefficient jumps and heterogeneous materials. A detailed set of numerical experiments, which includes the application of the preconditioner to 3D realistic cases, shows excellent weak scalability properties of the implementation of the proposed algorithms."},{"title":"Zero-error communication via quantum channels, non-commutative graphs\n  and a quantum Lovasz theta function","source":"Runyao Duan, Simone Severini, Andreas Winter","docs_id":"1002.2514","section":["quant-ph","cs.IT","math.IT","math.OA"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Zero-error communication via quantum channels, non-commutative graphs\n  and a quantum Lovasz theta function. We study the quantum channel version of Shannon's zero-error capacity problem. Motivated by recent progress on this question, we propose to consider a certain operator space as the quantum generalisation of the adjacency matrix, in terms of which the plain, quantum and entanglement-assisted capacity can be formulated, and for which we show some new basic properties. Most importantly, we define a quantum version of Lovasz' famous theta function, as the norm-completion (or stabilisation) of a \"naive\" generalisation of theta. We go on to show that this function upper bounds the number of entanglement-assisted zero-error messages, that it is given by a semidefinite programme, whose dual we write down explicitly, and that it is multiplicative with respect to the natural (strong) graph product. We explore various other properties of the new quantity, which reduces to Lovasz' original theta in the classical case, give several applications, and propose to study the operator spaces associated to channels as \"non-commutative graphs\", using the language of Hilbert modules."},{"title":"Facial Makeup Transfer Combining Illumination Transfer","source":"Xin Jin, Rui Han, Ning Ning, Xiaodong Li, Xiaokun Zhang","docs_id":"1907.03398","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Facial Makeup Transfer Combining Illumination Transfer. To meet the women appearance needs, we present a novel virtual experience approach of facial makeup transfer, developed into windows platform application software. The makeup effects could present on the user's input image in real time, with an only single reference image. The input image and reference image are divided into three layers by facial feature points landmarked: facial structure layer, facial color layer, and facial detail layer. Except for the above layers are processed by different algorithms to generate output image, we also add illumination transfer, so that the illumination effect of the reference image is automatically transferred to the input image. Our approach has the following three advantages: (1) Black or dark and white facial makeup could be effectively transferred by introducing illumination transfer; (2) Efficiently transfer facial makeup within seconds compared to those methods based on deep learning frameworks; (3) Reference images with the air-bangs could transfer makeup perfectly."},{"title":"Multiplication of sparse Laurent polynomials and Poisson series on\n  modern hardware architectures","source":"Francesco Biscani","docs_id":"1004.4548","section":["cs.SC"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Multiplication of sparse Laurent polynomials and Poisson series on\n  modern hardware architectures. In this paper we present two algorithms for the multiplication of sparse Laurent polynomials and Poisson series (the latter being algebraic structures commonly arising in Celestial Mechanics from the application of perturbation theories). Both algorithms first employ the Kronecker substitution technique to reduce multivariate multiplication to univariate multiplication, and then use the schoolbook method to perform the univariate multiplication. The first algorithm, suitable for moderately-sparse multiplication, uses the exponents of the monomials resulting from the univariate multiplication as trivial hash values in a one dimensional lookup array of coefficients. The second algorithm, suitable for highly-sparse multiplication, uses a cache-optimised hash table which stores the coefficient-exponent pairs resulting from the multiplication using the exponents as keys. Both algorithms have been implemented with attention to modern computer hardware architectures. Particular care has been devoted to the efficient exploitation of contemporary memory hierarchies through cache-blocking techniques and cache-friendly term ordering. The first algorithm has been parallelised for shared-memory multicore architectures, whereas the second algorithm is in the process of being parallelised. We present benchmarks comparing our algorithms to the routines of other computer algebra systems, both in sequential and parallel mode."},{"title":"In the Age of Web: Typed Functional-First Programming Revisited","source":"Tomas Petricek (University of Cambridge), Don Syme (Microsoft\n  Research), Zach Bray (Type Inferred Ltd)","docs_id":"1512.01896","section":["cs.PL","cs.SE"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"In the Age of Web: Typed Functional-First Programming Revisited. Most programming languages were designed before the age of web. This matters because the web changes many assumptions that typed functional language designers take for granted. For example, programs do not run in a closed world, but must instead interact with (changing and likely unreliable) services and data sources, communication is often asynchronous or event-driven, and programs need to interoperate with untyped environments. In this paper, we present how the F# language and libraries face the challenges posed by the web. Technically, this comprises using type providers for integration with external information sources and for integration with untyped programming environments, using lightweight meta-programming for targeting JavaScript and computation expressions for writing asynchronous code. In this inquiry, the holistic perspective is more important than each of the features in isolation. We use a practical case study as a starting point and look at how F# language and libraries approach the challenges posed by the web. The specific lessons learned are perhaps less interesting than our attempt to uncover hidden assumptions that no longer hold in the age of web."},{"title":"An Interference-Free Filter-Bank Multicarrier System Applicable for MIMO\n  Channels","source":"Mohammad Towliat","docs_id":"2006.03758","section":["eess.SP","cs.SY","eess.SY"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"An Interference-Free Filter-Bank Multicarrier System Applicable for MIMO\n  Channels. In filter-bank multicarrier (FBMC) systems the intrinsic interference is the major challenge to achieve a full gain of diversity over multi-input multi-output (MIMO) channels. In this paper, we develop a novel multicarrier system called FBMC offset upper-lower polyphase network (FBMC\/OULP) in which, to eliminate the intrinsic interference, the complex-valued symbols are alternatively transmitted via upper and lower half of polyphase network branches with an offset time. The symbol density of the FBMC\/OULP system is equal to one complex-valued symbol in time-frequency lattice. Also, for transmission over frequency selective channels, a minimum mean square error (MMSE) estimator is employed at the receiver of the FBMC\/OULP system to eliminate the interference caused by the frequency selectivity of the channel. The proposed scheme mitigates the produced interference between symbols in the upper and lower polyphase branches, based on the circular convolutional property. As a result of using complex-valued symbols and diminishing the interference, the full diversity gain of the orthogonal space-time block codes (OSTBC) can be achieved in MIMO channels by a low complex maximum likelihood (ML) detector. In comparison with the orthogonal frequency division multiplexing (OFDM) system, simulation results indicate that the proposed system achieves a superior performance in fast multi-path fading channels and a competitive performance in slow multi-path fading channels."},{"title":"Nearly-tight VC-dimension and pseudodimension bounds for piecewise\n  linear neural networks","source":"Peter L. Bartlett and Nick Harvey and Chris Liaw and Abbas Mehrabian","docs_id":"1703.02930","section":["cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Nearly-tight VC-dimension and pseudodimension bounds for piecewise\n  linear neural networks. We prove new upper and lower bounds on the VC-dimension of deep neural networks with the ReLU activation function. These bounds are tight for almost the entire range of parameters. Letting $W$ be the number of weights and $L$ be the number of layers, we prove that the VC-dimension is $O(W L \\log(W))$, and provide examples with VC-dimension $\\Omega( W L \\log(W\/L) )$. This improves both the previously known upper bounds and lower bounds. In terms of the number $U$ of non-linear units, we prove a tight bound $\\Theta(W U)$ on the VC-dimension. All of these bounds generalize to arbitrary piecewise linear activation functions, and also hold for the pseudodimensions of these function classes. Combined with previous results, this gives an intriguing range of dependencies of the VC-dimension on depth for networks with different non-linearities: there is no dependence for piecewise-constant, linear dependence for piecewise-linear, and no more than quadratic dependence for general piecewise-polynomial."},{"title":"Convolutional RNN: an Enhanced Model for Extracting Features from\n  Sequential Data","source":"Gil Keren and Bj\\\"orn Schuller","docs_id":"1602.05875","section":["stat.ML","cs.CL"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Convolutional RNN: an Enhanced Model for Extracting Features from\n  Sequential Data. Traditional convolutional layers extract features from patches of data by applying a non-linearity on an affine function of the input. We propose a model that enhances this feature extraction process for the case of sequential data, by feeding patches of the data into a recurrent neural network and using the outputs or hidden states of the recurrent units to compute the extracted features. By doing so, we exploit the fact that a window containing a few frames of the sequential data is a sequence itself and this additional structure might encapsulate valuable information. In addition, we allow for more steps of computation in the feature extraction process, which is potentially beneficial as an affine function followed by a non-linearity can result in too simple features. Using our convolutional recurrent layers we obtain an improvement in performance in two audio classification tasks, compared to traditional convolutional layers. Tensorflow code for the convolutional recurrent layers is publicly available in https:\/\/github.com\/cruvadom\/Convolutional-RNN."},{"title":"Playing against the fittest: A simple strategy that promotes the\n  emergence of cooperation","source":"M. Brede","docs_id":"1104.4532","section":["cs.GT","physics.bio-ph","physics.soc-ph","q-bio.PE"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Playing against the fittest: A simple strategy that promotes the\n  emergence of cooperation. Understanding the emergence and sustainability of cooperation is a fundamental problem in evolutionary biology and is frequently studied by the framework of evolutionary game theory. A very powerful mechanism to promote cooperation is network reciprocity, where the interaction patterns and opportunities for strategy spread of agents are constrained to limited sets of permanent interactions partners. Cooperation survives because it is possible for close-knit communities of cooperation to be shielded from invasion by defectors. Here we show that parameter ranges in which cooperation can survive are strongly expanded if game play on networks is skewed towards more frequent interactions with more successful neighbours. In particular, if agents exclusively select neighbors for game play that are more successful than themselves, cooperation can even dominate in situations in which it would die out if interaction neighbours were chosen without a bias or with a preference for less successful opponents. We demonstrate that the \"selecting fitter neighbours\" strategy is evolutionarily stable. Moreover, it will emerge as the dominant strategy out of an initially random population of agents."},{"title":"Communication-Compressed Adaptive Gradient Method for Distributed\n  Nonconvex Optimization","source":"Yujia Wang, Lu Lin and Jinghui Chen","docs_id":"2111.00705","section":["cs.LG","cs.AI","cs.DC","math.OC","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Communication-Compressed Adaptive Gradient Method for Distributed\n  Nonconvex Optimization. Due to the explosion in the size of the training datasets, distributed learning has received growing interest in recent years. One of the major bottlenecks is the large communication cost between the central server and the local workers. While error feedback compression has been proven to be successful in reducing communication costs with stochastic gradient descent (SGD), there are much fewer attempts in building communication-efficient adaptive gradient methods with provable guarantees, which are widely used in training large-scale machine learning models. In this paper, we propose a new communication-compressed AMSGrad for distributed nonconvex optimization problem, which is provably efficient. Our proposed distributed learning framework features an effective gradient compression strategy and a worker-side model update design. We prove that the proposed communication-efficient distributed adaptive gradient method converges to the first-order stationary point with the same iteration complexity as uncompressed vanilla AMSGrad in the stochastic nonconvex optimization setting. Experiments on various benchmarks back up our theory."},{"title":"A heuristic scheme for the Cooperative Team Orienteering Problem with\n  Time Windows","source":"Iman Roozbeh, Melih Ozlen, John W. Hearne","docs_id":"1608.05485","section":["cs.AI","math.OC"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A heuristic scheme for the Cooperative Team Orienteering Problem with\n  Time Windows. The Cooperative Orienteering Problem with Time Windows (COPTW)is a class of problems with some important applications and yet has received relatively little attention. In the COPTW a certain number of team members are required to collect the associated reward from each customer simultaneously and cooperatively. This requirement to have one or more team members simultaneously available at a vertex to collect the reward, poses a challenging OR task. Exact methods are not able to handle large scale instances of the COPTW and no heuristic schemes have been developed for this problem so far. In this paper, a new modification to the classical Clarke and Wright saving heuristic is proposed to handle this problem. A new benchmark set generated by adding the resource requirement attribute to the existing benchmarks. The heuristic algorithm followed by boosting operators achieves optimal solutions for 64.5% of instances for which the optimal results are known. The proposed solution approach attains an optimality gap of 2.61% for the same instances and solves benchmarks with realistic size within short computational times."},{"title":"Electromagnetic actuation for a vibrotactile display: Assessing stimuli\n  complexity and usability","source":"Michael J. Proulx, Theodoros Eracleous, Ben Spencer, Anna Passfield,\n  Alexandra de Sousa, and Ali Mohammadi","docs_id":"2105.13295","section":["cs.HC","cs.MM"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Electromagnetic actuation for a vibrotactile display: Assessing stimuli\n  complexity and usability. Sensory substitution has influenced the design of many tactile visual substitution systems with the aim of offering visual aids for the blind. This paper focuses on whether a novel electromagnetic vibrotactile display, a four by four vibrotactile matrix of taxels, can serve as an aid for dynamic communication for visually impaired people. A mixed methods approach was used to firstly assess whether pattern complexity affected undergraduate participants' perceptive success, and secondly, if participants total score positively correlated with their perceived success ratings. A thematic analysis was also conducted on participants' experiences with the vibrotactile display and what methods of interaction they used. The results indicated that complex patterns were less accurately perceived than simple and linear patterns respectively, and no significant correlation was found between participants' score and perceived success ratings. Additionally, most participants interacted with the vibrotactile display in similar ways using one finger to feel one taxel at a time; arguably, the most effective strategy from previous research. This technology could have applications to navigational and communication aids for the visually impaired and road users."},{"title":"Subtractive Color Mixture Computation","source":"Scott Allen Burns","docs_id":"1710.06364","section":["cs.GR"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Subtractive Color Mixture Computation. Modeling subtractive color mixture (e.g., the way that paints mix) is difficult when working with colors described only by three-dimensional color space values, such as RGB. Although RGB values are sufficient to describe a specific color sensation, they do not contain enough information to predict the RGB color that would result from a subtractive mixture of two specified RGB colors. Methods do exist for accurately modeling subtractive mixture, such as the Kubelka-Munk equations, but require extensive spectrophotometric measurements of the mixed components, making them unsuitable for many computer graphics applications. This paper presents a strategy for modeling subtractive color mixture given only the RGB information of the colors being mixed, written for a general audience. The RGB colors are first transformed to generic, representative spectral distributions, and then this spectral information is used to perform the subtractive mixture, using the weighted arithmetic-geometric mean. This strategy provides reasonable, representative subtractive mixture colors with only modest computational effort and no experimental measurements. As such, it provides a useful way to model subtractive color mixture in computer graphics applications."},{"title":"A Polynomial-Time Algorithm for Solving the Minimal Observability\n  Problem in Conjunctive Boolean Networks","source":"Eyal Weiss and Michael Margaliot","docs_id":"1706.04072","section":["math.OC","cs.SY","math.DS","q-bio.QM"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A Polynomial-Time Algorithm for Solving the Minimal Observability\n  Problem in Conjunctive Boolean Networks. Many complex systems in biology, physics, and engineering include a large number of state-variables, and measuring the full state of the system is often impossible. Typically, a set of sensors is used to measure part of the state-variables. A system is called observable if these measurements allow to reconstruct the entire state of the system. When the system is not observable, an important and practical problem is how to add a \\emph{minimal} number of sensors so that the system becomes observable. This minimal observability problem is practically useful and theoretically interesting, as it pinpoints the most informative nodes in the system. We consider the minimal observability problem for an important special class of Boolean networks, called conjunctive Boolean networks (CBNs). Using a graph-theoretic approach, we provide a necessary and sufficient condition for observability of a CBN with $n$ state-variables, and an efficient~$O(n^2)$-time algorithm for solving the minimal observability problem. We demonstrate the usefulness of these results by studying the properties of a class of random CBNs."},{"title":"Deep Reinforcement Learning for Producing Furniture Layout in Indoor\n  Scenes","source":"Xinhan Di, Pengqian Yu","docs_id":"2101.07462","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Deep Reinforcement Learning for Producing Furniture Layout in Indoor\n  Scenes. In the industrial interior design process, professional designers plan the size and position of furniture in a room to achieve a satisfactory design for selling. In this paper, we explore the interior scene design task as a Markov decision process (MDP), which is solved by deep reinforcement learning. The goal is to produce an accurate position and size of the furniture simultaneously for the indoor layout task. In particular, we first formulate the furniture layout task as a MDP problem by defining the state, action, and reward function. We then design the simulated environment and train reinforcement learning agents to produce the optimal layout for the MDP formulation. We conduct our experiments on a large-scale real-world interior layout dataset that contains industrial designs from professional designers. Our numerical results demonstrate that the proposed model yields higher-quality layouts as compared with the state-of-art model. The developed simulator and codes are available at \\url{https:\/\/github.com\/CODE-SUBMIT\/simulator1}."},{"title":"Exploring the Self-enhanced Mechanism of Interactive Advertising\n  Phenomenon---Based on the Research of Three Cases","source":"Jian Ren and Wanxing Ding","docs_id":"1505.04488","section":["cs.CY"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Exploring the Self-enhanced Mechanism of Interactive Advertising\n  Phenomenon---Based on the Research of Three Cases. Under the background of the new media era with the rapid development of interactive advertising, this paper used case study method based on the summary of the research of the communication effect of interactive advertising from both domestic and foreign academia. This paper divided interactive advertising into three types to examine ---- interactive ads on official website, interactive ads based on SNS and interactive ads based on mobile media. Furthermore, this paper induced and summarized a self-enhanced dissemination mechanism of the interactive advertising, including three parts which are micro level, meso level and macro level mechanism, micro level embodies core interaction, inner interaction and outer interaction which reveal the whole process of interact with contents, with people and with computer, and the communication approach and spread speed shown in meso level which is self-fission-type spread, finally in macro level the communication effect of IA achieved the spiral increasing. In a word, this article enriches research procedure of the interactive advertising communication effects."},{"title":"Modified second-order generalized integrators with modified frequency\n  locked loop for fast harmonics estimation of distorted single-phase signals\n  (LONG VERSION)","source":"Christoph M. Hackl and Markus Landerer","docs_id":"1902.04653","section":["cs.SY"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Modified second-order generalized integrators with modified frequency\n  locked loop for fast harmonics estimation of distorted single-phase signals\n  (LONG VERSION). This paper proposes modified Second-Order Generalized Integrators (mSOGIs) for a fast estimation of all harmonic components of arbitrarily distorted single-phase signals such as voltages or currents in power systems. The estimation is based on the internal model principle leading to an overall observer system consisting of parallelized mSOGIs. The observer is tuned by pole placement. For a constant fundamental frequency, the observer is capable of estimating all harmonic components with prescribed settling time by choosing the observer poles appropriately. For time-varying fundamental frequencies, the harmonic estimation is combined with a modified Frequency Locked Loop (mFLL) with gain normalization, sign-correct anti-windup and rate limitation. The estimation performances of the proposed parallelized mSOGIs with and without mFLL are illustrated and validated by measurement results. The results are compared to standard approaches such as parallelized standard SOGIs (sSOGIs) and adaptive notch filters (ANFs)."},{"title":"Sdf-GAN: Semi-supervised Depth Fusion with Multi-scale Adversarial\n  Networks","source":"Can Pu, Runzi Song, Radim Tylecek, Nanbo Li, Robert B Fisher","docs_id":"1803.06657","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Sdf-GAN: Semi-supervised Depth Fusion with Multi-scale Adversarial\n  Networks. Refining raw disparity maps from different algorithms to exploit their complementary advantages is still challenging. Uncertainty estimation and complex disparity relationships among pixels limit the accuracy and robustness of existing methods and there is no standard method for fusion of different kinds of depth data. In this paper, we introduce a new method to fuse disparity maps from different sources, while incorporating supplementary information (intensity, gradient, etc.) into a refiner network to better refine raw disparity inputs. A discriminator network classifies disparities at different receptive fields and scales. Assuming a Markov Random Field for the refined disparity map produces better estimates of the true disparity distribution. Both fully supervised and semi-supervised versions of the algorithm are proposed. The approach includes a more robust loss function to inpaint invalid disparity values and requires much less labeled data to train in the semi-supervised learning mode. The algorithm can be generalized to fuse depths from different kinds of depth sources. Experiments explored different fusion opportunities: stereo-monocular fusion, stereo-ToF fusion and stereo-stereo fusion. The experiments show the superiority of the proposed algorithm compared with the most recent algorithms on public synthetic datasets (Scene Flow, SYNTH3, our synthetic garden dataset) and real datasets (Kitti2015 dataset and Trimbot2020 Garden dataset)."},{"title":"Modular Action Concept Grounding in Semantic Video Prediction","source":"Wei Yu, Wenxin Chen, Songhenh Yin, Steve Easterbrook, Animesh Garg","docs_id":"2011.11201","section":["cs.CV","cs.AI","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Modular Action Concept Grounding in Semantic Video Prediction. Recent works in video prediction have mainly focused on passive forecasting and low-level action-conditional prediction, which sidesteps the learning of interaction between agents and objects. We introduce the task of semantic action-conditional video prediction, which uses semantic action labels to describe those interactions and can be regarded as an inverse problem of action recognition. The challenge of this new task primarily lies in how to effectively inform the model of semantic action information. Inspired by the idea of Mixture of Experts, we embody each abstract label by a structured combination of various visual concept learners and propose a novel video prediction model, Modular Action Concept Network (MAC). Our method is evaluated on two newly designed synthetic datasets, CLEVR-Building-Blocks and Sapien-Kitchen, and one real-world dataset called Tower-Creation. Extensive experiments demonstrate that MAC can correctly condition on given instructions and generate corresponding future frames without need of bounding boxes. We further show that the trained model can make out-of-distribution generalization, be quickly adapted to new object categories and exploit its learnt features for object detection, showing the progression towards higher-level cognitive abilities."},{"title":"Hybrid MPI-OpenMP Paradigm on SMP Clusters: MPEG-2 Encoder and N-Body\n  Simulation","source":"Truong Vinh Truong Duy, Katsuhiro Yamazaki, Kosai Ikegami, and Shigeru\n  Oyanagi","docs_id":"1211.2292","section":["cs.DC","cs.CE","cs.PF"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Hybrid MPI-OpenMP Paradigm on SMP Clusters: MPEG-2 Encoder and N-Body\n  Simulation. Clusters of SMP nodes provide support for a wide diversity of parallel programming paradigms. Combining both shared memory and message passing parallelizations within the same application, the hybrid MPI-OpenMP paradigm is an emerging trend for parallel programming to fully exploit distributed shared-memory architecture. In this paper, we improve the performance of MPEG-2 encoder and n-body simulation by employing the hybrid MPI-OpenMP programming paradigm on SMP clusters. The hierarchical image data structure of the MPEG bit-stream is eminently suitable for the hybrid model to achieve multiple levels of parallelism: MPI for parallelism at the group of pictures level across SMP nodes and OpenMP for parallelism within pictures at the slice level within each SMP node. Similarly, the work load of the force calculation which accounts for upwards of 90% of the cycles in typical computations in the n-body simulation is shared among OpenMP threads after ORB domain decomposition among MPI processes. Besides, loop scheduling of OpenMP threads is adopted with appropriate chunk size to provide better load balance of work, leading to enhanced performance. With the n-body simulation, experimental results demonstrate that the hybrid MPI-OpenMP program outperforms the corresponding pure MPI program by average factors of 1.52 on a 4-way cluster and 1.21 on a 2-way cluster. Likewise, the hybrid model offers a performance improvement of 18% compared to the MPI model for the MPEG-2 encoder."},{"title":"A Link Generator for Increasing the Utility of OpenAPI-to-GraphQL\n  Translations","source":"Dominik Adam Kus, Istv\\'an Koren, Ralf Klamma","docs_id":"2005.08708","section":["cs.DC","cs.DB","cs.SE"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A Link Generator for Increasing the Utility of OpenAPI-to-GraphQL\n  Translations. Standardized interfaces are the connecting link of today's distributed systems, facilitating access to data services in the cloud. REST APIs have been prevalent over the last years, despite several issues like over- and underfetching of resources. GraphQL enjoys rapid adoption, resolving these problems by using statically typed queries. However, the redevelopment of services to the new paradigm is costly. Therefore, several approaches for the successive migration from REST to GraphQL have been proposed, many leveraging OpenAPI service descriptions. In this article, we present the findings of our empirical evaluation on the APIs.guru directory and identify several schema translation challenges. These include less expressive schema types in GraphQL, as well as missing meta information about related resources in OpenAPI. To this end, we developed the open source Link Generator, that analyzes OpenAPI documents and automatically adds links to increase translation utility. This fundamentally benefits around 34% of APIs in the APIs.guru directory. Our findings and tool support contribute to the ongoing discussion about the migration of REST APIs to GraphQL, and provide developers with valuable insights into common pitfalls, to reduce friction during API transformation."},{"title":"Truth as Utility: A Conceptual Synthesis","source":"Enrique H. Ruspini","docs_id":"1303.5744","section":["cs.AI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Truth as Utility: A Conceptual Synthesis. This paper introduces conceptual relations that synthesize utilitarian and logical concepts, extending the logics of preference of Rescher. We define first, in the context of a possible worlds model, constraint-dependent measures that quantify the relative quality of alternative solutions of reasoning problems or the relative desirability of various policies in control, decision, and planning problems. We show that these measures may be interpreted as truth values in a multi valued logic and propose mechanisms for the representation of complex constraints as combinations of simpler restrictions. These extended logical operations permit also the combination and aggregation of goal-specific quality measures into global measures of utility. We identify also relations that represent differential preferences between alternative solutions and relate them to the previously defined desirability measures. Extending conventional modal logic formulations, we introduce structures for the representation of ignorance about the utility of alternative solutions. Finally, we examine relations between these concepts and similarity based semantic models of fuzzy logic."},{"title":"Are pre-trained text representations useful for multilingual and\n  multi-dimensional language proficiency modeling?","source":"Taraka Rama and Sowmya Vajjala","docs_id":"2102.12971","section":["cs.CL"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Are pre-trained text representations useful for multilingual and\n  multi-dimensional language proficiency modeling?. Development of language proficiency models for non-native learners has been an active area of interest in NLP research for the past few years. Although language proficiency is multidimensional in nature, existing research typically considers a single \"overall proficiency\" while building models. Further, existing approaches also considers only one language at a time. This paper describes our experiments and observations about the role of pre-trained and fine-tuned multilingual embeddings in performing multi-dimensional, multilingual language proficiency classification. We report experiments with three languages -- German, Italian, and Czech -- and model seven dimensions of proficiency ranging from vocabulary control to sociolinguistic appropriateness. Our results indicate that while fine-tuned embeddings are useful for multilingual proficiency modeling, none of the features achieve consistently best performance for all dimensions of language proficiency. All code, data and related supplementary material can be found at: https:\/\/github.com\/nishkalavallabhi\/MultidimCEFRScoring."},{"title":"Learning Deep Control Policies for Autonomous Aerial Vehicles with\n  MPC-Guided Policy Search","source":"Tianhao Zhang, Gregory Kahn, Sergey Levine, Pieter Abbeel","docs_id":"1509.06791","section":["cs.LG","cs.RO"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Learning Deep Control Policies for Autonomous Aerial Vehicles with\n  MPC-Guided Policy Search. Model predictive control (MPC) is an effective method for controlling robotic systems, particularly autonomous aerial vehicles such as quadcopters. However, application of MPC can be computationally demanding, and typically requires estimating the state of the system, which can be challenging in complex, unstructured environments. Reinforcement learning can in principle forego the need for explicit state estimation and acquire a policy that directly maps sensor readings to actions, but is difficult to apply to unstable systems that are liable to fail catastrophically during training before an effective policy has been found. We propose to combine MPC with reinforcement learning in the framework of guided policy search, where MPC is used to generate data at training time, under full state observations provided by an instrumented training environment. This data is used to train a deep neural network policy, which is allowed to access only the raw observations from the vehicle's onboard sensors. After training, the neural network policy can successfully control the robot without knowledge of the full state, and at a fraction of the computational cost of MPC. We evaluate our method by learning obstacle avoidance policies for a simulated quadrotor, using simulated onboard sensors and no explicit state estimation at test time."},{"title":"3D Shape Synthesis for Conceptual Design and Optimization Using\n  Variational Autoencoders","source":"Wentai Zhang, Zhangsihao Yang, Haoliang Jiang, Suyash Nigam, Soji\n  Yamakawa, Tomotake Furuhata, Kenji Shimada, Levent Burak Kara","docs_id":"1904.07964","section":["cs.LG","cs.CG","cs.NE","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"3D Shape Synthesis for Conceptual Design and Optimization Using\n  Variational Autoencoders. We propose a data-driven 3D shape design method that can learn a generative model from a corpus of existing designs, and use this model to produce a wide range of new designs. The approach learns an encoding of the samples in the training corpus using an unsupervised variational autoencoder-decoder architecture, without the need for an explicit parametric representation of the original designs. To facilitate the generation of smooth final surfaces, we develop a 3D shape representation based on a distance transformation of the original 3D data, rather than using the commonly utilized binary voxel representation. Once established, the generator maps the latent space representations to the high-dimensional distance transformation fields, which are then automatically surfaced to produce 3D representations amenable to physics simulations or other objective function evaluation modules. We demonstrate our approach for the computational design of gliders that are optimized to attain prescribed performance scores. Our results show that when combined with genetic optimization, the proposed approach can generate a rich set of candidate concept designs that achieve prescribed functional goals, even when the original dataset has only a few or no solutions that achieve these goals."},{"title":"Improving Grey-Box Fuzzing by Modeling Program Behavior","source":"Siddharth Karamcheti, Gideon Mann, and David Rosenberg","docs_id":"1811.08973","section":["cs.AI","cs.SE"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Improving Grey-Box Fuzzing by Modeling Program Behavior. Grey-box fuzzers such as American Fuzzy Lop (AFL) are popular tools for finding bugs and potential vulnerabilities in programs. While these fuzzers have been able to find vulnerabilities in many widely used programs, they are not efficient; of the millions of inputs executed by AFL in a typical fuzzing run, only a handful discover unseen behavior or trigger a crash. The remaining inputs are redundant, exhibiting behavior that has already been observed. Here, we present an approach to increase the efficiency of fuzzers like AFL by applying machine learning to directly model how programs behave. We learn a forward prediction model that maps program inputs to execution traces, training on the thousands of inputs collected during standard fuzzing. This learned model guides exploration by focusing on fuzzing inputs on which our model is the most uncertain (measured via the entropy of the predicted execution trace distribution). By focusing on executing inputs our learned model is unsure about, and ignoring any input whose behavior our model is certain about, we show that we can significantly limit wasteful execution. Through testing our approach on a set of binaries released as part of the DARPA Cyber Grand Challenge, we show that our approach is able to find a set of inputs that result in more code coverage and discovered crashes than baseline fuzzers with significantly fewer executions."},{"title":"Generalized Fast Decoding of Polar Codes","source":"Carlo Condo and Valerio Bioglio and Ingmar Land","docs_id":"1804.09508","section":["cs.IT","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Generalized Fast Decoding of Polar Codes. Research on polar codes has been constantly gaining attention over the last decade, by academia and industry alike, thanks to their capacity-achieving error-correction performance and low-complexity decoding algorithms. Recently, they have been selected as one of the coding schemes in the $5^{th}$ generation wireless standard (5G). Over the years various polar code decoding algorithms, like SC-list (SCL), have been proposed to improve the mediocre performance of the successive cancellation (SC) decoding algorithm for finite code lengths; however, like SC, they suffer from long decoding latency. Fast decoding of polar codes tries to overcome this problem by identifying particular subcodes in the polar code and decoding them with efficient decoders. In this work, we introduce a generalized approach to fast decoding of polar codes to further reduce SC-based decoding latency. We propose three multi-node polar code subcodes whose identification patterns include most of the existing subcodes, extending them to SCL decoding, and allow to apply fast decoding to larger subsets of bits. Without any error-correction performance degradation, the proposed technique shows up to $23.6\\%$ and $29.2\\%$ decoding latency gain with respect to fast SC and SCL decoding algorithms, respectively, and up to $63.6\\%$ and $49.8\\%$ if a performance loss is accepted, whose amount depends on code and decoding algorithm parameters, along with the desired speedup."},{"title":"Optimal Sensor Gain Control for Minimum-Information Estimation of\n  Continuous-Time Gauss-Markov Processes","source":"Vrushabh Zinage, Takashi Tanaka and Valeri Ugrinovskii","docs_id":"2109.13854","section":["eess.SY","cs.SY"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Optimal Sensor Gain Control for Minimum-Information Estimation of\n  Continuous-Time Gauss-Markov Processes. We consider the scenario in which a continuous-time Gauss-Markov process is estimated by the Kalman-Bucy filter over a Gaussian channel (sensor) with a variable sensor gain. The problem of scheduling the sensor gain over a finite time interval to minimize the weighted sum of the data rate (the mutual information between the sensor output and the underlying Gauss-Markov process) and the distortion (the mean-square estimation error) is formulated as an optimal control problem. A necessary optimality condition for a scheduled sensor gain is derived based on Pontryagin's minimum principle. For a scalar problem, we show that an optimal sensor gain control is of bang-bang type, except the possibility of taking an intermediate value when there exists a stationary point on the switching surface in the phase space of canonical dynamics. Furthermore, we show that the number of switches is at most two and the time instants at which the optimal gain must be switched can be computed from the analytical solutions to the canonical equations."},{"title":"A Compressive Method for Centralized PSD Map Construction with Imperfect\n  Reporting Channel","source":"Mohammad Eslami, Seyed Hamid Safavi, Farah Torkamani-Azar, Esfandiar\n  Mehrshahi","docs_id":"1703.05536","section":["cs.IT","cs.NI","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A Compressive Method for Centralized PSD Map Construction with Imperfect\n  Reporting Channel. Spectrum resources management of growing demands is a challenging problem and Cognitive Radio (CR) known to be capable of improving the spectrum utilization. Recently, Power Spectral Density (PSD) map is defined to enable the CR to reuse the frequency resources regarding to the area. For this reason, the sensed PSDs are collected by the distributed sensors in the area and fused by a Fusion Center (FC). But, for a given zone, the sensed PSDs by neighbor CR sensors may contain a shared common component for a while. This component can be exploited in the theory of the Distributed Source Coding (DSC) to make the sensors transmission data more compressed. However, uncertain channel fading and random shadowing would lead to varying signal strength at different CRs, even placed close to each other. Hence, existence of some perturbations in the transmission procedure yields to some imperfection in the reporting channel and as a result it degrades the performance remarkably. The main focus of this paper is to be able to reconstruct the PSDs of sensors \\textit{robustly} based on the Distributed Compressive Sensing (DCS) when the data transmission is slightly imperfect. Simulation results verify the robustness of the proposed scheme."},{"title":"Large Language Models Can Be Strong Differentially Private Learners","source":"Xuechen Li, Florian Tram\\`er, Percy Liang, Tatsunori Hashimoto","docs_id":"2110.05679","section":["cs.LG","cs.CL"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Large Language Models Can Be Strong Differentially Private Learners. Differentially Private (DP) learning has seen limited success for building large deep learning models of text, and attempts at straightforwardly applying Differentially Private Stochastic Gradient Descent (DP-SGD) to NLP tasks have resulted in large performance drops and high computational overhead. We show that this performance drop can be mitigated with (1) the use of large pretrained models; (2) hyperparameters that suit DP optimization; and (3) fine-tuning objectives aligned with the pretraining procedure. With these factors set right, we obtain private NLP models that outperform state-of-the-art private training approaches and strong non-private baselines -- by directly fine-tuning pretrained models with DP optimization on moderately-sized corpora. To address the computational challenge of running DP-SGD with large Transformers, we propose a memory saving technique that allows clipping in DP-SGD to run without instantiating per-example gradients for any layer in the model. The technique enables privately training Transformers with almost the same memory cost as non-private training at a modest run-time overhead. Contrary to conventional wisdom that DP optimization fails at learning high-dimensional models (due to noise that scales with dimension) empirical results reveal that private learning with pretrained models tends to not suffer from dimension-dependent performance degradation."},{"title":"Warwick Image Forensics Dataset for Device Fingerprinting In Multimedia\n  Forensics","source":"Yijun Quan, Chang-Tsun Li, Yujue Zhou and Li Li","docs_id":"2004.10469","section":["cs.CV","cs.CR","cs.MM","eess.IV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Warwick Image Forensics Dataset for Device Fingerprinting In Multimedia\n  Forensics. Device fingerprints like sensor pattern noise (SPN) are widely used for provenance analysis and image authentication. Over the past few years, the rapid advancement in digital photography has greatly reshaped the pipeline of image capturing process on consumer-level mobile devices. The flexibility of camera parameter settings and the emergence of multi-frame photography algorithms, especially high dynamic range (HDR) imaging, bring new challenges to device fingerprinting. The subsequent study on these topics requires a new purposefully built image dataset. In this paper, we present the Warwick Image Forensics Dataset, an image dataset of more than 58,600 images captured using 14 digital cameras with various exposure settings. Special attention to the exposure settings allows the images to be adopted by different multi-frame computational photography algorithms and for subsequent device fingerprinting. The dataset is released as an open-source, free for use for the digital forensic community."},{"title":"GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond","source":"Yue Cao and Jiarui Xu and Stephen Lin and Fangyun Wei and Han Hu","docs_id":"1904.11492","section":["cs.CV","cs.AI","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond. The Non-Local Network (NLNet) presents a pioneering approach for capturing long-range dependencies, via aggregating query-specific global context to each query position. However, through a rigorous empirical analysis, we have found that the global contexts modeled by non-local network are almost the same for different query positions within an image. In this paper, we take advantage of this finding to create a simplified network based on a query-independent formulation, which maintains the accuracy of NLNet but with significantly less computation. We further observe that this simplified design shares similar structure with Squeeze-Excitation Network (SENet). Hence we unify them into a three-step general framework for global context modeling. Within the general framework, we design a better instantiation, called the global context (GC) block, which is lightweight and can effectively model the global context. The lightweight property allows us to apply it for multiple layers in a backbone network to construct a global context network (GCNet), which generally outperforms both simplified NLNet and SENet on major benchmarks for various recognition tasks. The code and configurations are released at https:\/\/github.com\/xvjiarui\/GCNet."},{"title":"PaPy: Parallel and Distributed Data-processing Pipelines in Python","source":"Marcin Cieslik and Cameron Mura","docs_id":"1407.4378","section":["cs.PL","q-bio.QM"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"PaPy: Parallel and Distributed Data-processing Pipelines in Python. PaPy, which stands for parallel pipelines in Python, is a highly flexible framework that enables the construction of robust, scalable workflows for either generating or processing voluminous datasets. A workflow is created from user-written Python functions (nodes) connected by 'pipes' (edges) into a directed acyclic graph. These functions are arbitrarily definable, and can make use of any Python modules or external binaries. Given a user-defined topology and collection of input data, functions are composed into nested higher-order maps, which are transparently and robustly evaluated in parallel on a single computer or on remote hosts. Local and remote computational resources can be flexibly pooled and assigned to functional nodes, thereby allowing facile load-balancing and pipeline optimization to maximize computational throughput. Input items are processed by nodes in parallel, and traverse the graph in batches of adjustable size -- a trade-off between lazy-evaluation, parallelism, and memory consumption. The processing of a single item can be parallelized in a scatter\/gather scheme. The simplicity and flexibility of distributed workflows using PaPy bridges the gap between desktop -> grid, enabling this new computing paradigm to be leveraged in the processing of large scientific datasets."},{"title":"Scaling Properties of Deep Residual Networks","source":"Alain-Sam Cohen, Rama Cont, Alain Rossier, Renyuan Xu","docs_id":"2105.12245","section":["cs.LG","cs.NA","cs.NE","math.NA","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Scaling Properties of Deep Residual Networks. Residual networks (ResNets) have displayed impressive results in pattern recognition and, recently, have garnered considerable theoretical interest due to a perceived link with neural ordinary differential equations (neural ODEs). This link relies on the convergence of network weights to a smooth function as the number of layers increases. We investigate the properties of weights trained by stochastic gradient descent and their scaling with network depth through detailed numerical experiments. We observe the existence of scaling regimes markedly different from those assumed in neural ODE literature. Depending on certain features of the network architecture, such as the smoothness of the activation function, one may obtain an alternative ODE limit, a stochastic differential equation or neither of these. These findings cast doubts on the validity of the neural ODE model as an adequate asymptotic description of deep ResNets and point to an alternative class of differential equations as a better description of the deep network limit."},{"title":"Sosed: a tool for finding similar software projects","source":"Egor Bogomolov, Yaroslav Golubev, Artyom Lobanov, Vladimir Kovalenko,\n  Timofey Bryksin","docs_id":"2007.02599","section":["cs.SE"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Sosed: a tool for finding similar software projects. In this paper, we present Sosed, a tool for discovering similar software projects. We use fastText to compute the embeddings of subtokens into a dense space for 120,000 GitHub repositories in 200 languages. Then, we cluster embeddings to identify groups of semantically similar sub-tokens that reflect topics in source code. We use a dataset of 9 million GitHub projects as a reference search base. To identify similar projects, we compare the distributions of clusters among their sub-tokens. The tool receives an arbitrary project as input, extracts sub-tokens in 16 most popular programming languages, computes cluster distribution, and finds projects with the closest distribution in the search base. We labeled subtoken clusters with short descriptions to enable Sosed to produce interpretable output. Sosed is available at https:\/\/github.com\/JetBrains-Research\/sosed\/. The tool demo is available at https:\/\/www.youtube.com\/watch?v=LYLkztCGRt8. The multi-language extractor of sub-tokens is available separately at https:\/\/github.com\/JetBrains-Research\/buckwheat\/."},{"title":"ScienceWISE: Topic Modeling over Scientific Literature Networks","source":"Andrea Martini, Artem Lutov, Valerio Gemmetto, Andrii Magalich,\n  Alessio Cardillo, Alex Constantin, Vasyl Palchykov, Mourad Khayati, Philippe\n  Cudr\\'e-Mauroux, Alexey Boyarsky, Oleg Ruchayskiy, Diego Garlaschelli, Paolo\n  De Los Rios, Karl Aberer","docs_id":"1612.07636","section":["cs.DL","cs.SI","physics.soc-ph"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"ScienceWISE: Topic Modeling over Scientific Literature Networks. We provide an up-to-date view on the knowledge management system ScienceWISE (SW) and address issues related to the automatic assignment of articles to research topics. So far, SW has been proven to be an effective platform for managing large volumes of technical articles by means of ontological concept-based browsing. However, as the publication of research articles accelerates, the expressivity and the richness of the SW ontology turns into a double-edged sword: a more fine-grained characterization of articles is possible, but at the cost of introducing more spurious relations among them. In this context, the challenge of continuously recommending relevant articles to users lies in tackling a network partitioning problem, where nodes represent articles and co-occurring concepts create edges between them. In this paper, we discuss the three research directions we have taken for solving this issue: i) the identification of generic concepts to reinforce inter-article similarities; ii) the adoption of a bipartite network representation to improve scalability; iii) the design of a clustering algorithm to identify concepts for cross-disciplinary articles and obtain fine-grained topics for all articles."},{"title":"Multilingual Speech Recognition for Low-Resource Indian Languages using\n  Multi-Task conformer","source":"Krishna D N","docs_id":"2109.03969","section":["cs.CL","cs.SD","eess.AS"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Multilingual Speech Recognition for Low-Resource Indian Languages using\n  Multi-Task conformer. Transformers have recently become very popular for sequence-to-sequence applications such as machine translation and speech recognition. In this work, we propose a multi-task learning-based transformer model for low-resource multilingual speech recognition for Indian languages. Our proposed model consists of a conformer [1] encoder and two parallel transformer decoders. We use a phoneme decoder (PHN-DEC) for the phoneme recognition task and a grapheme decoder (GRP-DEC) to predict grapheme sequence. We consider the phoneme recognition task as an auxiliary task for our multi-task learning framework. We jointly optimize the network for both phoneme and grapheme recognition tasks using Joint CTC-Attention [2] training. We use a conditional decoding scheme to inject the language information into the model before predicting the grapheme sequence. Our experiments show that our proposed approach can obtain significant improvement over previous approaches [4]. We also show that our conformer-based dual-decoder approach outperforms both the transformer-based dual-decoder approach and single decoder approach. Finally, We compare monolingual ASR models with our proposed multilingual ASR approach."},{"title":"Theoretic Shaping Bounds for Single Letter Constraints and Mismatched\n  Decoding","source":"Stella Achtenberg and Dan Raphaeli","docs_id":"1308.5938","section":["cs.IT","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Theoretic Shaping Bounds for Single Letter Constraints and Mismatched\n  Decoding. Shaping gain is attained in schemes where a shaped subcode is chosen from a larger codebook by a codeword selection process. This includes the popular method of Trellis Shaping (TS), originally proposed by Forney for average power reduction. The decoding process of such schemes is mismatched, since it is aware of only the large codebook. This study models such schemes by a random code construction and derives achievable bounds on the transmission rate under matched and mismatched decoding. For matched decoding the bound is obtained using a modified asymptotic equipartition property (AEP) theorem derived to suit this particular code construction. For mismatched decoding, relying on the large codebook performance is generally wrong, since the performance of the non-typical codewords within the large codebook may differ substantially from the typical ones. Hence, we present two novel lower bounds on the capacity under mismatched decoding. The first is based upon Gallager's random exponent, whereas the second on a modified version of the joint-typicality decoder."},{"title":"Emergence of scaling in human-interest dynamics","source":"Zhi-Dan Zhao and Zimo Yang and Zike Zhang and Tao Zhou and Zi-Gang\n  Huang and Ying-Cheng Lai","docs_id":"1307.7796","section":["physics.soc-ph","cs.SI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Emergence of scaling in human-interest dynamics. Human behaviors are often driven by human interests. Despite intense recent efforts in exploring the dynamics of human behaviors, little is known about human-interest dynamics, partly due to the extreme difficulty in accessing the human mind from observations. However, the availability of large-scale data, such as those from e-commerce and smart-phone communications, makes it possible to probe into and quantify the dynamics of human interest. Using three prototypical \"big data\" sets, we investigate the scaling behaviors associated with human-interest dynamics. In particular, from the data sets we uncover power-law scaling associated with the three basic quantities: (1) the length of continuous interest, (2) the return time of visiting certain interest, and (3) interest ranking and transition. We argue that there are three basic ingredients underlying human-interest dynamics: preferential return to previously visited interests, inertial effect, and exploration of new interests. We develop a biased random-walk model, incorporating the three ingredients, to account for the observed power-law scaling relations. Our study represents the first attempt to understand the dynamical processes underlying human interest, which has significant applications in science and engineering, commerce, as well as defense, in terms of specific tasks such as recommendation and human-behavior prediction."},{"title":"A New Method of Construction of Permutation Trinomials with Coefficients\n  1","source":"Hua Guo, Shuo Wang, Hutao Song, Xiyong Zhang, Jianwei Liu","docs_id":"2112.14547","section":["math.NT","cs.IT","math.CO","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A New Method of Construction of Permutation Trinomials with Coefficients\n  1. Permutation polynomials over finite fields are an interesting and constantly active research subject of study for many years. They have important applications in areas of mathematics and engineering. In recent years, permutation binomials and permutation trinomials attract people's interests due to their simple algebraic forms. By reversely using Tu's method for the characterization of permutation polynomials with exponents of Niho type, we construct a class of permutation trinomials with coefficients 1 in this paper. As applications, two conjectures of [19] and a conjecture of [13] are all special cases of our result. To our knowledge, the construction method of permutation polynomials by polar decomposition in this paper is new. Moreover, we prove that in new class of permutation trinomials, there exists a permutation polynomial which is EA-inequivalent to known permutation polynomials for all m greater than or equal to 2. Also we give the explicit compositional inverses of the new permutation trinomials for a special case."},{"title":"Multimodal Recurrent Neural Networks with Information Transfer Layers\n  for Indoor Scene Labeling","source":"Abrar H. Abdulnabi, Bing Shuai, Zhen Zuo, Lap-Pui Chau, Gang Wang","docs_id":"1803.04687","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Multimodal Recurrent Neural Networks with Information Transfer Layers\n  for Indoor Scene Labeling. This paper proposes a new method called Multimodal RNNs for RGB-D scene semantic segmentation. It is optimized to classify image pixels given two input sources: RGB color channels and Depth maps. It simultaneously performs training of two recurrent neural networks (RNNs) that are crossly connected through information transfer layers, which are learnt to adaptively extract relevant cross-modality features. Each RNN model learns its representations from its own previous hidden states and transferred patterns from the other RNNs previous hidden states; thus, both model-specific and crossmodality features are retained. We exploit the structure of quad-directional 2D-RNNs to model the short and long range contextual information in the 2D input image. We carefully designed various baselines to efficiently examine our proposed model structure. We test our Multimodal RNNs method on popular RGB-D benchmarks and show how it outperforms previous methods significantly and achieves competitive results with other state-of-the-art works."},{"title":"Evolutionary Algorithm for Graph Coloring Problem","source":"Robiul Islam and Arup Kumar Pramanik","docs_id":"2111.09743","section":["cs.NE"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Evolutionary Algorithm for Graph Coloring Problem. The graph coloring problem (GCP) is one of the most studied NP-HARD problems in computer science. Given a graph , the task is to assign a color to all vertices such that no vertices sharing an edge receive the same color and that the number of used colors, is minimal. Different heuristic, meta-heuristic, machine learning and hybrid solution methods have been applied to obtain the solution. To solve this problem we use mutation of evolutionary algorithm. For this purpose we introduce binary encoding for Graph Coloring Problem. This binary encoding help us for mutation, evaluate, immune system and merge color easily and also reduce coloring dynamically. In the traditional evolutionary algorithm (EA) for graph coloring, k-coloring approach is used and the EA is run repeatedly until the lowest possible is reached. In our paper, we start with the theoretical upper bound of chromatic number, that is, maximum out-degree + 1 and in the process of evolution some of the colors are made unused to dynamically reduce the number of color in every generation. We test few standard DIMACS benchmark and compare resent paper. Maximum results are same as expected chromatic color and few data sets are larger than expected chromatic number"},{"title":"Lovasz Convolutional Networks","source":"Prateek Yadav, Madhav Nimishakavi, Naganand Yadati, Shikhar Vashishth,\n  Arun Rajkumar, Partha Talukdar","docs_id":"1805.11365","section":["cs.LG","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Lovasz Convolutional Networks. Semi-supervised learning on graph structured data has received significant attention with the recent introduction of Graph Convolution Networks (GCN). While traditional methods have focused on optimizing a loss augmented with Laplacian regularization framework, GCNs perform an implicit Laplacian type regularization to capture local graph structure. In this work, we propose Lovasz Convolutional Network (LCNs) which are capable of incorporating global graph properties. LCNs achieve this by utilizing Lovasz's orthonormal embeddings of the nodes. We analyse local and global properties of graphs and demonstrate settings where LCNs tend to work better than GCNs. We validate the proposed method on standard random graph models such as stochastic block models (SBM) and certain community structure based graphs where LCNs outperform GCNs and learn more intuitive embeddings. We also perform extensive binary and multi-class classification experiments on real world datasets to demonstrate LCN's effectiveness. In addition to simple graphs, we also demonstrate the use of LCNs on hyper-graphs by identifying settings where they are expected to work better than GCNs."},{"title":"Routing brain traffic through the von Neumann bottleneck: Efficient\n  cache usage in spiking neural network simulation code on general purpose\n  computers","source":"Jari Pronold, Jakob Jordan, Brian J. N. Wylie, Itaru Kitayama, Markus\n  Diesmann, Susanne Kunkel","docs_id":"2109.12855","section":["cs.DC"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Routing brain traffic through the von Neumann bottleneck: Efficient\n  cache usage in spiking neural network simulation code on general purpose\n  computers. Simulation is a third pillar next to experiment and theory in the study of complex dynamic systems such as biological neural networks. Contemporary brain-scale networks correspond to directed graphs of a few million nodes, each with an in-degree and out-degree of several thousands of edges, where nodes and edges correspond to the fundamental biological units, neurons and synapses, respectively. When considering a random graph, each node's edges are distributed across thousands of parallel processes. The activity in neuronal networks is also sparse. Each neuron occasionally transmits a brief signal, called spike, via its outgoing synapses to the corresponding target neurons. This spatial and temporal sparsity represents an inherent bottleneck for simulations on conventional computers: Fundamentally irregular memory-access patterns cause poor cache utilization. Using an established neuronal network simulation code as a reference implementation, we investigate how common techniques to recover cache performance such as software-induced prefetching and software pipelining can benefit a real-world application. The algorithmic changes reduce simulation time by up to 50%. The study exemplifies that many-core systems assigned with an intrinsically parallel computational problem can overcome the von Neumann bottleneck of conventional computer architectures."},{"title":"$k$-ported vs. $k$-lane Broadcast, Scatter, and Alltoall Algorithms","source":"Jesper Larsson Tr\\\"aff","docs_id":"2008.12144","section":["cs.DC"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"$k$-ported vs. $k$-lane Broadcast, Scatter, and Alltoall Algorithms. In $k$-ported message-passing systems, a processor can simultaneously receive $k$ different messages from $k$ other processors, and send $k$ different messages to $k$ other processors that may or may not be different from the processors from which messages are received. Modern clustered systems may not have such capabilities. Instead, compute nodes consisting of $n$ processors can simultaneously send and receive $k$ messages from other nodes, by letting $k$ processors on the nodes concurrently send and receive at most one message. We pose the question of how to design good algorithms for this $k$-lane model, possibly by adapting algorithms devised for the traditional $k$-ported model. We discuss and compare a number of (non-optimal) $k$-lane algorithms for the broadcast, scatter and alltoall collective operations (as found in, e.g., MPI), and experimentally evaluate these on a small $36\\times 32$-node cluster with a dual OmniPath network (corresponding to $k=2$). Results are preliminary."},{"title":"A Novel Paradigm for Calculating Ramsey Number via Artificial Bee Colony\n  Algorithm","source":"Wei-Hao Mao, Fei Gao, Yi-Jin Dong, Wen-Ming Li","docs_id":"1512.01613","section":["cs.AI","cs.NE","math.CO","math.OC"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A Novel Paradigm for Calculating Ramsey Number via Artificial Bee Colony\n  Algorithm. The Ramsey number is of vital importance in Ramsey's theorem. This paper proposed a novel methodology for constructing Ramsey graphs about R(3,10), which uses Artificial Bee Colony optimization(ABC) to raise the lower bound of Ramsey number R(3,10). The r(3,10)-graph contains two limitations, that is, neither complete graphs of order 3 nor independent sets of order 10. To resolve these limitations, a special mathematical model is put in the paradigm to convert the problems into discrete optimization whose smaller minimizers are correspondent to bigger lower bound as approximation of inf R(3,10). To demonstrate the potential of the proposed method, simulations are done to to minimize the amount of these two types of graphs. For the first time, four r(3,9,39) graphs with best approximation for inf R(3,10) are reported in simulations to support the current lower bound for R(3,10). The experiments' results show that the proposed paradigm for Ramsey number's calculation driven by ABC is a successful method with the advantages of high precision and robustness."},{"title":"Single-Camera 3D Head Fitting for Mixed Reality Clinical Applications","source":"Tejas Mane, Aylar Bayramova, Kostas Daniilidis, Philippos Mordohai,\n  Elena Bernardis","docs_id":"2109.02740","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Single-Camera 3D Head Fitting for Mixed Reality Clinical Applications. We address the problem of estimating the shape of a person's head, defined as the geometry of the complete head surface, from a video taken with a single moving camera, and determining the alignment of the fitted 3D head for all video frames, irrespective of the person's pose. 3D head reconstructions commonly tend to focus on perfecting the face reconstruction, leaving the scalp to a statistical approximation. Our goal is to reconstruct the head model of each person to enable future mixed reality applications. To do this, we recover a dense 3D reconstruction and camera information via structure-from-motion and multi-view stereo. These are then used in a new two-stage fitting process to recover the 3D head shape by iteratively fitting a 3D morphable model of the head with the dense reconstruction in canonical space and fitting it to each person's head, using both traditional facial landmarks and scalp features extracted from the head's segmentation mask. Our approach recovers consistent geometry for varying head shapes, from videos taken by different people, with different smartphones, and in a variety of environments from living rooms to outdoor spaces."},{"title":"Rewriting Theory for the Life Sciences: A Unifying Theory of CTMC\n  Semantics","source":"Nicolas Behr and Jean Krivine","docs_id":"2003.09395","section":["cs.LO"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Rewriting Theory for the Life Sciences: A Unifying Theory of CTMC\n  Semantics. The Kappa biochemistry and the M{\\O}D organo-chemistry frameworks are amongst the most intensely developed applications of rewriting theoretical methods in the life sciences to date. A typical feature of these types of rewriting theories is the necessity to implement certain structural constraints on the objects to be rewritten (a protein is empirically found to have a certain signature of sites, a carbon atom can form at most four bonds, ...). In this paper, we contribute to the theoretical foundations of these types of rewriting theory a number of conceptual and technical developments that permit to implement a universal theory of continuous-time Markov chains (CTMCs) for stochastic rewriting systems. Our core mathematical concepts are a novel rule algebra construction for the relevant setting of rewriting rules with conditions, both in Double- and in Sesqui-Pushout semantics, augmented by a suitable stochastic mechanics formalism extension that permits to derive dynamical evolution equations for pattern-counting statistics."},{"title":"A Novel Multi-task Deep Learning Model for Skin Lesion Segmentation and\n  Classification","source":"Xulei Yang, Zeng Zeng, Si Yong Yeo, Colin Tan, Hong Liang Tey, Yi Su","docs_id":"1703.01025","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A Novel Multi-task Deep Learning Model for Skin Lesion Segmentation and\n  Classification. In this study, a multi-task deep neural network is proposed for skin lesion analysis. The proposed multi-task learning model solves different tasks (e.g., lesion segmentation and two independent binary lesion classifications) at the same time by exploiting commonalities and differences across tasks. This results in improved learning efficiency and potential prediction accuracy for the task-specific models, when compared to training the individual models separately. The proposed multi-task deep learning model is trained and evaluated on the dermoscopic image sets from the International Skin Imaging Collaboration (ISIC) 2017 Challenge - Skin Lesion Analysis towards Melanoma Detection, which consists of 2000 training samples and 150 evaluation samples. The experimental results show that the proposed multi-task deep learning model achieves promising performances on skin lesion segmentation and classification. The average value of Jaccard index for lesion segmentation is 0.724, while the average values of area under the receiver operating characteristic curve (AUC) on two individual lesion classifications are 0.880 and 0.972, respectively."},{"title":"Property-based Polynomial Invariant Generation using Sums-of-Squares\n  Optimization","source":"Assal\\'e Adj\\'e (Toulouse), Pierre-Lo\\\"ic Garoche (Toulouse), Victor\n  Magron","docs_id":"1503.07025","section":["cs.LO"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Property-based Polynomial Invariant Generation using Sums-of-Squares\n  Optimization. While abstract interpretation is not theoretically restricted to specific kinds of properties, it is, in practice, mainly developed to compute linear over-approximations of reachable sets, aka. the collecting semantics of the program. The verification of user-provided properties is not easily compatible with the usual forward fixpoint computation using numerical abstract domains. We propose here to rely on sums-of-squares programming to characterize a property-driven polynomial invariant. This invariant generation can be guided by either boundedness, or in contrary, a given zone of the state space to avoid. While the target property is not necessarily inductive with respect to the program semantics, our method identifies a stronger inductive polynomial invariant using numerical optimization. Our method applies to a wide set of programs: a main while loop composed of a disjunction (if-then-else) of polynomial updates e.g. piecewise polynomial controllers. It has been evaluated on various programs."},{"title":"Training CNNs faster with Dynamic Input and Kernel Downsampling","source":"Zissis Poulos, Ali Nouri, Andreas Moshovos","docs_id":"1910.06548","section":["cs.LG","cs.CV","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Training CNNs faster with Dynamic Input and Kernel Downsampling. We reduce training time in convolutional networks (CNNs) with a method that, for some of the mini-batches: a) scales down the resolution of input images via downsampling, and b) reduces the forward pass operations via pooling on the convolution filters. Training is performed in an interleaved fashion; some batches undergo the regular forward and backpropagation passes with original network parameters, whereas others undergo a forward pass with pooled filters and downsampled inputs. Since pooling is differentiable, the gradients of the pooled filters propagate to the original network parameters for a standard parameter update. The latter phase requires fewer floating point operations and less storage due to the reduced spatial dimensions in feature maps and filters. The key idea is that this phase leads to smaller and approximate updates and thus slower learning, but at significantly reduced cost, followed by passes that use the original network parameters as a refinement stage. Deciding how often and for which batches the downsmapling occurs can be done either stochastically or deterministically, and can be defined as a training hyperparameter itself. Experiments on residual architectures show that we can achieve up to 23% reduction in training time with minimal loss in validation accuracy."},{"title":"Data-Driven and SE-assisted AI Model Signal-Awareness Enhancement and\n  Introspection","source":"Sahil Suneja, Yufan Zhuang, Yunhui Zheng, Jim Laredo, Alessandro\n  Morari","docs_id":"2111.05827","section":["cs.SE","cs.AI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Data-Driven and SE-assisted AI Model Signal-Awareness Enhancement and\n  Introspection. AI modeling for source code understanding tasks has been making significant progress, and is being adopted in production development pipelines. However, reliability concerns, especially whether the models are actually learning task-related aspects of source code, are being raised. While recent model-probing approaches have observed a lack of signal awareness in many AI-for-code models, i.e. models not capturing task-relevant signals, they do not offer solutions to rectify this problem. In this paper, we explore data-driven approaches to enhance models' signal-awareness: 1) we combine the SE concept of code complexity with the AI technique of curriculum learning; 2) we incorporate SE assistance into AI models by customizing Delta Debugging to generate simplified signal-preserving programs, augmenting them to the training dataset. With our techniques, we achieve up to 4.8x improvement in model signal awareness. Using the notion of code complexity, we further present a novel model learning introspection approach from the perspective of the dataset."},{"title":"Wireless-Powered Relays in Cooperative Communications: Time-Switching\n  Relaying Protocols and Throughput Analysis","source":"Ali Arshad Nasir, Xiangyun Zhou, Salman Durrani, and Rodney A. Kennedy","docs_id":"1310.7648","section":["cs.IT","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Wireless-Powered Relays in Cooperative Communications: Time-Switching\n  Relaying Protocols and Throughput Analysis. We consider wireless-powered amplify-and-forward and decode-and-forward relaying in cooperative communications, where an energy constrained relay node first harvests energy through the received radio-frequency signal from the source and then uses the harvested energy to forward the source information to the destination node. We propose time-switching based energy harvesting (EH) and information transmission (IT) protocols with two modes of EH at the relay. For continuous time EH, the EH time can be any percentage of the total transmission block time. For discrete time EH, the whole transmission block is either used for EH or IT. The proposed protocols are attractive because they do not require channel state information at the transmitter side and enable relay transmission with preset fixed transmission power. We derive analytical expressions of the achievable throughput for the proposed protocols. The derived expressions are verified by comparison with simulations and allow the system performance to be determined as a function of the system parameters. Finally, we show that the proposed protocols outperform the existing fixed time duration EH protocols in the literature, since they intelligently track the level of the harvested energy to switch between EH and IT in an online fashion, allowing efficient use of resources."},{"title":"Near-Optimal Algorithms for Minimax Optimization","source":"Tianyi Lin, Chi Jin and Michael. I. Jordan","docs_id":"2002.02417","section":["math.OC","cs.LG","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Near-Optimal Algorithms for Minimax Optimization. This paper resolves a longstanding open question pertaining to the design of near-optimal first-order algorithms for smooth and strongly-convex-strongly-concave minimax problems. Current state-of-the-art first-order algorithms find an approximate Nash equilibrium using $\\tilde{O}(\\kappa_{\\mathbf x}+\\kappa_{\\mathbf y})$ or $\\tilde{O}(\\min\\{\\kappa_{\\mathbf x}\\sqrt{\\kappa_{\\mathbf y}}, \\sqrt{\\kappa_{\\mathbf x}}\\kappa_{\\mathbf y}\\})$ gradient evaluations, where $\\kappa_{\\mathbf x}$ and $\\kappa_{\\mathbf y}$ are the condition numbers for the strong-convexity and strong-concavity assumptions. A gap still remains between these results and the best existing lower bound $\\tilde{\\Omega}(\\sqrt{\\kappa_{\\mathbf x}\\kappa_{\\mathbf y}})$. This paper presents the first algorithm with $\\tilde{O}(\\sqrt{\\kappa_{\\mathbf x}\\kappa_{\\mathbf y}})$ gradient complexity, matching the lower bound up to logarithmic factors. Our algorithm is designed based on an accelerated proximal point method and an accelerated solver for minimax proximal steps. It can be easily extended to the settings of strongly-convex-concave, convex-concave, nonconvex-strongly-concave, and nonconvex-concave functions. This paper also presents algorithms that match or outperform all existing methods in these settings in terms of gradient complexity, up to logarithmic factors."},{"title":"Representation for alphanumeric data type based on space and speed case\n  study: Student ID of X university","source":"Agus Pratondo","docs_id":"1109.1359","section":["cs.DB"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Representation for alphanumeric data type based on space and speed case\n  study: Student ID of X university. ID is derived from the word identity, derived from the first two characters in the word. ID is used to distinguish between an entity to another entity. Student ID (SID) is the key differentiator between a student with other students. On the concept of database, the differentiator is unique. SID can be numbers, letters, or a combination of both (alphanumeric). Viewed from the daily context, it is not important to determine which a SID belongs to the type of data. However, when reviewed on database design, determining the type of data, including SID in this case, is important. Problems arise because there is a contradiction between the data type viewed from the data characteristic and practical needs. Type of data for SID is a string, if it is evaluated from the basic concepts and its characteristic. It is acceptable because SID consists of a set of numbers which will not be meaningful if applied arithmetic operations like addition, subtraction, multiplication and division. But in terms of computer organization, data representation type will determine how much data space requirements, speed of access, and speed of operation. By considering the constraints of space and speed on the experiments conducted, SID is better expressed as an integer rather than a set of characters. KEYWORDS aphanumeric,representation, string, integer, space, speed"},{"title":"Discriminative Predicate Path Mining for Fact Checking in Knowledge\n  Graphs","source":"Baoxu Shi, Tim Weninger","docs_id":"1510.05911","section":["cs.DB","cs.AI","cs.IR","cs.SI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Discriminative Predicate Path Mining for Fact Checking in Knowledge\n  Graphs. Traditional fact checking by experts and analysts cannot keep pace with the volume of newly created information. It is important and necessary, therefore, to enhance our ability to computationally determine whether some statement of fact is true or false. We view this problem as a link-prediction task in a knowledge graph, and present a discriminative path-based method for fact checking in knowledge graphs that incorporates connectivity, type information, and predicate interactions. Given a statement S of the form (subject, predicate, object), for example, (Chicago, capitalOf, Illinois), our approach mines discriminative paths that alternatively define the generalized statement (U.S. city, predicate, U.S. state) and uses the mined rules to evaluate the veracity of statement S. We evaluate our approach by examining thousands of claims related to history, geography, biology, and politics using a public, million node knowledge graph extracted from Wikipedia and PubMedDB. Not only does our approach significantly outperform related models, we also find that the discriminative predicate path model is easily interpretable and provides sensible reasons for the final determination."},{"title":"Every decision tree has an influential variable","source":"Ryan O'Donnell, Michael Saks, Oded Schramm, Rocco A. Servedio","docs_id":"cs\/0508071","section":["cs.CC","cs.DM","math.PR"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Every decision tree has an influential variable. We prove that for any decision tree calculating a boolean function $f:\\{-1,1\\}^n\\to\\{-1,1\\}$, \\[ \\Var[f] \\le \\sum_{i=1}^n \\delta_i \\Inf_i(f), \\] where $\\delta_i$ is the probability that the $i$th input variable is read and $\\Inf_i(f)$ is the influence of the $i$th variable on $f$. The variance, influence and probability are taken with respect to an arbitrary product measure on $\\{-1,1\\}^n$. It follows that the minimum depth of a decision tree calculating a given balanced function is at least the reciprocal of the largest influence of any input variable. Likewise, any balanced boolean function with a decision tree of depth $d$ has a variable with influence at least $\\frac{1}{d}$. The only previous nontrivial lower bound known was $\\Omega(d 2^{-d})$. Our inequality has many generalizations, allowing us to prove influence lower bounds for randomized decision trees, decision trees on arbitrary product probability spaces, and decision trees with non-boolean outputs. As an application of our results we give a very easy proof that the randomized query complexity of nontrivial monotone graph properties is at least $\\Omega(v^{4\/3}\/p^{1\/3})$, where $v$ is the number of vertices and $p \\leq \\half$ is the critical threshold probability. This supersedes the milestone $\\Omega(v^{4\/3})$ bound of Hajnal and is sometimes superior to the best known lower bounds of Chakrabarti-Khot and Friedgut-Kahn-Wigderson."},{"title":"Industry-Relevant Implicit Large-Eddy Simulation of a High-Performance\n  Road Car via Spectral\/hp Element Methods","source":"Gianmarco Mengaldo, David Moxey, Michael Turner, Rodrigo C. Moura,\n  Ayad Jassim, Mark Taylor, Joaquim Peiro, Spencer J. Sherwin","docs_id":"2009.10178","section":["cs.CE"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Industry-Relevant Implicit Large-Eddy Simulation of a High-Performance\n  Road Car via Spectral\/hp Element Methods. We present a successful deployment of high-fidelity Large-Eddy Simulation (LES) technologies based on spectral\/hp element methods to industrial flow problems, which are characterized by high Reynolds numbers and complex geometries. In particular, we describe the numerical methods, software development and steps that were required to perform the implicit LES of a real automotive car, namely the Elemental Rp1 model. To the best of the authors' knowledge, this simulation represents the first fifth-order accurate transient LES of an entire real car geometry. Moreover, this constitutes a key milestone towards considerably expanding the computational design envelope currently allowed in industry, where steady-state modelling remains the standard. To this end, a number of novel developments had to be made in order to overcome obstacles in mesh generation and solver technology to achieve this simulation, which we detail in this paper. The main objective is to present to the industrial and applied mathematics community, a viable pathway to translate academic developments into industrial tools, that can substantially advance the analysis and design capabilities of high-end engineering stakeholders. The novel developments and results were achieved using the academic-driven open-source framework Nektar++."},{"title":"A Generalized Framework of Sequence Generation with Application to\n  Undirected Sequence Models","source":"Elman Mansimov, Alex Wang, Sean Welleck, Kyunghyun Cho","docs_id":"1905.12790","section":["cs.LG","cs.CL","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A Generalized Framework of Sequence Generation with Application to\n  Undirected Sequence Models. Undirected neural sequence models such as BERT (Devlin et al., 2019) have received renewed interest due to their success on discriminative natural language understanding tasks such as question-answering and natural language inference. The problem of generating sequences directly from these models has received relatively little attention, in part because generating from undirected models departs significantly from conventional monotonic generation in directed sequence models. We investigate this problem by proposing a generalized model of sequence generation that unifies decoding in directed and undirected models. The proposed framework models the process of generation rather than the resulting sequence, and under this framework, we derive various neural sequence models as special cases, such as autoregressive, semi-autoregressive, and refinement-based non-autoregressive models. This unification enables us to adapt decoding algorithms originally developed for directed sequence models to undirected sequence models. We demonstrate this by evaluating various handcrafted and learned decoding strategies on a BERT-like machine translation model (Lample & Conneau, 2019). The proposed approach achieves constant-time translation results on par with linear-time translation results from the same undirected sequence model, while both are competitive with the state-of-the-art on WMT'14 English-German translation."},{"title":"Emergence and structure of decentralised trade networks around dark web\n  marketplaces","source":"Matthieu Nadini, Alberto Bracci, Abeer ElBahrawy, Philip Gradwell,\n  Alexander Teytelboym, Andrea Baronchelli","docs_id":"2111.01774","section":["physics.soc-ph","cs.CY"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Emergence and structure of decentralised trade networks around dark web\n  marketplaces. Dark web marketplaces (DWMs) are online platforms that facilitate illicit trade among millions of users generating billions of dollars in annual revenue. Recently, two interview-based studies have suggested that DWMs may also promote the emergence of direct user-to-user (U2U) trading relationships. Here, we quantify the scale of, and thoroughly investigate, U2U trading around DWMs by analysing 31 million Bitcoin transactions among users of 40 DWMs between June 2011 and Jan 2021. We find that half of the DWM users trade through U2U pairs generating a total trading volume greater than DWMs themselves. We then show that hundreds of thousands of DWM users form stable trading pairs that are persistent over time. Users in stable pairs are typically the ones with the largest trading volume on DWMs. Then, we show that new U2U pairs often form while both users are active on the same DWM, suggesting the marketplace may serve as a catalyst for new direct trading relationships. Finally, we reveal that stable U2U pairs tend to survive DWM closures and that they were not affected by COVID-19, indicating that their trading activity is resilient to external shocks. Our work unveils sophisticated patterns of trade emerging in the dark web and highlights the importance of investigating user behaviour beyond the immediate buyer-seller network on a single marketplace."},{"title":"Deep Joint Source-Channel Coding for Wireless Image Transmission with\n  Adaptive Rate Control","source":"Mingyu Yang, Hun-Seok Kim","docs_id":"2110.04456","section":["eess.SP","cs.LG","eess.IV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Deep Joint Source-Channel Coding for Wireless Image Transmission with\n  Adaptive Rate Control. We present a novel adaptive deep joint source-channel coding (JSCC) scheme for wireless image transmission. The proposed scheme supports multiple rates using a single deep neural network (DNN) model and learns to dynamically control the rate based on the channel condition and image contents. Specifically, a policy network is introduced to exploit the tradeoff space between the rate and signal quality. To train the policy network, the Gumbel-Softmax trick is adopted to make the policy network differentiable and hence the whole JSCC scheme can be trained end-to-end. To the best of our knowledge, this is the first deep JSCC scheme that can automatically adjust its rate using a single network model. Experiments show that our scheme successfully learns a reasonable policy that decreases channel bandwidth utilization for high SNR scenarios or simple image contents. For an arbitrary target rate, our rate-adaptive scheme using a single model achieves similar performance compared to an optimized model specifically trained for that fixed target rate. To reproduce our results, we make the source code publicly available at https:\/\/github.com\/mingyuyng\/Dynamic_JSCC."},{"title":"Multi-Document Summarization via Discriminative Summary Reranking","source":"Xiaojun Wan, Ziqiang Cao, Furu Wei, Sujian Li and Ming Zhou","docs_id":"1507.02062","section":["cs.CL"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Multi-Document Summarization via Discriminative Summary Reranking. Existing multi-document summarization systems usually rely on a specific summarization model (i.e., a summarization method with a specific parameter setting) to extract summaries for different document sets with different topics. However, according to our quantitative analysis, none of the existing summarization models can always produce high-quality summaries for different document sets, and even a summarization model with good overall performance may produce low-quality summaries for some document sets. On the contrary, a baseline summarization model may produce high-quality summaries for some document sets. Based on the above observations, we treat the summaries produced by different summarization models as candidate summaries, and then explore discriminative reranking techniques to identify high-quality summaries from the candidates for difference document sets. We propose to extract a set of candidate summaries for each document set based on an ILP framework, and then leverage Ranking SVM for summary reranking. Various useful features have been developed for the reranking process, including word-level features, sentence-level features and summary-level features. Evaluation results on the benchmark DUC datasets validate the efficacy and robustness of our proposed approach."},{"title":"Efficient Online Estimation of Causal Effects by Deciding What to\n  Observe","source":"Shantanu Gupta, Zachary C. Lipton, David Childers","docs_id":"2108.09265","section":["cs.LG","econ.EM","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Efficient Online Estimation of Causal Effects by Deciding What to\n  Observe. Researchers often face data fusion problems, where multiple data sources are available, each capturing a distinct subset of variables. While problem formulations typically take the data as given, in practice, data acquisition can be an ongoing process. In this paper, we aim to estimate any functional of a probabilistic model (e.g., a causal effect) as efficiently as possible, by deciding, at each time, which data source to query. We propose online moment selection (OMS), a framework in which structural assumptions are encoded as moment conditions. The optimal action at each step depends, in part, on the very moments that identify the functional of interest. Our algorithms balance exploration with choosing the best action as suggested by current estimates of the moments. We propose two selection strategies: (1) explore-then-commit (OMS-ETC) and (2) explore-then-greedy (OMS-ETG), proving that both achieve zero asymptotic regret as assessed by MSE. We instantiate our setup for average treatment effect estimation, where structural assumptions are given by a causal graph and data sources may include subsets of mediators, confounders, and instrumental variables."},{"title":"TCDesc: Learning Topology Consistent Descriptors","source":"Honghu Pan, Fanyang Meng, Zhenyu He, Yongsheng Liang, Wei Liu","docs_id":"2006.03254","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"TCDesc: Learning Topology Consistent Descriptors. Triplet loss is widely used for learning local descriptors from image patch. However, triplet loss only minimizes the Euclidean distance between matching descriptors and maximizes that between the non-matching descriptors, which neglects the topology similarity between two descriptor sets. In this paper, we propose topology measure besides Euclidean distance to learn topology consistent descriptors by considering kNN descriptors of positive sample. First we establish a novel topology vector for each descriptor followed by Locally Linear Embedding (LLE) to indicate the topological relation among the descriptor and its kNN descriptors. Then we define topology distance between descriptors as the difference of their topology vectors. Last we employ the dynamic weighting strategy to fuse Euclidean distance and topology distance of matching descriptors and take the fusion result as the positive sample distance in the triplet loss. Experimental results on several benchmarks show that our method performs better than state-of-the-arts results and effectively improves the performance of triplet loss."},{"title":"Data Driven Control with Learned Dynamics: Model-Based versus Model-Free\n  Approach","source":"Wenjian Hao, Yiqiang Han","docs_id":"2006.09543","section":["cs.LG","cs.SY","eess.SY","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Data Driven Control with Learned Dynamics: Model-Based versus Model-Free\n  Approach. This paper compares two different types of data-driven control methods, representing model-based and model-free approaches. One is a recently proposed method - Deep Koopman Representation for Control (DKRC), which utilizes a deep neural network to map an unknown nonlinear dynamical system to a high-dimensional linear system, which allows for employing state-of-the-art control strategy. The other one is a classic model-free control method based on an actor-critic architecture - Deep Deterministic Policy Gradient (DDPG), which has been proved to be effective in various dynamical systems. The comparison is carried out in OpenAI Gym, which provides multiple control environments for benchmark purposes. Two examples are provided for comparison, i.e., classic Inverted Pendulum and Lunar Lander Continuous Control. From the results of the experiments, we compare these two methods in terms of control strategies and the effectiveness under various initialization conditions. We also examine the learned dynamic model from DKRC with the analytical model derived from the Euler-Lagrange Linearization method, which demonstrates the accuracy in the learned model for unknown dynamics from a data-driven sample-efficient approach."},{"title":"Near-Deterministic Inference of AS Relationships","source":"Yuval Shavitt, Eran Shir, Udi Weinsberg","docs_id":"0711.4562","section":["cs.NI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Near-Deterministic Inference of AS Relationships. The discovery of Autonomous Systems (ASes) interconnections and the inference of their commercial Type-of-Relationships (ToR) has been extensively studied during the last few years. The main motivation is to accurately calculate AS-level paths and to provide better topological view of the Internet. An inherent problem in current algorithms is their extensive use of heuristics. Such heuristics incur unbounded errors which are spread over all inferred relationships. We propose a near-deterministic algorithm for solving the ToR inference problem. Our algorithm uses as input the Internet core, which is a dense sub-graph of top-level ASes. We test several methods for creating such a core and demonstrate the robustness of the algorithm to the core's size and density, the inference period, and errors in the core. We evaluate our algorithm using AS-level paths collected from RouteViews BGP paths and DIMES traceroute measurements. Our proposed algorithm deterministically infers over 95% of the approximately 58,000 AS topology links. The inference becomes stable when using a week worth of data and as little as 20 ASes in the core. The algorithm infers 2-3 times more peer-to-peer relationships in edges discovered only by DIMES than in RouteViews edges, validating the DIMES promise to discover periphery AS edges."},{"title":"Gini Index based Initial Coin Offering Mechanism","source":"Mingyu Guo, Zhenghui Wang, Yuko Sakurai","docs_id":"2002.11387","section":["cs.GT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Gini Index based Initial Coin Offering Mechanism. As a fundraising method, Initial Coin Offering (ICO) has raised billions of dollars for thousands of startups in the past two years. Existing ICO mechanisms place more emphasis on the short-term benefits of maximal fundraising while ignoring the problem of unbalanced token allocation, which negatively impacts subsequent fundraising and has bad effects on introducing new investors and resources. We propose a new ICO mechanism which uses the concept of Gini index for the very first time as a mechanism design constraint to control allocation inequality. Our mechanism maintains an elegant and straightforward structure. It allows the agents to modify their bids as a price discovery process, while limiting the bids of whales. We analyze the agents' equilibrium behaviors under our mechanism. Under natural technical assumptions, we show that most agents have simple dominant strategies and the equilibrium revenue approaches the optimal revenue asymptotically in the number of agents. We verify our mechanism using real ICO dataset we collected, and confirm that our mechanism performs well in terms of both allocation fairness and revenue."},{"title":"Encoding Legal Balancing: Automating an Abstract Ethico-Legal Value\n  Ontology in Preference Logic","source":"Christoph Benzm\\\"uller and David Fuenmayor and Bertram Lomfeld","docs_id":"2006.12789","section":["cs.AI","cs.LO"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Encoding Legal Balancing: Automating an Abstract Ethico-Legal Value\n  Ontology in Preference Logic. Enabling machines to legal balancing is a non-trivial task challenged by a multitude of factors some of which are addressed and explored in this work. We propose a holistic approach to formal modelling at different abstraction layers supported by a pluralistic framework in which the encoding of an ethico-legal value ontology is developed in combination with the exploration of a formalisation logic, with legal domain knowledge and with exemplary use cases until a reflective equilibrium is reached. Our work is enabled by a meta-logical approach to universal logical reasoning and it applies the recently introduced LOGIKEY methodology for designing normative theories for ethical and legal reasoning. We explore and illustrate the application of the multilayered LOGIKEY approach for the modelling of legal and world knowledge that is constrained by context-dependent value preferences. The framework is then exemplary applied for explaining and resolving legal conflicts in property law (wild animal cases) within a modern proof assistant system."},{"title":"Achieving Efficient Realization of Kalman Filter on CGRA through\n  Algorithm-Architecture Co-design","source":"Farhad Merchant, Tarun Vatwani, Anupam Chattopadhyay, Soumyendu Raha,\n  S K Nandy, Ranjani Narayan","docs_id":"1802.03650","section":["cs.MS","cs.AR"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Achieving Efficient Realization of Kalman Filter on CGRA through\n  Algorithm-Architecture Co-design. In this paper, we present efficient realization of Kalman Filter (KF) that can achieve up to 65% of the theoretical peak performance of underlying architecture platform. KF is realized using Modified Faddeeva Algorithm (MFA) as a basic building block due to its versatility and REDEFINE Coarse Grained Reconfigurable Architecture (CGRA) is used as a platform for experiments since REDEFINE is capable of supporting realization of a set algorithmic compute structures at run-time on a Reconfigurable Data-path (RDP). We perform several hardware and software based optimizations in the realization of KF to achieve 116% improvement in terms of Gflops over the first realization of KF. Overall, with the presented approach for KF, 4-105x performance improvement in terms of Gflops\/watt over several academically and commercially available realizations of KF is attained. In REDEFINE, we show that our implementation is scalable and the performance attained is commensurate with the underlying hardware resources"},{"title":"Outlier detection at the parcel-level in wheat and rapeseed crops using\n  multispectral and SAR time series","source":"Florian Mouret and Mohanad Albughdadi and Sylvie Duthoit and Denis\n  Kouam\\'e and Guillaume Rieu and Jean-Yves Tourneret","docs_id":"2004.08431","section":["eess.IV","cs.LG","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Outlier detection at the parcel-level in wheat and rapeseed crops using\n  multispectral and SAR time series. This paper studies the detection of anomalous crop development at the parcel-level based on an unsupervised outlier detection technique. The experimental validation is conducted on rapeseed and wheat parcels located in Beauce (France). The proposed methodology consists of four sequential steps: 1) preprocessing of synthetic aperture radar (SAR) and multispectral images acquired using Sentinel-1 and Sentinel-2 satellites, 2) extraction of SAR and multispectral pixel-level features, 3) computation of parcel-level features using zonal statistics and 4) outlier detection. The different types of anomalies that can affect the studied crops are analyzed and described. The different factors that can influence the outlier detection results are investigated with a particular attention devoted to the synergy between Sentinel-1 and Sentinel-2 data. Overall, the best performance is obtained when using jointly a selection of Sentinel-1 and Sentinel-2 features with the isolation forest algorithm. The selected features are VV and VH backscattering coefficients for Sentinel-1 and 5 Vegetation Indexes for Sentinel-2 (among us, the Normalized Difference Vegetation Index and two variants of the Normalized Difference Water). When using these features with an outlier ratio of 10%, the percentage of detected true positives (i.e., crop anomalies) is equal to 94.1% for rapeseed parcels and 95.5% for wheat parcels."},{"title":"CalQNet -- Detection of Calibration Quality for Life-Long Stereo Camera\n  Setups","source":"Jiapeng Zhong, Zheyu Ye, Andrei Cramariuc, Florian Tschopp, Jen Jen\n  Chung, Roland Siegwart, Cesar Cadena","docs_id":"2104.04837","section":["cs.RO"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"CalQNet -- Detection of Calibration Quality for Life-Long Stereo Camera\n  Setups. Many mobile robotic platforms rely on an accurate knowledge of the extrinsic calibration parameters, especially systems performing visual stereo matching. Although a number of accurate stereo camera calibration methods have been developed, which provide good initial \"factory\" calibrations, the determined parameters can lose their validity over time as the sensors are exposed to environmental conditions and external effects. Thus, on autonomous platforms on-board diagnostic methods for an early detection of the need to repeat calibration procedures have the potential to prevent critical failures of crucial systems, such as state estimation or obstacle detection. In this work, we present a novel data-driven method to estimate the calibration quality and detect discrepancies between the original calibration and the current system state for stereo camera systems. The framework consists of a novel dataset generation pipeline to train CalQNet, a deep convolutional neural network. CalQNet can estimate the calibration quality using a new metric that approximates the degree of miscalibration in stereo setups. We show the framework's ability to predict from a single stereo frame if a state-of-the-art stereo-visual odometry system will diverge due to a degraded calibration in two real-world experiments."},{"title":"Dense Relational Image Captioning via Multi-task Triple-Stream Networks","source":"Dong-Jin Kim, Tae-Hyun Oh, Jinsoo Choi, In So Kweon","docs_id":"2010.03855","section":["cs.CV","cs.AI","cs.CL"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Dense Relational Image Captioning via Multi-task Triple-Stream Networks. We introduce dense relational captioning, a novel image captioning task which aims to generate multiple captions with respect to relational information between objects in a visual scene. Relational captioning provides explicit descriptions for each relationship between object combinations. This framework is advantageous in both diversity and amount of information, leading to a comprehensive image understanding based on relationships, e.g., relational proposal generation. For relational understanding between objects, the part-of-speech (POS; i.e., subject-object-predicate categories) can be a valuable prior information to guide the causal sequence of words in a caption. We enforce our framework to learn not only to generate captions but also to understand the POS of each word. To this end, we propose the multi-task triple-stream network (MTTSNet) which consists of three recurrent units responsible for each POS which is trained by jointly predicting the correct captions and POS for each word. In addition, we found that the performance of MTTSNet can be improved by modulating the object embeddings with an explicit relational module. We demonstrate that our proposed model can generate more diverse and richer captions, via extensive experimental analysis on large scale datasets and several metrics. Then, we present applications of our framework to holistic image captioning, scene graph generation, and retrieval tasks."},{"title":"Review on Master Patient Index","source":"Dr W.G Prabath Jayatissa, Prof Vajira H W Dissanayake, Dr Roshan\n  Hewapathirane","docs_id":"1803.05994","section":["cs.CY"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Review on Master Patient Index. In today's health care establishments there is a great diversity of information systems. Each with different specificities and capacities, proprietary communication methods, and hardly allow scalability. This set of characteristics hinders the interoperability of all these systems, in the search for the good of the patient. It is vulgar that, when we look at all the databases of each of these information systems, we come across different registers that refer to the same person; records with insufficient data; records with erroneous data due to errors or misunderstandings when inserting patient data; and records with outdated data. These problems cause duplicity, incoherence, discontinuation and dispersion in patient data. With the intention of minimizing these problems that the concept of a Master Patient Index is necessary. A Master Patient Index proposes a centralized repository, which indexes all patient records of a given set of information systems. Which is composed of a set of demographic data sufficient to unambiguously identify a person and a list of identifiers that identify the various records that the patient has in the repositories of each information system. This solution allows for synchronization between all the actors, minimizing incoherence, out datedness, lack of data, and a decrease in duplicate registrations. The Master Patient Index is an asset to patients, the medical staff and health care providers."},{"title":"FQ-ViT: Fully Quantized Vision Transformer without Retraining","source":"Yang Lin, Tianyu Zhang, Peiqin Sun, Zheng Li, Shuchang Zhou","docs_id":"2111.13824","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"FQ-ViT: Fully Quantized Vision Transformer without Retraining. Network quantization significantly reduces model inference complexity and has been widely used in real-world deployments. However, most existing quantization methods have been developed and tested mainly on Convolutional Neural Networks (CNN), and suffer severe degradation when applied to Transformer-based architectures. In this work, we present a systematic method to reduce the performance degradation and inference complexity of Quantized Transformers. In particular, we propose Powers-of-Two Scale (PTS) to deal with the serious inter-channel variation of LayerNorm inputs in a hardware-friendly way. In addition, we propose Log-Int-Softmax (LIS) that can sustain the extreme non-uniform distribution of the attention maps while simplifying inference by using 4-bit quantization and the BitShift operator. Comprehensive experiments on various Transformer-based architectures and benchmarks show that our methods outperform previous works in performance while using even lower bit-width in attention maps. For instance, we reach 85.17% Top-1 accuracy with ViT-L on ImageNet and 51.4 mAP with Cascade Mask R-CNN (Swin-S) on COCO. To our knowledge, we are the first to achieve comparable accuracy degradation (~1%) on fully quantized Vision Transformers. Code is available at https:\/\/github.com\/linyang-zhh\/FQ-ViT."},{"title":"Learnable Manifold Alignment (LeMA) : A Semi-supervised Cross-modality\n  Learning Framework for Land Cover and Land Use Classification","source":"Danfeng Hong, Naoto Yokoya, Nan Ge, Jocelyn Chanussot, Xiao Xiang Zhu","docs_id":"1901.02838","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Learnable Manifold Alignment (LeMA) : A Semi-supervised Cross-modality\n  Learning Framework for Land Cover and Land Use Classification. In this paper, we aim at tackling a general but interesting cross-modality feature learning question in remote sensing community --- can a limited amount of highly-discrimin-ative (e.g., hyperspectral) training data improve the performance of a classification task using a large amount of poorly-discriminative (e.g., multispectral) data? Traditional semi-supervised manifold alignment methods do not perform sufficiently well for such problems, since the hyperspectral data is very expensive to be largely collected in a trade-off between time and efficiency, compared to the multispectral data. To this end, we propose a novel semi-supervised cross-modality learning framework, called learnable manifold alignment (LeMA). LeMA learns a joint graph structure directly from the data instead of using a given fixed graph defined by a Gaussian kernel function. With the learned graph, we can further capture the data distribution by graph-based label propagation, which enables finding a more accurate decision boundary. Additionally, an optimization strategy based on the alternating direction method of multipliers (ADMM) is designed to solve the proposed model. Extensive experiments on two hyperspectral-multispectral datasets demonstrate the superiority and effectiveness of the proposed method in comparison with several state-of-the-art methods."},{"title":"Simulation and estimation of an agent-based market-model with a matching\n  engine","source":"Ivan Jericevich and Patrick Chang and Tim Gebbie","docs_id":"2108.07806","section":["q-fin.TR","cs.MA","q-fin.CP"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Simulation and estimation of an agent-based market-model with a matching\n  engine. An agent-based model with interacting low frequency liquidity takers inter-mediated by high-frequency liquidity providers acting collectively as market makers can be used to provide realistic simulated price impact curves. This is possible when agent-based model interactions occur asynchronously via order matching using a matching engine in event time to replace sequential calendar time market clearing. Here the matching engine infrastructure has been modified to provide a continuous feed of order confirmations and updates as message streams in order to conform more closely to live trading environments. The resulting trade and quote message data from the simulations are then aggregated, calibrated and visualised. Various stylised facts are presented along with event visualisations and price impact curves. We argue that additional realism in modelling can be achieved with a small set of agent parameters and simple interaction rules once interactions are reactive, asynchronous and in event time. We argue that the reactive nature of market agents may be a fundamental property of financial markets and when accounted for can allow for parsimonious modelling without recourse to additional sources of noise."},{"title":"Communication in a Poisson Field of Interferers -- Part I: Interference\n  Distribution and Error Probability","source":"Pedro C. Pinto, Moe Z. Win","docs_id":"1001.4519","section":["cs.IT","cs.NI","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Communication in a Poisson Field of Interferers -- Part I: Interference\n  Distribution and Error Probability. We present a mathematical model for communication subject to both network interference and noise. We introduce a framework where the interferers are scattered according to a spatial Poisson process, and are operating asynchronously in a wireless environment subject to path loss, shadowing, and multipath fading. We consider both cases of slow and fast-varying interferer positions. The paper is comprised of two separate parts. In Part I, we determine the distribution of the aggregate network interference at the output of a linear receiver. We characterize the error performance of the link, in terms of average and outage probabilities. The proposed model is valid for any linear modulation scheme (e.g., M-ary phase shift keying or M-ary quadrature amplitude modulation), and captures all the essential physical parameters that affect network interference. Our work generalizes the conventional analysis of communication in the presence of additive white Gaussian noise and fast fading, allowing the traditional results to be extended to include the effect of network interference. In Part II of the paper, we derive the capacity of the link when subject to network interference and noise, and characterize the spectrum of the aggregate interference."},{"title":"Interactive Multi-level Stroke Control for Neural Style Transfer","source":"Max Reimann and Benito Buchheim and Amir Semmo and J\\\"urgen D\\\"ollner\n  and Matthias Trapp","docs_id":"2106.13787","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Interactive Multi-level Stroke Control for Neural Style Transfer. We present StyleTune, a mobile app for interactive multi-level control of neural style transfers that facilitates creative adjustments of style elements and enables high output fidelity. In contrast to current mobile neural style transfer apps, StyleTune supports users to adjust both the size and orientation of style elements, such as brushstrokes and texture patches, on a global as well as local level. To this end, we propose a novel stroke-adaptive feed-forward style transfer network, that enables control over stroke size and intensity and allows a larger range of edits than current approaches. For additional level-of-control, we propose a network agnostic method for stroke-orientation adjustment by utilizing the rotation-variance of CNNs. To achieve high output fidelity, we further add a patch-based style transfer method that enables users to obtain output resolutions of more than 20 Megapixel. Our approach empowers users to create many novel results that are not possible with current mobile neural style transfer apps."},{"title":"Task-Driven Deep Image Enhancement Network for Autonomous Driving in Bad\n  Weather","source":"Younkwan Lee, Jihyo Jeon, Yeongmin Ko, Byunggwan Jeon, Moongu Jeon","docs_id":"2110.07206","section":["cs.CV","cs.RO"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Task-Driven Deep Image Enhancement Network for Autonomous Driving in Bad\n  Weather. Visual perception in autonomous driving is a crucial part of a vehicle to navigate safely and sustainably in different traffic conditions. However, in bad weather such as heavy rain and haze, the performance of visual perception is greatly affected by several degrading effects. Recently, deep learning-based perception methods have addressed multiple degrading effects to reflect real-world bad weather cases but have shown limited success due to 1) high computational costs for deployment on mobile devices and 2) poor relevance between image enhancement and visual perception in terms of the model ability. To solve these issues, we propose a task-driven image enhancement network connected to the high-level vision task, which takes in an image corrupted by bad weather as input. Specifically, we introduce a novel low memory network to reduce most of the layer connections of dense blocks for less memory and computational cost while maintaining high performance. We also introduce a new task-driven training strategy to robustly guide the high-level task model suitable for both high-quality restoration of images and highly accurate perception. Experiment results demonstrate that the proposed method improves the performance among lane and 2D object detection, and depth estimation largely under adverse weather in terms of both low memory and accuracy."},{"title":"Localization Efficiency in Massive MIMO Systems","source":"Masoud Arash, Hamed Mirghasemi, Ivan Stupia and Luc Vandendorpe","docs_id":"2003.07978","section":["cs.IT","eess.SP","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Localization Efficiency in Massive MIMO Systems. In the next generation of wireless systems, Massive MIMO offers high angular resolution for localization. By virtue of large number of antennas, the Angle of Arrival (AoA) of User Terminals (UTs) can be estimated with high accuracy. According to Dense Multipath Component (DMC) channel model, local scatters around UTs can create different multipath signals for each antenna at the Base Station (BS). We obtain a deterministic form for the Cramer-Rao Lower Bound (CRLB) in a multi-user scenario when the contribution of the multipath signals is considered. We do this when the multipath signals are independent and identically distributed (i.i.d) with arbitrary distribution. Then, we redefine a localization efficiency function for a multi-user scenario and numerically optimize it with respect to (w.r.t) the number of antennas. We prove when only a subset of the available antennas is used, CRLB can be minimized w.r.t which set of antennas is used. Then, an antenna selection strategy that minimizes CRLB is proposed. As a benchmark, we apply the proposed antenna selection scheme to the MUltiple SIgnal Classification (MUSIC) algorithm and study its efficiency. Numerical results validate the accuracy of our analysis and show significant improvement in efficiency when the proposed antenna selection strategy is employed."},{"title":"VPIC 2.0: Next Generation Particle-in-Cell Simulations","source":"Robert Bird, Nigel Tan, Scott V. Luedtke, Stephen Lien Harrell,\n  Michela Taufer, Brian Albright","docs_id":"2102.13133","section":["cs.DC"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"VPIC 2.0: Next Generation Particle-in-Cell Simulations. VPIC is a general purpose Particle-in-Cell simulation code for modeling plasma phenomena such as magnetic reconnection, fusion, solar weather, and laser-plasma interaction in three dimensions using large numbers of particles. VPIC's capacity in both fidelity and scale makes it particularly well-suited for plasma research on pre-exascale and exascale platforms. In this paper we demonstrate the unique challenges involved in preparing the VPIC code for operation at exascale, outlining important optimizations to make VPIC efficient on accelerators. Specifically, we show the work undertaken in adapting VPIC to exploit the portability-enabling framework Kokkos and highlight the enhancements to VPIC's modeling capabilities to achieve performance at exascale. We assess the achieved performance-portability trade-off through a suite of studies on nine different varieties of modern pre-exascale hardware. Our performance-portability study includes weak-scaling runs on three of the top ten TOP500 supercomputers, as well as a comparison of low-level system performance of hardware from four different vendors."},{"title":"Event Detection: Gate Diversity and Syntactic Importance Scoresfor Graph\n  Convolution Neural Networks","source":"Viet Dac Lai, Tuan Ngo Nguyen, Thien Huu Nguyen","docs_id":"2010.14123","section":["cs.CL"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Event Detection: Gate Diversity and Syntactic Importance Scoresfor Graph\n  Convolution Neural Networks. Recent studies on event detection (ED) haveshown that the syntactic dependency graph canbe employed in graph convolution neural net-works (GCN) to achieve state-of-the-art per-formance. However, the computation of thehidden vectors in such graph-based models isagnostic to the trigger candidate words, po-tentially leaving irrelevant information for thetrigger candidate for event prediction. In addi-tion, the current models for ED fail to exploitthe overall contextual importance scores of thewords, which can be obtained via the depen-dency tree, to boost the performance. In thisstudy, we propose a novel gating mechanismto filter noisy information in the hidden vec-tors of the GCN models for ED based on theinformation from the trigger candidate. Wealso introduce novel mechanisms to achievethe contextual diversity for the gates and theimportance score consistency for the graphsand models in ED. The experiments show thatthe proposed model achieves state-of-the-artperformance on two ED datasets"},{"title":"A Novel Model for Distributed Big Data Service Composition using\n  Stratified Functional Graph Matching","source":"Carlos R. Rivero and Hasan M. Jamil","docs_id":"1607.02669","section":["cs.DB"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A Novel Model for Distributed Big Data Service Composition using\n  Stratified Functional Graph Matching. A significant number of current industrial applications rely on web services. A cornerstone task in these applications is discovering a suitable service that meets the threshold of some user needs. Then, those services can be composed to perform specific functionalities. We argue that the prevailing approach to compose services based on the \"all or nothing\" paradigm is limiting and leads to exceedingly high rejection of potentially suitable services. Furthermore, contemporary models do not allow \"mix and match\" composition from atomic services of different composite services when binary matching is not possible or desired. In this paper, we propose a new model for service composition based on \"stratified graph summarization\" and \"service stitching\". We discuss the limitations of existing approaches with a motivating example, present our approach to overcome these limitations, and outline a possible architecture for service composition from atomic services. Our thesis is that, with the advent of Big Data, our approach will reduce latency in service discovery, and will improve efficiency and accuracy of matchmaking and composition of services."},{"title":"Automating Cluster Management with Weave","source":"Lalith Suresh, Joao Loff, Faria Kalim, Nina Narodytska, Leonid Ryzhyk,\n  Sahan Gamage, Brian Oki, Zeeshan Lokhandwala, Mukesh Hira, Mooly Sagiv","docs_id":"1909.03130","section":["cs.DC"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Automating Cluster Management with Weave. Modern cluster management systems like Kubernetes and Openstack grapple with hard combinatorial optimization problems: load balancing, placement, scheduling, and configuration. Currently, developers tackle these problems by designing custom application-specific algorithms---an approach that is proving unsustainable, as ad-hoc solutions both perform poorly and introduce overwhelming complexity to the system, making it challenging to add important new features. We propose a radically different architecture, where programmers drive cluster management tasks declaratively, using SQL queries over cluster state stored in a relational database. These queries capture in a natural way both constraints on the cluster configuration as well as optimization objectives. When a cluster reconfiguration is required at runtime, our tool, called Weave, synthesizes an encoding of these queries into an optimization model, which it solves using an off-the-shelf solver. We demonstrate Weave's efficacy by powering three production-grade systems with it: a Kubernetes scheduler, a virtual machine management solution, and a distributed transactional datastore. Using Weave, we expressed complex cluster management policies in under 20 lines of SQL, easily added new features to these existing systems, and significantly improved placement quality and convergence times."},{"title":"Translational Equivariance in Kernelizable Attention","source":"Max Horn, Kumar Shridhar, Elrich Groenewald, Philipp F. M. Baumann","docs_id":"2102.07680","section":["cs.LG","cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Translational Equivariance in Kernelizable Attention. While Transformer architectures have show remarkable success, they are bound to the computation of all pairwise interactions of input element and thus suffer from limited scalability. Recent work has been successful by avoiding the computation of the complete attention matrix, yet leads to problems down the line. The absence of an explicit attention matrix makes the inclusion of inductive biases relying on relative interactions between elements more challenging. An extremely powerful inductive bias is translational equivariance, which has been conjectured to be responsible for much of the success of Convolutional Neural Networks on image recognition tasks. In this work we show how translational equivariance can be implemented in efficient Transformers based on kernelizable attention - Performers. Our experiments highlight that the devised approach significantly improves robustness of Performers to shifts of input images compared to their naive application. This represents an important step on the path of replacing Convolutional Neural Networks with more expressive Transformer architectures and will help to improve sample efficiency and robustness in this realm."},{"title":"Improving Electron Micrograph Signal-to-Noise with an Atrous\n  Convolutional Encoder-Decoder","source":"Jeffrey M. Ede","docs_id":"1807.11234","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Improving Electron Micrograph Signal-to-Noise with an Atrous\n  Convolutional Encoder-Decoder. We present an atrous convolutional encoder-decoder trained to denoise 512$\\times$512 crops from electron micrographs. It consists of a modified Xception backbone, atrous convoltional spatial pyramid pooling module and a multi-stage decoder. Our neural network was trained end-to-end to remove Poisson noise applied to low-dose ($\\ll$ 300 counts ppx) micrographs created from a new dataset of 17267 2048$\\times$2048 high-dose ($>$ 2500 counts ppx) micrographs and then fine-tuned for ordinary doses (200-2500 counts ppx). Its performance is benchmarked against bilateral, non-local means, total variation, wavelet, Wiener and other restoration methods with their default parameters. Our network outperforms their best mean squared error and structural similarity index performances by 24.6% and 9.6% for low doses and by 43.7% and 5.5% for ordinary doses. In both cases, our network's mean squared error has the lowest variance. Source code and links to our new high-quality dataset and trained network have been made publicly available at https:\/\/github.com\/Jeffrey-Ede\/Electron-Micrograph-Denoiser"},{"title":"Federated Learning with Heterogeneous Labels and Models for Mobile\n  Activity Monitoring","source":"Gautham Krishna Gudur, Satheesh K. Perepu","docs_id":"2012.02539","section":["cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Federated Learning with Heterogeneous Labels and Models for Mobile\n  Activity Monitoring. Various health-care applications such as assisted living, fall detection, etc., require modeling of user behavior through Human Activity Recognition (HAR). Such applications demand characterization of insights from multiple resource-constrained user devices using machine learning techniques for effective personalized activity monitoring. On-device Federated Learning proves to be an effective approach for distributed and collaborative machine learning. However, there are a variety of challenges in addressing statistical (non-IID data) and model heterogeneities across users. In addition, in this paper, we explore a new challenge of interest -- to handle heterogeneities in labels (activities) across users during federated learning. To this end, we propose a framework for federated label-based aggregation, which leverages overlapping information gain across activities using Model Distillation Update. We also propose that federated transfer of model scores is sufficient rather than model weight transfer from device to server. Empirical evaluation with the Heterogeneity Human Activity Recognition (HHAR) dataset (with four activities for effective elucidation of results) on Raspberry Pi 2 indicates an average deterministic accuracy increase of at least ~11.01%, thus demonstrating the on-device capabilities of our proposed framework."},{"title":"Divide & Concur and Difference-Map BP Decoders for LDPC Codes","source":"Jonathan S. Yedidia, Yige Wang, and Stark C. Draper","docs_id":"1001.1730","section":["cs.IT","cs.DS","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Divide & Concur and Difference-Map BP Decoders for LDPC Codes. The \"Divide and Concur'' (DC) algorithm, recently introduced by Gravel and Elser, can be considered a competitor to the belief propagation (BP) algorithm, in that both algorithms can be applied to a wide variety of constraint satisfaction, optimization, and probabilistic inference problems. We show that DC can be interpreted as a message-passing algorithm on a constraint graph, which helps make the comparison with BP more clear. The \"difference-map'' dynamics of the DC algorithm enables it to avoid \"traps'' which may be related to the \"trapping sets'' or \"pseudo-codewords'' that plague BP decoders of low-density parity check (LDPC) codes in the error-floor regime. We investigate two decoders for low-density parity-check (LDPC) codes based on these ideas. The first decoder is based directly on DC, while the second decoder borrows the important \"difference-map'' concept from the DC algorithm and translates it into a BP-like decoder. We show that this \"difference-map belief propagation'' (DMBP) decoder has dramatically improved error-floor performance compared to standard BP decoders, while maintaining a similar computational complexity. We present simulation results for LDPC codes on the additive white Gaussian noise and binary symmetric channels, comparing DC and DMBP decoders with other decoders based on BP, linear programming, and mixed-integer linear programming."},{"title":"Differentially Private Inference for Binomial Data","source":"Jordan Awan and Aleksandra Slavkovic","docs_id":"1904.00459","section":["math.ST","cs.CR","stat.TH"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Differentially Private Inference for Binomial Data. We derive uniformly most powerful (UMP) tests for simple and one-sided hypotheses for a population proportion within the framework of Differential Privacy (DP), optimizing finite sample performance. We show that in general, DP hypothesis tests can be written in terms of linear constraints, and for exchangeable data can always be expressed as a function of the empirical distribution. Using this structure, we prove a 'Neyman-Pearson lemma' for binomial data under DP, where the DP-UMP only depends on the sample sum. Our tests can also be stated as a post-processing of a random variable, whose distribution we coin ''Truncated-Uniform-Laplace'' (Tulap), a generalization of the Staircase and discrete Laplace distributions. Furthermore, we obtain exact $p$-values, which are easily computed in terms of the Tulap random variable. Using the above techniques, we show that our tests can be applied to give uniformly most accurate one-sided confidence intervals and optimal confidence distributions. We also derive uniformly most powerful unbiased (UMPU) two-sided tests, which lead to uniformly most accurate unbiased (UMAU) two-sided confidence intervals. We show that our results can be applied to distribution-free hypothesis tests for continuous data. Our simulation results demonstrate that all our tests have exact type I error, and are more powerful than current techniques."},{"title":"ST++: Make Self-training Work Better for Semi-supervised Semantic\n  Segmentation","source":"Lihe Yang, Wei Zhuo, Lei Qi, Yinghuan Shi, Yang Gao","docs_id":"2106.05095","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"ST++: Make Self-training Work Better for Semi-supervised Semantic\n  Segmentation. In this paper, we investigate if we could make the self-training -- a simple but popular framework -- work better for semi-supervised segmentation. Since the core issue in semi-supervised setting lies in effective and efficient utilization of unlabeled data, we notice that increasing the diversity and hardness of unlabeled data is crucial to performance improvement. Being aware of this fact, we propose to adopt the most plain self-training scheme coupled with appropriate strong data augmentations on unlabeled data (namely ST) for this task, which surprisingly outperforms previous methods under various settings without any bells and whistles. Moreover, to alleviate the negative impact of the wrongly pseudo labeled images, we further propose an advanced self-training framework (namely ST++), that performs selective re-training via selecting and prioritizing the more reliable unlabeled images. As a result, the proposed ST++ boosts the performance of semi-supervised model significantly and surpasses existing methods by a large margin on the Pascal VOC 2012 and Cityscapes benchmark. Overall, we hope this straightforward and simple framework will serve as a strong baseline or competitor for future works. Code is available at https:\/\/github.com\/LiheYoung\/ST-PlusPlus."},{"title":"Estimation of Bandlimited Signals in Additive Gaussian Noise: a\n  \"Precision Indifference\" Principle","source":"Animesh Kumar and Vinod M. Prabhakaran","docs_id":"1211.6598","section":["cs.IT","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Estimation of Bandlimited Signals in Additive Gaussian Noise: a\n  \"Precision Indifference\" Principle. The sampling, quantization, and estimation of a bounded dynamic-range bandlimited signal affected by additive independent Gaussian noise is studied in this work. For bandlimited signals, the distortion due to additive independent Gaussian noise can be reduced by oversampling (statistical diversity). The pointwise expected mean-squared error is used as a distortion metric for signal estimate in this work. Two extreme scenarios of quantizer precision are considered: (i) infinite precision (real scalars); and (ii) one-bit quantization (sign information). If $N$ is the oversampling ratio with respect to the Nyquist rate, then the optimal law for distortion is $O(1\/N)$. We show that a distortion of $O(1\/N)$ can be achieved irrespective of the quantizer precision by considering the above-mentioned two extreme scenarios of quantization. Thus, a quantization precision indifference principle is discovered, where the reconstruction distortion law, up to a proportionality constant, is unaffected by quantizer's accuracy."},{"title":"Near-field Perception for Low-Speed Vehicle Automation using\n  Surround-view Fisheye Cameras","source":"Ciaran Eising, Jonathan Horgan and Senthil Yogamani","docs_id":"2103.17001","section":["cs.CV","cs.RO"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Near-field Perception for Low-Speed Vehicle Automation using\n  Surround-view Fisheye Cameras. Cameras are the primary sensor in automated driving systems. They provide high information density and are optimal for detecting road infrastructure cues laid out for human vision. Surround-view camera systems typically comprise of four fisheye cameras with 190{\\deg}+ field of view covering the entire 360{\\deg} around the vehicle focused on near-field sensing. They are the principal sensors for low-speed, high accuracy, and close-range sensing applications, such as automated parking, traffic jam assistance, and low-speed emergency braking. In this work, we provide a detailed survey of such vision systems, setting up the survey in the context of an architecture that can be decomposed into four modular components namely Recognition, Reconstruction, Relocalization, and Reorganization. We jointly call this the 4R Architecture. We discuss how each component accomplishes a specific aspect and provide a positional argument that they can be synergized to form a complete perception system for low-speed automation. We support this argument by presenting results from previous works and by presenting architecture proposals for such a system. Qualitative results are presented in the video at https:\/\/youtu.be\/ae8bCOF77uY."},{"title":"Reinforced Deep Markov Models With Applications in Automatic Trading","source":"Tadeu A. Ferreira","docs_id":"2011.04391","section":["q-fin.TR","cs.LG","stat.AP","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Reinforced Deep Markov Models With Applications in Automatic Trading. Inspired by the developments in deep generative models, we propose a model-based RL approach, coined Reinforced Deep Markov Model (RDMM), designed to integrate desirable properties of a reinforcement learning algorithm acting as an automatic trading system. The network architecture allows for the possibility that market dynamics are partially visible and are potentially modified by the agent's actions. The RDMM filters incomplete and noisy data, to create better-behaved input data for RL planning. The policy search optimisation also properly accounts for state uncertainty. Due to the complexity of the RKDF model architecture, we performed ablation studies to understand the contributions of individual components of the approach better. To test the financial performance of the RDMM we implement policies using variants of Q-Learning, DynaQ-ARIMA and DynaQ-LSTM algorithms. The experiments show that the RDMM is data-efficient and provides financial gains compared to the benchmarks in the optimal execution problem. The performance improvement becomes more pronounced when price dynamics are more complex, and this has been demonstrated using real data sets from the limit order book of Facebook, Intel, Vodafone and Microsoft."},{"title":"Leveraging blur information for plenoptic camera calibration","source":"Mathieu Labussi\\`ere, C\\'eline Teuli\\`ere, Fr\\'ed\\'eric Bernardin,\n  Omar Ait-Aider","docs_id":"2111.05226","section":["eess.IV","cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Leveraging blur information for plenoptic camera calibration. This paper presents a novel calibration algorithm for plenoptic cameras, especially the multi-focus configuration, where several types of micro-lenses are used, using raw images only. Current calibration methods rely on simplified projection models, use features from reconstructed images, or require separated calibrations for each type of micro-lens. In the multi-focus configuration, the same part of a scene will demonstrate different amounts of blur according to the micro-lens focal length. Usually, only micro-images with the smallest amount of blur are used. In order to exploit all available data, we propose to explicitly model the defocus blur in a new camera model with the help of our newly introduced Blur Aware Plenoptic (BAP) feature. First, it is used in a pre-calibration step that retrieves initial camera parameters, and second, to express a new cost function to be minimized in our single optimization process. Third, it is exploited to calibrate the relative blur between micro-images. It links the geometric blur, i.e., the blur circle, to the physical blur, i.e., the point spread function. Finally, we use the resulting blur profile to characterize the camera's depth of field. Quantitative evaluations in controlled environment on real-world data demonstrate the effectiveness of our calibrations."},{"title":"Channel Hardening in Massive MIMO: Model Parameters and Experimental\n  Assessment","source":"Sara Gunnarsson, Jos\\'e Flordelis, Liesbet Van der Perre, Fredrik\n  Tufvesson","docs_id":"2004.06772","section":["eess.SP","cs.IT","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Channel Hardening in Massive MIMO: Model Parameters and Experimental\n  Assessment. Reliability is becoming increasingly important for many applications envisioned for future wireless systems. A technology that could improve reliability in these systems is massive MIMO (Multiple-Input Multiple-Output). One reason for this is a phenomenon called channel hardening, which means that as the number of antennas in the system increases, the variations of channel gain decrease in both the time- and frequency domain. Our analysis of channel hardening is based on a joint comparison of theory, measurements and simulations. Data from measurement campaigns including both indoor and outdoor scenarios, as well as cylindrical and planar base station arrays, are analyzed. The simulation analysis includes a comparison with the COST 2100 channel model with its massive MIMO extension. The conclusion is that the COST 2100 model is well suited to represent real scenarios, and provides a reasonable match to actual measurements up to the uncertainty of antenna patterns and user interaction. Also, the channel hardening effect in practical massive MIMO channels is less pronounced than in complex independent and identically distributed (i.i.d.) Gaussian channels, which are often considered in theoretical work."},{"title":"Mixed-Resolution Image Representation and Compression with Convolutional\n  Neural Networks","source":"Lijun Zhao, Huihui Bai, Feng Li, Anhong Wang and Yao Zhao","docs_id":"1802.01447","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Mixed-Resolution Image Representation and Compression with Convolutional\n  Neural Networks. In this paper, we propose an end-to-end mixed-resolution image compression framework with convolutional neural networks. Firstly, given one input image, feature description neural network (FDNN) is used to generate a new representation of this image, so that this image representation can be more efficiently compressed by standard codec, as compared to the input image. Furthermore, we use post-processing neural network (PPNN) to remove the coding artifacts caused by quantization of codec. Secondly, low-resolution image representation is adopted for high efficiency compression in terms of most of bit spent by image's structures under low bit-rate. However, more bits should be assigned to image details in the high-resolution, when most of structures have been kept after compression at the high bit-rate. This comes from a fact that the low-resolution image representation can't burden more information than high-resolution representation beyond a certain bit-rate. Finally, to resolve the problem of error back-propagation from the PPNN network to the FDNN network, we introduce to learn a virtual codec neural network to imitate two continuous procedures of standard compression and post-processing. The objective experimental results have demonstrated the proposed method has a large margin improvement, when comparing with several state-of-the-art approaches."},{"title":"Detecting Algebraic Manipulation in Leaky Storage Systems","source":"Fuchun Lin, Reihaneh Safavi-Naini, Pengwei Wang","docs_id":"1607.00089","section":["cs.IT","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Detecting Algebraic Manipulation in Leaky Storage Systems. Algebraic Manipulation Detection (AMD) Codes detect adversarial noise that is added to a coded message and stored in a storage that is opaque to the adversary. We study AMD codes when the storage can leak up to \\rho\\log|G| bits of information about the stored codeword, where G is the group in which the stored codeword lives and \\rho is a constant. We propose \\rho-AMD codes that provide protection in this new setting, and define weak and strong \\rho-AMD codes that provide security for a random and an arbitrary message, respectively. We derive concrete and asymptotic bounds for the efficiency of these codes featuring a rate upper bound of 1-\\rho for the strong codes. We also define the class of \\rho^{LV}-AMD codes that provide protection when leakage is in the form of a number of codeword components, and give constructions featuring a strong \\rho^{LV}-AMD codes that asymptotically achieve the rate 1-\\rho. We describe applications of \\rho-AMD codes to, (i) robust ramp secret sharing scheme and, (ii) wiretap II channel when the adversary can eavesdrop a \\rho fraction of codeword components and tamper with all components of the codeword."},{"title":"Dynamic Interaction of Transportation and Power Distribution Networks\n  With Electric Vehicles","source":"Li Jiaqi, Xu Xiaoyuan, Yan Zheng, Wang Han, Chen Yue","docs_id":"2112.04683","section":["eess.SY","cs.SY"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Dynamic Interaction of Transportation and Power Distribution Networks\n  With Electric Vehicles. The increasing global spread of electric vehicles has introduced significant interdependence between transportation and power networks. Most of the previous studies on the coupled networks are based on static models, and the spatial and temporal variations of traffic and power flows are neglected, which is not suitable for short-term operation. This paper constructs a dynamic interaction model of coupled networks. First, the dynamic traffic assignment (DTA) model is established considering departure time and route choices simultaneously, and a nested diagonalization method is exploited to solve it. Then, based on DTA and multi-period optimal power flow, the equilibrium state of coupled networks is designed as the solution of a fixed-point problem. Moreover, the solution existence is proved based on mild assumptions. Third, the linearization and convex relaxation techniques are used to improve computational efficiency. A Monte Carlo simulation technique is developed to evaluate the influence of uncertain travel demands on coupled networks. Numerical simulations of the interaction analyses of coupled networks in both deterministic and uncertain conditions are presented."},{"title":"Analysis of the Relationships among Longest Common Subsequences,\n  Shortest Common Supersequences and Patterns and its application on Pattern\n  Discovery in Biological Sequences","source":"Kang Ning, Hoong Kee Ng, Hon Wai Leong","docs_id":"0903.2310","section":["cs.DS","cs.DM","cs.IR","cs.OH","q-bio.QM"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Analysis of the Relationships among Longest Common Subsequences,\n  Shortest Common Supersequences and Patterns and its application on Pattern\n  Discovery in Biological Sequences. For a set of mulitple sequences, their patterns,Longest Common Subsequences (LCS) and Shortest Common Supersequences (SCS) represent different aspects of these sequences profile, and they can all be used for biological sequence comparisons and analysis. Revealing the relationship between the patterns and LCS,SCS might provide us with a deeper view of the patterns of biological sequences, in turn leading to better understanding of them. However, There is no careful examinaton about the relationship between patterns, LCS and SCS. In this paper, we have analyzed their relation, and given some lemmas. Based on their relations, a set of algorithms called the PALS (PAtterns by Lcs and Scs) algorithms are propsoed to discover patterns in a set of biological sequences. These algorithms first generate the results for LCS and SCS of sequences by heuristic, and consequently derive patterns from these results. Experiments show that the PALS algorithms perform well (both in efficiency and in accuracy) on a variety of sequences. The PALS approach also provides us with a solution for transforming between the heuristic results of SCS and LCS."},{"title":"Energy Efficient Cross Layer Time Synchronization in Cognitive Radio\n  Networks","source":"S.M. Usman Hashmi, Muntazir Hussain, S.M. Nashit Arshad, Kashif Inayat\n  and Seong Oun Hwang","docs_id":"2007.03841","section":["cs.NI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Energy Efficient Cross Layer Time Synchronization in Cognitive Radio\n  Networks. Time synchronization is a vital concern for any Cognitive Radio Network (CRN) to perform dynamic spectrum management. Each Cognitive Radio (CR) node has to be environment aware and self adaptive and must have the ability to switch between multiple modulation schemes and frequencies. Achieving same notion of time within these CR nodes is essential to fulfill the requirements for simultaneous quiet periods for spectrum sensing. Current application layer time synchronization protocols require multiple timestamp exchanges to estimate skew between the clocks of CRN nodes. The proposed symbol timing recovery method already estimates the skew of hardware clock at the physical layer and use it for skew correction of application layer clock of each node. The heart of application layer clock is the hardware clock and hence application layer clock skew will be same as of physical layer and can be corrected from symbol timing recovery process. So one timestamp is enough to synchronize two CRN nodes. This conserves the energy utilized by application layer protocol and makes a CRN energy efficient and can achieve time synchronization in short span."},{"title":"Asymmetric Action Abstractions for Multi-Unit Control in Adversarial\n  Real-Time Games","source":"Rubens O. Moraes and Levi H. S. Lelis","docs_id":"1711.08101","section":["cs.AI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Asymmetric Action Abstractions for Multi-Unit Control in Adversarial\n  Real-Time Games. Action abstractions restrict the number of legal actions available during search in multi-unit real-time adversarial games, thus allowing algorithms to focus their search on a set of promising actions. Optimal strategies derived from un-abstracted spaces are guaranteed to be no worse than optimal strategies derived from action-abstracted spaces. In practice, however, due to real-time constraints and the state space size, one is only able to derive good strategies in un-abstracted spaces in small-scale games. In this paper we introduce search algorithms that use an action abstraction scheme we call asymmetric abstraction. Asymmetric abstractions retain the un-abstracted spaces' theoretical advantage over regularly abstracted spaces while still allowing the search algorithms to derive effective strategies, even in large-scale games. Empirical results on combat scenarios that arise in a real-time strategy game show that our search algorithms are able to substantially outperform state-of-the-art approaches."},{"title":"Context-aware Goodness of Pronunciation for Computer-Assisted\n  Pronunciation Training","source":"Jiatong Shi, Nan Huo, Qin Jin","docs_id":"2008.08647","section":["eess.AS","cs.SD"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Context-aware Goodness of Pronunciation for Computer-Assisted\n  Pronunciation Training. Mispronunciation detection is an essential component of the Computer-Assisted Pronunciation Training (CAPT) systems. State-of-the-art mispronunciation detection models use Deep Neural Networks (DNN) for acoustic modeling, and a Goodness of Pronunciation (GOP) based algorithm for pronunciation scoring. However, GOP based scoring models have two major limitations: i.e., (i) They depend on forced alignment which splits the speech into phonetic segments and independently use them for scoring, which neglects the transitions between phonemes within the segment; (ii) They only focus on phonetic segments, which fails to consider the context effects across phonemes (such as liaison, omission, incomplete plosive sound, etc.). In this work, we propose the Context-aware Goodness of Pronunciation (CaGOP) scoring model. Particularly, two factors namely the transition factor and the duration factor are injected into CaGOP scoring. The transition factor identifies the transitions between phonemes and applies them to weight the frame-wise GOP. Moreover, a self-attention based phonetic duration modeling is proposed to introduce the duration factor into the scoring model. The proposed scoring model significantly outperforms baselines, achieving 20% and 12% relative improvement over the GOP model on the phoneme-level and sentence-level mispronunciation detection respectively."},{"title":"A Vision-Guided Multi-Robot Cooperation Framework for\n  Learning-by-Demonstration and Task Reproduction","source":"Bidan Huang, Menglong Ye, Su-Lin Lee, Guang-Zhong Yang","docs_id":"1706.00508","section":["cs.RO"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A Vision-Guided Multi-Robot Cooperation Framework for\n  Learning-by-Demonstration and Task Reproduction. This paper presents a vision-based learning-by-demonstration approach to enable robots to learn and complete a manipulation task cooperatively. With this method, a vision system is involved in both the task demonstration and reproduction stages. An expert first demonstrates how to use tools to perform a task, while the tool motion is observed using a vision system. The demonstrations are then encoded using a statistical model to generate a reference motion trajectory. Equipped with the same tools and the learned model, the robot is guided by vision to reproduce the task. The task performance was evaluated in terms of both accuracy and speed. However, simply increasing the robot's speed could decrease the reproduction accuracy. To this end, a dual-rate Kalman filter is employed to compensate for latency between the robot and vision system. More importantly, the sampling rates of the reference trajectory and the robot speed are optimised adaptively according to the learned motion model. We demonstrate the effectiveness of our approach by performing two tasks: a trajectory reproduction task and a bimanual sewing task. We show that using our vision-based approach, the robots can conduct effective learning by demonstrations and perform accurate and fast task reproduction. The proposed approach is generalisable to other manipulation tasks, where bimanual or multi-robot cooperation is required."},{"title":"A Two-stage Unsupervised Approach for Low light Image Enhancement","source":"Junjie Hu, Xiyue Guo, Junfeng Chen, Guanqi Liang, Fuqin Deng and Tin\n  lun Lam","docs_id":"2010.09316","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A Two-stage Unsupervised Approach for Low light Image Enhancement. As vision based perception methods are usually built on the normal light assumption, there will be a serious safety issue when deploying them into low light environments. Recently, deep learning based methods have been proposed to enhance low light images by penalizing the pixel-wise loss of low light and normal light images. However, most of them suffer from the following problems: 1) the need of pairs of low light and normal light images for training, 2) the poor performance for dark images, 3) the amplification of noise. To alleviate these problems, in this paper, we propose a two-stage unsupervised method that decomposes the low light image enhancement into a pre-enhancement and a post-refinement problem. In the first stage, we pre-enhance a low light image with a conventional Retinex based method. In the second stage, we use a refinement network learned with adversarial training for further improvement of the image quality. The experimental results show that our method outperforms previous methods on four benchmark datasets. In addition, we show that our method can significantly improve feature points matching and simultaneous localization and mapping in low light conditions."},{"title":"Learning in an Uncertain World: Representing Ambiguity Through Multiple\n  Hypotheses","source":"Christian Rupprecht, Iro Laina, Robert DiPietro, Maximilian Baust,\n  Federico Tombari, Nassir Navab, Gregory D. Hager","docs_id":"1612.00197","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Learning in an Uncertain World: Representing Ambiguity Through Multiple\n  Hypotheses. Many prediction tasks contain uncertainty. In some cases, uncertainty is inherent in the task itself. In future prediction, for example, many distinct outcomes are equally valid. In other cases, uncertainty arises from the way data is labeled. For example, in object detection, many objects of interest often go unlabeled, and in human pose estimation, occluded joints are often labeled with ambiguous values. In this work we focus on a principled approach for handling such scenarios. In particular, we propose a framework for reformulating existing single-prediction models as multiple hypothesis prediction (MHP) models and an associated meta loss and optimization procedure to train them. To demonstrate our approach, we consider four diverse applications: human pose estimation, future prediction, image classification and segmentation. We find that MHP models outperform their single-hypothesis counterparts in all cases, and that MHP models simultaneously expose valuable insights into the variability of predictions."},{"title":"Learning Hidden Markov Models with Geometrical Constraints","source":"Hagit Shatkay","docs_id":"1301.6740","section":["cs.AI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Learning Hidden Markov Models with Geometrical Constraints. Hidden Markov models (HMMs) and partially observable Markov decision processes (POMDPs) form a useful tool for modeling dynamical systems. They are particularly useful for representing environments such as road networks and office buildings, which are typical for robot navigation and planning. The work presented here is concerned with acquiring such models. We demonstrate how domain-specific information and constraints can be incorporated into the statistical estimation process, greatly improving the learned models in terms of the model quality, the number of iterations required for convergence and robustness to reduction in the amount of available data. We present new initialization heuristics which can be used even when the data suffers from cumulative rotational error, new update rules for the model parameters, as an instance of generalized EM, and a strategy for enforcing complete geometrical consistency in the model. Experimental results demonstrate the effectiveness of our approach for both simulated and real robot data, in traditionally hard-to-learn environments."},{"title":"Infinite Shift-invariant Grouped Multi-task Learning for Gaussian\n  Processes","source":"Yuyang Wang, Roni Khardon, Pavlos Protopapas","docs_id":"1203.0970","section":["cs.LG","astro-ph.IM","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Infinite Shift-invariant Grouped Multi-task Learning for Gaussian\n  Processes. Multi-task learning leverages shared information among data sets to improve the learning performance of individual tasks. The paper applies this framework for data where each task is a phase-shifted periodic time series. In particular, we develop a novel Bayesian nonparametric model capturing a mixture of Gaussian processes where each task is a sum of a group-specific function and a component capturing individual variation, in addition to each task being phase shifted. We develop an efficient \\textsc{em} algorithm to learn the parameters of the model. As a special case we obtain the Gaussian mixture model and \\textsc{em} algorithm for phased-shifted periodic time series. Furthermore, we extend the proposed model by using a Dirichlet Process prior and thereby leading to an infinite mixture model that is capable of doing automatic model selection. A Variational Bayesian approach is developed for inference in this model. Experiments in regression, classification and class discovery demonstrate the performance of the proposed models using both synthetic data and real-world time series data from astrophysics. Our methods are particularly useful when the time series are sparsely and non-synchronously sampled."},{"title":"Trusted Authentication using hybrid security algorithm in VANET","source":"Prasanna Venkatesan E, Kristen Titus W","docs_id":"2105.06105","section":["cs.CR","cs.NI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Trusted Authentication using hybrid security algorithm in VANET. Vehicular Ad Hoc Networks (VANETs) improves traffic management and reduce the amount of road accidents by providing safety applications. However, VANETs are vulnerable to variety of security attacks from malicious entities. An authentication is an integral a neighborhood of trust establishment and secure communications between vehicles. The Road-side Unit (RSU) evaluates trust-value and the Agent Trusted Authority (ATA) helps in computing the trust-value of auto supported its reward-points. The communication between nodes is enhanced, this can reduce 50% of road accidents. The security of the VANET is improved. We propose the utilization of Elliptic Curve Cryptography in the design of an efficient data encryption\/decryption system for sensor nodes in a wireless network. Elliptic Curve Cryptography can provide impressive levels of security standards while keeping down the cost of certain issues, primarily storage space. Sensors will benefit from having to store relatively smaller keys coupled with increased computational capability and this will be a stronger design as the bit-level security is improved. Thus, reducing the time delay between the nodes and to provide better results between them we have made use of this method. The implementation of this work is done with NS2 software."},{"title":"Adaptive control of a mechatronic system using constrained residual\n  reinforcement learning","source":"Tom Staessens, Tom Lefebvre and Guillaume Crevecoeur","docs_id":"2110.02566","section":["eess.SY","cs.LG","cs.SY"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Adaptive control of a mechatronic system using constrained residual\n  reinforcement learning. We propose a simple, practical and intuitive approach to improve the performance of a conventional controller in uncertain environments using deep reinforcement learning while maintaining safe operation. Our approach is motivated by the observation that conventional controllers in industrial motion control value robustness over adaptivity to deal with different operating conditions and are suboptimal as a consequence. Reinforcement learning on the other hand can optimize a control signal directly from input-output data and thus adapt to operational conditions, but lacks safety guarantees, impeding its use in industrial environments. To realize adaptive control using reinforcement learning in such conditions, we follow a residual learning methodology, where a reinforcement learning algorithm learns corrective adaptations to a base controller's output to increase optimality. We investigate how constraining the residual agent's actions enables to leverage the base controller's robustness to guarantee safe operation. We detail the algorithmic design and propose to constrain the residual actions relative to the base controller to increase the method's robustness. Building on Lyapunov stability theory, we prove stability for a broad class of mechatronic closed-loop systems. We validate our method experimentally on a slider-crank setup and investigate how the constraints affect the safety during learning and optimality after convergence."},{"title":"Tensor Relational Algebra for Machine Learning System Design","source":"Binhang Yuan and Dimitrije Jankov and Jia Zou and Yuxin Tang and\n  Daniel Bourgeois and Chris Jermaine","docs_id":"2009.00524","section":["cs.DB","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Tensor Relational Algebra for Machine Learning System Design. We consider the question: what is the abstraction that should be implemented by the computational engine of a machine learning system? Current machine learning systems typically push whole tensors through a series of compute kernels such as matrix multiplications or activation functions, where each kernel runs on an AI accelerator (ASIC) such as a GPU. This implementation abstraction provides little built-in support for ML systems to scale past a single machine, or for handling large models with matrices or tensors that do not easily fit into the RAM of an ASIC. In this paper, we present an alternative implementation abstraction called the tensor relational algebra (TRA). The TRA is a set-based algebra based on the relational algebra. Expressions in the TRA operate over binary tensor relations, where keys are multi-dimensional arrays and values are tensors. The TRA is easily executed with high efficiency in a parallel or distributed environment, and amenable to automatic optimization. Our empirical study shows that the optimized TRA-based back-end can significantly outperform alternatives for running ML workflows in distributed clusters."},{"title":"Approximation algorithms for maximally balanced connected graph\n  partition","source":"Yong Chen, Zhi-Zhong Chen, Guohui Lin, Yao Xu, An Zhang","docs_id":"1910.02470","section":["cs.DS","cs.DM"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Approximation algorithms for maximally balanced connected graph\n  partition. Given a simple connected graph $G = (V, E)$, we seek to partition the vertex set $V$ into $k$ non-empty parts such that the subgraph induced by each part is connected, and the partition is maximally balanced in the way that the maximum cardinality of these $k$ parts is minimized. We refer this problem to as {\\em min-max balanced connected graph partition} into $k$ parts and denote it as {\\sc $k$-BGP}. The general vertex-weighted version of this problem on trees has been studied since about four decades ago, which admits a linear time exact algorithm; the vertex-weighted {\\sc $2$-BGP} and {\\sc $3$-BGP} admit a $5\/4$-approximation and a $3\/2$-approximation, respectively; but no approximability result exists for {\\sc $k$-BGP} when $k \\ge 4$, except a trivial $k$-approximation. In this paper, we present another $3\/2$-approximation for our cardinality {\\sc $3$-BGP} and then extend it to become a $k\/2$-approximation for {\\sc $k$-BGP}, for any constant $k \\ge 3$. Furthermore, for {\\sc $4$-BGP}, we propose an improved $24\/13$-approximation. To these purposes, we have designed several local improvement operations, which could be useful for related graph partition problems."},{"title":"A ground-truth dataset and classification model for detecting bots in\n  GitHub issue and PR comments","source":"Mehdi Golzadeh, Alexandre Decan, Damien Legay and Tom Mens","docs_id":"2010.03303","section":["cs.SE","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A ground-truth dataset and classification model for detecting bots in\n  GitHub issue and PR comments. Bots are frequently used in Github repositories to automate repetitive activities that are part of the distributed software development process. They communicate with human actors through comments. While detecting their presence is important for many reasons, no large and representative ground-truth dataset is available, nor are classification models to detect and validate bots on the basis of such a dataset. This paper proposes a ground-truth dataset, based on a manual analysis with high interrater agreement, of pull request and issue comments in 5,000 distinct Github accounts of which 527 have been identified as bots. Using this dataset we propose an automated classification model to detect bots, taking as main features the number of empty and non-empty comments of each account, the number of comment patterns, and the inequality between comments within comment patterns. We obtained a very high weighted average precision, recall and F1-score of 0.98 on a test set containing 40% of the data. We integrated the classification model into an open source command-line tool to allow practitioners to detect which accounts in a given Github repository actually correspond to bots."},{"title":"Predicting Goal-directed Attention Control Using Inverse-Reinforcement\n  Learning","source":"Gregory J. Zelinsky, Yupei Chen, Seoyoung Ahn, Hossein Adeli, Zhibo\n  Yang, Lihan Huang, Dimitrios Samaras, Minh Hoai","docs_id":"2001.11921","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Predicting Goal-directed Attention Control Using Inverse-Reinforcement\n  Learning. Understanding how goal states control behavior is a question ripe for interrogation by new methods from machine learning. These methods require large and labeled datasets to train models. To annotate a large-scale image dataset with observed search fixations, we collected 16,184 fixations from people searching for either microwaves or clocks in a dataset of 4,366 images (MS-COCO). We then used this behaviorally-annotated dataset and the machine learning method of Inverse-Reinforcement Learning (IRL) to learn target-specific reward functions and policies for these two target goals. Finally, we used these learned policies to predict the fixations of 60 new behavioral searchers (clock = 30, microwave = 30) in a disjoint test dataset of kitchen scenes depicting both a microwave and a clock (thus controlling for differences in low-level image contrast). We found that the IRL model predicted behavioral search efficiency and fixation-density maps using multiple metrics. Moreover, reward maps from the IRL model revealed target-specific patterns that suggest, not just attention guidance by target features, but also guidance by scene context (e.g., fixations along walls in the search of clocks). Using machine learning and the psychologically-meaningful principle of reward, it is possible to learn the visual features used in goal-directed attention control."},{"title":"Pretraining boosts out-of-domain robustness for pose estimation","source":"Alexander Mathis, Thomas Biasi, Steffen Schneider, Mert\n  Y\\\"uksekg\\\"on\\\"ul, Byron Rogers, Matthias Bethge, Mackenzie W. Mathis","docs_id":"1909.11229","section":["cs.CV","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Pretraining boosts out-of-domain robustness for pose estimation. Neural networks are highly effective tools for pose estimation. However, as in other computer vision tasks, robustness to out-of-domain data remains a challenge, especially for small training sets that are common for real-world applications. Here, we probe the generalization ability with three architecture classes (MobileNetV2s, ResNets, and EfficientNets) for pose estimation. We developed a dataset of 30 horses that allowed for both \"within-domain\" and \"out-of-domain\" (unseen horse) benchmarking - this is a crucial test for robustness that current human pose estimation benchmarks do not directly address. We show that better ImageNet-performing architectures perform better on both within- and out-of-domain data if they are first pretrained on ImageNet. We additionally show that better ImageNet models generalize better across animal species. Furthermore, we introduce Horse-C, a new benchmark for common corruptions for pose estimation, and confirm that pretraining increases performance in this domain shift context as well. Overall, our results demonstrate that transfer learning is beneficial for out-of-domain robustness."},{"title":"Elicitation of SME Requirements for Cybersecurity Solutions by Studying\n  Adherence to Recommendations","source":"Alireza Shojaifar, Samuel A. Fricker, Martin Gwerder","docs_id":"2007.08177","section":["cs.CY"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Elicitation of SME Requirements for Cybersecurity Solutions by Studying\n  Adherence to Recommendations. Small and medium-sized enterprises (SME) have become the weak spot of our economy for cyber attacks. These companies are large in number and often do not have the controls in place to prevent successful attacks, respectively are not prepared to systematically manage their cybersecurity capabilities. One of the reasons for why many SME do not adopt cybersecurity is that developers of cybersecurity solutions understand little the SME context and the requirements for successful use of these solutions. We elicit requirements by studying how cybersecurity experts provide advice to SME. The experts recommendations offer insights into what important capabilities of the solution are and how these capabilities ought to be used for mitigating cybersecurity threats. The adoption of a recommendation hints at a correct match of the solution, hence successful consideration of requirements. Abandoned recommendations point to a misalignment that can be used as a source to inquire missed requirements. Re-occurrence of adoption or abandonment decisions corroborate the presence of requirements. This poster describes the challenges of SME regarding cybersecurity and introduces our proposed approach to elicit requirements for cybersecurity solutions. The poster describes CYSEC, our tool used to capture cybersecurity advice and help to scale cybersecurity requirements elicitation to a large number of participating SME. We conclude by outlining the planned research to develop and validate CYSEC."},{"title":"RoboChain: A Secure Data-Sharing Framework for Human-Robot Interaction","source":"Eduardo Castell\\'o Ferrer, Ognjen Rudovic, Thomas Hardjono, Alex\n  Pentland","docs_id":"1802.04480","section":["cs.RO","cs.HC"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"RoboChain: A Secure Data-Sharing Framework for Human-Robot Interaction. Robots have potential to revolutionize the way we interact with the world around us. One of their largest potentials is in the domain of mobile health where they can be used to facilitate clinical interventions. However, to accomplish this, robots need to have access to our private data in order to learn from these data and improve their interaction capabilities. Furthermore, to enhance this learning process, the knowledge sharing among multiple robot units is the natural step forward. However, to date, there is no well-established framework which allows for such data sharing while preserving the privacy of the users (e.g., the hospital patients). To this end, we introduce RoboChain - the first learning framework for secure, decentralized and computationally efficient data and model sharing among multiple robot units installed at multiple sites (e.g., hospitals). RoboChain builds upon and combines the latest advances in open data access and blockchain technologies, as well as machine learning. We illustrate this framework using the example of a clinical intervention conducted in a private network of hospitals. Specifically, we lay down the system architecture that allows multiple robot units, conducting the interventions at different hospitals, to perform efficient learning without compromising the data privacy."},{"title":"Balancing transparency, efficiency and security in pervasive systems","source":"Mark Wenstrom, Eloisa Bentivegna and Ali Hurson (Pennsylvania State\n  University)","docs_id":"0801.3102","section":["cs.HC","cs.IR"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Balancing transparency, efficiency and security in pervasive systems. This chapter will survey pervasive computing with a look at how its constraint for transparency affects issues of resource management and security. The goal of pervasive computing is to render computing transparent, such that computing resources are ubiquitously offered to the user and services are proactively performed for a user without his or her intervention. The task of integrating computing infrastructure into everyday life without making it excessively invasive brings about tradeoffs between flexibility and robustness, efficiency and effectiveness, as well as autonomy and reliability. As the feasibility of ubiquitous computing and its real potential for mass applications are still a matter of controversy, this chapter will look into the underlying issues of resource management and authentication to discover how these can be handled in a least invasive fashion. The discussion will be closed by an overview of the solutions proposed by current pervasive computing efforts, both in the area of generic platforms and for dedicated applications such as pervasive education and healthcare."},{"title":"An Information-Theoretic Framework for Identifying Age-Related Genes\n  Using Human Dermal Fibroblast Transcriptome Data","source":"Salman Mohamadi, Donald Adjeroh","docs_id":"2111.02595","section":["q-bio.GN","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"An Information-Theoretic Framework for Identifying Age-Related Genes\n  Using Human Dermal Fibroblast Transcriptome Data. Investigation of age-related genes is of great importance for multiple purposes, for instance, improving our understanding of the mechanism of ageing, increasing life expectancy, age prediction, and other healthcare applications. In his work, starting with a set of 27,142 genes, we develop an information-theoretic framework for identifying genes that are associated with aging by applying unsupervised and semi-supervised learning techniques on human dermal fibroblast gene expression data. First, we use unsupervised learning and apply information-theoretic measures to identify key features for effective representation of gene expression values in the transcriptome data. Using the identified features, we perform clustering on the data. Finally, we apply semi-supervised learning on the clusters using different distance measures to identify novel genes that are potentially associated with aging. Performance assessment for both unsupervised and semi-supervised methods show the effectiveness of the framework."},{"title":"Theoretical links between universal and Bayesian compressed sensing\n  algorithms","source":"Shirin Jalali","docs_id":"1801.01069","section":["cs.IT","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Theoretical links between universal and Bayesian compressed sensing\n  algorithms. Quantized maximum a posteriori (Q-MAP) is a recently-proposed Bayesian compressed sensing algorithm that, given the source distribution, recovers $X^n$ from its linear measurements $Y^m=AX^n$, where $A\\in R^{m\\times n}$ denotes the known measurement matrix. On the other hand, Lagrangian minimum entropy pursuit (L-MEP) is a universal compressed sensing algorithm that aims at recovering $X^n$ from its linear measurements $Y^m=AX^n$, without having access to the source distribution. Both Q-MAP and L-MEP provably achieve the minimum required sampling rates, in noiseless cases where such fundamental limits are known. L-MEP is based on minimizing a cost function that consists of a linear combination of the conditional empirical entropy of a potential reconstruction vector and its corresponding measurement error. In this paper, using a first-order linear approximation of the conditional empirical entropy function, L-MEP is connected with Q-MAP. The established connection between L-MEP and Q-MAP leads to variants of Q-MAP which have the same asymptotic performance as Q-MAP in terms of their required sampling rates. Moreover, these variants suggest that Q-MAP is robust to small error in estimating the source distribution. This robustness is theoretically proven and the effect of a non-vanishing estimation error on the required sampling rate is characterized."},{"title":"A generalized optimal fourth-order finite difference scheme for a 2D\n  Helmholtz equation with the perfectly matched layer boundary condition","source":"Hatef Dastour and Wenyuan Liao","docs_id":"1908.07403","section":["math.NA","cs.NA"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A generalized optimal fourth-order finite difference scheme for a 2D\n  Helmholtz equation with the perfectly matched layer boundary condition. A crucial part of successful wave propagation related inverse problems is an efficient and accurate numerical scheme for solving the seismic wave equations. In particular, the numerical solution to a multi-dimensional Helmholtz equation can be troublesome when the perfectly matched layer (PML) boundary condition is implemented. In this paper, we present a general approach for constructing fourth-order finite difference schemes for the Helmholtz equation with PML in the two-dimensional domain based on point-weighting strategy. Particularly, we develop two optimal fourth-order finite difference schemes, optimal point-weighting 25p and optimal point-weighting 17p. It is shown that the two schemes are consistent with the Helmholtz equation with PML. Moreover, an error analysis for the numerical approximation of the exact wavenumber is provided. Based on minimizing the numerical dispersion, we implement the refined choice strategy for selecting optimal parameters and present refined point-weighting 25p and refined point-weighting 17p finite difference schemes. Furthermore, three numerical examples are provided to illustrate the accuracy and effectiveness of the new methods in reducing numerical dispersion."},{"title":"Harnessing GANs for Zero-shot Learning of New Classes in Visual Speech\n  Recognition","source":"Yaman Kumar, Dhruva Sahrawat, Shubham Maheshwari, Debanjan Mahata,\n  Amanda Stent, Yifang Yin, Rajiv Ratn Shah, Roger Zimmermann","docs_id":"1901.10139","section":["cs.LG","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Harnessing GANs for Zero-shot Learning of New Classes in Visual Speech\n  Recognition. Visual Speech Recognition (VSR) is the process of recognizing or interpreting speech by watching the lip movements of the speaker. Recent machine learning based approaches model VSR as a classification problem; however, the scarcity of training data leads to error-prone systems with very low accuracies in predicting unseen classes. To solve this problem, we present a novel approach to zero-shot learning by generating new classes using Generative Adversarial Networks (GANs), and show how the addition of unseen class samples increases the accuracy of a VSR system by a significant margin of 27% and allows it to handle speaker-independent out-of-vocabulary phrases. We also show that our models are language agnostic and therefore capable of seamlessly generating, using English training data, videos for a new language (Hindi). To the best of our knowledge, this is the first work to show empirical evidence of the use of GANs for generating training samples of unseen classes in the domain of VSR, hence facilitating zero-shot learning. We make the added videos for new classes publicly available along with our code."},{"title":"Application of Bernoulli Polynomials for Solving Variable-Order\n  Fractional Optimal Control-Affine Problems","source":"Somayeh Nemati, Delfim F. M. Torres","docs_id":"2010.02833","section":["math.OC","cs.NA","math.NA"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Application of Bernoulli Polynomials for Solving Variable-Order\n  Fractional Optimal Control-Affine Problems. We propose two efficient numerical approaches for solving variable-order fractional optimal control-affine problems. The variable-order fractional derivative is considered in the Caputo sense, which together with the Riemann-Liouville integral operator is used in our new techniques. An accurate operational matrix of variable-order fractional integration for Bernoulli polynomials is introduced. Our methods proceed as follows. First, a specific approximation of the differentiation order of the state function is considered, in terms of Bernoulli polynomials. Such approximation, together with the initial conditions, help us to obtain some approximations for the other existing functions in the dynamical control-affine system. Using these approximations, and the Gauss-Legendre integration formula, the problem is reduced to a system of nonlinear algebraic equations. Some error bounds are then given for the approximate optimal state and control functions, which allow us to obtain an error bound for the approximate value of the performance index. We end by solving some test problems, which demonstrate the high accuracy of our results."},{"title":"UAV-Enabled Communication Using NOMA","source":"Ali A. Nasir, Hoang D. Tuan, Trung Q. Duong and H. Vincent Poor","docs_id":"1806.03604","section":["cs.IT","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"UAV-Enabled Communication Using NOMA. Unmanned aerial vehicles (UAVs) can be deployed as flying base stations (BSs) to leverage the strength of line-of-sight connections and effectively support the coverage and throughput of wireless communication. This paper considers a multiuser communication system, in which a single-antenna UAV-BS serves a large number of ground users by employing non-orthogonal multiple access (NOMA). The max-min rate optimization problem is formulated under total power, total bandwidth, UAV altitude, and antenna beamwdith constraints. The objective of max-min rate optimization is non-convex in all optimization variables, i.e. UAV altitude, transmit antenna beamwidth, power allocation and bandwidth allocation for multiple users. A path-following algorithm is proposed to solve the formulated problem. Next, orthogonal multiple access (OMA) and dirty paper coding (DPC)-based max-min rate optimization problems are formulated and respective path-following algorithms are developed to solve them. Numerical results show that NOMA outperforms OMA and achieves rates similar to those attained by DPC. In addition, a clear rate gain is observed by jointly optimizing all the parameters rather than optimizing a subset of parameters, which confirms the desirability of their joint optimization."},{"title":"Conceptual Framework for Internet of Things' Virtualization via OpenFlow\n  in Context-aware Networks","source":"Theo Kanter, Rahim Rahmani, and Arif Mahmud","docs_id":"1401.7437","section":["cs.NI","cs.DC"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Conceptual Framework for Internet of Things' Virtualization via OpenFlow\n  in Context-aware Networks. A novel conceptual framework is presented in this paper with an aim to standardize and virtualize Internet of Things(IoT) infrastructure through deploying OpenFlow technology. The framework can receivee services based on context information leaving the current infrastructure unchanged. This framework allows the active collaboration of heterogeneous devices and protocols. Moreover it is capable to model placement of physical objects, manage the system and to collect information for services deployed on an IoT infrastructure. Our proposed IoT virtualization is applicable to a random topology scenario which makes it possible to 1) share flow sensors resources 2) establish multioperational sensor networks, and 3) extend reachability within the framework without establishing any further physical networks. Flow sensors achieve better results comparable to the typical sensors with respect to packet generation, reachability, simulation time, throughput, energy consumption point of view. Even better results are possible through utilizing multicast groups in large scale networks."},{"title":"Complexity of Detectability, Opacity and A-Diagnosability for Modular\n  Discrete Event Systems","source":"Tom\\'a\\v{s} Masopust and Xiang Yin","docs_id":"1710.02877","section":["cs.SY","cs.FL"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Complexity of Detectability, Opacity and A-Diagnosability for Modular\n  Discrete Event Systems. We study the complexity of deciding whether a modular discrete event system is detectable (resp. opaque, A-diagnosable). Detectability arises in the state estimation of discrete event systems, opacity is related to the privacy and security analysis, and A-diagnosability appears in the fault diagnosis of stochastic discrete event systems. Previously, deciding weak detectability (opacity, A-diagnosability) for monolithic systems was shown to be PSPACE-complete. In this paper, we study the complexity of deciding weak detectability (opacity, A-diagnosability) for modular systems. We show that the complexities of these problems are significantly worse than in the monolithic case. Namely, we show that deciding modular weak detectability (opacity, A-diagnosability) is EXPSPACE-complete. We further discuss a special case where all unobservable events are private, and show that in this case the problems are PSPACE-complete. Consequently, if the systems are all fully observable, then deciding weak detectability (opacity) for modular systems is PSPACE-complete."},{"title":"UnrealROX: An eXtremely Photorealistic Virtual Reality Environment for\n  Robotics Simulations and Synthetic Data Generation","source":"Pablo Martinez-Gonzalez, Sergiu Oprea, Alberto Garcia-Garcia, Alvaro\n  Jover-Alvarez, Sergio Orts-Escolano, Jose Garcia-Rodriguez","docs_id":"1810.06936","section":["cs.RO","cs.CV","cs.MM"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"UnrealROX: An eXtremely Photorealistic Virtual Reality Environment for\n  Robotics Simulations and Synthetic Data Generation. Data-driven algorithms have surpassed traditional techniques in almost every aspect in robotic vision problems. Such algorithms need vast amounts of quality data to be able to work properly after their training process. Gathering and annotating that sheer amount of data in the real world is a time-consuming and error-prone task. Those problems limit scale and quality. Synthetic data generation has become increasingly popular since it is faster to generate and automatic to annotate. However, most of the current datasets and environments lack realism, interactions, and details from the real world. UnrealROX is an environment built over Unreal Engine 4 which aims to reduce that reality gap by leveraging hyperrealistic indoor scenes that are explored by robot agents which also interact with objects in a visually realistic manner in that simulated world. Photorealistic scenes and robots are rendered by Unreal Engine into a virtual reality headset which captures gaze so that a human operator can move the robot and use controllers for the robotic hands; scene information is dumped on a per-frame basis so that it can be reproduced offline to generate raw data and ground truth annotations. This virtual reality environment enables robotic vision researchers to generate realistic and visually plausible data with full ground truth for a wide variety of problems such as class and instance semantic segmentation, object detection, depth estimation, visual grasping, and navigation."},{"title":"Exact partial information decompositions for Gaussian systems based on\n  dependency constraints","source":"James W. Kay and Robin A. A. Ince","docs_id":"1803.02030","section":["cond-mat.stat-mech","cs.IT","math.IT","physics.data-an","q-bio.QM","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Exact partial information decompositions for Gaussian systems based on\n  dependency constraints. The Partial Information Decomposition (PID) [arXiv:1004.2515] provides a theoretical framework to characterize and quantify the structure of multivariate information sharing. A new method (Idep) has recently been proposed for computing a two-predictor PID over discrete spaces. [arXiv:1709.06653] A lattice of maximum entropy probability models is constructed based on marginal dependency constraints, and the unique information that a particular predictor has about the target is defined as the minimum increase in joint predictor-target mutual information when that particular predictor-target marginal dependency is constrained. Here, we apply the Idep approach to Gaussian systems, for which the marginally constrained maximum entropy models are Gaussian graphical models. Closed form solutions for the Idep PID are derived for both univariate and multivariate Gaussian systems. Numerical and graphical illustrations are provided, together with practical and theoretical comparisons of the Idep PID with the minimum mutual information PID (Immi). [arXiv:1411.2832] In particular, it is proved that the Immi method generally produces larger estimates of redundancy and synergy than does the Idep method. In discussion of the practical examples, the PIDs are complemented by the use of deviance tests for the comparison of Gaussian graphical models."},{"title":"Deformation-aware Unpaired Image Translation for Pose Estimation on\n  Laboratory Animals","source":"Siyuan Li, Semih G\\\"unel, Mirela Ostrek, Pavan Ramdya, Pascal Fua, and\n  Helge Rhodin","docs_id":"2001.08601","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Deformation-aware Unpaired Image Translation for Pose Estimation on\n  Laboratory Animals. Our goal is to capture the pose of neuroscience model organisms, without using any manual supervision, to be able to study how neural circuits orchestrate behaviour. Human pose estimation attains remarkable accuracy when trained on real or simulated datasets consisting of millions of frames. However, for many applications simulated models are unrealistic and real training datasets with comprehensive annotations do not exist. We address this problem with a new sim2real domain transfer method. Our key contribution is the explicit and independent modeling of appearance, shape and poses in an unpaired image translation framework. Our model lets us train a pose estimator on the target domain by transferring readily available body keypoint locations from the source domain to generated target images. We compare our approach with existing domain transfer methods and demonstrate improved pose estimation accuracy on Drosophila melanogaster (fruit fly), Caenorhabditis elegans (worm) and Danio rerio (zebrafish), without requiring any manual annotation on the target domain and despite using simplistic off-the-shelf animal characters for simulation, or simple geometric shapes as models. Our new datasets, code, and trained models will be published to support future neuroscientific studies."},{"title":"Data based reconstruction of complex multiplex networks","source":"Chuang Ma, Han-Shuang Chen, Xiang Li, Ying-Cheng Lai, Hai-Feng Zhang","docs_id":"1806.03405","section":["physics.soc-ph","cs.SI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Data based reconstruction of complex multiplex networks. It has been recognized that many complex dynamical systems in the real world require a description in terms of multiplex networks, where a set of common, mutually connected nodes belong to distinct network layers and play a different role in each layer. In spite of recent progress towards data based inference of single-layer networks, to reconstruct complex systems with a multiplex structure remains largely open. We articulate a mean-field based maximum likelihood estimation framework to solve this outstanding and challenging problem. We demonstrate the power of the reconstruction framework and characterize its performance using binary time series from a class of prototypical duplex network systems that host two distinct types of spreading dynamics. In addition to validating the framework using synthetic and real-world multiplex networks, we carry out a detailed analysis to elucidate the impacts of structural and dynamical parameters as well as noise on the reconstruction accuracy and robustness."},{"title":"DTCA: Decision Tree-based Co-Attention Networks for Explainable Claim\n  Verification","source":"Lianwei Wu, Yuan Rao, Yongqiang Zhao, Hao Liang, Ambreen Nazir","docs_id":"2004.13455","section":["cs.CL","cs.AI","cs.CY","cs.SI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"DTCA: Decision Tree-based Co-Attention Networks for Explainable Claim\n  Verification. Recently, many methods discover effective evidence from reliable sources by appropriate neural networks for explainable claim verification, which has been widely recognized. However, in these methods, the discovery process of evidence is nontransparent and unexplained. Simultaneously, the discovered evidence only roughly aims at the interpretability of the whole sequence of claims but insufficient to focus on the false parts of claims. In this paper, we propose a Decision Tree-based Co-Attention model (DTCA) to discover evidence for explainable claim verification. Specifically, we first construct Decision Tree-based Evidence model (DTE) to select comments with high credibility as evidence in a transparent and interpretable way. Then we design Co-attention Self-attention networks (CaSa) to make the selected evidence interact with claims, which is for 1) training DTE to determine the optimal decision thresholds and obtain more powerful evidence; and 2) utilizing the evidence to find the false parts in the claim. Experiments on two public datasets, RumourEval and PHEME, demonstrate that DTCA not only provides explanations for the results of claim verification but also achieves the state-of-the-art performance, boosting the F1-score by 3.11%, 2.41%, respectively."},{"title":"Formal characterization and efficient verification of a biological\n  robustness property","source":"Lucia Nasti, Roberta Gori, Paolo Milazzo","docs_id":"2104.13831","section":["cs.FL"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Formal characterization and efficient verification of a biological\n  robustness property. Robustness is an observable property for which a chemical reaction network (CRN) can maintain its functionalities despite the influence of different perturbations. In general, to verify whether a network is robust, it is necessary to consider all the possible parameter configurations. This is a process that can entail a massive computational effort. In the work of Rizk et al., the authors propose a definition of robustness in linear temporal logic (LTL) through which, on the basis of multiple numerical timed traces obtained by considering different parameter configurations, they verify the robustness of a reaction network. In this paper, we focus on a notion of initial concentration robustness ($\\alpha$-robustness), which is related to the influence of the perturbation of the initial concentration of one species (i.e., the input) on the concentration of another species (i.e., the output) at the steady state. We characterize this notion of robustness in the framework proposed by Rizk et al., and we show that, for monotonic reaction networks, this allows us to drastically reduce the number of traces necessary to verify robustness of the CRN."},{"title":"Hierarchical Point-Edge Interaction Network for Point Cloud Semantic\n  Segmentation","source":"Li Jiang, Hengshuang Zhao, Shu Liu, Xiaoyong Shen, Chi-Wing Fu, Jiaya\n  Jia","docs_id":"1909.10469","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Hierarchical Point-Edge Interaction Network for Point Cloud Semantic\n  Segmentation. We achieve 3D semantic scene labeling by exploring semantic relation between each point and its contextual neighbors through edges. Besides an encoder-decoder branch for predicting point labels, we construct an edge branch to hierarchically integrate point features and generate edge features. To incorporate point features in the edge branch, we establish a hierarchical graph framework, where the graph is initialized from a coarse layer and gradually enriched along the point decoding process. For each edge in the final graph, we predict a label to indicate the semantic consistency of the two connected points to enhance point prediction. At different layers, edge features are also fed into the corresponding point module to integrate contextual information for message passing enhancement in local regions. The two branches interact with each other and cooperate in segmentation. Decent experimental results on several 3D semantic labeling datasets demonstrate the effectiveness of our work."},{"title":"Generating Realistic Synthetic Population Datasets","source":"Hao Wu, Yue Ning, Prithwish Chakraborty, Jilles Vreeken, Nikolaj Tatti\n  and Naren Ramakrishnan","docs_id":"1602.06844","section":["cs.DB"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Generating Realistic Synthetic Population Datasets. Modern studies of societal phenomena rely on the availability of large datasets capturing attributes and activities of synthetic, city-level, populations. For instance, in epidemiology, synthetic population datasets are necessary to study disease propagation and intervention measures before implementation. In social science, synthetic population datasets are needed to understand how policy decisions might affect preferences and behaviors of individuals. In public health, synthetic population datasets are necessary to capture diagnostic and procedural characteristics of patient records without violating confidentialities of individuals. To generate such datasets over a large set of categorical variables, we propose the use of the maximum entropy principle to formalize a generative model such that in a statistically well-founded way we can optimally utilize given prior information about the data, and are unbiased otherwise. An efficient inference algorithm is designed to estimate the maximum entropy model, and we demonstrate how our approach is adept at estimating underlying data distributions. We evaluate this approach against both simulated data and on US census datasets, and demonstrate its feasibility using an epidemic simulation application."},{"title":"Through the Looking Glass: Diminishing Occlusions in Robot Vision\n  Systems with Mirror Reflections","source":"Kentaro Yoshioka, Hidenori Okuni, Tuan Thanh Ta, Akihide Sai","docs_id":"2108.13599","section":["cs.RO"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Through the Looking Glass: Diminishing Occlusions in Robot Vision\n  Systems with Mirror Reflections. The quality of robot vision greatly affects the performance of automation systems, where occlusions stand as one of the biggest challenges. If the target is occluded from the sensor, detecting and grasping such objects become very challenging. For example, when multiple robot arms cooperate in a single workplace, occlusions will be created under the robot arm itself and hide objects underneath. While occlusions can be greatly reduced by installing multiple sensors, the increase in sensor costs cannot be ignored. Moreover, the sensor placements must be rearranged every time the robot operation routine and layout change. To diminish occlusions, we propose the first robot vision system with tilt-type mirror reflection sensing. By instantly tilting the sensor itself, we obtain two sensing results with different views: conventional direct line-of-sight sensing and non-line-of-sight sensing via mirror reflections. Our proposed system removes occlusions adaptively by detecting the occlusions in the scene and dynamically configuring the sensor tilt angle to sense the detected occluded area. Thus, sensor rearrangements are not required even after changes in robot operation or layout. Since the required hardware is the tilt-unit and a commercially available mirror, the cost increase is marginal. Through experiments, we show that our system can achieve a similar detection accuracy as systems with multiple sensors, regardless of the single-sensor implementation."},{"title":"Single Reduct Generation Based on Relative Indiscernibility of Rough Set\n  Theory","source":"Shampa Sengupta and Asit Kr. Das","docs_id":"1203.3170","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Single Reduct Generation Based on Relative Indiscernibility of Rough Set\n  Theory. In real world everything is an object which represents particular classes. Every object can be fully described by its attributes. Any real world dataset contains large number of attributes and objects. Classifiers give poor performance when these huge datasets are given as input to it for proper classification. So from these huge dataset most useful attributes need to be extracted that contribute the maximum to the decision. In the paper, attribute set is reduced by generating reducts using the indiscernibility relation of Rough Set Theory (RST). The method measures similarity among the attributes using relative indiscernibility relation and computes attribute similarity set. Then the set is minimized and an attribute similarity table is constructed from which attribute similar to maximum number of attributes is selected so that the resultant minimum set of selected attributes (called reduct) cover all attributes of the attribute similarity table. The method has been applied on glass dataset collected from the UCI repository and the classification accuracy is calculated by various classifiers. The result shows the efficiency of the proposed method."},{"title":"Adversarial Teacher-Student Learning for Unsupervised Domain Adaptation","source":"Zhong Meng, Jinyu Li, Yifan Gong, Biing-Hwang (Fred) Juang","docs_id":"1804.00644","section":["eess.AS","cs.CL","cs.LG","cs.SD"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Adversarial Teacher-Student Learning for Unsupervised Domain Adaptation. The teacher-student (T\/S) learning has been shown effective in unsupervised domain adaptation [1]. It is a form of transfer learning, not in terms of the transfer of recognition decisions, but the knowledge of posteriori probabilities in the source domain as evaluated by the teacher model. It learns to handle the speaker and environment variability inherent in and restricted to the speech signal in the target domain without proactively addressing the robustness to other likely conditions. Performance degradation may thus ensue. In this work, we advance T\/S learning by proposing adversarial T\/S learning to explicitly achieve condition-robust unsupervised domain adaptation. In this method, a student acoustic model and a condition classifier are jointly optimized to minimize the Kullback-Leibler divergence between the output distributions of the teacher and student models, and simultaneously, to min-maximize the condition classification loss. A condition-invariant deep feature is learned in the adapted student model through this procedure. We further propose multi-factorial adversarial T\/S learning which suppresses condition variabilities caused by multiple factors simultaneously. Evaluated with the noisy CHiME-3 test set, the proposed methods achieve relative word error rate improvements of 44.60% and 5.38%, respectively, over a clean source model and a strong T\/S learning baseline model."},{"title":"Improve Learning from Crowds via Generative Augmentation","source":"Zhendong Chu, Hongning Wang","docs_id":"2107.10449","section":["cs.LG","cs.CV","cs.HC"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Improve Learning from Crowds via Generative Augmentation. Crowdsourcing provides an efficient label collection schema for supervised machine learning. However, to control annotation cost, each instance in the crowdsourced data is typically annotated by a small number of annotators. This creates a sparsity issue and limits the quality of machine learning models trained on such data. In this paper, we study how to handle sparsity in crowdsourced data using data augmentation. Specifically, we propose to directly learn a classifier by augmenting the raw sparse annotations. We implement two principles of high-quality augmentation using Generative Adversarial Networks: 1) the generated annotations should follow the distribution of authentic ones, which is measured by a discriminator; 2) the generated annotations should have high mutual information with the ground-truth labels, which is measured by an auxiliary network. Extensive experiments and comparisons against an array of state-of-the-art learning from crowds methods on three real-world datasets proved the effectiveness of our data augmentation framework. It shows the potential of our algorithm for low-budget crowdsourcing in general."},{"title":"TANTRA: Timing-Based Adversarial Network Traffic Reshaping Attack","source":"Yam Sharon and David Berend and Yang Liu and Asaf Shabtai and Yuval\n  Elovici","docs_id":"2103.06297","section":["cs.CR","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"TANTRA: Timing-Based Adversarial Network Traffic Reshaping Attack. Network intrusion attacks are a known threat. To detect such attacks, network intrusion detection systems (NIDSs) have been developed and deployed. These systems apply machine learning models to high-dimensional vectors of features extracted from network traffic to detect intrusions. Advances in NIDSs have made it challenging for attackers, who must execute attacks without being detected by these systems. Prior research on bypassing NIDSs has mainly focused on perturbing the features extracted from the attack traffic to fool the detection system, however, this may jeopardize the attack's functionality. In this work, we present TANTRA, a novel end-to-end Timing-based Adversarial Network Traffic Reshaping Attack that can bypass a variety of NIDSs. Our evasion attack utilizes a long short-term memory (LSTM) deep neural network (DNN) which is trained to learn the time differences between the target network's benign packets. The trained LSTM is used to set the time differences between the malicious traffic packets (attack), without changing their content, such that they will \"behave\" like benign network traffic and will not be detected as an intrusion. We evaluate TANTRA on eight common intrusion attacks and three state-of-the-art NIDS systems, achieving an average success rate of 99.99\\% in network intrusion detection system evasion. We also propose a novel mitigation technique to address this new evasion attack."},{"title":"An Extensible and Personalizable Multi-Modal Trip Planner","source":"Xudong Liu, Christian Fritz, Matthew Klenk","docs_id":"1909.11604","section":["cs.AI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"An Extensible and Personalizable Multi-Modal Trip Planner. Despite a tremendous amount of work in the literature and in the commercial sectors, current approaches to multi-modal trip planning still fail to consistently generate plans that users deem optimal in practice. We believe that this is due to the fact that current planners fail to capture the true preferences of users, e.g., their preferences depend on aspects that are not modeled. An example of this could be a preference not to walk through an unsafe area at night. We present a novel multi-modal trip planner that allows users to upload auxiliary geographic data (e.g., crime rates) and to specify temporal constraints and preferences over these data in combination with typical metrics such as time and cost. Concretely, our planner supports the modes walking, biking, driving, public transit, and taxi, uses linear temporal logic to capture temporal constraints, and preferential cost functions to represent preferences. We show by examples that this allows the expression of very interesting preferences and constraints that, naturally, lead to quite diverse optimal plans."},{"title":"The Capacity Region of Distributed Multi-User Secret Sharing","source":"Ali Khalesi, Mahtab Mirmohseni, and Mohammad Ali Maddah-Ali","docs_id":"2103.01568","section":["cs.IT","cs.CR","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"The Capacity Region of Distributed Multi-User Secret Sharing. In this paper, we study the problem of distributed multi-user secret sharing, including a trusted master node, $N\\in \\mathbb{N}$ storage nodes, and $K$ users, where each user has access to the contents of a subset of storage nodes. Each user has an independent secret message with certain rate, defined as the size of the message normalized by the size of a storage node. Having access to the secret messages, the trusted master node places encoded shares in the storage nodes, such that (i) each user can recover its own message from the content of the storage nodes that it has access to, (ii) each user cannot gain any information about the message of any other user. We characterize the capacity region of the distributed multi-user secret sharing, defined as the set of all achievable rate tuples, subject to the correctness and privacy constraints. In the achievable scheme, for each user, the master node forms a polynomial with the degree equal to the number of its accessible storage nodes minus one, where the value of this polynomial at certain points are stored as the encoded shares. The message of that user is embedded in some of the coefficients of the polynomial. The remaining coefficients are determined such that the content of each storage node serves as the encoded shares for all users that have access to that storage node."},{"title":"Linear Bounds between Contraction Coefficients for $f$-Divergences","source":"Anuran Makur and Lizhong Zheng","docs_id":"1510.01844","section":["cs.IT","math.IT","math.PR","math.ST","stat.TH"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Linear Bounds between Contraction Coefficients for $f$-Divergences. Data processing inequalities for $f$-divergences can be sharpened using constants called \"contraction coefficients\" to produce strong data processing inequalities. For any discrete source-channel pair, the contraction coefficients for $f$-divergences are lower bounded by the contraction coefficient for $\\chi^2$-divergence. In this paper, we elucidate that this lower bound can be achieved by driving the input $f$-divergences of the contraction coefficients to zero. Then, we establish a linear upper bound on the contraction coefficients for a certain class of $f$-divergences using the contraction coefficient for $\\chi^2$-divergence, and refine this upper bound for the salient special case of Kullback-Leibler (KL) divergence. Furthermore, we present an alternative proof of the fact that the contraction coefficients for KL and $\\chi^2$-divergences are equal for a Gaussian source with an additive Gaussian noise channel (where the former coefficient can be power constrained). Finally, we generalize the well-known result that contraction coefficients of channels (after extremizing over all possible sources) for all $f$-divergences with non-linear operator convex $f$ are equal. In particular, we prove that the so called \"less noisy\" preorder over channels can be equivalently characterized by any non-linear operator convex $f$-divergence."},{"title":"Complexity of Equilibrium in Diffusion Games on Social Networks","source":"Seyed Rasoul Etesami, Tamer Basar","docs_id":"1403.3881","section":["cs.GT","cs.CC","cs.DC","cs.DM","cs.MA"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Complexity of Equilibrium in Diffusion Games on Social Networks. In this paper, we consider the competitive diffusion game, and study the existence of its pure-strategy Nash equilibrium when defined over general undirected networks. We first determine the set of pure-strategy Nash equilibria for two special but well-known classes of networks, namely the lattice and the hypercube. Characterizing the utility of the players in terms of graphical distances of their initial seed placements to other nodes in the network, we show that in general networks the decision process on the existence of pure-strategy Nash equilibrium is an NP-hard problem. Following this, we provide some necessary conditions for a given profile to be a Nash equilibrium. Furthermore, we study players' utilities in the competitive diffusion game over Erdos-Renyi random graphs and show that as the size of the network grows, the utilities of the players are highly concentrated around their expectation, and are bounded below by some threshold based on the parameters of the network. Finally, we obtain a lower bound for the maximum social welfare of the game with two players, and study sub-modularity of the players' utilities."},{"title":"Reinforcement Learning Based Text Style Transfer without Parallel\n  Training Corpus","source":"Hongyu Gong, Suma Bhat, Lingfei Wu, Jinjun Xiong, Wen-mei Hwu","docs_id":"1903.10671","section":["cs.CL"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Reinforcement Learning Based Text Style Transfer without Parallel\n  Training Corpus. Text style transfer rephrases a text from a source style (e.g., informal) to a target style (e.g., formal) while keeping its original meaning. Despite the success existing works have achieved using a parallel corpus for the two styles, transferring text style has proven significantly more challenging when there is no parallel training corpus. In this paper, we address this challenge by using a reinforcement-learning-based generator-evaluator architecture. Our generator employs an attention-based encoder-decoder to transfer a sentence from the source style to the target style. Our evaluator is an adversarially trained style discriminator with semantic and syntactic constraints that score the generated sentence for style, meaning preservation, and fluency. Experimental results on two different style transfer tasks (sentiment transfer and formality transfer) show that our model outperforms state-of-the-art approaches. Furthermore, we perform a manual evaluation that demonstrates the effectiveness of the proposed method using subjective metrics of generated text quality."},{"title":"GmCN: Graph Mask Convolutional Network","source":"Bo Jiang, Beibei Wang, Jin Tang and Bin Luo","docs_id":"1910.01735","section":["cs.CV","cs.LG","cs.SI","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"GmCN: Graph Mask Convolutional Network. Graph Convolutional Networks (GCNs) have shown very powerful for graph data representation and learning tasks. Existing GCNs usually conduct feature aggregation on a fixed neighborhood graph in which each node computes its representation by aggregating the feature representations of all its neighbors which is biased by its own representation. However, this fixed aggregation strategy is not guaranteed to be optimal for GCN based graph learning and also can be affected by some graph structure noises, such as incorrect or undesired edge connections. To address these issues, we propose a novel Graph mask Convolutional Network (GmCN) in which nodes can adaptively select the optimal neighbors in their feature aggregation to better serve GCN learning. GmCN can be theoretically interpreted by a regularization framework, based on which we derive a simple update algorithm to determine the optimal mask adaptively in GmCN training process. Experiments on several datasets validate the effectiveness of GmCN."},{"title":"Unconstrained Facial Expression Transfer using Style-based Generator","source":"Chao Yang and Ser-Nam Lim","docs_id":"1912.06253","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Unconstrained Facial Expression Transfer using Style-based Generator. Facial expression transfer and reenactment has been an important research problem given its applications in face editing, image manipulation, and fabricated videos generation. We present a novel method for image-based facial expression transfer, leveraging the recent style-based GAN shown to be very effective for creating realistic looking images. Given two face images, our method can create plausible results that combine the appearance of one image and the expression of the other. To achieve this, we first propose an optimization procedure based on StyleGAN to infer hierarchical style vector from an image that disentangle different attributes of the face. We further introduce a linear combination scheme that fuses the style vectors of the two given images and generate a new face that combines the expression and appearance of the inputs. Our method can create high-quality synthesis with accurate facial reenactment. Unlike many existing methods, we do not rely on geometry annotations, and can be applied to unconstrained facial images of any identities without the need for retraining, making it feasible to generate large-scale expression-transferred results."},{"title":"Applications and Techniques for Fast Machine Learning in Science","source":"Allison McCarn Deiana (coordinator), Nhan Tran (coordinator), Joshua\n  Agar, Michaela Blott, Giuseppe Di Guglielmo, Javier Duarte, Philip Harris,\n  Scott Hauck, Mia Liu, Mark S. Neubauer, Jennifer Ngadiuba, Seda\n  Ogrenci-Memik, Maurizio Pierini, Thea Aarrestad, Steffen Bahr, Jurgen Becker,\n  Anne-Sophie Berthold, Richard J. Bonventre, Tomas E. Muller Bravo, Markus\n  Diefenthaler, Zhen Dong, Nick Fritzsche, Amir Gholami, Ekaterina Govorkova,\n  Kyle J Hazelwood, Christian Herwig, Babar Khan, Sehoon Kim, Thomas Klijnsma,\n  Yaling Liu, Kin Ho Lo, Tri Nguyen, Gianantonio Pezzullo, Seyedramin\n  Rasoulinezhad, Ryan A. Rivera, Kate Scholberg, Justin Selig, Sougata Sen,\n  Dmitri Strukov, William Tang, Savannah Thais, Kai Lukas Unger, Ricardo\n  Vilalta, Belinavon Krosigk, Thomas K. Warburton, Maria Acosta Flechas,\n  Anthony Aportela, Thomas Calvet, Leonardo Cristella, Daniel Diaz, Caterina\n  Doglioni, Maria Domenica Galati, Elham E Khoda, Farah Fahim, Davide Giri,\n  Benjamin Hawks, Duc Hoang, Burt Holzman, Shih-Chieh Hsu, Sergo Jindariani,\n  Iris Johnson, Raghav Kansal, Ryan Kastner, Erik Katsavounidis, Jeffrey Krupa,\n  Pan Li, Sandeep Madireddy, Ethan Marx, Patrick McCormack, Andres Meza, Jovan\n  Mitrevski, Mohammed Attia Mohammed, Farouk Mokhtar, Eric Moreno, Srishti\n  Nagu, Rohin Narayan, Noah Palladino, Zhiqiang Que, Sang Eon Park, Subramanian\n  Ramamoorthy, Dylan Rankin, Simon Rothman, Ashish Sharma, Sioni Summers,\n  Pietro Vischia, Jean-Roch Vlimant, Olivia Weng","docs_id":"2110.13041","section":["cs.LG","cs.AR","physics.data-an","physics.ins-det"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Applications and Techniques for Fast Machine Learning in Science. In this community review report, we discuss applications and techniques for fast machine learning (ML) in science -- the concept of integrating power ML methods into the real-time experimental data processing loop to accelerate scientific discovery. The material for the report builds on two workshops held by the Fast ML for Science community and covers three main areas: applications for fast ML across a number of scientific domains; techniques for training and implementing performant and resource-efficient ML algorithms; and computing architectures, platforms, and technologies for deploying these algorithms. We also present overlapping challenges across the multiple scientific domains where common solutions can be found. This community report is intended to give plenty of examples and inspiration for scientific discovery through integrated and accelerated ML solutions. This is followed by a high-level overview and organization of technical advances, including an abundance of pointers to source material, which can enable these breakthroughs."},{"title":"End-to-End Deep Convolutional Active Contours for Image Segmentation","source":"Ali Hatamizadeh, Debleena Sengupta and Demetri Terzopoulos","docs_id":"1909.13359","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"End-to-End Deep Convolutional Active Contours for Image Segmentation. The Active Contour Model (ACM) is a standard image analysis technique whose numerous variants have attracted an enormous amount of research attention across multiple fields. Incorrectly, however, the ACM's differential-equation-based formulation and prototypical dependence on user initialization have been regarded as being largely incompatible with the recently popular deep learning approaches to image segmentation. This paper introduces the first tight unification of these two paradigms. In particular, we devise Deep Convolutional Active Contours (DCAC), a truly end-to-end trainable image segmentation framework comprising a Convolutional Neural Network (CNN) and an ACM with learnable parameters. The ACM's Eulerian energy functional includes per-pixel parameter maps predicted by the backbone CNN, which also initializes the ACM. Importantly, both the CNN and ACM components are fully implemented in TensorFlow, and the entire DCAC architecture is end-to-end automatically differentiable and backpropagation trainable without user intervention. As a challenging test case, we tackle the problem of building instance segmentation in aerial images and evaluate DCAC on two publicly available datasets, Vaihingen and Bing Huts. Our reseults demonstrate that, for building segmentation, the DCAC establishes a new state-of-the-art performance by a wide margin."},{"title":"End to End Video Segmentation for Driving : Lane Detection For\n  Autonomous Car","source":"Wenhui Zhang, Tejas Mahale","docs_id":"1812.05914","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"End to End Video Segmentation for Driving : Lane Detection For\n  Autonomous Car. Safety and decline of road traffic accidents remain important issues of autonomous driving. Statistics show that unintended lane departure is a leading cause of worldwide motor vehicle collisions, making lane detection the most promising and challenge task for self-driving. Today, numerous groups are combining deep learning techniques with computer vision problems to solve self-driving problems. In this paper, a Global Convolution Networks (GCN) model is used to address both classification and localization issues for semantic segmentation of lane. We are using color-based segmentation is presented and the usability of the model is evaluated. A residual-based boundary refinement and Adam optimization is also used to achieve state-of-art performance. As normal cars could not afford GPUs on the car, and training session for a particular road could be shared by several cars. We propose a framework to get it work in real world. We build a real time video transfer system to get video from the car, get the model trained in edge server (which is equipped with GPUs), and send the trained model back to the car."},{"title":"Maximizing submodular functions using probabilistic graphical models","source":"K. S. Sesh Kumar (LIENS, INRIA Paris - Rocquencourt), Francis Bach\n  (LIENS, INRIA Paris - Rocquencourt)","docs_id":"1309.2593","section":["cs.LG","math.OC"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Maximizing submodular functions using probabilistic graphical models. We consider the problem of maximizing submodular functions; while this problem is known to be NP-hard, several numerically efficient local search techniques with approximation guarantees are available. In this paper, we propose a novel convex relaxation which is based on the relationship between submodular functions, entropies and probabilistic graphical models. In a graphical model, the entropy of the joint distribution decomposes as a sum of marginal entropies of subsets of variables; moreover, for any distribution, the entropy of the closest distribution factorizing in the graphical model provides an bound on the entropy. For directed graphical models, this last property turns out to be a direct consequence of the submodularity of the entropy function, and allows the generalization of graphical-model-based upper bounds to any submodular functions. These upper bounds may then be jointly maximized with respect to a set, while minimized with respect to the graph, leading to a convex variational inference scheme for maximizing submodular functions, based on outer approximations of the marginal polytope and maximum likelihood bounded treewidth structures. By considering graphs of increasing treewidths, we may then explore the trade-off between computational complexity and tightness of the relaxation. We also present extensions to constrained problems and maximizing the difference of submodular functions, which include all possible set functions."},{"title":"Improving Universal Sound Separation Using Sound Classification","source":"Efthymios Tzinis, Scott Wisdom, John R. Hershey, Aren Jansen, Daniel\n  P. W. Ellis","docs_id":"1911.07951","section":["cs.SD","cs.LG","eess.AS","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Improving Universal Sound Separation Using Sound Classification. Deep learning approaches have recently achieved impressive performance on both audio source separation and sound classification. Most audio source separation approaches focus only on separating sources belonging to a restricted domain of source classes, such as speech and music. However, recent work has demonstrated the possibility of \"universal sound separation\", which aims to separate acoustic sources from an open domain, regardless of their class. In this paper, we utilize the semantic information learned by sound classifier networks trained on a vast amount of diverse sounds to improve universal sound separation. In particular, we show that semantic embeddings extracted from a sound classifier can be used to condition a separation network, providing it with useful additional information. This approach is especially useful in an iterative setup, where source estimates from an initial separation stage and their corresponding classifier-derived embeddings are fed to a second separation network. By performing a thorough hyperparameter search consisting of over a thousand experiments, we find that classifier embeddings from clean sources provide nearly one dB of SNR gain, and our best iterative models achieve a significant fraction of this oracle performance, establishing a new state-of-the-art for universal sound separation."},{"title":"Effects of Persuasive Dialogues: Testing Bot Identities and Inquiry\n  Strategies","source":"Weiyan Shi, Xuewei Wang, Yoo Jung Oh, Jingwen Zhang, Saurav Sahay,\n  Zhou Yu","docs_id":"2001.04564","section":["cs.HC"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Effects of Persuasive Dialogues: Testing Bot Identities and Inquiry\n  Strategies. Intelligent conversational agents, or chatbots, can take on various identities and are increasingly engaging in more human-centered conversations with persuasive goals. However, little is known about how identities and inquiry strategies influence the conversation's effectiveness. We conducted an online study involving 790 participants to be persuaded by a chatbot for charity donation. We designed a two by four factorial experiment (two chatbot identities and four inquiry strategies) where participants were randomly assigned to different conditions. Findings showed that the perceived identity of the chatbot had significant effects on the persuasion outcome (i.e., donation) and interpersonal perceptions (i.e., competence, confidence, warmth, and sincerity). Further, we identified interaction effects among perceived identities and inquiry strategies. We discuss the findings for theoretical and practical implications for developing ethical and effective persuasive chatbots. Our published data, codes, and analyses serve as the first step towards building competent ethical persuasive chatbots."},{"title":"Mapping Rule Estimation for Power Flow Analysis in Distribution Grids","source":"Jiafan Yu, Yang Weng, Ram Rajagopal","docs_id":"1702.07948","section":["cs.SY"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Mapping Rule Estimation for Power Flow Analysis in Distribution Grids. The increasing integration of distributed energy resources (DERs) calls for new monitoring and operational planning tools to ensure stability and sustainability in distribution grids. One idea is to use existing monitoring tools in transmission grids and some primary distribution grids. However, they usually depend on the knowledge of the system model, e.g., the topology and line parameters, which may be unavailable in primary and secondary distribution grids. Furthermore, a utility usually has limited modeling ability of active controllers for solar panels as they may belong to a third party like residential customers. To solve the modeling problem in traditional power flow analysis, we propose a support vector regression (SVR) approach to reveal the mapping rules between different variables and recover useful variables based on physical understanding and data mining. We illustrate the advantages of using the SVR model over traditional regression method which finds line parameters in distribution grids. Specifically, the SVR model is robust enough to recover the mapping rules while the regression method fails when 1) there are measurement outliers and missing data, 2) there are active controllers, or 3) measurements are only available at some part of a distribution grid. We demonstrate the superior performance of our method through extensive numerical validation on different scales of distribution grids."},{"title":"From NoSQL Accumulo to NewSQL Graphulo: Design and Utility of Graph\n  Algorithms inside a BigTable Database","source":"Dylan Hutchison, Jeremy Kepner, Vijay Gadepally, Bill Howe","docs_id":"1606.07085","section":["cs.DB","cs.DC","cs.MS"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"From NoSQL Accumulo to NewSQL Graphulo: Design and Utility of Graph\n  Algorithms inside a BigTable Database. Google BigTable's scale-out design for distributed key-value storage inspired a generation of NoSQL databases. Recently the NewSQL paradigm emerged in response to analytic workloads that demand distributed computation local to data storage. Many such analytics take the form of graph algorithms, a trend that motivated the GraphBLAS initiative to standardize a set of matrix math kernels for building graph algorithms. In this article we show how it is possible to implement the GraphBLAS kernels in a BigTable database by presenting the design of Graphulo, a library for executing graph algorithms inside the Apache Accumulo database. We detail the Graphulo implementation of two graph algorithms and conduct experiments comparing their performance to two main-memory matrix math systems. Our results shed insight into the conditions that determine when executing a graph algorithm is faster inside a database versus an external system---in short, that memory requirements and relative I\/O are critical factors."},{"title":"Face Alignment Robust to Pose, Expressions and Occlusions","source":"Vishnu Naresh Boddeti, Myung-Cheol Roh, Jongju Shin, Takaharu Oguri,\n  Takeo Kanade","docs_id":"1707.05938","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Face Alignment Robust to Pose, Expressions and Occlusions. We propose an Ensemble of Robust Constrained Local Models for alignment of faces in the presence of significant occlusions and of any unknown pose and expression. To account for partial occlusions we introduce, Robust Constrained Local Models, that comprises of a deformable shape and local landmark appearance model and reasons over binary occlusion labels. Our occlusion reasoning proceeds by a hypothesize-and-test search over occlusion labels. Hypotheses are generated by Constrained Local Model based shape fitting over randomly sampled subsets of landmark detector responses and are evaluated by the quality of face alignment. To span the entire range of facial pose and expression variations we adopt an ensemble of independent Robust Constrained Local Models to search over a discretized representation of pose and expression. We perform extensive evaluation on a large number of face images, both occluded and unoccluded. We find that our face alignment system trained entirely on facial images captured \"in-the-lab\" exhibits a high degree of generalization to facial images captured \"in-the-wild\". Our results are accurate and stable over a wide spectrum of occlusions, pose and expression variations resulting in excellent performance on many real-world face datasets."},{"title":"Distribution-free Contextual Dynamic Pricing","source":"Yiyun Luo and Will Wei Sun and and Yufeng Liu","docs_id":"2109.07340","section":["stat.ML","cs.LG","math.ST","stat.TH"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Distribution-free Contextual Dynamic Pricing. Contextual dynamic pricing aims to set personalized prices based on sequential interactions with customers. At each time period, a customer who is interested in purchasing a product comes to the platform. The customer's valuation for the product is a linear function of contexts, including product and customer features, plus some random market noise. The seller does not observe the customer's true valuation, but instead needs to learn the valuation by leveraging contextual information and historical binary purchase feedbacks. Existing models typically assume full or partial knowledge of the random noise distribution. In this paper, we consider contextual dynamic pricing with unknown random noise in the valuation model. Our distribution-free pricing policy learns both the contextual function and the market noise simultaneously. A key ingredient of our method is a novel perturbed linear bandit framework, where a modified linear upper confidence bound algorithm is proposed to balance the exploration of market noise and the exploitation of the current knowledge for better pricing. We establish the regret upper bound and a matching lower bound of our policy in the perturbed linear bandit framework and prove a sub-linear regret bound in the considered pricing problem. Finally, we demonstrate the superior performance of our policy on simulations and a real-life auto-loan dataset."},{"title":"Exhaustive and Efficient Constraint Propagation: A Semi-Supervised\n  Learning Perspective and Its Applications","source":"Zhiwu Lu, Horace H.S. Ip, Yuxin Peng","docs_id":"1109.4684","section":["cs.AI","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Exhaustive and Efficient Constraint Propagation: A Semi-Supervised\n  Learning Perspective and Its Applications. This paper presents a novel pairwise constraint propagation approach by decomposing the challenging constraint propagation problem into a set of independent semi-supervised learning subproblems which can be solved in quadratic time using label propagation based on k-nearest neighbor graphs. Considering that this time cost is proportional to the number of all possible pairwise constraints, our approach actually provides an efficient solution for exhaustively propagating pairwise constraints throughout the entire dataset. The resulting exhaustive set of propagated pairwise constraints are further used to adjust the similarity matrix for constrained spectral clustering. Other than the traditional constraint propagation on single-source data, our approach is also extended to more challenging constraint propagation on multi-source data where each pairwise constraint is defined over a pair of data points from different sources. This multi-source constraint propagation has an important application to cross-modal multimedia retrieval. Extensive results have shown the superior performance of our approach."},{"title":"Semi and Weakly Supervised Semantic Segmentation Using Generative\n  Adversarial Network","source":"Nasim Souly, Concetto Spampinato and Mubarak Shah","docs_id":"1703.09695","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Semi and Weakly Supervised Semantic Segmentation Using Generative\n  Adversarial Network. Semantic segmentation has been a long standing challenging task in computer vision. It aims at assigning a label to each image pixel and needs significant number of pixellevel annotated data, which is often unavailable. To address this lack, in this paper, we leverage, on one hand, massive amount of available unlabeled or weakly labeled data, and on the other hand, non-real images created through Generative Adversarial Networks. In particular, we propose a semi-supervised framework ,based on Generative Adversarial Networks (GANs), which consists of a generator network to provide extra training examples to a multi-class classifier, acting as discriminator in the GAN framework, that assigns sample a label y from the K possible classes or marks it as a fake sample (extra class). The underlying idea is that adding large fake visual data forces real samples to be close in the feature space, enabling a bottom-up clustering process, which, in turn, improves multiclass pixel classification. To ensure higher quality of generated images for GANs with consequent improved pixel classification, we extend the above framework by adding weakly annotated data, i.e., we provide class level information to the generator. We tested our approaches on several challenging benchmarking visual datasets, i.e. PASCAL, SiftFLow, Stanford and CamVid, achieving competitive performance also compared to state-of-the-art semantic segmentation method"},{"title":"Differentiable Factor Graph Optimization for Learning Smoothers","source":"Brent Yi, Michelle A. Lee, Alina Kloss, Roberto Mart\\'in-Mart\\'in,\n  Jeannette Bohg","docs_id":"2105.08257","section":["cs.RO"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Differentiable Factor Graph Optimization for Learning Smoothers. A recent line of work has shown that end-to-end optimization of Bayesian filters can be used to learn state estimators for systems whose underlying models are difficult to hand-design or tune, while retaining the core advantages of probabilistic state estimation. As an alternative approach for state estimation in these settings, we present an end-to-end approach for learning state estimators modeled as factor graph-based smoothers. By unrolling the optimizer we use for maximum a posteriori inference in these probabilistic graphical models, we can learn probabilistic system models in the full context of an overall state estimator, while also taking advantage of the distinct accuracy and runtime advantages that smoothers offer over recursive filters. We study this approach using two fundamental state estimation problems, object tracking and visual odometry, where we demonstrate a significant improvement over existing baselines. Our work comes with an extensive code release, which includes training and evaluation scripts, as well as Python libraries for Lie theory and factor graph optimization: https:\/\/sites.google.com\/view\/diffsmoothing\/"},{"title":"Interpretable Run-Time Prediction and Planning in Co-Robotic\n  Environments","source":"Rahul Peddi and Nicola Bezzo","docs_id":"2109.03893","section":["cs.RO"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Interpretable Run-Time Prediction and Planning in Co-Robotic\n  Environments. Mobile robots are traditionally developed to be reactive and avoid collisions with surrounding humans, often moving in unnatural ways without following social protocols, forcing people to behave very differently from human-human interaction rules. Humans, on the other hand, are seamlessly able to understand why they may interfere with surrounding humans and change their behavior based on their reasoning, resulting in smooth, intuitive avoiding behaviors. In this paper, we propose an approach for a mobile robot to avoid interfering with the desired paths of surrounding humans. We leverage a library of previously observed trajectories to design a decision-tree based interpretable monitor that: i) predicts whether the robot is interfering with surrounding humans, ii) explains what behaviors are causing either prediction, and iii) plans corrective behaviors if interference is predicted. We also propose a validation scheme to improve the predictive model at run-time. The proposed approach is validated with simulations and experiments involving an unmanned ground vehicle (UGV) performing go-to-goal operations in the presence of humans, demonstrating non-interfering behaviors and run-time learning."},{"title":"The SensorCloud Protocol: Securely Outsourcing Sensor Data to the Cloud","source":"Martin Henze, Ren\\'e Hummen, Roman Matzutt, Klaus Wehrle","docs_id":"1607.03239","section":["cs.NI","cs.CR","cs.DC"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"The SensorCloud Protocol: Securely Outsourcing Sensor Data to the Cloud. The increasing deployment of sensor networks, ranging from home networks to industrial automation, leads to a similarly growing demand for storing and processing the collected sensor data. To satisfy this demand, the most promising approach to date is the utilization of the dynamically scalable, on-demand resources made available via the cloud computing paradigm. However, prevalent security and privacy concerns are a huge obstacle for the outsourcing of sensor data to the cloud. Hence, sensor data needs to be secured properly before it can be outsourced to the cloud. When securing the outsourcing of sensor data to the cloud, one important challenge lies in the representation of sensor data and the choice of security measures applied to it. In this paper, we present the SensorCloud protocol, which enables the representation of sensor data and actuator commands using JSON as well as the encoding of the object security mechanisms applied to a given sensor data item. Notably, we solely utilize mechanisms that have been or currently are in the process of being standardized at the IETF to aid the wide applicability of our approach."},{"title":"A Game-Theoretical Self-Adaptation Framework for Securing\n  Software-Intensive Systems","source":"Mingyue Zhang, Nianyu Li, Sridhar Adepu, Eunsuk Kang, Zhi Jin","docs_id":"2112.07588","section":["cs.SE","cs.GT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A Game-Theoretical Self-Adaptation Framework for Securing\n  Software-Intensive Systems. The increasing prevalence of security attacks on software-intensive systems calls for new, effective methods for detecting and responding to these attacks. As one promising approach, game theory provides analytical tools for modeling the interaction between the system and the adversarial environment and designing reliable defense. In this paper, we propose an approach for securing software-intensive systems using a rigorous game-theoretical framework. First, a self-adaptation framework is deployed on a component-based software intensive system, which periodically monitors the system for anomalous behaviors. A learning-based method is proposed to detect possible on-going attacks on the system components and predict potential threats to components. Then, an algorithm is designed to automatically build a \\emph{Bayesian game} based on the system architecture (of which some components might have been compromised) once an attack is detected, in which the system components are modeled as independent players in the game. Finally, an optimal defensive policy is computed by solving the Bayesian game to achieve the best system utility, which amounts to minimizing the impact of the attack. We conduct two sets of experiments on two general benchmark tasks for security domain. Moreover, we systematically present a case study on a real-world water treatment testbed, i.e. the Secure Water Treatment System. Experiment results show the applicability and the effectiveness of our approach."},{"title":"Rare-Allele Detection Using Compressed Se(que)nsing","source":"Noam Shental, Amnon Amir and Or Zuk","docs_id":"0909.0400","section":["q-bio.GN","cs.IT","cs.LG","math.IT","q-bio.QM","stat.AP","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Rare-Allele Detection Using Compressed Se(que)nsing. Detection of rare variants by resequencing is important for the identification of individuals carrying disease variants. Rapid sequencing by new technologies enables low-cost resequencing of target regions, although it is still prohibitive to test more than a few individuals. In order to improve cost trade-offs, it has recently been suggested to apply pooling designs which enable the detection of carriers of rare alleles in groups of individuals. However, this was shown to hold only for a relatively low number of individuals in a pool, and requires the design of pooling schemes for particular cases. We propose a novel pooling design, based on a compressed sensing approach, which is both general, simple and efficient. We model the experimental procedure and show via computer simulations that it enables the recovery of rare allele carriers out of larger groups than were possible before, especially in situations where high coverage is obtained for each individual. Our approach can also be combined with barcoding techniques to enhance performance and provide a feasible solution based on current resequencing costs. For example, when targeting a small enough genomic region (~100 base-pairs) and using only ~10 sequencing lanes and ~10 distinct barcodes, one can recover the identity of 4 rare allele carriers out of a population of over 4000 individuals."},{"title":"A New Generation of Brain-Computer Interface Based on Riemannian\n  Geometry","source":"Marco Congedo, Alexandre Barachant and Anton Andreev","docs_id":"1310.8115","section":["cs.HC","math.DG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A New Generation of Brain-Computer Interface Based on Riemannian\n  Geometry. Based on the cumulated experience over the past 25 years in the field of Brain-Computer Interface (BCI) we can now envision a new generation of BCI. Such BCIs will not require training; instead they will be smartly initialized using remote massive databases and will adapt to the user fast and effectively in the first minute of use. They will be reliable, robust and will maintain good performances within and across sessions. A general classification framework based on recent advances in Riemannian geometry and possessing these characteristics is presented. It applies equally well to BCI based on event-related potentials (ERP), sensorimotor (mu) rhythms and steady-state evoked potential (SSEP). The framework is very simple, both algorithmically and computationally. Due to its simplicity, its ability to learn rapidly (with little training data) and its good across-subject and across-session generalization, this strategy a very good candidate for building a new generation of BCIs, thus we hereby propose it as a benchmark method for the field."},{"title":"Uncertainty Quantification in Medical Image Segmentation with\n  Multi-decoder U-Net","source":"Yanwu Yang, Xutao Guo, Yiwei Pan, Pengcheng Shi, Haiyan Lv, Ting Ma","docs_id":"2109.07045","section":["eess.IV","cs.AI","cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Uncertainty Quantification in Medical Image Segmentation with\n  Multi-decoder U-Net. Accurate medical image segmentation is crucial for diagnosis and analysis. However, the models without calibrated uncertainty estimates might lead to errors in downstream analysis and exhibit low levels of robustness. Estimating the uncertainty in the measurement is vital to making definite, informed conclusions. Especially, it is difficult to make accurate predictions on ambiguous areas and focus boundaries for both models and radiologists, even harder to reach a consensus with multiple annotations. In this work, the uncertainty under these areas is studied, which introduces significant information with anatomical structure and is as important as segmentation performance. We exploit the medical image segmentation uncertainty quantification by measuring segmentation performance with multiple annotations in a supervised learning manner and propose a U-Net based architecture with multiple decoders, where the image representation is encoded with the same encoder, and segmentation referring to each annotation is estimated with multiple decoders. Nevertheless, a cross-loss function is proposed for bridging the gap between different branches. The proposed architecture is trained in an end-to-end manner and able to improve predictive uncertainty estimates. The model achieves comparable performance with fewer parameters to the integrated training model that ranked the runner-up in the MICCAI-QUBIQ 2020 challenge."},{"title":"Data-Driven Characterization and Detection of COVID-19 Themed Malicious\n  Websites","source":"Mir Mehedi Ahsan Pritom, Kristin M. Schweitzer, Raymond M. Bateman,\n  Min Xu, Shouhuai Xu","docs_id":"2102.13226","section":["cs.CR"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Data-Driven Characterization and Detection of COVID-19 Themed Malicious\n  Websites. COVID-19 has hit hard on the global community, and organizations are working diligently to cope with the new norm of \"work from home\". However, the volume of remote work is unprecedented and creates opportunities for cyber attackers to penetrate home computers. Attackers have been leveraging websites with COVID-19 related names, dubbed COVID-19 themed malicious websites. These websites mostly contain false information, fake forms, fraudulent payments, scams, or malicious payloads to steal sensitive information or infect victims' computers. In this paper, we present a data-driven study on characterizing and detecting COVID-19 themed malicious websites. Our characterization study shows that attackers are agile and are deceptively crafty in designing geolocation targeted websites, often leveraging popular domain registrars and top-level domains. Our detection study shows that the Random Forest classifier can detect COVID-19 themed malicious websites based on the lexical and WHOIS features defined in this paper, achieving a 98% accuracy and 2.7% false-positive rate."},{"title":"3D Reconstruction from public webcams","source":"Tianyu Wu, Konrad Schindler and Cenek Albl","docs_id":"2108.09476","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"3D Reconstruction from public webcams. We investigate the possibility of 3D scene reconstruction from two or more overlapping webcam streams. A large, and growing, number of webcams observe places of interest and are publicly accessible. The question naturally arises: can we make use of this free data source for 3D computer vision? It turns out that the task to reconstruct scene structure from webcam streams is very different from standard structure-from-motion (SfM), and conventional SfM pipelines fail. In the webcam setting there are very few views of the same scene, in most cases only the minimum of two. These viewpoints often have large baselines and\/or scale differences, their overlap is rather limited, and besides unknown internal and external calibration also their temporal synchronisation is unknown. On the other hand, they record rather large fields of view continuously over long time spans, so that they regularly observe dynamic objects moving through the scene. We show how to leverage recent advances in several areas of computer vision to adapt SfM reconstruction to this particular scenario and reconstruct the unknown camera poses, the 3D scene structure, and the 3D trajectories of dynamic objects."},{"title":"Revisit the Fundamental Theorem of Linear Algebra","source":"Jun Lu","docs_id":"2108.04432","section":["cs.LG","cs.NA","math.NA"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Revisit the Fundamental Theorem of Linear Algebra. This survey is meant to provide an introduction to the fundamental theorem of linear algebra and the theories behind them. Our goal is to give a rigorous introduction to the readers with prior exposure to linear algebra. Specifically, we provide some details and proofs of some results from (Strang, 1993). We then describe the fundamental theorem of linear algebra from different views and find the properties and relationships behind the views. The fundamental theorem of linear algebra is essential in many fields, such as electrical engineering, computer science, machine learning, and deep learning. This survey is primarily a summary of purpose, significance of important theories behind it. The sole aim of this survey is to give a self-contained introduction to concepts and mathematical tools in theory behind the fundamental theorem of linear algebra and rigorous analysis in order to seamlessly introduce its properties in four subspaces in subsequent sections. However, we clearly realize our inability to cover all the useful and interesting results and given the paucity of scope to present this discussion, e.g., the separated analysis of the (orthogonal) projection matrices. We refer the reader to literature in the field of linear algebra for a more detailed introduction to the related fields. Some excellent examples include (Rose, 1982; Strang, 2009; Trefethen and Bau III, 1997; Strang, 2019, 2021)."},{"title":"RFBNet: Deep Multimodal Networks with Residual Fusion Blocks for RGB-D\n  Semantic Segmentation","source":"Liuyuan Deng, Ming Yang, Tianyi Li, Yuesheng He, and Chunxiang Wang","docs_id":"1907.00135","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"RFBNet: Deep Multimodal Networks with Residual Fusion Blocks for RGB-D\n  Semantic Segmentation. RGB-D semantic segmentation methods conventionally use two independent encoders to extract features from the RGB and depth data. However, there lacks an effective fusion mechanism to bridge the encoders, for the purpose of fully exploiting the complementary information from multiple modalities. This paper proposes a novel bottom-up interactive fusion structure to model the interdependencies between the encoders. The structure introduces an interaction stream to interconnect the encoders. The interaction stream not only progressively aggregates modality-specific features from the encoders but also computes complementary features for them. To instantiate this structure, the paper proposes a residual fusion block (RFB) to formulate the interdependences of the encoders. The RFB consists of two residual units and one fusion unit with gate mechanism. It learns complementary features for the modality-specific encoders and extracts modality-specific features as well as cross-modal features. Based on the RFB, the paper presents the deep multimodal networks for RGB-D semantic segmentation called RFBNet. The experiments on two datasets demonstrate the effectiveness of modeling the interdependencies and that the RFBNet achieved state-of-the-art performance."},{"title":"High Performance and Portable Convolution Operators for ARM-based\n  Multicore Processors","source":"Pablo San Juan, Adri\\'an Castell\\'o, Manuel F. Dolz, Pedro\n  Alonso-Jord\\'a, Enrique S. Quintana-Ort\\'i","docs_id":"2005.06410","section":["cs.PF"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"High Performance and Portable Convolution Operators for ARM-based\n  Multicore Processors. The considerable impact of Convolutional Neural Networks on many Artificial Intelligence tasks has led to the development of various high performance algorithms for the convolution operator present in this type of networks. One of these approaches leverages the \\imcol transform followed by a general matrix multiplication (GEMM) in order to take advantage of the highly optimized realizations of the GEMM kernel in many linear algebra libraries. The main problems of this approach are 1) the large memory workspace required to host the intermediate matrices generated by the IM2COL transform; and 2) the time to perform the IM2COL transform, which is not negligible for complex neural networks. This paper presents a portable high performance convolution algorithm based on the BLIS realization of the GEMM kernel that avoids the use of the intermediate memory by taking advantage of the BLIS structure. In addition, the proposed algorithm eliminates the cost of the explicit IM2COL transform, while maintaining the portability and performance of the underlying realization of GEMM in BLIS."},{"title":"Neural Networks for Semantic Gaze Analysis in XR Settings","source":"Lena Stubbemann, Dominik D\\\"urrschnabel, Robert Refflinghaus","docs_id":"2103.10451","section":["cs.CV","cs.HC","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Neural Networks for Semantic Gaze Analysis in XR Settings. Virtual-reality (VR) and augmented-reality (AR) technology is increasingly combined with eye-tracking. This combination broadens both fields and opens up new areas of application, in which visual perception and related cognitive processes can be studied in interactive but still well controlled settings. However, performing a semantic gaze analysis of eye-tracking data from interactive three-dimensional scenes is a resource-intense task, which so far has been an obstacle to economic use. In this paper we present a novel approach which minimizes time and information necessary to annotate volumes of interest (VOIs) by using techniques from object recognition. To do so, we train convolutional neural networks (CNNs) on synthetic data sets derived from virtual models using image augmentation techniques. We evaluate our method in real and virtual environments, showing that the method can compete with state-of-the-art approaches, while not relying on additional markers or preexisting databases but instead offering cross-platform use."},{"title":"Optimal Perimeter Guarding with Heterogeneous Robot Teams: Complexity\n  Analysis and Effective Algorithms","source":"Si Wei Feng and Jingjin Yu","docs_id":"1912.08591","section":["math.OC","cs.RO"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Optimal Perimeter Guarding with Heterogeneous Robot Teams: Complexity\n  Analysis and Effective Algorithms. We perform structural and algorithmic studies of significantly generalized versions of the optimal perimeter guarding (OPG) problem. As compared with the original OPG where robots are uniform, in this paper, many mobile robots with heterogeneous sensing capabilities are to be deployed to optimally guard a set of one-dimensional segments. Two complimentary formulations are investigated where one limits the number of available robots (OPG_LR) and the other seeks to minimize the total deployment cost (OPG_MC). In contrast to the original OPG which admits low-polynomial time solutions, both OPG_LR and OPG_MC are computationally intractable with OPG_LR being strongly NP-hard. Nevertheless, we develop fairly scalable pseudo-polynomial time algorithms for practical, fixed-parameter subcase of OPG_LR; we also develop pseudo-polynomial time algorithm for general OPG_MC and polynomial time algorithm for the fixed-parameter OPG_MC case. The applicability and effectiveness of selected algorithms are demonstrated through extensive numerical experiments."},{"title":"SoK: Achieving State Machine Replication in Blockchains based on\n  Repeated Consensus","source":"Silvia Bonomi and Antonella Del Pozzo and \\'Alvaro Garc\\'ia-P\\'erez\n  and Sara Tucci-Piergiovanni","docs_id":"2105.13732","section":["cs.DC"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"SoK: Achieving State Machine Replication in Blockchains based on\n  Repeated Consensus. This paper revisits the ubiquitous problem of achieving state machine replication in blockchains based on repeated consensus, like Tendermint. To achieve state machine replication in blockchains built on top of consensus, one needs to guarantee fairness of user transactions. A huge body of work has been carried out on the relation between state machine replication and consensus in the past years, in a variety of system models and with respect to varied problem specifications. We systematize this work by proposing novel and rigorous abstractions for state machine replication and repeated consensus in a system model that accounts for realistic blockchains in which blocks may contain several transactions issued by one or more users, and where validity and order of transactions within a block is determined by an external application-dependent function that can capture various approaches for order-fairness in the literature. Based on these abstractions, we propose a reduction from state machine replication to repeated consensus, such that user fairness is achieved using the consensus module as a black box. This approach allows to achieve fairness as an add-on on top of preexisting consensus modules in blockchains based on repeated consensus."},{"title":"Noisy population recovery in polynomial time","source":"Anindya De and Michael Saks and Sijian Tang","docs_id":"1602.07616","section":["cs.CC","cs.DS","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Noisy population recovery in polynomial time. In the noisy population recovery problem of Dvir et al., the goal is to learn an unknown distribution $f$ on binary strings of length $n$ from noisy samples. For some parameter $\\mu \\in [0,1]$, a noisy sample is generated by flipping each coordinate of a sample from $f$ independently with probability $(1-\\mu)\/2$. We assume an upper bound $k$ on the size of the support of the distribution, and the goal is to estimate the probability of any string to within some given error $\\varepsilon$. It is known that the algorithmic complexity and sample complexity of this problem are polynomially related to each other. We show that for $\\mu > 0$, the sample complexity (and hence the algorithmic complexity) is bounded by a polynomial in $k$, $n$ and $1\/\\varepsilon$ improving upon the previous best result of $\\mathsf{poly}(k^{\\log\\log k},n,1\/\\varepsilon)$ due to Lovett and Zhang. Our proof combines ideas from Lovett and Zhang with a \\emph{noise attenuated} version of M\\\"{o}bius inversion. In turn, the latter crucially uses the construction of \\emph{robust local inverse} due to Moitra and Saks."},{"title":"Non-Differentiable Supervised Learning with Evolution Strategies and\n  Hybrid Methods","source":"Karel Lenc, Erich Elsen, Tom Schaul, Karen Simonyan","docs_id":"1906.03139","section":["cs.NE","cs.LG","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Non-Differentiable Supervised Learning with Evolution Strategies and\n  Hybrid Methods. In this work we show that Evolution Strategies (ES) are a viable method for learning non-differentiable parameters of large supervised models. ES are black-box optimization algorithms that estimate distributions of model parameters; however they have only been used for relatively small problems so far. We show that it is possible to scale ES to more complex tasks and models with millions of parameters. While using ES for differentiable parameters is computationally impractical (although possible), we show that a hybrid approach is practically feasible in the case where the model has both differentiable and non-differentiable parameters. In this approach we use standard gradient-based methods for learning differentiable weights, while using ES for learning non-differentiable parameters - in our case sparsity masks of the weights. This proposed method is surprisingly competitive, and when parallelized over multiple devices has only negligible training time overhead compared to training with gradient descent. Additionally, this method allows to train sparse models from the first training step, so they can be much larger than when using methods that require training dense models first. We present results and analysis of supervised feed-forward models (such as MNIST and CIFAR-10 classification), as well as recurrent models, such as SparseWaveRNN for text-to-speech."},{"title":"Coverage in mmWave Cellular Networks with Base station Cooperation","source":"Diana Maamari, Natasha Devroye, Daniela Tuninetti","docs_id":"1503.05269","section":["cs.IT","cs.NI","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Coverage in mmWave Cellular Networks with Base station Cooperation. The presence of signal outage, due to shadowing and blockage, is expected to be the main bottleneck in millimeter wave (mmWave) networks. Moreover, with the anticipated vision that mmWave networks would have a dense deployment of base stations, interference from strong line-of-sight base stations increases too, thus further increasing the probability of outage. To address the issue of reducing outage, this paper explores the possibility of base station cooperation in the downlink of a mmWave heterogenous network. The main focus of this work is showing that, in a stochastic geometry framework, cooperation from randomly located base stations decreases outage probability. With the presumed vision that less severe fading will be experienced due to highly directional transmissions, one might expect that cooperation would increase the coverage probability; our numerical examples suggest that is in fact the case. Coverage probabilities are derived accounting for: different fading distributions, antenna directionality and blockage. Numerical results suggest that coverage with base station cooperation in dense mmWave systems and with no small scale fading considerably exceeds coverage with no cooperation. In contrast, an insignificant increase is reported when mmWave networks are less dense with a high probability of signal blockage and with Rayleigh fading."},{"title":"Blockchain for Genomics: A Systematic Literature Review","source":"Mohammed Alghazwi, Fatih Turkmen, Joeri van der Velde, Dimka\n  Karastoyanova","docs_id":"2111.10153","section":["cs.CR"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Blockchain for Genomics: A Systematic Literature Review. Human genomic data carry unique information about an individual and offer unprecedented opportunities for healthcare. The clinical interpretations derived from large genomic datasets can greatly improve healthcare and pave the way for personalized medicine. Sharing genomic datasets, however, pose major challenges, as genomic data is different from traditional medical data, indirectly revealing information about descendants and relatives of the data owner and carrying valid information even after the owner passes away. Therefore, stringent data ownership and control measures are required when dealing with genomic data. In order to provide secure and accountable infrastructure, blockchain technologies offer a promising alternative to traditional distributed systems. Indeed, the research on blockchain-based infrastructures tailored to genomics is on the rise. However, there is a lack of a comprehensive literature review that summarizes the current state-of-the-art methods in the applications of blockchain in genomics. In this paper, we systematically look at the existing work both commercial and academic, and discuss the major opportunities and challenges. Our study is driven by five research questions that we aim to answer in our review. We also present our projections of future research directions which we hope the researchers interested in the area can benefit from."},{"title":"Learning to Amend Facial Expression Representation via De-albino and\n  Affinity","source":"Jiawei Shi and Songhao Zhu and Zhiwei Liang","docs_id":"2103.10189","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Learning to Amend Facial Expression Representation via De-albino and\n  Affinity. Facial Expression Recognition (FER) is a classification task that points to face variants. Hence, there are certain affinity features between facial expressions, receiving little attention in the FER literature. Convolution padding, despite helping capture the edge information, causes erosion of the feature map simultaneously. After multi-layer filling convolution, the output feature map named albino feature definitely weakens the representation of the expression. To tackle these challenges, we propose a novel architecture named Amending Representation Module (ARM). ARM is a substitute for the pooling layer. Theoretically, it can be embedded in the back end of any network to deal with the Padding Erosion. ARM efficiently enhances facial expression representation from two different directions: 1) reducing the weight of eroded features to offset the side effect of padding, and 2) decomposing facial features to simplify representation learning. Experiments on public benchmarks prove that our ARM boosts the performance of FER remarkably. The validation accuracies are respectively 90.42% on RAF-DB, 65.2% on Affect-Net, and 58.71% on SFEW, exceeding current state-of-the-art methods. Our implementation and trained models are available at https:\/\/github.com\/JiaweiShiCV\/Amend-Representation-Module."},{"title":"Estimating Redundancy in Clinical Text","source":"Thomas Searle, Zina Ibrahim, James Teo, Richard JB Dobson","docs_id":"2105.11832","section":["cs.CL"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Estimating Redundancy in Clinical Text. The current mode of use of Electronic Health Record (EHR) elicits text redundancy. Clinicians often populate new documents by duplicating existing notes, then updating accordingly. Data duplication can lead to a propagation of errors, inconsistencies and misreporting of care. Therefore, quantifying information redundancy can play an essential role in evaluating innovations that operate on clinical narratives. This work is a quantitative examination of information redundancy in EHR notes. We present and evaluate two strategies to measure redundancy: an information-theoretic approach and a lexicosyntactic and semantic model. We evaluate the measures by training large Transformer-based language models using clinical text from a large openly available US-based ICU dataset and a large multi-site UK based Trust. By comparing the information-theoretic content of the trained models with open-domain language models, the language models trained using clinical text have shown ~1.5x to ~3x less efficient than open-domain corpora. Manual evaluation shows a high correlation with lexicosyntactic and semantic redundancy, with averages ~43 to ~65%."},{"title":"Design Verifiably Correct Model Patterns to Facilitate Modeling Medical\n  Best Practice Guidelines with Statecharts (Technical Report)","source":"Chunhui Guo, Zhicheng Fu, Zhenyu Zhang, Shangping Ren, Lui Sha","docs_id":"1811.00694","section":["cs.SE","cs.FL","cs.SY"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Design Verifiably Correct Model Patterns to Facilitate Modeling Medical\n  Best Practice Guidelines with Statecharts (Technical Report). Improving patient care safety is an ultimate objective for medical cyber-physical systems. A recent study shows that the patients' death rate can be significantly reduced by computerizing medical best practice guidelines. To facilitate the development of computerized medical best practice guidelines, statecharts are often used as a modeling tool because of their high resemblances to disease and treatment models and their capabilities to provide rapid prototyping and simulation for clinical validations. However, some implementations of statecharts, such as Yakindu statecharts, are priority-based and have synchronous execution semantics which makes it difficult to model certain functionalities that are essential in modeling medical guidelines, such as two-way communications and configurable execution orders. Rather than introducing new statechart elements or changing the statechart implementation's underline semantics, we use existing basic statechart elements to design model patterns for the commonly occurring issues. In particular, we show the design of model patterns for two-way communications and configurable execution orders and formally prove the correctness of these model patterns. We further use a simplified airway laser surgery scenario as a case study to demonstrate how the developed model patterns address the two-way communication and configurable execution order issues and their impact on validation and verification of medical safety properties."},{"title":"A Review on Explainability in Multimodal Deep Neural Nets","source":"Gargi Joshi, Rahee Walambe, Ketan Kotecha","docs_id":"2105.07878","section":["cs.AI","cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A Review on Explainability in Multimodal Deep Neural Nets. Artificial Intelligence techniques powered by deep neural nets have achieved much success in several application domains, most significantly and notably in the Computer Vision applications and Natural Language Processing tasks. Surpassing human-level performance propelled the research in the applications where different modalities amongst language, vision, sensory, text play an important role in accurate predictions and identification. Several multimodal fusion methods employing deep learning models are proposed in the literature. Despite their outstanding performance, the complex, opaque and black-box nature of the deep neural nets limits their social acceptance and usability. This has given rise to the quest for model interpretability and explainability, more so in the complex tasks involving multimodal AI methods. This paper extensively reviews the present literature to present a comprehensive survey and commentary on the explainability in multimodal deep neural nets, especially for the vision and language tasks. Several topics on multimodal AI and its applications for generic domains have been covered in this paper, including the significance, datasets, fundamental building blocks of the methods and techniques, challenges, applications, and future trends in this domain"},{"title":"Learning without feedback: Fixed random learning signals allow for\n  feedforward training of deep neural networks","source":"Charlotte Frenkel, Martin Lefebvre, David Bol","docs_id":"1909.01311","section":["stat.ML","cs.LG","cs.NE"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Learning without feedback: Fixed random learning signals allow for\n  feedforward training of deep neural networks. While the backpropagation of error algorithm enables deep neural network training, it implies (i) bidirectional synaptic weight transport and (ii) update locking until the forward and backward passes are completed. Not only do these constraints preclude biological plausibility, but they also hinder the development of low-cost adaptive smart sensors at the edge, as they severely constrain memory accesses and entail buffering overhead. In this work, we show that the one-hot-encoded labels provided in supervised classification problems, denoted as targets, can be viewed as a proxy for the error sign. Therefore, their fixed random projections enable a layerwise feedforward training of the hidden layers, thus solving the weight transport and update locking problems while relaxing the computational and memory requirements. Based on these observations, we propose the direct random target projection (DRTP) algorithm and demonstrate that it provides a tradeoff between accuracy and computational cost that is suitable for adaptive edge computing devices."},{"title":"Crosstalk Noise based Configurable Computing: A New Paradigm for Digital\n  Electronics","source":"Naveen Kumar Macha, Md Arif Iqbal, Bhavana Tejaswini Repalle, Sehtab\n  Hossain, Mostafizur Rahman","docs_id":"2004.08040","section":["cs.ET"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Crosstalk Noise based Configurable Computing: A New Paradigm for Digital\n  Electronics. The past few decades have seen exponential growth in capabilities of digital electronics primarily due to the ability to scale Integrated Circuits (ICs) to smaller dimensions while attaining power and performance benefits. That scalability is now being challenged due to the lack of scaled transistor performance and also manufacturing complexities [1]-[5]. In addition, the growing cyber threat in fabless manufacturing era poses a new front that modern ICs need to withstand. We present a new noise based computing where the interconnect interference between nanoscale metal lines is intentionally engineered to exhibit programmable Boolean logic behavior. The reliance on just coupling between metal lines and not on transistors for computing, and the programmability are the foundations for better scalability, and security by obscurity. Here, we show experimental evidence of a functioning Crosstalk computing chip at 65nm technology. Our demonstration of computing constructs, gate level configurability and utilization of foundry processes show feasibility. These results in conjunction with our simulation results at 7nm for various benchmarks, which show over 48%, 57%, and 10% density, power and performance respectively, gains over equivalent CMOS in the best case, show potentials. The benefits of Crosstalk circuits and inherent programmable features set it apart and make it a promising prospect for future electronics."},{"title":"Measures of path-based nonlinear expansion rates and Lagrangian\n  uncertainty in stochastic flows","source":"Michal Branicki and Kenneth Uda","docs_id":"1810.07567","section":["math.DS","cs.IT","math.IT","math.PR"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Measures of path-based nonlinear expansion rates and Lagrangian\n  uncertainty in stochastic flows. We develop a probabilistic characterisation of trajectorial expansion rates in non-autonomous stochastic dynamical systems that can be defined over a finite time interval and used for the subsequent uncertainty quantification in Lagrangian (trajectory-based) predictions. These expansion rates are quantified via certain divergences (pre-metrics) between probability measures induced by the laws of the stochastic flow associated with the underlying dynamics. We construct scalar fields of finite-time divergence\/expansion rates, show their existence and space-time continuity for general stochastic flows. Combining these divergence rate fields with our 'information inequalities' derived in allows for quantification and mitigation of the uncertainty in path-based observables estimated from simplified models in a way that is amenable to algorithmic implementations, and it can be utilised in information-geometric analysis of statistical estimation and inference, as well as in a data-driven machine\/deep learning of coarse-grained models. We also derive a link between the divergence rates and finite-time Lyapunov exponents for probability measures and for path-based observables."},{"title":"Toxicity Detection can be Sensitive to the Conversational Context","source":"Alexandros Xenos, John Pavlopoulos, Ion Androutsopoulos, Lucas Dixon,\n  Jeffrey Sorensen and Leo Laugier","docs_id":"2111.10223","section":["cs.CL"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Toxicity Detection can be Sensitive to the Conversational Context. User posts whose perceived toxicity depends on the conversational context are rare in current toxicity detection datasets. Hence, toxicity detectors trained on existing datasets will also tend to disregard context, making the detection of context-sensitive toxicity harder when it does occur. We construct and publicly release a dataset of 10,000 posts with two kinds of toxicity labels: (i) annotators considered each post with the previous one as context; and (ii) annotators had no additional context. Based on this, we introduce a new task, context sensitivity estimation, which aims to identify posts whose perceived toxicity changes if the context (previous post) is also considered. We then evaluate machine learning systems on this task, showing that classifiers of practical quality can be developed, and we show that data augmentation with knowledge distillation can improve the performance further. Such systems could be used to enhance toxicity detection datasets with more context-dependent posts, or to suggest when moderators should consider the parent posts, which often may be unnecessary and may otherwise introduce significant additional cost."},{"title":"Smaller generalization error derived for a deep residual neural network\n  compared to shallow networks","source":"Aku Kammonen, Jonas Kiessling, Petr Plech\\'a\\v{c}, Mattias Sandberg,\n  Anders Szepessy, Ra\\'ul Tempone","docs_id":"2010.01887","section":["math.NA","cs.NA","math.OC"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Smaller generalization error derived for a deep residual neural network\n  compared to shallow networks. Estimates of the generalization error are proved for a residual neural network with $L$ random Fourier features layers $\\bar z_{\\ell+1}=\\bar z_\\ell + \\mathrm{Re}\\sum_{k=1}^K\\bar b_{\\ell k}e^{\\mathrm{i}\\omega_{\\ell k}\\bar z_\\ell}+ \\mathrm{Re}\\sum_{k=1}^K\\bar c_{\\ell k}e^{\\mathrm{i}\\omega'_{\\ell k}\\cdot x}$. An optimal distribution for the frequencies $(\\omega_{\\ell k},\\omega'_{\\ell k})$ of the random Fourier features $e^{\\mathrm{i}\\omega_{\\ell k}\\bar z_\\ell}$ and $e^{\\mathrm{i}\\omega'_{\\ell k}\\cdot x}$ is derived. This derivation is based on the corresponding generalization error for the approximation of the function values $f(x)$. The generalization error turns out to be smaller than the estimate ${\\|\\hat f\\|^2_{L^1(\\mathbb{R}^d)}}\/{(KL)}$ of the generalization error for random Fourier features with one hidden layer and the same total number of nodes $KL$, in the case the $L^\\infty$-norm of $f$ is much less than the $L^1$-norm of its Fourier transform $\\hat f$. This understanding of an optimal distribution for random features is used to construct a new training method for a deep residual network. Promising performance of the proposed new algorithm is demonstrated in computational experiments."},{"title":"Hardware Accelerated SDR Platform for Adaptive Air Interfaces","source":"Tarik Kazaz, Christophe Van Praet, Merima Kulin, Pieter Willemen,\n  Ingrid Moerman","docs_id":"1705.00115","section":["cs.NI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Hardware Accelerated SDR Platform for Adaptive Air Interfaces. The future 5G wireless infrastructure will support any-to-any connectivity between densely deployed smart objects that form the emerging paradigm known as the Internet of Everything (IoE). Compared to traditional wireless networks that enable communication between devices using a single technology, 5G networks will need to support seamless connectivity between heterogeneous wireless objects and IoE networks. To tackle the complexity and versatility of future IoE networks, 5G will need to guarantee optimal usage of both spectrum and energy resources and further support technology-agnostic connectivity between objects. One way to realize this is to combine intelligent network control with adaptive software defined air interfaces. In this paper, a flexible and compact platform is proposed for on-the-fly composition of low-power adaptive air interfaces, based on hardware\/software co-processing. Compared to traditional Software Defined Radio (SDR) systems that perform computationally-intensive signal processing algorithms in software, consume significantly power and have a large form factor, the proposed platform uses modern hybrid FPGA technology combined with novel ideas such as RF Network-on-Chip (RFNoC) and partial reconfiguration. The resulting system enables composition of reconfigurable air interfaces based on hardware\/software co-processing on a single chip, allowing high processing throughput, at a smaller form factor and reduced power consumption."},{"title":"View-Invariant Probabilistic Embedding for Human Pose","source":"Jennifer J. Sun, Jiaping Zhao, Liang-Chieh Chen, Florian Schroff,\n  Hartwig Adam, Ting Liu","docs_id":"1912.01001","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"View-Invariant Probabilistic Embedding for Human Pose. Depictions of similar human body configurations can vary with changing viewpoints. Using only 2D information, we would like to enable vision algorithms to recognize similarity in human body poses across multiple views. This ability is useful for analyzing body movements and human behaviors in images and videos. In this paper, we propose an approach for learning a compact view-invariant embedding space from 2D joint keypoints alone, without explicitly predicting 3D poses. Since 2D poses are projected from 3D space, they have an inherent ambiguity, which is difficult to represent through a deterministic mapping. Hence, we use probabilistic embeddings to model this input uncertainty. Experimental results show that our embedding model achieves higher accuracy when retrieving similar poses across different camera views, in comparison with 2D-to-3D pose lifting models. We also demonstrate the effectiveness of applying our embeddings to view-invariant action recognition and video alignment. Our code is available at https:\/\/github.com\/google-research\/google-research\/tree\/master\/poem."},{"title":"Minimum Complexity Pursuit for Universal Compressed Sensing","source":"Shirin Jalali, Arian Maleki, Richard Baraniuk","docs_id":"1208.5814","section":["cs.IT","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Minimum Complexity Pursuit for Universal Compressed Sensing. The nascent field of compressed sensing is founded on the fact that high-dimensional signals with \"simple structure\" can be recovered accurately from just a small number of randomized samples. Several specific kinds of structures have been explored in the literature, from sparsity and group sparsity to low-rankness. However, two fundamental questions have been left unanswered, namely: What are the general abstract meanings of \"structure\" and \"simplicity\"? And do there exist universal algorithms for recovering such simple structured objects from fewer samples than their ambient dimension? In this paper, we address these two questions. Using algorithmic information theory tools such as the Kolmogorov complexity, we provide a unified definition of structure and simplicity. Leveraging this new definition, we develop and analyze an abstract algorithm for signal recovery motivated by Occam's Razor.Minimum complexity pursuit (MCP) requires just O(3\\kappa) randomized samples to recover a signal of complexity \\kappa and ambient dimension n. We also discuss the performance of MCP in the presence of measurement noise and with approximately simple signals."},{"title":"User Review-Based Change File Localization for Mobile Applications","source":"Yu Zhou, Yanqi Su, Taolue Chen, Zhiqiu Huang, Harald Gall, Sebastiano\n  Panichella","docs_id":"1903.00894","section":["cs.SE"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"User Review-Based Change File Localization for Mobile Applications. In the current mobile app development, novel and emerging DevOps practices (e.g., Continuous Delivery, Integration, and user feedback analysis) and tools are becoming more widespread. For instance, the integration of user feedback (provided in the form of user reviews) in the software release cycle represents a valuable asset for the maintenance and evolution of mobile apps. To fully make use of these assets, it is highly desirable for developers to establish semantic links between the user reviews and the software artefacts to be changed (e.g., source code and documentation), and thus to localize the potential files to change for addressing the user feedback. In this paper, we propose RISING (Review Integration via claSsification, clusterIng, and linkiNG), an automated approach to support the continuous integration of user feedback via classification, clustering, and linking of user reviews. RISING leverages domain-specific constraint information and semi-supervised learning to group user reviews into multiple fine-grained clusters concerning similar users' requests. Then, by combining the textual information from both commit messages and source code, it automatically localizes potential change files to accommodate the users' requests. Our empirical studies demonstrate that the proposed approach outperforms the state-of-the-art baseline work in terms of clustering and localization accuracy, and thus produces more reliable results."},{"title":"Is It Safe to Uplift This Patch? An Empirical Study on Mozilla Firefox","source":"Marco Castelluccio, Le An and Foutse Khomh","docs_id":"1709.08852","section":["cs.SE"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Is It Safe to Uplift This Patch? An Empirical Study on Mozilla Firefox. In rapid release development processes, patches that fix critical issues, or implement high-value features are often promoted directly from the development channel to a stabilization channel, potentially skipping one or more stabilization channels. This practice is called patch uplift. Patch uplift is risky, because patches that are rushed through the stabilization phase can end up introducing regressions in the code. This paper examines patch uplift operations at Mozilla, with the aim to identify the characteristics of uplifted patches that introduce regressions. Through statistical and manual analyses, we quantitatively and qualitatively investigate the reasons behind patch uplift decisions and the characteristics of uplifted patches that introduced regressions. Additionally, we interviewed three Mozilla release managers to understand organizational factors that affect patch uplift decisions and outcomes. Results show that most patches are uplifted because of a wrong functionality or a crash. Uplifted patches that lead to faults tend to have larger patch size, and most of the faults are due to semantic or memory errors in the patches. Also, release managers are more inclined to accept patch uplift requests that concern certain specific components, and-or that are submitted by certain specific developers."},{"title":"Signal recovery from a few linear measurements of its high-order spectra","source":"Tamir Bendory, Dan Edidin, Shay Kreymer","docs_id":"2103.01551","section":["cs.IT","eess.SP","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Signal recovery from a few linear measurements of its high-order spectra. The $q$-th order spectrum is a polynomial of degree $q$ in the entries of a signal $x\\in\\mathbb{C}^N$, which is invariant under circular shifts of the signal. For $q\\geq 3$, this polynomial determines the signal uniquely, up to a circular shift, and is called a high-order spectrum. The high-order spectra, and in particular the bispectrum ($q=3$) and the trispectrum ($q=4$), play a prominent role in various statistical signal processing and imaging applications, such as phase retrieval and single-particle reconstruction. However, the dimension of the $q$-th order spectrum is $N^{q-1}$, far exceeding the dimension of $x$, leading to increased computational load and storage requirements. In this work, we show that it is unnecessary to store and process the full high-order spectra: a signal can be characterized uniquely, up to symmetries, from only $N+1$ linear measurements of its high-order spectra. The proof relies on tools from algebraic geometry and is corroborated by numerical experiments."},{"title":"An $O(\\log n)$-approximation for the Set Cover Problem with Set\n  Ownership","source":"Mira Gonen and Yuval Shavitt","docs_id":"0807.3326","section":["cs.NI","cs.CC"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"An $O(\\log n)$-approximation for the Set Cover Problem with Set\n  Ownership. In highly distributed Internet measurement systems distributed agents periodically measure the Internet using a tool called {\\tt traceroute}, which discovers a path in the network graph. Each agent performs many traceroute measurement to a set of destinations in the network, and thus reveals a portion of the Internet graph as it is seen from the agent locations. In every period we need to check whether previously discovered edges still exist in this period, a process termed {\\em validation}. For this end we maintain a database of all the different measurements performed by each agent. Our aim is to be able to {\\em validate} the existence of all previously discovered edges in the minimum possible time. In this work we formulate the validation problem as a generalization of the well know set cover problem. We reduce the set cover problem to the validation problem, thus proving that the validation problem is ${\\cal NP}$-hard. We present a $O(\\log n)$-approximation algorithm to the validation problem, where $n$ in the number of edges that need to be validated. We also show that unless ${\\cal P = NP}$ the approximation ratio of the validation problem is $\\Omega(\\log n)$."},{"title":"Variational Information Bottleneck for Effective Low-Resource\n  Fine-Tuning","source":"Rabeeh Karimi Mahabadi, Yonatan Belinkov, James Henderson","docs_id":"2106.05469","section":["cs.CL"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Variational Information Bottleneck for Effective Low-Resource\n  Fine-Tuning. While large-scale pretrained language models have obtained impressive results when fine-tuned on a wide variety of tasks, they still often suffer from overfitting in low-resource scenarios. Since such models are general-purpose feature extractors, many of these features are inevitably irrelevant for a given target task. We propose to use Variational Information Bottleneck (VIB) to suppress irrelevant features when fine-tuning on low-resource target tasks, and show that our method successfully reduces overfitting. Moreover, we show that our VIB model finds sentence representations that are more robust to biases in natural language inference datasets, and thereby obtains better generalization to out-of-domain datasets. Evaluation on seven low-resource datasets in different tasks shows that our method significantly improves transfer learning in low-resource scenarios, surpassing prior work. Moreover, it improves generalization on 13 out of 15 out-of-domain natural language inference benchmarks. Our code is publicly available in https:\/\/github.com\/rabeehk\/vibert."},{"title":"Accuracy-Efficiency Trade-Offs and Accountability in Distributed ML\n  Systems","source":"A. Feder Cooper, Karen Levy, Christopher De Sa","docs_id":"2007.02203","section":["cs.CY","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Accuracy-Efficiency Trade-Offs and Accountability in Distributed ML\n  Systems. Trade-offs between accuracy and efficiency pervade law, public health, and other non-computing domains, which have developed policies to guide how to balance the two in conditions of uncertainty. While computer science also commonly studies accuracy-efficiency trade-offs, their policy implications remain poorly examined. Drawing on risk assessment practices in the US, we argue that, since examining these trade-offs has been useful for guiding governance in other domains, we need to similarly reckon with these trade-offs in governing computer systems. We focus our analysis on distributed machine learning systems. Understanding the policy implications in this area is particularly urgent because such systems, which include autonomous vehicles, tend to be high-stakes and safety-critical. We 1) describe how the trade-off takes shape for these systems, 2) highlight gaps between existing US risk assessment standards and what these systems require to be properly assessed, and 3) make specific calls to action to facilitate accountability when hypothetical risks concerning the accuracy-efficiency trade-off become realized as accidents in the real world. We close by discussing how such accountability mechanisms encourage more just, transparent governance aligned with public values."},{"title":"Dual Graph-Laplacian PCA: A Closed-Form Solution for Bi-clustering to\n  Find \"Checkerboard\" Structures on Gene Expression Data","source":"Jin-Xing Liu, Chun-Mei Feng, Xiang-Zhen Kong, Yong Xu","docs_id":"1901.06794","section":["q-bio.GN","cs.CE","q-bio.QM"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Dual Graph-Laplacian PCA: A Closed-Form Solution for Bi-clustering to\n  Find \"Checkerboard\" Structures on Gene Expression Data. In the context of cancer, internal \"checkerboard\" structures are normally found in the matrices of gene expression data, which correspond to genes that are significantly up- or down-regulated in patients with specific types of tumors. In this paper, we propose a novel method, called dual graph-regularization principal component analysis (DGPCA). The main innovation of this method is that it simultaneously considers the internal geometric structures of the condition manifold and the gene manifold. Specifically, we obtain principal components (PCs) to represent the data and approximate the cluster membership indicators through Laplacian embedding. This new method is endowed with internal geometric structures, such as the condition manifold and gene manifold, which are both suitable for bi-clustering. A closed-form solution is provided for DGPCA. We apply this new method to simultaneously cluster genes and conditions (e.g., different samples) with the aim of finding internal \"checkerboard\" structures on gene expression data, if they exist. Then, we use this new method to identify regulatory genes under the particular conditions and to compare the results with those of other state-of-the-art PCA-based methods. Promising results on gene expression data have been verified by extensive experiments"},{"title":"Communication-Avoiding Optimization Methods for Distributed\n  Massive-Scale Sparse Inverse Covariance Estimation","source":"Penporn Koanantakool, Alnur Ali, Ariful Azad, Aydin Buluc, Dmitriy\n  Morozov, Leonid Oliker, Katherine Yelick, Sang-Yun Oh","docs_id":"1710.10769","section":["stat.ML","cs.DC","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Communication-Avoiding Optimization Methods for Distributed\n  Massive-Scale Sparse Inverse Covariance Estimation. Across a variety of scientific disciplines, sparse inverse covariance estimation is a popular tool for capturing the underlying dependency relationships in multivariate data. Unfortunately, most estimators are not scalable enough to handle the sizes of modern high-dimensional data sets (often on the order of terabytes), and assume Gaussian samples. To address these deficiencies, we introduce HP-CONCORD, a highly scalable optimization method for estimating a sparse inverse covariance matrix based on a regularized pseudolikelihood framework, without assuming Gaussianity. Our parallel proximal gradient method uses a novel communication-avoiding linear algebra algorithm and runs across a multi-node cluster with up to 1k nodes (24k cores), achieving parallel scalability on problems with up to ~819 billion parameters (1.28 million dimensions); even on a single node, HP-CONCORD demonstrates scalability, outperforming a state-of-the-art method. We also use HP-CONCORD to estimate the underlying dependency structure of the brain from fMRI data, and use the result to identify functional regions automatically. The results show good agreement with a clustering from the neuroscience literature."},{"title":"Differentially Private Regret Minimization in Episodic Markov Decision\n  Processes","source":"Sayak Ray Chowdhury, Xingyu Zhou","docs_id":"2112.10599","section":["cs.LG","cs.CR","math.PR"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Differentially Private Regret Minimization in Episodic Markov Decision\n  Processes. We study regret minimization in finite horizon tabular Markov decision processes (MDPs) under the constraints of differential privacy (DP). This is motivated by the widespread applications of reinforcement learning (RL) in real-world sequential decision making problems, where protecting users' sensitive and private information is becoming paramount. We consider two variants of DP -- joint DP (JDP), where a centralized agent is responsible for protecting users' sensitive data and local DP (LDP), where information needs to be protected directly on the user side. We first propose two general frameworks -- one for policy optimization and another for value iteration -- for designing private, optimistic RL algorithms. We then instantiate these frameworks with suitable privacy mechanisms to satisfy JDP and LDP requirements, and simultaneously obtain sublinear regret guarantees. The regret bounds show that under JDP, the cost of privacy is only a lower order additive term, while for a stronger privacy protection under LDP, the cost suffered is multiplicative. Finally, the regret bounds are obtained by a unified analysis, which, we believe, can be extended beyond tabular MDPs."},{"title":"Emergent Behaviors over Signed Random Dynamical Networks:\n  Relative-State-Flipping Model","source":"Guodong Shi, Alexandre Proutiere, Mikael Johansson, John. S. Baras,\n  and Karl H. Johansson","docs_id":"1412.1990","section":["cs.SI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Emergent Behaviors over Signed Random Dynamical Networks:\n  Relative-State-Flipping Model. We study asymptotic dynamical patterns that emerge among a set of nodes interacting in a dynamically evolving signed random network, where positive links carry out standard consensus and negative links induce relative-state flipping. A sequence of deterministic signed graphs define potential node interactions that take place independently. Each node receives a positive recommendation consistent with the standard consensus algorithm from its positive neighbors, and a negative recommendation defined by relative-state flipping from its negative neighbors. After receiving these recommendations, each node puts a deterministic weight to each recommendation, and then encodes these weighted recommendations in its state update through stochastic attentions defined by two Bernoulli random variables. We establish a number of conditions regarding almost sure convergence and divergence of the node states. We also propose a condition for almost sure state clustering for essentially weakly balanced graphs, with the help of several martingale convergence lemmas. Some fundamental differences on the impact of the deterministic weights and stochastic attentions to the node state evolution are highlighted between the current relative-state-flipping model and the state-flipping model considered in Altafini 2013 and Shi et al. 2014."},{"title":"Which Digraphs with Ring Structure are Essentially Cyclic?","source":"Rafig Agaev and Pavel Chebotarev","docs_id":"0910.3113","section":["math.CO","cs.DM","cs.MA","math.SP"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Which Digraphs with Ring Structure are Essentially Cyclic?. We say that a digraph is essentially cyclic if its Laplacian spectrum is not completely real. The essential cyclicity implies the presence of directed cycles, but not vice versa. The problem of characterizing essential cyclicity in terms of graph topology is difficult and yet unsolved. Its solution is important for some applications of graph theory, including that in decentralized control. In the present paper, this problem is solved with respect to the class of digraphs with ring structure, which models some typical communication networks. It is shown that the digraphs in this class are essentially cyclic, except for certain specified digraphs. The main technical tool we employ is the Chebyshev polynomials of the second kind. A by-product of this study is a theorem on the zeros of polynomials that differ by one from the products of Chebyshev polynomials of the second kind. We also consider the problem of essential cyclicity for weighted digraphs and enumerate the spanning trees in some digraphs with ring structure."},{"title":"A non-iterative domain decomposition method for the interaction between\n  a fluid and a thick structure","source":"Anyastassia Seboldt, Martina Buka\\v{c}","docs_id":"2007.00781","section":["math.NA","cs.NA"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A non-iterative domain decomposition method for the interaction between\n  a fluid and a thick structure. This work focuses on the development and analysis of a partitioned numerical method for moving domain, fluid-structure interaction problems. We model the fluid using incompressible Navier-Stokes equations, and the structure using linear elasticity equations. We assume that the structure is thick, i.e., described in the same dimension as the fluid. We propose a non-iterative, domain decomposition method where the fluid and the structure sub-problems are solved separately. The method is based on generalized Robin boundary conditions, which are used in both fluid and structure sub-problems. Using energy estimates, we show that the proposed method applied to a moving domain problem is unconditionally stable. We also analyze the convergence of the method and show $\\mathcal{O}(\\Delta t^\\frac12)$ convergence in time and optimal convergence in space. Numerical examples are used to demonstrate the performance of the method. In particular, we explore the relation between the combination parameter used in the derivation of the generalized Robin boundary conditions and the accuracy of the scheme. We also compare the performance of the method to a monolithic solver."},{"title":"Securing of Unmanned Aerial Systems (UAS) against security threats using\n  human immune system","source":"Reza Fotohi","docs_id":"2003.04984","section":["cs.CR","cs.AI","cs.PF"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Securing of Unmanned Aerial Systems (UAS) against security threats using\n  human immune system. UASs form a large part of the fighting ability of the advanced military forces. In particular, these systems that carry confidential information are subject to security attacks. Accordingly, an Intrusion Detection System (IDS) has been proposed in the proposed design to protect against the security problems using the human immune system (HIS). The IDSs are used to detect and respond to attempts to compromise the target system. Since the UASs operate in the real world, the testing and validation of these systems with a variety of sensors is confronted with problems. This design is inspired by HIS. In the mapping, insecure signals are equivalent to an antigen that are detected by antibody-based training patterns and removed from the operation cycle. Among the main uses of the proposed design are the quick detection of intrusive signals and quarantining their activity. Moreover, SUAS-HIS method is evaluated here via extensive simulations carried out in NS-3 environment. The simulation results indicate that the UAS network performance metrics are improved in terms of false positive rate, false negative rate, detection rate, and packet delivery rate."},{"title":"An Arithmetic Analogue of Fox's Triangle Removal Argument","source":"Pooya Hatami, Sushant Sachdeva, Madhur Tulsiani","docs_id":"1304.4921","section":["math.CO","cs.DM"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"An Arithmetic Analogue of Fox's Triangle Removal Argument. We give an arithmetic version of the recent proof of the triangle removal lemma by Fox [Fox11], for the group $\\mathbb{F}_2^n$. A triangle in $\\mathbb{F}_2^n$ is a triple $(x,y,z)$ such that $x+y+z = 0$. The triangle removal lemma for $\\mathbb{F}_2^n$ states that for every $\\epsilon > 0$ there is a $\\delta > 0$, such that if a subset $A$ of $\\mathbb{F}_2^n$ requires the removal of at least $\\epsilon \\cdot 2^n$ elements to make it triangle-free, then it must contain at least $\\delta \\cdot 2^{2n}$ triangles. This problem was first studied by Green [Gre05] who proved a lower bound on $\\delta$ using an arithmetic regularity lemma. Regularity based lower bounds for triangle removal in graphs were recently improved by Fox and we give a direct proof of an analogous improvement for triangle removal in $\\mathbb{F}_2^n$. The improved lower bound was already known to follow (for triangle-removal in all groups), using Fox's removal lemma for directed cycles and a reduction by Kr\\'{a}l, Serra and Vena [KSV09] (see [Fox11,CF13]). The purpose of this note is to provide a direct Fourier-analytic proof for the group $\\mathbb{F}_2^n.$"},{"title":"Efficient and principled score estimation with Nystr\\\"om kernel\n  exponential families","source":"Danica J. Sutherland, Heiko Strathmann, Michael Arbel, Arthur Gretton","docs_id":"1705.08360","section":["stat.ML","cs.LG","stat.ME"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Efficient and principled score estimation with Nystr\\\"om kernel\n  exponential families. We propose a fast method with statistical guarantees for learning an exponential family density model where the natural parameter is in a reproducing kernel Hilbert space, and may be infinite-dimensional. The model is learned by fitting the derivative of the log density, the score, thus avoiding the need to compute a normalization constant. Our approach improves the computational efficiency of an earlier solution by using a low-rank, Nystr\\\"om-like solution. The new solution retains the consistency and convergence rates of the full-rank solution (exactly in Fisher distance, and nearly in other distances), with guarantees on the degree of cost and storage reduction. We evaluate the method in experiments on density estimation and in the construction of an adaptive Hamiltonian Monte Carlo sampler. Compared to an existing score learning approach using a denoising autoencoder, our estimator is empirically more data-efficient when estimating the score, runs faster, and has fewer parameters (which can be tuned in a principled and interpretable way), in addition to providing statistical guarantees."},{"title":"Pixel-wise Orthogonal Decomposition for Color Illumination Invariant and\n  Shadow-free Image","source":"Liangqiong Qu, Jiandong Tian, Zhi Han, and Yandong Tang","docs_id":"1407.0010","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Pixel-wise Orthogonal Decomposition for Color Illumination Invariant and\n  Shadow-free Image. In this paper, we propose a novel, effective and fast method to obtain a color illumination invariant and shadow-free image from a single outdoor image. Different from state-of-the-art methods for shadow-free image that either need shadow detection or statistical learning, we set up a linear equation set for each pixel value vector based on physically-based shadow invariants, deduce a pixel-wise orthogonal decomposition for its solutions, and then get an illumination invariant vector for each pixel value vector on an image. The illumination invariant vector is the unique particular solution of the linear equation set, which is orthogonal to its free solutions. With this illumination invariant vector and Lab color space, we propose an algorithm to generate a shadow-free image which well preserves the texture and color information of the original image. A series of experiments on a diverse set of outdoor images and the comparisons with the state-of-the-art methods validate our method."},{"title":"Anchor: Locating Android Framework-specific Crashing Faults","source":"Pingfan Kong, Li Li, Jun Gao, Timoth\\'ee Riom, Yanjie Zhao,\n  Tegawend\\'e F. Bissyand\\'e, Jacques Klein","docs_id":"2008.01676","section":["cs.SE"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Anchor: Locating Android Framework-specific Crashing Faults. Android framework-specific app crashes are hard to debug. Indeed, the callback-based event-driven mechanism of Android challenges crash localization techniques that are developed for traditional Java programs. The key challenge stems from the fact that the buggy code location may not even be listed within the stack trace. For example, our empirical study on 500 framework-specific crashes from an open benchmark has revealed that 37 percent of the crash types are related to bugs that are outside the stack traces. Moreover, Android programs are a mixture of code and extra-code artifacts such as the Manifest file. The fact that any artifact can lead to failures in the app execution creates the need to position the localization target beyond the code realm. In this paper, we propose Anchor, a two-phase suspicious bug location suggestion tool. Anchor specializes in finding crash-inducing bugs outside the stack trace. Anchor is lightweight and source code independent since it only requires the crash message and the apk file to locate the fault. Experimental results, collected via cross-validation and in-the-wild dataset evaluation, show that Anchor is effective in locating Android framework-specific crashing faults."},{"title":"Fusion of Mobile Device Signal Data Attributes Enables Multi-Protocol\n  Entity Resolution and Enhanced Large-Scale Tracking","source":"Brian Thompson (The MITRE Corporation), Dave Cedel (The MITRE\n  Corporation), Jeremy Martin (The MITRE Corporation), Peter Ryan (The MITRE\n  Corporation), Sarah Kern (The MITRE Corporation)","docs_id":"1906.02686","section":["eess.SP","cs.CR"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Fusion of Mobile Device Signal Data Attributes Enables Multi-Protocol\n  Entity Resolution and Enhanced Large-Scale Tracking. Use of persistent identifiers in wireless communication protocols is a known privacy concern as they can be used to track the location of mobile devices. Furthermore, inherent structure in the assignment of hardware identifiers as well as upper-layer network protocol data attributes can leak additional device information. We introduce SEXTANT, a computational framework that combines improvements on previously published device identification techniques with novel spatio-temporal correlation algorithms to perform multi-protocol entity resolution, enabling large-scale tracking of mobile devices across protocol domains. Experiments using simulated data representing Las Vegas residents and visitors over a 30-day period, consisting of about 300,000 multi-protocol mobile devices generating over 200 million sensor observations, demonstrate SEXTANT's ability to perform effectively at scale while being robust to data heterogeneity, sparsity, and noise, highlighting the urgent need for the adoption of new standards to protect the privacy of mobile device users."},{"title":"Levels of Automation for a Mobile Robot Teleoperated by a Caregiver","source":"Samuel Olatunji, Andre Potenza, Andrey Kiselev, Tal Oron-Gilad, Amy\n  Loutfi, Yael Edan","docs_id":"2107.09992","section":["cs.RO"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Levels of Automation for a Mobile Robot Teleoperated by a Caregiver. Caregivers in eldercare can benefit from telepresence robots that allow them to perform a variety of tasks remotely. In order for such robots to be operated effectively and efficiently by non-technical users, it is important to examine if and how the robotic system's level of automation (LOA) impacts their performance. The objective of this work was to develop suitable LOA modes for a mobile robotic telepresence (MRP) system for eldercare and assess their influence on users' performance, workload, awareness of the environment and usability at two different levels of task complexity. For this purpose, two LOA modes were implemented on the MRP platform: assisted teleoperation (low LOA mode) and autonomous navigation (high LOA mode). The system was evaluated in a user study with 20 participants, who, in the role of the caregiver, navigated the robot through a home-like environment to perform various control and perception tasks. Results revealed that performance improved at high LOA when the task complexity was low. However, when task complexity increased, lower LOA improved performance. This opposite trend was also observed in the results for workload and situation awareness. We discuss the results in terms of the LOAs' impact on users' attitude towards automation and implications on usability."},{"title":"On Unimodality of Independence Polynomials of Trees","source":"Ron Yosef, Matan Mizrachi and Ohr Kadrawi","docs_id":"2101.06744","section":["cs.DM","math.CO"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"On Unimodality of Independence Polynomials of Trees. An independent set in a graph is a set of pairwise non-adjacent vertices. The independence number $\\alpha{(G)}$ is the size of a maximum independent set in the graph $G$. The independence polynomial of a graph is the generating function for the sequence of numbers of independent sets of each size. In other words, the $k$-th coefficient of the independence polynomial equals the number of independent sets comprised of $k$ vertices. For instance, the degree of the independence polynomial of the graph $G$ is equal to $\\alpha{(G)}$. In 1987, Alavi, Malde, Schwenk, and Erd{\\\"o}s conjectured that the independence polynomial of a tree is unimodal. In what follows, we provide support to this assertion considering trees with up to $20$ vertices. Moreover, we show that the corresponding independence polynomials are log-concave and, consequently, unimodal. The algorithm computing the independence polynomial of a given tree makes use of a database of non-isomorphic unlabeled trees to prevent repeated computations."},{"title":"Computing Optimal Repairs for Functional Dependencies","source":"Ester Livshits, Benny Kimelfeld, Sudeepa Roy","docs_id":"1712.07705","section":["cs.DB"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Computing Optimal Repairs for Functional Dependencies. We investigate the complexity of computing an optimal repair of an inconsistent database, in the case where integrity constraints are Functional Dependencies (FDs). We focus on two types of repairs: an optimal subset repair (optimal S-repair) that is obtained by a minimum number of tuple deletions, and an optimal update repair (optimal U-repair) that is obtained by a minimum number of value (cell) updates. For computing an optimal S-repair, we present a polynomial-time algorithm that succeeds on certain sets of FDs and fails on others. We prove the following about the algorithm. When it succeeds, it can also incorporate weighted tuples and duplicate tuples. When it fails, the problem is NP-hard, and in fact, APX-complete (hence, cannot be approximated better than some constant). Thus, we establish a dichotomy in the complexity of computing an optimal S-repair. We present general analysis techniques for the complexity of computing an optimal U-repair, some based on the dichotomy for S-repairs. We also draw a connection to a past dichotomy in the complexity of finding a \"most probable database\" that satisfies a set of FDs with a single attribute on the left hand side; the case of general FDs was left open, and we show how our dichotomy provides the missing generalization and thereby settles the open problem."},{"title":"A multi-modal approach towards mining social media data during natural\n  disasters -- a case study of Hurricane Irma","source":"Somya D. Mohanty and Brown Biggers and Saed Sayedahmed and Nastaran\n  Pourebrahim and Evan B. Goldstein and Rick Bunch and Guangqing Chi and\n  Fereidoon Sadri and Tom P. McCoy and Arthur Cosby","docs_id":"2101.00480","section":["cs.SI","cs.IR","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A multi-modal approach towards mining social media data during natural\n  disasters -- a case study of Hurricane Irma. Streaming social media provides a real-time glimpse of extreme weather impacts. However, the volume of streaming data makes mining information a challenge for emergency managers, policy makers, and disciplinary scientists. Here we explore the effectiveness of data learned approaches to mine and filter information from streaming social media data from Hurricane Irma's landfall in Florida, USA. We use 54,383 Twitter messages (out of 784K geolocated messages) from 16,598 users from Sept. 10 - 12, 2017 to develop 4 independent models to filter data for relevance: 1) a geospatial model based on forcing conditions at the place and time of each tweet, 2) an image classification model for tweets that include images, 3) a user model to predict the reliability of the tweeter, and 4) a text model to determine if the text is related to Hurricane Irma. All four models are independently tested, and can be combined to quickly filter and visualize tweets based on user-defined thresholds for each submodel. We envision that this type of filtering and visualization routine can be useful as a base model for data capture from noisy sources such as Twitter. The data can then be subsequently used by policy makers, environmental managers, emergency managers, and domain scientists interested in finding tweets with specific attributes to use during different stages of the disaster (e.g., preparedness, response, and recovery), or for detailed research."},{"title":"Battery Asset Management with Cycle Life Prognosis","source":"Xinyang Liu, Pingfeng Wang, Esra B\\\"uy\\\"uktahtak{\\i}n Toy and Zhi Zhou","docs_id":"2011.14903","section":["eess.SY","cs.SY"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Battery Asset Management with Cycle Life Prognosis. Battery Asset Management problem determines the minimum cost replacement schedules for each individual asset in a group of battery assets that operate in parallel. Battery cycle life varies under different operating conditions including temperature, depth of discharge, charge rate, etc., and a battery deteriorates due to usage, which cannot be handled by current asset management models. This paper presents battery cycle life prognosis and its integration with parallel asset management to reduce lifecycle cost of the Battery Energy Storage System (BESS). A nonlinear capacity fade model is incorporated in the parallel asset management model to update battery capacity. Parametric studies have been conducted to explore the influence of different model inputs (e.g. usage rate, unit battery capacity, operating condition and periodical demand) for a five-year time horizon. Experiment results verify the reasonableness of this new framework and suggest that the increase in battery lifetime leads to decrease in lifecycle cost."},{"title":"MU-MIMO Communications with MIMO Radar: From Co-existence to Joint\n  Transmission","source":"Fan Liu, Christos Masouros, Ang Li, Huafei Sun, Lajos Hanzo","docs_id":"1707.00519","section":["cs.IT","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"MU-MIMO Communications with MIMO Radar: From Co-existence to Joint\n  Transmission. Beamforming techniques are proposed for a joint multi-input-multi-output (MIMO) radar-communication (RadCom) system, where a single device acts both as a radar and a communication base station (BS) by simultaneously communicating with downlink users and detecting radar targets. Two operational options are considered, where we first split the antennas into two groups, one for radar and the other for communication. Under this deployment, the radar signal is designed to fall into the null-space of the downlink channel. The communication beamformer is optimized such that the beampattern obtained matches the radar's beampattern while satisfying the communication performance requirements. To reduce the optimizations' constraints, we consider a second operational option, where all the antennas transmit a joint waveform that is shared by both radar and communications. In this case, we formulate an appropriate probing beampattern, while guaranteeing the performance of the downlink communications. By incorporating the SINR constraints into objective functions as penalty terms, we further simplify the original beamforming designs to weighted optimizations, and solve them by efficient manifold algorithms. Numerical results show that the shared deployment outperforms the separated case significantly, and the proposed weighted optimizations achieve a similar performance to the original optimizations, despite their significantly lower computational complexity."},{"title":"Efficient Inner-product Algorithm for Stabilizer States","source":"Hector J. Garcia, Igor L. Markov and Andrew W. Cross","docs_id":"1210.6646","section":["cs.ET","cs.CG","cs.DS","quant-ph"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Efficient Inner-product Algorithm for Stabilizer States. Large-scale quantum computation is likely to require massive quantum error correction (QEC). QEC codes and circuits are described via the stabilizer formalism, which represents stabilizer states by keeping track of the operators that preserve them. Such states are obtained by stabilizer circuits (consisting of CNOT, Hadamard and Phase only) and can be represented compactly on conventional computers using Omega(n^2) bits, where n is the number of qubits. Although techniques for the efficient simulation of stabilizer circuits have been studied extensively, techniques for efficient manipulation of stabilizer states are not currently available. To this end, we design new algorithms for: (i) obtaining canonical generators for stabilizer states, (ii) obtaining canonical stabilizer circuits, and (iii) computing the inner product between stabilizer states. Our inner-product algorithm takes O(n^3) time in general, but observes quadratic behavior for many practical instances relevant to QECC (e.g., GHZ states). We prove that each n-qubit stabilizer state has exactly 4(2^n - 1) nearest-neighbor stabilizer states, and verify this claim experimentally using our algorithms. We design techniques for representing arbitrary quantum states using stabilizer frames and generalize our algorithms to compute the inner product between two such frames."},{"title":"Tight FPT Approximation for Socially Fair Clustering","source":"Dishant Goyal and Ragesh Jaiswal","docs_id":"2106.06755","section":["cs.DS","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Tight FPT Approximation for Socially Fair Clustering. In this work, we study the socially fair $k$-median\/$k$-means problem. We are given a set of points $P$ in a metric space $\\mathcal{X}$ with a distance function $d(.,.)$. There are $\\ell$ groups: $P_1,\\dotsc,P_{\\ell} \\subseteq P$. We are also given a set $F$ of feasible centers in $\\mathcal{X}$. The goal in the socially fair $k$-median problem is to find a set $C \\subseteq F$ of $k$ centers that minimizes the maximum average cost over all the groups. That is, find $C$ that minimizes the objective function $\\Phi(C,P) \\equiv \\max_{j} \\Big\\{ \\sum_{x \\in P_j} d(C,x)\/|P_j| \\Big\\}$, where $d(C,x)$ is the distance of $x$ to the closest center in $C$. The socially fair $k$-means problem is defined similarly by using squared distances, i.e., $d^{2}(.,.)$ instead of $d(.,.)$. The current best approximation guarantee for both the problems is $O\\left( \\frac{\\log \\ell}{\\log \\log \\ell} \\right)$ due to Makarychev and Vakilian [COLT 2021]. In this work, we study the fixed parameter tractability of the problems with respect to parameter $k$. We design $(3+\\varepsilon)$ and $(9 + \\varepsilon)$ approximation algorithms for the socially fair $k$-median and $k$-means problems, respectively, in FPT (fixed parameter tractable) time $f(k,\\varepsilon) \\cdot n^{O(1)}$, where $f(k,\\varepsilon) = (k\/\\varepsilon)^{{O}(k)}$ and $n = |P \\cup F|$. Furthermore, we show that if Gap-ETH holds, then better approximation guarantees are not possible in FPT time."},{"title":"Pursuing Open-Source Development of Predictive Algorithms: The Case of\n  Criminal Sentencing Algorithms","source":"Philip D. Waggoner, Alec Macmillen","docs_id":"2011.06422","section":["stat.AP","cs.CY"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Pursuing Open-Source Development of Predictive Algorithms: The Case of\n  Criminal Sentencing Algorithms. Currently, there is uncertainty surrounding the merits of open-source versus proprietary algorithm development. Though justification in favor of each exists, we argue that open-source algorithm development should be the standard in highly consequential contexts that affect people's lives for reasons of transparency and collaboration, which contribute to greater predictive accuracy and enjoy the additional advantage of cost-effectiveness. To make this case, we focus on criminal sentencing algorithms, as criminal sentencing is highly consequential, and impacts society and individual people. Further, the popularity of this topic has surged in the wake of recent studies uncovering racial bias in proprietary sentencing algorithms among other issues of over-fitting and model complexity. We suggest these issues are exacerbated by the proprietary and expensive nature of virtually all widely used criminal sentencing algorithms. Upon replicating a major algorithm using real criminal profiles, we fit three penalized regressions and demonstrate an increase in predictive power of these open-source and relatively computationally inexpensive options. The result is a data-driven suggestion that if judges who are making sentencing decisions want to craft appropriate sentences based on a high degree of accuracy and at low costs, then they should be pursuing open-source options."},{"title":"Auto-clustering Output Layer: Automatic Learning of Latent Annotations\n  in Neural Networks","source":"Ozsel Kilinc, Ismail Uysal","docs_id":"1702.08648","section":["cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Auto-clustering Output Layer: Automatic Learning of Latent Annotations\n  in Neural Networks. In this paper, we discuss a different type of semi-supervised setting: a coarse level of labeling is available for all observations but the model has to learn a fine level of latent annotation for each one of them. Problems in this setting are likely to be encountered in many domains such as text categorization, protein function prediction, image classification as well as in exploratory scientific studies such as medical and genomics research. We consider this setting as simultaneously performed supervised classification (per the available coarse labels) and unsupervised clustering (within each one of the coarse labels) and propose a novel output layer modification called auto-clustering output layer (ACOL) that allows concurrent classification and clustering based on Graph-based Activity Regularization (GAR) technique. As the proposed output layer modification duplicates the softmax nodes at the output layer for each class, GAR allows for competitive learning between these duplicates on a traditional error-correction learning framework to ultimately enable a neural network to learn the latent annotations in this partially supervised setup. We demonstrate how the coarse label supervision impacts performance and helps propagate useful clustering information between sub-classes. Comparative tests on three of the most popular image datasets MNIST, SVHN and CIFAR-100 rigorously demonstrate the effectiveness and competitiveness of the proposed approach."},{"title":"OCTAVA: an open-source toolbox for quantitative analysis of optical\n  coherence tomography angiography images","source":"Gavrielle R. Untracht, Rolando Matos, Nikolaos Dikaios, Mariam Bapir,\n  Abdullah K. Durrani, Teemapron Butsabong, Paola Campagnolo, David D. Sampson,\n  Christian Heiss and Danuta M. Sampson","docs_id":"2109.01835","section":["eess.IV","cs.CV","q-bio.TO"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"OCTAVA: an open-source toolbox for quantitative analysis of optical\n  coherence tomography angiography images. Optical coherence tomography angiography (OCTA) performs non-invasive visualization and characterization of microvasculature in research and clinical applications mainly in ophthalmology and dermatology. A wide variety of instruments, imaging protocols, processing methods and metrics have been used to describe the microvasculature, such that comparing different study outcomes is currently not feasible. With the goal of contributing to standardization of OCTA data analysis, we report a user-friendly, open-source toolbox, OCTAVA (OCTA Vascular Analyzer), to automate the pre-processing, segmentation, and quantitative analysis of en face OCTA maximum intensity projection images in a standardized workflow. We present each analysis step, including optimization of filtering and choice of segmentation algorithm, and definition of metrics. We perform quantitative analysis of OCTA images from different commercial and non-commercial instruments and samples and show OCTAVA can accurately and reproducibly determine metrics for characterization of microvasculature. Wide adoption could enable studies and aggregation of data on a scale sufficient to develop reliable microvascular biomarkers for early detection, and to guide treatment, of microvascular disease."},{"title":"Formalization of malware through process calculi","source":"Gregoire Jacob, Eric Filiol and Herve Debar","docs_id":"0902.0469","section":["cs.CR"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Formalization of malware through process calculi. Since the seminal work from F. Cohen in the eighties, abstract virology has seen the apparition of successive viral models, all based on Turing-equivalent formalisms. But considering recent malware such as rootkits or k-ary codes, these viral models only partially cover these evolved threats. The problem is that Turing-equivalent models do not support interactive computations. New models have thus appeared, offering support for these evolved malware, but loosing the unified approach in the way. This article provides a basis for a unified malware model founded on process algebras and in particular the Join-Calculus. In terms of expressiveness, the new model supports the fundamental definitions based on self-replication and adds support for interactions, concurrency and non-termination allows the definition of more complex behaviors. Evolved malware such as rootkits can now be thoroughly modeled. In terms of detection and prevention, the fundamental results of undecidability and isolation still hold. However the process-based model has permitted to establish new results: identification of fragments from the Join-Calculus where malware detection becomes decidable, formal definition of the non-infection property, approximate solutions to restrict malware propagation."},{"title":"A probabilistic deep learning approach to automate the interpretation of\n  multi-phase diffraction spectra","source":"Nathan J. Szymanski, Christopher J. Bartel, Yan Zeng, Qingsong Tu,\n  Gerbrand Ceder","docs_id":"2103.16664","section":["cond-mat.mtrl-sci","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A probabilistic deep learning approach to automate the interpretation of\n  multi-phase diffraction spectra. Autonomous synthesis and characterization of inorganic materials requires the automatic and accurate analysis of X-ray diffraction spectra. For this task, we designed a probabilistic deep learning algorithm to identify complex multi-phase mixtures. At the core of this algorithm lies an ensemble convolutional neural network trained on simulated diffraction spectra, which are systematically augmented with physics-informed perturbations to account for artifacts that can arise during experimental sample preparation and synthesis. Larger perturbations associated with off-stoichiometry are also captured by supplementing the training set with hypothetical solid solutions. Spectra containing mixtures of materials are analyzed with a newly developed branching algorithm that utilizes the probabilistic nature of the neural network to explore suspected mixtures and identify the set of phases that maximize confidence in the prediction. Our model is benchmarked on simulated and experimentally measured diffraction spectra, showing exceptional performance with accuracies exceeding those given by previously reported methods based on profile matching and deep learning. We envision that the algorithm presented here may be integrated in experimental workflows to facilitate the high-throughput and autonomous discovery of inorganic materials."},{"title":"A Malaria Control Model Using Mobility Data: An Early Explanation of\n  Kedougou's Case in Senegal","source":"Lynda Bouzid Khiri, Ibrahima Gueye, Hubert Naacke, Idrissa Sarr and\n  St\\'ephane Gan\\c{c}arski","docs_id":"2004.06482","section":["q-bio.PE","cs.CY","cs.SY","eess.SY"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A Malaria Control Model Using Mobility Data: An Early Explanation of\n  Kedougou's Case in Senegal. Studies in malaria control cover many areas such as medicine, sociology, biology, mathematic, physic, computer science and so forth. Researches in the realm of mathematic are conducted to predict the occurrence of the disease and to support the eradication process. Basically, the modeling methodology is predominantly deterministic and differential equation based while selecting clinical and biological features that seem to be important. Yet, if the individual characteristics matter when modeling the disease, the overall estimation of the malaria is not done based on the health status of each individual but in a non-specified percentage of the global population. The goal of this paper is to propose a model that relies on a daily evolution of the individual's state, which depends on their mobility and the characteristics of the area they visit. Thus, the mobility data of a single person moving from one area to another, gathered thanks to mobile networks, is the essential building block to predict the outcome of the disease. We implement our solution and demonstrate its effectiveness through empirical experiments. The results show how promising the model is in providing possible insights into the failure of the disease control in the Kedougou region."},{"title":"Reactive Turing Machines","source":"Jos C. M. Baeten, Bas Luttik, Paul van Tilburg","docs_id":"1104.1738","section":["cs.LO","cs.FL"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Reactive Turing Machines. We propose reactive Turing machines (RTMs), extending classical Turing machines with a process-theoretical notion of interaction, and use it to define a notion of executable transition system. We show that every computable transition system with a bounded branching degree is simulated modulo divergence-preserving branching bisimilarity by an RTM, and that every effective transition system is simulated modulo the variant of branching bisimilarity that does not require divergence preservation. We conclude from these results that the parallel composition of (communicating) RTMs can be simulated by a single RTM. We prove that there exist universal RTMs modulo branching bisimilarity, but these essentially employ divergence to be able to simulate an RTM of arbitrary branching degree. We also prove that modulo divergence-preserving branching bisimilarity there are RTMs that are universal up to their own branching degree. Finally, we establish a correspondence between executability and finite definability in a simple process calculus."},{"title":"Frank-Wolfe with Subsampling Oracle","source":"Thomas Kerdreux, Fabian Pedregosa and Alexandre d'Aspremont","docs_id":"1803.07348","section":["math.OC","cs.LG","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Frank-Wolfe with Subsampling Oracle. We analyze two novel randomized variants of the Frank-Wolfe (FW) or conditional gradient algorithm. While classical FW algorithms require solving a linear minimization problem over the domain at each iteration, the proposed method only requires to solve a linear minimization problem over a small \\emph{subset} of the original domain. The first algorithm that we propose is a randomized variant of the original FW algorithm and achieves a $\\mathcal{O}(1\/t)$ sublinear convergence rate as in the deterministic counterpart. The second algorithm is a randomized variant of the Away-step FW algorithm, and again as its deterministic counterpart, reaches linear (i.e., exponential) convergence rate making it the first provably convergent randomized variant of Away-step FW. In both cases, while subsampling reduces the convergence rate by a constant factor, the linear minimization step can be a fraction of the cost of that of the deterministic versions, especially when the data is streamed. We illustrate computational gains of the algorithms on regression problems, involving both $\\ell_1$ and latent group lasso penalties."},{"title":"A hybrid algorithm based on Community Detection and Multi-Attribute\n  Decision-Making for Influence Maximization","source":"Masoud Jalayer, Morvarid Azheian, Mehrdad Mohammad Ali Kermani","docs_id":"2105.09507","section":["cs.SI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A hybrid algorithm based on Community Detection and Multi-Attribute\n  Decision-Making for Influence Maximization. The influence maximization problem is trying to identify a set of K nodes by which the spread of influence, diseases, or information is maximized. The optimization of influence by finding such a set is an NP-hard problem and a key issue in analyzing complex networks. In this paper, a new greedy and hybrid approach based on a community detection algorithm and a MADM technique (TOPSIS) is proposed to cope with the problem, called, Greedy TOPSIS and Community-Based (GTaCB) algorithm. The paper concisely introduces community detection and the TOPSIS technique, then it presents the pseudo-code of the proposed algorithm. Afterward, it compares the performance of the solution which is found by GTaCB with some well-known greedy algorithms, based on Degree Centrality, Closeness Centrality, Betweenness Centrality, PageRank as well as TOPSIS, from two aspects: diffusion quality and diffusion speed. In order to evaluate the performance of GTaCB, computational experiments on nine different types of real-world networks are provided. The tests are conducted via one of the renowned epidemic diffusion models, namely, Susceptible-Infected-Recovered (SIR) model. The simulations exhibit that in most of the cases the proposed algorithm significantly outperforms the others, chiefly as the number of initial nodes or probability of infection increases."},{"title":"Multiwave COVID-19 Prediction via Social Awareness-Based Graph Neural\n  Networks using Mobility and Web Search Data","source":"J. Xue, T. Yabe, K. Tsubouchi, J. Ma, S. V. Ukkusuri","docs_id":"2110.11584","section":["cs.SI","cs.AI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Multiwave COVID-19 Prediction via Social Awareness-Based Graph Neural\n  Networks using Mobility and Web Search Data. Recurring outbreaks of COVID-19 have posed enduring effects on global society, which calls for a predictor of pandemic waves using various data with early availability. Existing prediction models that forecast the first outbreak wave using mobility data may not be applicable to the multiwave prediction, because the evidence in the USA and Japan has shown that mobility patterns across different waves exhibit varying relationships with fluctuations in infection cases. Therefore, to predict the multiwave pandemic, we propose a Social Awareness-Based Graph Neural Network (SAB-GNN) that considers the decay of symptom-related web search frequency to capture the changes in public awareness across multiple waves. SAB-GNN combines GNN and LSTM to model the complex relationships among urban districts, inter-district mobility patterns, web search history, and future COVID-19 infections. We train our model to predict future pandemic outbreaks in the Tokyo area using its mobility and web search data from April 2020 to May 2021 across four pandemic waves collected by _ANONYMOUS_COMPANY_ under strict privacy protection rules. Results show our model outperforms other baselines including ST-GNN and MPNN+LSTM. Though our model is not computationally expensive (only 3 layers and 10 hidden neurons), the proposed model enables public agencies to anticipate and prepare for future pandemic outbreaks."},{"title":"Fault Tolerance for Remote Memory Access Programming Models","source":"Maciej Besta, Torsten Hoefler","docs_id":"2010.09025","section":["cs.DC"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Fault Tolerance for Remote Memory Access Programming Models. Remote Memory Access (RMA) is an emerging mechanism for programming high-performance computers and datacenters. However, little work exists on resilience schemes for RMA-based applications and systems. In this paper we analyze fault tolerance for RMA and show that it is fundamentally different from resilience mechanisms targeting the message passing (MP) model. We design a model for reasoning about fault tolerance for RMA, addressing both flat and hierarchical hardware. We use this model to construct several highly-scalable mechanisms that provide efficient low-overhead in-memory checkpointing, transparent logging of remote memory accesses, and a scheme for transparent recovery of failed processes. Our protocols take into account diminishing amounts of memory per core, one of major features of future exascale machines. The implementation of our fault-tolerance scheme entails negligible additional overheads. Our reliability model shows that in-memory checkpointing and logging provide high resilience. This study enables highly-scalable resilience mechanisms for RMA and fills a research gap between fault tolerance and emerging RMA programming models."},{"title":"Cross-modal Zero-shot Hashing by Label Attributes Embedding","source":"Runmin Wang, Guoxian Yu, Lei Liu, Lizhen Cui, Carlotta Domeniconi,\n  Xiangliang Zhang","docs_id":"2111.04080","section":["cs.CV","cs.AI","cs.LG","cs.MM"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Cross-modal Zero-shot Hashing by Label Attributes Embedding. Cross-modal hashing (CMH) is one of the most promising methods in cross-modal approximate nearest neighbor search. Most CMH solutions ideally assume the labels of training and testing set are identical. However, the assumption is often violated, causing a zero-shot CMH problem. Recent efforts to address this issue focus on transferring knowledge from the seen classes to the unseen ones using label attributes. However, the attributes are isolated from the features of multi-modal data. To reduce the information gap, we introduce an approach called LAEH (Label Attributes Embedding for zero-shot cross-modal Hashing). LAEH first gets the initial semantic attribute vectors of labels by word2vec model and then uses a transformation network to transform them into a common subspace. Next, it leverages the hash vectors and the feature similarity matrix to guide the feature extraction network of different modalities. At the same time, LAEH uses the attribute similarity as the supplement of label similarity to rectify the label embedding and common subspace. Experiments show that LAEH outperforms related representative zero-shot and cross-modal hashing methods."},{"title":"An Attention-Based Word-Level Interaction Model: Relation Detection for\n  Knowledge Base Question Answering","source":"Hongzhi Zhang, Guandong Xu, Xiao Liang, Tinglei Huang and Kun fu","docs_id":"1801.09893","section":["cs.CL"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"An Attention-Based Word-Level Interaction Model: Relation Detection for\n  Knowledge Base Question Answering. Relation detection plays a crucial role in Knowledge Base Question Answering (KBQA) because of the high variance of relation expression in the question. Traditional deep learning methods follow an encoding-comparing paradigm, where the question and the candidate relation are represented as vectors to compare their semantic similarity. Max- or average- pooling operation, which compresses the sequence of words into fixed-dimensional vectors, becomes the bottleneck of information. In this paper, we propose to learn attention-based word-level interactions between questions and relations to alleviate the bottleneck issue. Similar to the traditional models, the question and relation are firstly represented as sequences of vectors. Then, instead of merging the sequence into a single vector with pooling operation, soft alignments between words from the question and the relation are learned. The aligned words are subsequently compared with the convolutional neural network (CNN) and the comparison results are merged finally. Through performing the comparison on low-level representations, the attention-based word-level interaction model (ABWIM) relieves the information loss issue caused by merging the sequence into a fixed-dimensional vector before the comparison. The experimental results of relation detection on both SimpleQuestions and WebQuestions datasets show that ABWIM achieves state-of-the-art accuracy, demonstrating its effectiveness."},{"title":"A High Order Sliding Mode Control with PID Sliding Surface: Simulation\n  on a Torpedo","source":"Ahmed Rhif","docs_id":"1202.2419","section":["cs.SY"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A High Order Sliding Mode Control with PID Sliding Surface: Simulation\n  on a Torpedo. Position and speed control of the torpedo present a real problem for the actuators because of the high level of the system non linearity and because of the external disturbances. The non linear systems control is based on several different approaches, among it the sliding mode control. The sliding mode control has proved its effectiveness through the different studies. The advantage that makes such an important approach is its robustness versus the disturbances and the model uncertainties. However, this approach implies a disadvantage which is the chattering phenomenon caused by the discontinuous part of this control and which can have a harmful effect on the actuators. This paper deals with the basic concepts, mathematics, and design aspects of a control for nonlinear systems that make the chattering effect lower. As solution to this problem we will adopt as a starting point the high order sliding mode approaches then the PID sliding surface. Simulation results show that this control strategy can attain excellent control performance with no chattering problem."},{"title":"Gait Recovery System for Parkinson's Disease using Machine Learning on\n  Embedded Platforms","source":"Gokul H., Prithvi Suresh, Hari Vignesh B, Pravin Kumaar R, Vineeth\n  Vijayaraghavan","docs_id":"2004.05811","section":["eess.SP","cs.LG","cs.SY","eess.SY"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Gait Recovery System for Parkinson's Disease using Machine Learning on\n  Embedded Platforms. Freezing of Gait (FoG) is a common gait deficit among patients diagnosed with Parkinson's Disease (PD). In order to help these patients recover from FoG episodes, Rhythmic Auditory Stimulation (RAS) is needed. The authors propose a ubiquitous embedded system that detects FOG events with a Machine Learning (ML) subsystem from accelerometer signals . By making inferences on-device, we avoid issues prevalent in cloud-based systems such as latency and network connection dependency. The resource-efficient classifier used, reduces the model size requirements by approximately 400 times compared to the best performing standard ML systems, with a trade-off of a mere 1.3% in best classification accuracy. The aforementioned trade-off facilitates deployability in a wide range of embedded devices including microcontroller based systems. The research also explores the optimization procedure to deploy the model on an ATMega2560 microcontroller with a minimum system latency of 44.5 ms. The smallest model size of the proposed resource efficient ML model was 1.4 KB with an average recall score of 93.58%."},{"title":"Dynamic Interference Mitigation for Generalized Partially Connected\n  Quasi-static MIMO Interference Channel","source":"Liangzhong Ruan, Vincent K.N. Lau","docs_id":"1105.0286","section":["cs.IT","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Dynamic Interference Mitigation for Generalized Partially Connected\n  Quasi-static MIMO Interference Channel. Recent works on MIMO interference channels have shown that interference alignment can significantly increase the achievable degrees of freedom (DoF) of the network. However, most of these works have assumed a fully connected interference graph. In this paper, we investigate how the partial connectivity can be exploited to enhance system performance in MIMO interference networks. We propose a novel interference mitigation scheme which introduces constraints for the signal subspaces of the precoders and decorrelators to mitigate \"many\" interference nulling constraints at a cost of \"little\" freedoms in precoder and decorrelator design so as to extend the feasibility region of the interference alignment scheme. Our analysis shows that the proposed algorithm can significantly increase system DoF in symmetric partially connected MIMO interference networks. We also compare the performance of the proposed scheme with various baselines and show via simulations that the proposed algorithms could achieve significant gain in the system performance of randomly connected interference networks."},{"title":"Zero-Shot Semantic Parsing for Instructions","source":"Ofer Givoli and Roi Reichart","docs_id":"1911.08827","section":["cs.CL","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Zero-Shot Semantic Parsing for Instructions. We consider a zero-shot semantic parsing task: parsing instructions into compositional logical forms, in domains that were not seen during training. We present a new dataset with 1,390 examples from 7 application domains (e.g. a calendar or a file manager), each example consisting of a triplet: (a) the application's initial state, (b) an instruction, to be carried out in the context of that state, and (c) the state of the application after carrying out the instruction. We introduce a new training algorithm that aims to train a semantic parser on examples from a set of source domains, so that it can effectively parse instructions from an unknown target domain. We integrate our algorithm into the floating parser of Pasupat and Liang (2015), and further augment the parser with features and a logical form candidate filtering logic, to support zero-shot adaptation. Our experiments with various zero-shot adaptation setups demonstrate substantial performance gains over a non-adapted parser."},{"title":"Learning Boolean Circuits with Neural Networks","source":"Eran Malach, Shai Shalev-Shwartz","docs_id":"1910.11923","section":["cs.LG","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Learning Boolean Circuits with Neural Networks. While on some natural distributions, neural-networks are trained efficiently using gradient-based algorithms, it is known that learning them is computationally hard in the worst-case. To separate hard from easy to learn distributions, we observe the property of local correlation: correlation between local patterns of the input and the target label. We focus on learning deep neural-networks using a gradient-based algorithm, when the target function is a tree-structured Boolean circuit. We show that in this case, the existence of correlation between the gates of the circuit and the target label determines whether the optimization succeeds or fails. Using this result, we show that neural-networks can learn the (log n)-parity problem for most product distributions. These results hint that local correlation may play an important role in separating easy\/hard to learn distributions. We also obtain a novel depth separation result, in which we show that a shallow network cannot express some functions, while there exists an efficient gradient-based algorithm that can learn the very same functions using a deep network. The negative expressivity result for shallow networks is obtained by a reduction from results in communication complexity, that may be of independent interest."},{"title":"Towards Machine Learning-Based Optimal HAS","source":"Christian Sieber, Korbinian Hagn, Christian Moldovan, Tobias\n  Ho{\\ss}feld, Wolfgang Kellerer","docs_id":"1808.08065","section":["cs.MM","cs.LG","cs.NI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Towards Machine Learning-Based Optimal HAS. Mobile video consumption is increasing and sophisticated video quality adaptation strategies are required to deal with mobile throughput fluctuations. These adaptation strategies have to keep the switching frequency low, the average quality high and prevent stalling occurrences to ensure customer satisfaction. This paper proposes a novel methodology for the design of machine learning-based adaptation logics named HASBRAIN. Furthermore, the performance of a trained neural network against two algorithms from the literature is evaluated. We first use a modified existing optimization formulation to calculate optimal adaptation paths with a minimum number of quality switches for a wide range of videos and for challenging mobile throughput patterns. Afterwards we use the resulting optimal adaptation paths to train and compare different machine learning models. The evaluation shows that an artificial neural network-based model can reach a high average quality with a low number of switches in the mobile scenario. The proposed methodology is general enough to be extended for further designs of machine learning-based algorithms and the provided model can be deployed in on-demand streaming scenarios or be further refined using reward-based mechanisms such as reinforcement learning. All tools, models and datasets created during the work are provided as open-source software."},{"title":"Dominant Resource Fairness with Meta-Types","source":"Steven Yin, Shatian Wang, Lingyi Zhang, Christian Kroer","docs_id":"2007.11961","section":["econ.TH","cs.GT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Dominant Resource Fairness with Meta-Types. Inspired by the recent COVID-19 pandemic, we study a generalization of the multi-resource allocation problem with heterogeneous demands and Leontief utilities. Unlike existing settings, we allow each agent to specify requirements to only accept allocations from a subset of the total supply for each resource. These requirements can take form in location constraints (e.g. A hospital can only accept volunteers who live nearby due to commute limitations). This can also model a type of substitution effect where some agents need 1 unit of resource A \\emph{or} B, both belonging to the same meta-type. But some agents specifically want A, and others specifically want B. We propose a new mechanism called Dominant Resource Fairness with Meta Types which determines the allocations by solving a small number of linear programs. The proposed method satisfies Pareto optimality, envy-freeness, strategy-proofness, and a notion of sharing incentive for our setting. To the best of our knowledge, we are the first to study this problem formulation, which improved upon existing work by capturing more constraints that often arise in real life situations. Finally, we show numerically that our method scales better to large problems than alternative approaches."},{"title":"A Crossing Lemma for Families of Jordan Curves with a Bounded\n  Intersection Number","source":"Maya Bechler-Speicher","docs_id":"1911.07287","section":["cs.CG","cs.DM"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A Crossing Lemma for Families of Jordan Curves with a Bounded\n  Intersection Number. A family of closed simple (i.e., Jordan) curves is $m$-intersecting if any pair of its curves have at most $m$ points of common intersection. We say that a pair of such curves touch if they intersect at a single point of common tangency. In this work we show that any $m$-intersecting family of $n$ Jordan curves in general position in the plane contains $O\\left(n^{2-\\frac{1}{3m+15}}\\right)$ touching pairs Furthermore, we use the string separator theorem of Fox and Pach in order to establish the following Crossing Lemma for contact graphs of Jordan curves: Let $\\Gamma$ be an $m$-intersecting family of closed Jordan curves in general position in the plane with exactly $T=\\Omega(n)$ touching pairs of curves, then the curves of $\\Gamma$ determine $\\Omega\\left(T\\cdot\\left(\\frac{T}{n}\\right)^{\\frac{1}{9m+45}}\\right)$ intersection points. This extends the similar bounds that were previously established by Salazar for the special case of pairwise intersecting (and $m$-intersecting) curves. Specializing to the case at hand, this substantially improves the bounds that were recently derived by Pach, Rubin and Tardos for arbitrary families of Jordan curves."},{"title":"Learning to predict synchronization of coupled oscillators on\n  heterogeneous graphs","source":"Hardeep Bassi, Richard Yim, Rohith Kodukula, Joshua Vendrow, Cherlin\n  Zhu, Hanbaek Lyu","docs_id":"2012.14048","section":["math.DS","cs.LG","nlin.AO"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Learning to predict synchronization of coupled oscillators on\n  heterogeneous graphs. Suppose we are given a system of coupled oscillators on an arbitrary graph along with the trajectory of the system during some period. Can we predict whether the system will eventually synchronize? This is an important but analytically intractable question especially when the structure of the underlying graph is highly varied. In this work, we take an entirely different approach that we call \"learning to predict synchronization\" (L2PSync), by viewing it as a classification problem for sets of graphs paired with initial dynamics into two classes: `synchronizing' or `non-synchronizing'. Our conclusion is that, once trained on large enough datasets of synchronizing and non-synchronizing dynamics on heterogeneous sets of graphs, a number of binary classification algorithms can successfully predict the future of an unknown system with surprising accuracy. We also propose an \"ensemble prediction\" algorithm that scales up our method to large graphs by training on dynamics observed from multiple random subgraphs. We find that in many instances, the first few iterations of the dynamics are far more important than the static features of the graphs. We demonstrate our method on three models of continuous and discrete coupled oscillators -- The Kuramoto model, the Firefly Cellular Automata, and the Greenberg-Hastings model."},{"title":"Ladder Polynomial Neural Networks","source":"Li-Ping Liu, Ruiyuan Gu, Xiaozhe Hu","docs_id":"2106.13834","section":["cs.LG","cs.NE"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Ladder Polynomial Neural Networks. Polynomial functions have plenty of useful analytical properties, but they are rarely used as learning models because their function class is considered to be restricted. This work shows that when trained properly polynomial functions can be strong learning models. Particularly this work constructs polynomial feedforward neural networks using the product activation, a new activation function constructed from multiplications. The new neural network is a polynomial function and provides accurate control of its polynomial order. It can be trained by standard training techniques such as batch normalization and dropout. This new feedforward network covers several previous polynomial models as special cases. Compared with common feedforward neural networks, the polynomial feedforward network has closed-form calculations of a few interesting quantities, which are very useful in Bayesian learning. In a series of regression and classification tasks in the empirical study, the proposed model outperforms previous polynomial models."},{"title":"Hardware Acceleration for Boolean Satisfiability Solver by Applying\n  Belief Propagation Algorithm","source":"Te-Hsuan Chen and Ju-Yi Lu","docs_id":"1603.05314","section":["cs.AI","cs.LO"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Hardware Acceleration for Boolean Satisfiability Solver by Applying\n  Belief Propagation Algorithm. Boolean satisfiability (SAT) has an extensive application domain in computer science, especially in electronic design automation applications. Circuit synthesis, optimization, and verification problems can be solved by transforming original problems to SAT problems. However, the SAT problem is known as NP-complete, which means there is no efficient method to solve it. Therefore, an efficient SAT solver to enhance the performance is always desired. We propose a hardware acceleration method for SAT problems. By surveying the properties of SAT problems and the decoding of low-density parity-check (LDPC) codes, a special class of error-correcting codes, we discover that both of them are constraint satisfaction problems. The belief propagation algorithm has been successfully applied to the decoding of LDPC, and the corresponding decoder hardware designs are extensively studied. Therefore, we proposed a belief propagation based algorithm to solve SAT problems. With this algorithm, the SAT solver can be accelerated by hardware. A software simulator is implemented to verify the proposed algorithm and the performance improvement is estimated. Our experiment results show that time complexity does not increase with the size of SAT problems and the proposed method can achieve at least 30x speedup compared to MiniSat."},{"title":"A Bayesian nonparametric approach to count-min sketch under power-law\n  data streams","source":"Emanuele Dolera, Stefano Favaro, Stefano Peluchetti","docs_id":"2102.03743","section":["stat.ML","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A Bayesian nonparametric approach to count-min sketch under power-law\n  data streams. The count-min sketch (CMS) is a randomized data structure that provides estimates of tokens' frequencies in a large data stream using a compressed representation of the data by random hashing. In this paper, we rely on a recent Bayesian nonparametric (BNP) view on the CMS to develop a novel learning-augmented CMS under power-law data streams. We assume that tokens in the stream are drawn from an unknown discrete distribution, which is endowed with a normalized inverse Gaussian process (NIGP) prior. Then, using distributional properties of the NIGP, we compute the posterior distribution of a token's frequency in the stream, given the hashed data, and in turn corresponding BNP estimates. Applications to synthetic and real data show that our approach achieves a remarkable performance in the estimation of low-frequency tokens. This is known to be a desirable feature in the context of natural language processing, where it is indeed common in the context of the power-law behaviour of the data."},{"title":"Active Hypothesis Testing for Quickest Anomaly Detection","source":"Kobi Cohen and Qing Zhao","docs_id":"1403.1023","section":["cs.IT","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Active Hypothesis Testing for Quickest Anomaly Detection. The problem of quickest detection of an anomalous process among M processes is considered. At each time, a subset of the processes can be observed, and the observations from each chosen process follow two different distributions, depending on whether the process is normal or abnormal. The objective is a sequential search strategy that minimizes the expected detection time subject to an error probability constraint. This problem can be considered as a special case of active hypothesis testing first considered by Chernoff in 1959 where a randomized strategy, referred to as the Chernoff test, was proposed and shown to be asymptotically (as the error probability approaches zero) optimal. For the special case considered in this paper, we show that a simple deterministic test achieves asymptotic optimality and offers better performance in the finite regime. We further extend the problem to the case where multiple anomalous processes are present. In particular, we examine the case where only an upper bound on the number of anomalous processes is known."},{"title":"System-in-the-loop Design Space Exploration for Efficient Communication\n  in Large-scale IoT-based Warehouse Systems","source":"Robert Falkenberg and Jens Drenhaus and Benjamin Sliwa and Christian\n  Wietfeld","docs_id":"1802.03033","section":["cs.NI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"System-in-the-loop Design Space Exploration for Efficient Communication\n  in Large-scale IoT-based Warehouse Systems. Instead of treating inventory items as static resources, future intelligent warehouses will transcend containers to Cyber Physical Systems (CPS) that actively and autonomously participate in the optimization of the logistical processes. Consequently, new challenges that are system-immanent for the massive Internet of Things (IoT) context, such as channel access in a shared communication medium, have to be addressed. In this paper, we present a multi-methodological system model that brings together testbed experiments for measuring real hardware properties and simulative evaluations for large-scale considerations. As an example case study, we will particularly focus on parametrization of the 802.15.4-based radio communication system, which has to be energy-efficient due to scarce amount of harvested energy, but avoid latencies for the maintenance of scalability of the overlaying warehouse system. The results show, that a modification of the initial backoff time can lead to both, energy and time savings in the order of 50% compared to the standard."},{"title":"A Game-Theoretic Algorithm for Link Prediction","source":"Mateusz Tarkowski, Tomasz Michalak, Michael Wooldridge","docs_id":"1912.12846","section":["cs.SI","physics.soc-ph"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A Game-Theoretic Algorithm for Link Prediction. Predicting edges in networks is a key problem in social network analysis and involves reasoning about the relationships between nodes based on the structural properties of a network. In particular, link prediction can be used to analyse how a network will develop or - given incomplete information about relationships - to discover \"missing\" links. Our approach to this problem is rooted in cooperative game theory, where we propose a new, quasi-local approach (i.e., one which considers nodes within some radius k) that combines generalised group closeness centrality and semivalue interaction indices. We develop fast algorithms for computing our measure and evaluate it on a number of real-world networks, where it outperforms a selection of other state-of-the-art methods from the literature. Importantly, choosing the optimal radius k for quasi-local methods is difficult, and there is no assurance that the choice is optimal. Additionally, when compared to other quasi-local methods, ours achieves very good results even when given a suboptimal radius k as a parameter."},{"title":"How fine can fine-tuning be? Learning efficient language models","source":"Evani Radiya-Dixit and Xin Wang","docs_id":"2004.14129","section":["cs.CL","cs.LG","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"How fine can fine-tuning be? Learning efficient language models. State-of-the-art performance on language understanding tasks is now achieved with increasingly large networks; the current record holder has billions of parameters. Given a language model pre-trained on massive unlabeled text corpora, only very light supervised fine-tuning is needed to learn a task: the number of fine-tuning steps is typically five orders of magnitude lower than the total parameter count. Does this mean that fine-tuning only introduces small differences from the pre-trained model in the parameter space? If so, can one avoid storing and computing an entire model for each task? In this work, we address these questions by using Bidirectional Encoder Representations from Transformers (BERT) as an example. As expected, we find that the fine-tuned models are close in parameter space to the pre-trained one, with the closeness varying from layer to layer. We show that it suffices to fine-tune only the most critical layers. Further, we find that there are surprisingly many good solutions in the set of sparsified versions of the pre-trained model. As a result, fine-tuning of huge language models can be achieved by simply setting a certain number of entries in certain layers of the pre-trained parameters to zero, saving both task-specific parameter storage and computational cost."},{"title":"A Unified Particle-Optimization Framework for Scalable Bayesian Sampling","source":"Changyou Chen, Ruiyi Zhang, Wenlin Wang, Bai Li and Liqun Chen","docs_id":"1805.11659","section":["stat.ML","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A Unified Particle-Optimization Framework for Scalable Bayesian Sampling. There has been recent interest in developing scalable Bayesian sampling methods such as stochastic gradient MCMC (SG-MCMC) and Stein variational gradient descent (SVGD) for big-data analysis. A standard SG-MCMC algorithm simulates samples from a discrete-time Markov chain to approximate a target distribution, thus samples could be highly correlated, an undesired property for SG-MCMC. In contrary, SVGD directly optimizes a set of particles to approximate a target distribution, and thus is able to obtain good approximations with relatively much fewer samples. In this paper, we propose a principle particle-optimization framework based on Wasserstein gradient flows to unify SG-MCMC and SVGD, and to allow new algorithms to be developed. Our framework interprets SG-MCMC as particle optimization on the space of probability measures, revealing a strong connection between SG-MCMC and SVGD. The key component of our framework is several particle-approximate techniques to efficiently solve the original partial differential equations on the space of probability measures. Extensive experiments on both synthetic data and deep neural networks demonstrate the effectiveness and efficiency of our framework for scalable Bayesian sampling."},{"title":"Adversarial Attacks in Cooperative AI","source":"Ted Fujimoto and Arthur Paul Pedersen","docs_id":"2111.14833","section":["cs.LG","cs.AI","cs.MA"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Adversarial Attacks in Cooperative AI. Single-agent reinforcement learning algorithms in a multi-agent environment are inadequate for fostering cooperation. If intelligent agents are to interact and work together to solve complex problems, methods that counter non-cooperative behavior are needed to facilitate the training of multiple agents. This is the goal of cooperative AI. Recent work in adversarial machine learning, however, shows that models (e.g., image classifiers) can be easily deceived into making incorrect decisions. In addition, some past research in cooperative AI has relied on new notions of representations, like public beliefs, to accelerate the learning of optimally cooperative behavior. Hence, cooperative AI might introduce new weaknesses not investigated in previous machine learning research. In this paper, our contributions include: (1) arguing that three algorithms inspired by human-like social intelligence introduce new vulnerabilities, unique to cooperative AI, that adversaries can exploit, and (2) an experiment showing that simple, adversarial perturbations on the agents' beliefs can negatively impact performance. This evidence points to the possibility that formal representations of social behavior are vulnerable to adversarial attacks."},{"title":"More on zeros and approximation of the Ising partition function","source":"Alexander Barvinok and Nicholas Barvinok","docs_id":"2005.11232","section":["math.PR","cs.DS","math-ph","math.CO","math.MP"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"More on zeros and approximation of the Ising partition function. We consider the problem of computing the partition function $\\sum_x e^{f(x)}$, where $f: \\{-1, 1\\}^n \\longrightarrow {\\Bbb R}$ is a quadratic or cubic polynomial on the Boolean cube $\\{-1, 1\\}^n$. In the case of a quadratic polynomial $f$, we show that the partition function can be approximated within relative error $0 < \\epsilon < 1$ in quasi-polynomial $n^{O(\\ln n - \\ln \\epsilon)}$ time if the Lipschitz constant of the non-linear part of $f$ with respect to the $\\ell^1$ metric on the Boolean cube does not exceed $1-\\delta$, for any $\\delta >0$, fixed in advance. For a cubic polynomial $f$, we get the same result under a somewhat stronger condition. We apply the method of polynomial interpolation, for which we prove that $\\sum_x e^{\\tilde{f}(x)} \\ne 0$ for complex-valued polynomials $\\tilde{f}$ in a neighborhood of a real-valued $f$ satisfying the above mentioned conditions. The bounds are asymptotically optimal. Results on the zero-free region are interpreted as the absence of a phase transition in the Lee - Yang sense in the corresponding Ising model. The novel feature of the bounds is that they control the total interaction of each vertex but not every single interaction of sets of vertices."},{"title":"Efficient $\\mathbb{Z}_2$ synchronization on $\\mathbb{Z}^d$ under\n  symmetry-preserving side information","source":"Ahmed El Alaoui","docs_id":"2106.02111","section":["math.PR","cs.IT","math.IT","math.ST","stat.TH"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Efficient $\\mathbb{Z}_2$ synchronization on $\\mathbb{Z}^d$ under\n  symmetry-preserving side information. We consider $\\mathbb{Z}_2$-synchronization on the Euclidean lattice. Every vertex of $\\mathbb{Z}^d$ is assigned an independent symmetric random sign $\\theta_u$, and for every edge $(u,v)$ of the lattice, one observes the product $\\theta_u\\theta_v$ flipped independently with probability $p$. The task is to reconstruct products $\\theta_u\\theta_v$ for pairs of vertices $u$ and $v$ which are arbitrarily far apart. Abb\\'e, Massouli\\'e, Montanari, Sly and Srivastava (2018) showed that synchronization is possible if and only if $p$ is below a critical threshold $\\tilde{p}_c(d)$, and efficiently so for $p$ small enough. We augment this synchronization setting with a model of side information preserving the sign symmetry of $\\theta$, and propose an \\emph{efficient} algorithm which synchronizes a randomly chosen pair of far away vertices on average, up to a differently defined critical threshold $p_c(d)$. We conjecture that $ p_c(d)=\\tilde{p}_c(d)$ for all $d \\ge 2$. Our strategy is to \\emph{renormalize} the synchronization model in order to reduce the effective noise parameter, and then apply a variant of the multiscale algorithm of AMMSS. The success of the renormalization procedure is conditional on a plausible but unproved assumption about the regularity of the free energy of an Ising spin glass model on $\\mathbb{Z}^d$."},{"title":"iPromoter-BnCNN: a Novel Branched CNN Based Predictor for Identifying\n  and Classifying Sigma Promoters","source":"Ruhul Amin, Chowdhury Rafeed Rahman, Md. Habibur Rahman Sifat, Md\n  Nazmul Khan Liton, Md. Moshiur Rahman, Swakkhar Shatabda and Sajid Ahmed","docs_id":"1912.10251","section":["q-bio.QM","cs.LG","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"iPromoter-BnCNN: a Novel Branched CNN Based Predictor for Identifying\n  and Classifying Sigma Promoters. Promoter is a short region of DNA which is responsible for initiating transcription of specific genes. Development of computational tools for automatic identification of promoters is in high demand. According to the difference of functions, promoters can be of different types. Promoters may have both intra and inter class variation and similarity in terms of consensus sequences. Accurate classification of various types of sigma promoters still remains a challenge. We present iPromoter-BnCNN for identification and accurate classification of six types of promoters - sigma24, sigma28, sigma32, sigma38, sigma54, sigma70. It is a Convolutional Neural Network (CNN) based classifier which combines local features related to monomer nucleotide sequence, trimer nucleotide sequence, dimer structural properties and trimer structural properties through the use of parallel branching. We conducted experiments on a benchmark dataset and compared with two state-of-the-art tools to show our supremacy on 5-fold cross-validation. Moreover, we tested our classifier on an independent test dataset. Our proposed tool iPromoter-BnCNN web server is freely available at http:\/\/103.109.52.8\/iPromoter-BnCNN. The runnable source code can be found at https:\/\/colab.research.google.com\/drive\/1yWWh7BXhsm8U4PODgPqlQRy23QGjF2DZ."},{"title":"Profit-Maximizing Planning and Control of Battery Energy Storage Systems\n  for Primary Frequency Control","source":"Ying Jun (Angela) Zhang, Changhong Zhao, Wanrong Tang, Steven H. Low","docs_id":"1604.00952","section":["cs.SY"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Profit-Maximizing Planning and Control of Battery Energy Storage Systems\n  for Primary Frequency Control. We consider a two-level profit-maximizing strategy, including planning and control, for battery energy storage system (BESS) owners that participate in the primary frequency control (PFC) market. Specifically, the optimal BESS control minimizes the operating cost by keeping the state of charge (SoC) in an optimal range. Through rigorous analysis, we prove that the optimal BESS control is a \"state-invariant\" strategy in the sense that the optimal SoC range does not vary with the state of the system. As such, the optimal control strategy can be computed offline once and for all with very low complexity. Regarding the BESS planning, we prove that the the minimum operating cost is a decreasing convex function of the BESS energy capacity. This leads to the optimal BESS sizing that strikes a balance between the capital investment and operating cost. Our work here provides a useful theoretical framework for understanding the planning and control strategies that maximize the economic benefits of BESSs in ancillary service markets."},{"title":"Efficient Spatio-Temporal Recurrent Neural Network for Video Deblurring","source":"Zhihang Zhong, Ye Gao, Yinqiang Zheng, Bo Zheng, and Imari Sato","docs_id":"2106.16028","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Efficient Spatio-Temporal Recurrent Neural Network for Video Deblurring. Real-time video deblurring still remains a challenging task due to the complexity of spatially and temporally varying blur itself and the requirement of low computational cost. To improve the network efficiency, we adopt residual dense blocks into RNN cells, so as to efficiently extract the spatial features of the current frame. Furthermore, a global spatio-temporal attention module is proposed to fuse the effective hierarchical features from past and future frames to help better deblur the current frame. Another issue needs to be addressed urgently is the lack of a real-world benchmark dataset. Thus, we contribute a novel dataset (BSD) to the community, by collecting paired blurry\/sharp video clips using a co-axis beam splitter acquisition system. Experimental results show that the proposed method (ESTRNN) can achieve better deblurring performance both quantitatively and qualitatively with less computational cost against state-of-the-art video deblurring methods. In addition, cross-validation experiments between datasets illustrate the high generality of BSD over the synthetic datasets. The code and dataset are released at https:\/\/github.com\/zzh-tech\/ESTRNN."},{"title":"Deep Neural Networks with Koopman Operators for Modeling and Control of\n  Autonomous Vehicles","source":"Yongqian Xiao, Xinglong Zhang, Xin Xu, Xueqing Liu, Jiahang Liu","docs_id":"2007.02219","section":["eess.SY","cs.SY"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Deep Neural Networks with Koopman Operators for Modeling and Control of\n  Autonomous Vehicles. Autonomous driving technologies have received notable attention in the past decades. In autonomous driving systems, identifying a precise dynamical model for motion control is nontrivial due to the strong nonlinearity and uncertainty in vehicle dynamics. Recent efforts have resorted to machine learning techniques for building vehicle dynamical models, but the generalization ability and interpretability of existing methods still need to be improved. In this paper, we propose a data-driven vehicle modeling approach based on deep neural networks with an interpretable Koopman operator. The main advantage of using the Koopman operator is to represent the nonlinear dynamics in a linear lifted feature space. In the proposed approach, a deep learning-based extended dynamic mode decomposition algorithm is presented to learn a finite-dimensional approximation of the Koopman operator. Furthermore, a data-driven model predictive controller with the learned Koopman model is designed for path tracking control of autonomous vehicles. Simulation results in a high-fidelity CarSim environment show that our approach exhibit a high modeling precision at a wide operating range and outperforms previously developed methods in terms of modeling performance. Path tracking tests of the autonomous vehicle are also performed in the CarSim environment and the results show the effectiveness of the proposed approach."},{"title":"Solving Power System Differential Algebraic Equations Using Differential\n  Transformation","source":"Yang Liu, Kai Sun","docs_id":"1903.00935","section":["eess.SY","cs.SY"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Solving Power System Differential Algebraic Equations Using Differential\n  Transformation. This paper proposes a novel non-iterative method to solve power system differential algebraic equations (DAEs) using the differential transformation, a mathematical tool that can obtain power series coefficients by transformation rules instead of calculating high order derivatives and has proved to be effective in solving state variables of nonlinear differential equations in our previous study. This paper further solves non-state variables, e.g. current injections and bus voltages, directly with a realistic DAE model of power grids. These non-state variables, nonlinearly coupled in network equations, are conventionally solved by numerical methods with time-consuming iterations, but their differential transformations are proved to satisfy formally linear equations in this paper. Thus, a non-iterative algorithm is designed to analytically solve all variables of a power system DAE model with ZIP loads. From test results on a Polish 2383-bus system, the proposed method demonstrates fast and reliable time performance compared to traditional numerical methods."},{"title":"Transformer Language Models with LSTM-based Cross-utterance Information\n  Representation","source":"G. Sun, C. Zhang, P. C. Woodland","docs_id":"2102.06474","section":["cs.CL","cs.AI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Transformer Language Models with LSTM-based Cross-utterance Information\n  Representation. The effective incorporation of cross-utterance information has the potential to improve language models (LMs) for automatic speech recognition (ASR). To extract more powerful and robust cross-utterance representations for the Transformer LM (TLM), this paper proposes the R-TLM which uses hidden states in a long short-term memory (LSTM) LM. To encode the cross-utterance information, the R-TLM incorporates an LSTM module together with a segment-wise recurrence in some of the Transformer blocks. In addition to the LSTM module output, a shortcut connection using a fusion layer that bypasses the LSTM module is also investigated. The proposed system was evaluated on the AMI meeting corpus, the Eval2000 and the RT03 telephone conversation evaluation sets. The best R-TLM achieved 0.9%, 0.6%, and 0.8% absolute WER reductions over the single-utterance TLM baseline, and 0.5%, 0.3%, 0.2% absolute WER reductions over a strong cross-utterance TLM baseline on the AMI evaluation set, Eval2000 and RT03 respectively. Improvements on Eval2000 and RT03 were further supported by significance tests. R-TLMs were found to have better LM scores on words where recognition errors are more likely to occur. The R-TLM WER can be further reduced by interpolation with an LSTM-LM."},{"title":"Sampling and Estimation for (Sparse) Exchangeable Graphs","source":"Victor Veitch and Daniel M. Roy","docs_id":"1611.00843","section":["math.ST","cs.SI","math.CO","stat.TH"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Sampling and Estimation for (Sparse) Exchangeable Graphs. Sparse exchangeable graphs on $\\mathbb{R}_+$, and the associated graphex framework for sparse graphs, generalize exchangeable graphs on $\\mathbb{N}$, and the associated graphon framework for dense graphs. We develop the graphex framework as a tool for statistical network analysis by identifying the sampling scheme that is naturally associated with the models of the framework, and by introducing a general consistent estimator for the parameter (the graphex) underlying these models. The sampling scheme is a modification of independent vertex sampling that throws away vertices that are isolated in the sampled subgraph. The estimator is a dilation of the empirical graphon estimator, which is known to be a consistent estimator for dense exchangeable graphs; both can be understood as graph analogues to the empirical distribution in the i.i.d. sequence setting. Our results may be viewed as a generalization of consistent estimation via the empirical graphon from the dense graph regime to also include sparse graphs."},{"title":"Image Aesthetics Assessment Using Composite Features from off-the-Shelf\n  Deep Models","source":"Xin Fu, Jia Yan, Cien Fan","docs_id":"1902.08546","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Image Aesthetics Assessment Using Composite Features from off-the-Shelf\n  Deep Models. Deep convolutional neural networks have recently achieved great success on image aesthetics assessment task. In this paper, we propose an efficient method which takes the global, local and scene-aware information of images into consideration and exploits the composite features extracted from corresponding pretrained deep learning models to classify the derived features with support vector machine. Contrary to popular methods that require fine-tuning or training a new model from scratch, our training-free method directly takes the deep features generated by off-the-shelf models for image classification and scene recognition. Also, we analyzed the factors that could influence the performance from two aspects: the architecture of the deep neural network and the contribution of local and scene-aware information. It turns out that deep residual network could produce more aesthetics-aware image representation and composite features lead to the improvement of overall performance. Experiments on common large-scale aesthetics assessment benchmarks demonstrate that our method outperforms the state-of-the-art results in photo aesthetics assessment."},{"title":"The Internet AS-Level Topology: Three Data Sources and One Definitive\n  Metric","source":"Priya Mahadevan, Dmitri Krioukov, Marina Fomenkov, Bradley Huffaker,\n  Xenofontas Dimitropoulos, kc claffy, Amin Vahdat","docs_id":"cs\/0512095","section":["cs.NI","physics.soc-ph"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"The Internet AS-Level Topology: Three Data Sources and One Definitive\n  Metric. We calculate an extensive set of characteristics for Internet AS topologies extracted from the three data sources most frequently used by the research community: traceroutes, BGP, and WHOIS. We discover that traceroute and BGP topologies are similar to one another but differ substantially from the WHOIS topology. Among the widely considered metrics, we find that the joint degree distribution appears to fundamentally characterize Internet AS topologies as well as narrowly define values for other important metrics. We discuss the interplay between the specifics of the three data collection mechanisms and the resulting topology views. In particular, we show how the data collection peculiarities explain differences in the resulting joint degree distributions of the respective topologies. Finally, we release to the community the input topology datasets, along with the scripts and output of our calculations. This supplement should enable researchers to validate their models against real data and to make more informed selection of topology data sources for their specific needs."},{"title":"Predicting Code Review Completion Time in Modern Code Review","source":"Moataz Chouchen, Jefferson Olongo, Ali Ouni, Mohamed Wiem Mkaouer","docs_id":"2109.15141","section":["cs.SE","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Predicting Code Review Completion Time in Modern Code Review. Context. Modern Code Review (MCR) is being adopted in both open source and commercial projects as a common practice. MCR is a widely acknowledged quality assurance practice that allows early detection of defects as well as poor coding practices. It also brings several other benefits such as knowledge sharing, team awareness, and collaboration. Problem. In practice, code reviews can experience significant delays to be completed due to various socio-technical factors which can affect the project quality and cost. For a successful review process, peer reviewers should perform their review tasks in a timely manner while providing relevant feedback about the code change being reviewed. However, there is a lack of tool support to help developers estimating the time required to complete a code review prior to accepting or declining a review request. Objective. Our objective is to build and validate an effective approach to predict the code review completion time in the context of MCR and help developers better manage and prioritize their code review tasks. Method. We formulate the prediction of the code review completion time as a learning problem. In particular, we propose a framework based on regression models to (i) effectively estimate the code review completion time, and (ii) understand the main factors influencing code review completion time."},{"title":"Elegant Object-oriented Software Design via Interactive, Evolutionary\n  Computation","source":"Christopher L. Simons and Ian C. Parmee","docs_id":"1210.1184","section":["cs.SE","cs.AI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Elegant Object-oriented Software Design via Interactive, Evolutionary\n  Computation. Design is fundamental to software development but can be demanding to perform. Thus to assist the software designer, evolutionary computing is being increasingly applied using machine-based, quantitative fitness functions to evolve software designs. However, in nature, elegance and symmetry play a crucial role in the reproductive fitness of various organisms. In addition, subjective evaluation has also been exploited in Interactive Evolutionary Computation (IEC). Therefore to investigate the role of elegance and symmetry in software design, four novel elegance measures are proposed based on the evenness of distribution of design elements. In controlled experiments in a dynamic interactive evolutionary computation environment, designers are presented with visualizations of object-oriented software designs, which they rank according to a subjective assessment of elegance. For three out of the four elegance measures proposed, it is found that a significant correlation exists between elegance values and reward elicited. These three elegance measures assess the evenness of distribution of (a) attributes and methods among classes, (b) external couples between classes, and (c) the ratio of attributes to methods. It is concluded that symmetrical elegance is in some way significant in software design, and that this can be exploited in dynamic, multi-objective interactive evolutionary computation to produce elegant software designs."},{"title":"Normal numbers with digit dependencies","source":"Christoph Aistleitner and Veronica Becher and Olivier Carton","docs_id":"1804.02844","section":["math.NT","cs.FL","math.CO","math.PR"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Normal numbers with digit dependencies. We give metric theorems for the property of Borel normality for real numbers under the assumption of digit dependencies in their expansion in a given integer base. We quantify precisely how much digit dependence can be allowed such that, still, almost all real numbers are normal. Our theorem states that almost all real numbers are normal when at least slightly more than $\\log \\log n$ consecutive digits with indices starting at position $n$ are independent. As the main application, we consider the Toeplitz set $T_P$, which is the set of all sequences $a_1a_2 \\ldots $ of symbols from $\\{0, \\ldots, b-1\\}$ such that $a_n$ is equal to $a_{pn}$, for every $p$ in $P$ and $n=1,2,\\ldots$. Here $b$ is an integer base and $P$ is a finite set of prime numbers. We show that almost every real number whose base $b$ expansion is in $T_P$ is normal to base $b$. In the case when $P$ is the singleton set $\\{2\\}$ we prove that more is true: almost every real number whose base $b$ expansion is in $T_P$ is normal to all integer bases. We also consider the Toeplitz transform which maps the set of all sequences to the set $T_P$ and we characterize the normal sequences whose Toeplitz transform is normal as well."},{"title":"DNN Speaker Tracking with Embeddings","source":"Carlos Rodrigo Castillo-Sanchez, Leibny Paola Garcia-Perera, Anabel\n  Martin-Gonzalez","docs_id":"2007.10248","section":["cs.SD","cs.LG","eess.AS"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"DNN Speaker Tracking with Embeddings. In multi-speaker applications is common to have pre-computed models from enrolled speakers. Using these models to identify the instances in which these speakers intervene in a recording is the task of speaker tracking. In this paper, we propose a novel embedding-based speaker tracking method. Specifically, our design is based on a convolutional neural network that mimics a typical speaker verification PLDA (probabilistic linear discriminant analysis) classifier and finds the regions uttered by the target speakers in an online fashion. The system was studied from two different perspectives: diarization and tracking; results on both show a significant improvement over the PLDA baseline under the same experimental conditions. Two standard public datasets, CALLHOME and DIHARD II single channel, were modified to create two-speaker subsets with overlapping and non-overlapping regions. We evaluate the robustness of our supervised approach with models generated from different segment lengths. A relative improvement of 17% in DER for DIHARD II single channel shows promising performance. Furthermore, to make the baseline system similar to speaker tracking, non-target speakers were added to the recordings. Even in these adverse conditions, our approach is robust enough to outperform the PLDA baseline."},{"title":"Kinetic Geodesic Voronoi Diagrams in a Simple Polygon","source":"Matias Korman, Andr\\'e van Renssen, Marcel Roeloffzen, Frank Staals","docs_id":"2002.05910","section":["cs.CG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Kinetic Geodesic Voronoi Diagrams in a Simple Polygon. We study the geodesic Voronoi diagram of a set $S$ of $n$ linearly moving sites inside a static simple polygon $P$ with $m$ vertices. We identify all events where the structure of the Voronoi diagram changes, bound the number of such events, and then develop a kinetic data structure (KDS) that maintains the geodesic Voronoi diagram as the sites move. To this end, we first analyze how often a single bisector, defined by two sites, or a single Voronoi center, defined by three sites, can change. For both these structures we prove that the number of such changes is at most $O(m^3)$, and that this is tight in the worst case. Moreover, we develop compact, responsive, local, and efficient kinetic data structures for both structures. Our data structures use linear space and process a worst-case optimal number of events. Our bisector KDS handles each event in $O(\\log m)$ time, and our Voronoi center handles each event in $O(\\log^2 m)$ time. Both structures can be extended to efficiently support updating the movement of the sites as well. Using these data structures as building blocks we obtain a compact KDS for maintaining the full geodesic Voronoi diagram."},{"title":"Inferring Species Trees from Incongruent Multi-Copy Gene Trees Using the\n  Robinson-Foulds Distance","source":"Ruchi Chaudhary, J. Gordon Burleigh and David Fern\\'andez-Baca","docs_id":"1210.2665","section":["cs.DS","q-bio.PE"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Inferring Species Trees from Incongruent Multi-Copy Gene Trees Using the\n  Robinson-Foulds Distance. We present a new method for inferring species trees from multi-copy gene trees. Our method is based on a generalization of the Robinson-Foulds (RF) distance to multi-labeled trees (mul-trees), i.e., gene trees in which multiple leaves can have the same label. Unlike most previous phylogenetic methods using gene trees, this method does not assume that gene tree incongruence is caused by a single, specific biological process, such as gene duplication and loss, deep coalescence, or lateral gene transfer. We prove that it is NP-hard to compute the RF distance between two mul-trees, but it is easy to calculate the generalized RF distance between a mul-tree and a singly-labeled tree. Motivated by this observation, we formulate the RF supertree problem for mul-trees (MulRF), which takes a collection of mul-trees and constructs a species tree that minimizes the total RF distance from the input mul-trees. We present a fast heuristic algorithm for the MulRF supertree problem. Simulation experiments demonstrate that the MulRF method produces more accurate species trees than gene tree parsimony methods when incongruence is caused by gene tree error, duplications and losses, and\/or lateral gene transfer. Furthermore, the MulRF heuristic runs quickly on data sets containing hundreds of trees with up to a hundred taxa."},{"title":"Hearing your touch: A new acoustic side channel on smartphones","source":"Ilia Shumailov, Laurent Simon, Jeff Yan, Ross Anderson","docs_id":"1903.11137","section":["cs.CR","cs.AI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Hearing your touch: A new acoustic side channel on smartphones. We present the first acoustic side-channel attack that recovers what users type on the virtual keyboard of their touch-screen smartphone or tablet. When a user taps the screen with a finger, the tap generates a sound wave that propagates on the screen surface and in the air. We found the device's microphone(s) can recover this wave and \"hear\" the finger's touch, and the wave's distortions are characteristic of the tap's location on the screen. Hence, by recording audio through the built-in microphone(s), a malicious app can infer text as the user enters it on their device. We evaluate the effectiveness of the attack with 45 participants in a real-world environment on an Android tablet and an Android smartphone. For the tablet, we recover 61% of 200 4-digit PIN-codes within 20 attempts, even if the model is not trained with the victim's data. For the smartphone, we recover 9 words of size 7--13 letters with 50 attempts in a common side-channel attack benchmark. Our results suggest that it not always sufficient to rely on isolation mechanisms such as TrustZone to protect user input. We propose and discuss hardware, operating-system and application-level mechanisms to block this attack more effectively. Mobile devices may need a richer capability model, a more user-friendly notification system for sensor usage and a more thorough evaluation of the information leaked by the underlying hardware."},{"title":"Stiffness modeling of non-perfect parallel manipulators","source":"Alexandr Klimchik (EMN, IRCCyN), Anatol Pashkevich (EMN, IRCCyN),\n  Damien Chablat (IRCCyN)","docs_id":"1211.5795","section":["cs.RO"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Stiffness modeling of non-perfect parallel manipulators. The paper focuses on the stiffness modeling of parallel manipulators composed of non-perfect serial chains, whose geometrical parameters differ from the nominal ones. In these manipulators, there usually exist essential internal forces\/torques that considerably affect the stiffness properties and also change the end-effector location. These internal load-ings are caused by elastic deformations of the manipulator ele-ments during assembling, while the geometrical errors in the chains are compensated for by applying appropriate forces. For this type of manipulators, a non-linear stiffness modeling tech-nique is proposed that allows us to take into account inaccuracy in the chains and to aggregate their stiffness models for the case of both small and large deflections. Advantages of the developed technique and its ability to compute and compensate for the compliance errors caused by different factors are illustrated by an example that deals with parallel manipulators of the Or-thoglide family"},{"title":"Robustness and modular structure in networks","source":"James P. Bagrow and Sune Lehmann and Yong-Yeol Ahn","docs_id":"1102.5085","section":["physics.soc-ph","cond-mat.stat-mech","cs.SI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Robustness and modular structure in networks. Complex networks have recently attracted much interest due to their prevalence in nature and our daily lives [1, 2]. A critical property of a network is its resilience to random breakdown and failure [3-6], typically studied as a percolation problem [7-9] or by modeling cascading failures [10-12]. Many complex systems, from power grids and the Internet to the brain and society [13-15], can be modeled using modular networks comprised of small, densely connected groups of nodes [16, 17]. These modules often overlap, with network elements belonging to multiple modules [18, 19]. Yet existing work on robustness has not considered the role of overlapping, modular structure. Here we study the robustness of these systems to the failure of elements. We show analytically and empirically that it is possible for the modules themselves to become uncoupled or non-overlapping well before the network disintegrates. If overlapping modular organization plays a role in overall functionality, networks may be far more vulnerable than predicted by conventional percolation theory."},{"title":"A Lipschitz Matrix for Parameter Reduction in Computational Science","source":"Jeffrey M. Hokanson and Paul G. Constantine","docs_id":"1906.00105","section":["math.NA","cs.NA"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A Lipschitz Matrix for Parameter Reduction in Computational Science. We introduce the Lipschitz matrix: a generalization of the scalar Lipschitz constant for functions with many inputs. Among the Lipschitz matrices compatible a particular function, we choose the smallest such matrix in the Frobenius norm to encode the structure of this function. The Lipschitz matrix then provides a function-dependent metric on the input space. Altering this metric to reflect a particular function improves the performance of many tasks in computational science. Compared to the Lipschitz constant, the Lipschitz matrix reduces the worst-case cost of approximation, integration, and optimization; if the Lipschitz matrix is low-rank, this cost no longer depends on the dimension of the input, but instead on the rank of the Lipschitz matrix defeating the curse of dimensionality. Both the Lipschitz constant and matrix define uncertainty away from point queries of the function and by using the Lipschitz matrix we can reduce uncertainty. If we build a minimax space-filling design of experiments in the Lipschitz matrix metric, we can further reduce this uncertainty. When the Lipschitz matrix is approximately low-rank, we can perform parameter reduction by constructing a ridge approximation whose active subspace is the span of the dominant eigenvectors of the Lipschitz matrix. In summary, the Lipschitz matrix provides a new tool for analyzing and performing parameter reduction in complex models arising in computational science."},{"title":"A dynamic mode decomposition extension for the forecasting of parametric\n  dynamical systems","source":"Francesco Andreuzzi and Nicola Demo and Gianluigi Rozza","docs_id":"2110.09155","section":["math.NA","cs.NA","math.DS"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A dynamic mode decomposition extension for the forecasting of parametric\n  dynamical systems. Dynamic mode decomposition (DMD) has recently become a popular tool for the non-intrusive analysis of dynamical systems. Exploiting the proper orthogonal decomposition as dimensionality reduction technique, DMD is able to approximate a dynamical system as a sum of (spatial) basis evolving linearly in time, allowing for a better understanding of the physical phenomena or for a future forecasting. We propose in this contribution an extension of the DMD to parametrized dynamical systems, focusing on the future forecasting of the output of interest in a parametric context. Initially, all the snapshots -- for different parameters and different time instants -- are projected to the reduced space, employing the DMD (or one of its variants) to approximate the reduced snapshots for a future instants. Still exploiting the low dimension of the reduced space, the predicted reduced snapshots are then combined using a regression technique, enabling the possibility to approximate any untested parametric configuration in any future instant. We are going to present here the algorithmic core of the aforementioned method, presenting at the end three different test cases with incremental complexity: a simple dynamical system with a linear parameter dependency, a heat problem with nonlinear parameter dependency and a fluid dynamics problem with nonlinear parameter dependency."},{"title":"Data-efficient Deep Reinforcement Learning for Dexterous Manipulation","source":"Ivaylo Popov, Nicolas Heess, Timothy Lillicrap, Roland Hafner, Gabriel\n  Barth-Maron, Matej Vecerik, Thomas Lampe, Yuval Tassa, Tom Erez, Martin\n  Riedmiller","docs_id":"1704.03073","section":["cs.LG","cs.RO"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Data-efficient Deep Reinforcement Learning for Dexterous Manipulation. Deep learning and reinforcement learning methods have recently been used to solve a variety of problems in continuous control domains. An obvious application of these techniques is dexterous manipulation tasks in robotics which are difficult to solve using traditional control theory or hand-engineered approaches. One example of such a task is to grasp an object and precisely stack it on another. Solving this difficult and practically relevant problem in the real world is an important long-term goal for the field of robotics. Here we take a step towards this goal by examining the problem in simulation and providing models and techniques aimed at solving it. We introduce two extensions to the Deep Deterministic Policy Gradient algorithm (DDPG), a model-free Q-learning based method, which make it significantly more data-efficient and scalable. Our results show that by making extensive use of off-policy data and replay, it is possible to find control policies that robustly grasp objects and stack them. Further, our results hint that it may soon be feasible to train successful stacking policies by collecting interactions on real robots."},{"title":"Speeding Up String Matching by Weak Factor Recognition","source":"Domenico Cantone, Simone Faro and Arianna Pavone","docs_id":"1707.00469","section":["cs.DS","cs.IR"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Speeding Up String Matching by Weak Factor Recognition. String matching is the problem of finding all the substrings of a text which match a given pattern. It is one of the most investigated problems in computer science, mainly due to its very diverse applications in several fields. Recently, much research in the string matching field has focused on the efficiency and flexibility of the searching procedure and quite effective techniques have been proposed for speeding up the existing solutions. In this context, algorithms based on factors recognition are among the best solutions. In this paper, we present a simple and very efficient algorithm for string matching based on a weak factor recognition and hashing. Our algorithm has a quadratic worst-case running time. However, despite its quadratic complexity, experimental results show that our algorithm obtains in most cases the best running times when compared, under various conditions, against the most effective algorithms present in literature. In the case of small alphabets and long patterns, the gain in running times reaches 28%. This makes our proposed algorithm one of the most flexible solutions in practical cases."},{"title":"Functional dependencies with null markers","source":"Antonio Badia and Daniel Lemire","docs_id":"1404.4963","section":["cs.DB"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Functional dependencies with null markers. Functional dependencies are an integral part of database design. However, they are only defined when we exclude null markers. Yet we commonly use null markers in practice. To bridge this gap between theory and practice, researchers have proposed definitions of functional dependencies over relations with null markers. Though sound, these definitions lack some qualities that we find desirable. For example, some fail to satisfy Armstrong's axioms---while these axioms are part of the foundation of common database methodologies. We propose a set of properties that any extension of functional dependencies over relations with null markers should possess. We then propose two new extensions having these properties. These extensions attempt to allow null markers where they make sense to practitioners. They both support Armstrong's axioms and provide realizable null markers: at any time, some or all of the null markers can be replaced by actual values without causing an anomaly. Our proposals may improve database designs."},{"title":"Ontological Entities for Planning and Describing Cultural Heritage 3D\n  Models Creation","source":"Nicola Amico and Achille Felicetti","docs_id":"2106.07277","section":["cs.DL"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Ontological Entities for Planning and Describing Cultural Heritage 3D\n  Models Creation. In the last decades the rapid development of technologies and methodologies in the field of digitization and 3D modelling has led to an increasing proliferation of 3D technologies in the Cultural Heritage domain. Despite the great potential of 3D digital heritage, the \"special effects\" of 3D may often overwhelm its importance in research. Projects and consortia of scholars have tried to put order in the different fields of application of these technologies, providing guidelines and proposing workflows. The use of computer graphics as an effective methodology for CH research and communication highlighted the need of transparent provenance data to properly document digital assets and understand the degree of scientific quality and reliability of their outcomes. The building and release of provenance knowledge, consisting in the complete formal documentation of each phase of the process, is therefore of fundamental importance to ensure its repeatability and to guarantee the integration and interoperability of the generated metadata on the Semantic Web. This paper proposes a methodology for documenting the planning and creation of 3D models used in archaeology and Cultural Heritage, by means of an application profile based on the CIDOC CRM ecosystem and other international standards."},{"title":"Deep Layer Aggregation","source":"Fisher Yu, Dequan Wang, Evan Shelhamer, Trevor Darrell","docs_id":"1707.06484","section":["cs.CV","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Deep Layer Aggregation. Visual recognition requires rich representations that span levels from low to high, scales from small to large, and resolutions from fine to coarse. Even with the depth of features in a convolutional network, a layer in isolation is not enough: compounding and aggregating these representations improves inference of what and where. Architectural efforts are exploring many dimensions for network backbones, designing deeper or wider architectures, but how to best aggregate layers and blocks across a network deserves further attention. Although skip connections have been incorporated to combine layers, these connections have been \"shallow\" themselves, and only fuse by simple, one-step operations. We augment standard architectures with deeper aggregation to better fuse information across layers. Our deep layer aggregation structures iteratively and hierarchically merge the feature hierarchy to make networks with better accuracy and fewer parameters. Experiments across architectures and tasks show that deep layer aggregation improves recognition and resolution compared to existing branching and merging schemes. The code is at https:\/\/github.com\/ucbdrive\/dla."},{"title":"Simulation Studies on Deep Reinforcement Learning for Building Control\n  with Human Interaction","source":"Donghwan Lee, Niao He, Seungjae Lee, Panagiota Karava, Jianghai Hu","docs_id":"2103.07919","section":["cs.AI","cs.SY","eess.SY"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Simulation Studies on Deep Reinforcement Learning for Building Control\n  with Human Interaction. The building sector consumes the largest energy in the world, and there have been considerable research interests in energy consumption and comfort management of buildings. Inspired by recent advances in reinforcement learning (RL), this paper aims at assessing the potential of RL in building climate control problems with occupant interaction. We apply a recent RL approach, called DDPG (deep deterministic policy gradient), for the continuous building control tasks and assess its performance with simulation studies in terms of its ability to handle (a) the partial state observability due to sensor limitations; (b) complex stochastic system with high-dimensional state-spaces, which are jointly continuous and discrete; (c) uncertainties due to ambient weather conditions, occupant's behavior, and comfort feelings. Especially, the partial observability and uncertainty due to the occupant interaction significantly complicate the control problem. Through simulation studies, the policy learned by DDPG demonstrates reasonable performance and computational tractability."},{"title":"$\\Pi-$nets: Deep Polynomial Neural Networks","source":"Grigorios G. Chrysos, Stylianos Moschoglou, Giorgos Bouritsas, Yannis\n  Panagakis, Jiankang Deng, Stefanos Zafeiriou","docs_id":"2003.03828","section":["cs.LG","cs.CV","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"$\\Pi-$nets: Deep Polynomial Neural Networks. Deep Convolutional Neural Networks (DCNNs) is currently the method of choice both for generative, as well as for discriminative learning in computer vision and machine learning. The success of DCNNs can be attributed to the careful selection of their building blocks (e.g., residual blocks, rectifiers, sophisticated normalization schemes, to mention but a few). In this paper, we propose $\\Pi$-Nets, a new class of DCNNs. $\\Pi$-Nets are polynomial neural networks, i.e., the output is a high-order polynomial of the input. $\\Pi$-Nets can be implemented using special kind of skip connections and their parameters can be represented via high-order tensors. We empirically demonstrate that $\\Pi$-Nets have better representation power than standard DCNNs and they even produce good results without the use of non-linear activation functions in a large battery of tasks and signals, i.e., images, graphs, and audio. When used in conjunction with activation functions, $\\Pi$-Nets produce state-of-the-art results in challenging tasks, such as image generation. Lastly, our framework elucidates why recent generative models, such as StyleGAN, improve upon their predecessors, e.g., ProGAN."},{"title":"ColloQL: Robust Cross-Domain Text-to-SQL Over Search Queries","source":"Karthik Radhakrishnan, Arvind Srikantan, Xi Victoria Lin","docs_id":"2010.09927","section":["cs.CL","cs.AI","cs.DB","cs.IR"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"ColloQL: Robust Cross-Domain Text-to-SQL Over Search Queries. Translating natural language utterances to executable queries is a helpful technique in making the vast amount of data stored in relational databases accessible to a wider range of non-tech-savvy end users. Prior work in this area has largely focused on textual input that is linguistically correct and semantically unambiguous. However, real-world user queries are often succinct, colloquial, and noisy, resembling the input of a search engine. In this work, we introduce data augmentation techniques and a sampling-based content-aware BERT model (ColloQL) to achieve robust text-to-SQL modeling over natural language search (NLS) questions. Due to the lack of evaluation data, we curate a new dataset of NLS questions and demonstrate the efficacy of our approach. ColloQL's superior performance extends to well-formed text, achieving 84.9% (logical) and 90.7% (execution) accuracy on the WikiSQL dataset, making it, to the best of our knowledge, the highest performing model that does not use execution guided decoding."},{"title":"Using learning to control artificial avatars in human motor coordination\n  tasks","source":"Maria Lombardi, Davide Liuzza, Mario di Bernardo","docs_id":"1810.04191","section":["cs.SY","q-bio.NC"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Using learning to control artificial avatars in human motor coordination\n  tasks. Designing artificial cyber-agents able to interact with human safely, smartly and in a natural way is a current open problem in control. Solving such an issue will allow the design of cyber-agents capable of co-operatively interacting with people in order to fulfil common joint tasks in a multitude of different applications. This is particularly relevant in the context of healthcare applications. Indeed, the use has been proposed of artificial agents interacting and coordinating their movements with those of a patient suffering from social or motor disorders. Specifically, it has been shown that an artificial agent exhibiting certain kinematic properties could provide innovative and efficient rehabilitation strategies for these patients. Moreover, it has also been shown that the level of motor coordination is enhanced if these kinematic properties are similar to those of the individual it is interacting with. In this paper we discuss, first, a new method based on Markov Chains to confer \"human motor characteristics\" on a virtual agent, so as that it can coordinate its motion with that of a target individual while exhibiting specific kinematic properties. Then, we embed such synthetic model in a control architecture based on reinforcement learning to synthesize a cyber-agent able to mimic the behaviour of a specific human performing a joint motor task with one or more individuals."},{"title":"Parameter Sensitivity Analysis of the SparTen High Performance Sparse\n  Tensor Decomposition Software: Extended Analysis","source":"Jeremy M. Myers, Daniel M. Dunlavy, Keita Teranishi, D. S. Hollman","docs_id":"2012.01520","section":["math.NA","cs.MS","cs.NA","cs.PF","stat.CO"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Parameter Sensitivity Analysis of the SparTen High Performance Sparse\n  Tensor Decomposition Software: Extended Analysis. Tensor decomposition models play an increasingly important role in modern data science applications. One problem of particular interest is fitting a low-rank Canonical Polyadic (CP) tensor decomposition model when the tensor has sparse structure and the tensor elements are nonnegative count data. SparTen is a high-performance C++ library which computes a low-rank decomposition using different solvers: a first-order quasi-Newton or a second-order damped Newton method, along with the appropriate choice of runtime parameters. Since default parameters in SparTen are tuned to experimental results in prior published work on a single real-world dataset conducted using MATLAB implementations of these methods, it remains unclear if the parameter defaults in SparTen are appropriate for general tensor data. Furthermore, it is unknown how sensitive algorithm convergence is to changes in the input parameter values. This report addresses these unresolved issues with large-scale experimentation on three benchmark tensor data sets. Experiments were conducted on several different CPU architectures and replicated with many initial states to establish generalized profiles of algorithm convergence behavior."},{"title":"Secure Wireless Communications via Cooperation","source":"Lun Dong, Zhu Han, Athina P. Petropulu, H. Vincent Poor","docs_id":"0809.4807","section":["cs.IT","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Secure Wireless Communications via Cooperation. The feasibility of physical-layer-based security approaches for wireless communications in the presence of one or more eavesdroppers is hampered by channel conditions. In this paper, cooperation is investigated as an approach to overcome this problem and improve the performance of secure communications. In particular, a decode-and-forward (DF) based cooperative protocol is considered, and the objective is to design the system for secrecy capacity maximization or transmit power minimization. System design for the DF-based cooperative protocol is first studied by assuming the availability of global channel state information (CSI). For the case of one eavesdropper, an iterative scheme is proposed to obtain the optimal solution for the problem of transmit power minimization. For the case of multiple eavesdroppers, the problem of secrecy capacity maximization or transmit power minimization is in general intractable. Suboptimal system design is proposed by adding an additional constraint, i.e., the complete nulling of signals at all eavesdroppers, which yields simple closed-form solutions for the aforementioned two problems. Then, the impact of imperfect CSI of eavesdroppers on system design is studied, in which the ergodic secrecy capacity is of interest."},{"title":"Siamese Labels Auxiliary Network(SiLaNet)","source":"Wenrui Gan, Zhulin Liu, C. L. Philip Chen, Tong Zhang","docs_id":"2103.00200","section":["cs.AI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Siamese Labels Auxiliary Network(SiLaNet). Auxiliary information attracts more and more attention in the area of machine learning. Attempts so far to include such auxiliary information in state-of-the-art learning process have often been based on simply appending these auxiliary features to the data level or feature level. In this paper, we intend to propose a novel training method with new options and architectures. Siamese labels, which were used in the training phase as auxiliary modules. While in the testing phase, the auxiliary module should be removed. Siamese label module makes it easier to train and improves the performance in testing process. In general, the main contributions can be summarized as, 1) Siamese Labels are firstly proposed as auxiliary information to improve the learning efficiency; 2) We establish a new architecture, Siamese Labels Auxiliary Network (SilaNet), which is to assist the training of the model; 3) Siamese Labels Auxiliary Network is applied to compress the model parameters by 50% and ensure the high accuracy at the same time. For the purpose of comparison, we tested the network on CIFAR-10 and CIFAR100 using some common models. The proposed SilaNet performs excellent efficiency both on the accuracy and robustness."},{"title":"DRST: Deep Residual Shearlet Transform for Densely Sampled Light Field\n  Reconstruction","source":"Yuan Gao, Robert Bregovic, Reinhard Koch and Atanas Gotchev","docs_id":"2003.08865","section":["cs.MM","cs.CV","eess.IV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"DRST: Deep Residual Shearlet Transform for Densely Sampled Light Field\n  Reconstruction. The Image-Based Rendering (IBR) approach using Shearlet Transform (ST) is one of the most effective methods for Densely-Sampled Light Field (DSLF) reconstruction. The ST-based DSLF reconstruction typically relies on an iterative thresholding algorithm for Epipolar-Plane Image (EPI) sparse regularization in shearlet domain, involving dozens of transformations between image domain and shearlet domain, which are in general time-consuming. To overcome this limitation, a novel learning-based ST approach, referred to as Deep Residual Shearlet Transform (DRST), is proposed in this paper. Specifically, for an input sparsely-sampled EPI, DRST employs a deep fully Convolutional Neural Network (CNN) to predict the residuals of the shearlet coefficients in shearlet domain in order to reconstruct a densely-sampled EPI in image domain. The DRST network is trained on synthetic Sparsely-Sampled Light Field (SSLF) data only by leveraging elaborately-designed masks. Experimental results on three challenging real-world light field evaluation datasets with varying moderate disparity ranges (8 - 16 pixels) demonstrate the superiority of the proposed learning-based DRST approach over the non-learning-based ST method for DSLF reconstruction. Moreover, DRST provides a 2.4x speedup over ST, at least."},{"title":"Counterfactual Mean Embeddings","source":"Krikamol Muandet, Motonobu Kanagawa, Sorawit Saengkyongam, Sanparith\n  Marukatat","docs_id":"1805.08845","section":["stat.ML","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Counterfactual Mean Embeddings. Counterfactual inference has become a ubiquitous tool in online advertisement, recommendation systems, medical diagnosis, and econometrics. Accurate modeling of outcome distributions associated with different interventions -- known as counterfactual distributions -- is crucial for the success of these applications. In this work, we propose to model counterfactual distributions using a novel Hilbert space representation called counterfactual mean embedding (CME). The CME embeds the associated counterfactual distribution into a reproducing kernel Hilbert space (RKHS) endowed with a positive definite kernel, which allows us to perform causal inference over the entire landscape of the counterfactual distribution. Based on this representation, we propose a distributional treatment effect (DTE) that can quantify the causal effect over entire outcome distributions. Our approach is nonparametric as the CME can be estimated under the unconfoundedness assumption from observational data without requiring any parametric assumption about the underlying distributions. We also establish a rate of convergence of the proposed estimator which depends on the smoothness of the conditional mean and the Radon-Nikodym derivative of the underlying marginal distributions. Furthermore, our framework allows for more complex outcomes such as images, sequences, and graphs. Our experimental results on synthetic data and off-policy evaluation tasks demonstrate the advantages of the proposed estimator."},{"title":"Relaxed Queues and Stacks from Read\/Write Operations","source":"Armando Casta\\~neda and Sergio Rajsbaum and Michel Raynal","docs_id":"2005.05427","section":["cs.DC"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Relaxed Queues and Stacks from Read\/Write Operations. Considering asynchronous shared memory systems in which any number of processes may crash, this work identifies and formally defines relaxations of queues and stacks that can be non-blocking or wait-free while being implemented using only read\/write operations. Set-linearizability and Interval-linearizability are used to specify the relaxations formally, and precisely identify the subset of executions which preserve the original sequential behavior. The relaxations allow for an item to be returned more than once by different operations, but only in case of concurrency; we call such a property multiplicity. The stack implementation is wait-free, while the queue implementation is non-blocking. Interval-linearizability is used to describe a queue with multiplicity, with the additional relaxation that a dequeue operation can return weak-empty, which means that the queue might be empty. We present a read\/write wait-free interval-linearizable algorithm of a concurrent queue. As far as we know, this work is the first that provides formalizations of the notions of multiplicity and weak-emptiness, which can be implemented on top of read\/write registers only."},{"title":"A Novel Adaptive Channel Allocation Scheme to Handle Handoffs","source":"Siva Alagu and T. Meyyappan","docs_id":"1206.3061","section":["cs.NI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A Novel Adaptive Channel Allocation Scheme to Handle Handoffs. Wireless networking is becoming an increasingly important and popular way of providing global information access to users on the move. One of the main challenges for seamless mobility is the availability of simple and robust handoff algorithms, which allow a mobile node to roam among heterogeneous wireless networks. In this paper, the authors devise a scheme, A Novel Adaptive Channel Allocation Scheme (ACAS) where the number of guard channel(s) is adjusted automatically based on the average handoff blocking rate measured in the past certain period of time. The handoff blocking rate is controlled under the designated threshold and the new call blocking rate is minimized. The performance evaluation of the ACAS is done through simulation of nodes. The result shows that the ACAS scheme outperforms the Static Channel Allocation Scheme by controlling a hard constraint on the handoff rejection probability. The proposed scheme achieves the optimal performance by maximizing the resource utilization and adapts itself to changing traffic conditions automatically."},{"title":"Equivalence of Systematic Linear Data Structures and Matrix Rigidity","source":"Sivaramakrishnan Natarajan Ramamoorthy, Cyrus Rashtchian","docs_id":"1910.11921","section":["cs.CC","cs.DS","math.CO"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Equivalence of Systematic Linear Data Structures and Matrix Rigidity. Recently, Dvir, Golovnev, and Weinstein have shown that sufficiently strong lower bounds for linear data structures would imply new bounds for rigid matrices. However, their result utilizes an algorithm that requires an $NP$ oracle, and hence, the rigid matrices are not explicit. In this work, we derive an equivalence between rigidity and the systematic linear model of data structures. For the $n$-dimensional inner product problem with $m$ queries, we prove that lower bounds on the query time imply rigidity lower bounds for the query set itself. In particular, an explicit lower bound of $\\omega\\left(\\frac{n}{r}\\log m\\right)$ for $r$ redundant storage bits would yield better rigidity parameters than the best bounds due to Alon, Panigrahy, and Yekhanin. We also prove a converse result, showing that rigid matrices directly correspond to hard query sets for the systematic linear model. As an application, we prove that the set of vectors obtained from rank one binary matrices is rigid with parameters matching the known results for explicit sets. This implies that the vector-matrix-vector problem requires query time $\\Omega(n^{3\/2}\/r)$ for redundancy $r \\geq \\sqrt{n}$ in the systematic linear model, improving a result of Chakraborty, Kamma, and Larsen. Finally, we prove a cell probe lower bound for the vector-matrix-vector problem in the high error regime, improving a result of Chattopadhyay, Kouck\\'{y}, Loff, and Mukhopadhyay."},{"title":"Forbidden minor characterizations for low-rank optimal solutions to\n  semidefinite programs over the elliptope","source":"Marianna Eisenberg-Nagy, Monique Laurent, Antonios Varvitsiotis","docs_id":"1205.2040","section":["math.CO","cs.DM","math.OC"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Forbidden minor characterizations for low-rank optimal solutions to\n  semidefinite programs over the elliptope. We study a new geometric graph parameter $\\egd(G)$, defined as the smallest integer $r\\ge 1$ for which any partial symmetric matrix which is completable to a correlation matrix and whose entries are specified at the positions of the edges of $G$, can be completed to a matrix in the convex hull of correlation matrices of $\\rank $ at most $r$. This graph parameter is motivated by its relevance to the problem of finding low rank solutions to semidefinite programs over the elliptope, and also by its relevance to the bounded rank Grothendieck constant. Indeed, $\\egd(G)\\le r$ if and only if the rank-$r$ Grothendieck constant of $G$ is equal to 1. We show that the parameter $\\egd(G)$ is minor monotone, we identify several classes of forbidden minors for $\\egd(G)\\le r$ and we give the full characterization for the case $r=2$. We also show an upper bound for $\\egd(G)$ in terms of a new tree-width-like parameter $\\sla(G)$, defined as the smallest $r$ for which $G$ is a minor of the strong product of a tree and $K_r$. We show that, for any 2-connected graph $G\\ne K_{3,3}$ on at least 6 nodes, $\\egd(G)\\le 2$ if and only if $\\sla(G)\\le 2$."},{"title":"A Fair Power Allocation Approach to NOMA in Multi-user SISO Systems","source":"Jose Armando Oviedo and Hamid R. Sadjadpour","docs_id":"1703.09394","section":["cs.IT","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A Fair Power Allocation Approach to NOMA in Multi-user SISO Systems. A non-orthogonal multiple access (NOMA) approach that always outperforms orthogonal multiple access (OMA) called Fair-NOMA is introduced. In Fair-NOMA, each mobile user is allocated its share of the transmit power such that its capacity is always greater than or equal to the capacity that can be achieved using OMA. For any slow-fading channel gains of the two users, the set of possible power allocation coefficients are derived. For the infimum and supremum of this set, the individual capacity gains and the sum-rate capacity gain are derived. It is shown that the ergodic sum-rate capacity gain approaches 1 b\/s\/Hz when the transmit power increases for the case when pairing two random users with i.i.d. channel gains. The outage probability of this approach is derived and shown to be better than OMA. The Fair-NOMA approach is applied to the case of pairing a near base-station user and a cell-edge user and the ergodic capacity gap is derived as a function of total number of users in the cell at high SNR. This is then compared to the conventional case of fixed-power NOMA with user-pairing. Finally, Fair-NOMA is extended to $K$ users and prove that the capacity can always be improved for each user, while using less than the total transmit power required to achieve OMA capacities per user."},{"title":"Open-book Video Captioning with Retrieve-Copy-Generate Network","source":"Ziqi Zhang, Zhongang Qi, Chunfeng Yuan, Ying Shan, Bing Li, Ying Deng,\n  Weiming Hu","docs_id":"2103.05284","section":["cs.CV","cs.CL"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Open-book Video Captioning with Retrieve-Copy-Generate Network. Due to the rapid emergence of short videos and the requirement for content understanding and creation, the video captioning task has received increasing attention in recent years. In this paper, we convert traditional video captioning task into a new paradigm, \\ie, Open-book Video Captioning, which generates natural language under the prompts of video-content-relevant sentences, not limited to the video itself. To address the open-book video captioning problem, we propose a novel Retrieve-Copy-Generate network, where a pluggable video-to-text retriever is constructed to retrieve sentences as hints from the training corpus effectively, and a copy-mechanism generator is introduced to extract expressions from multi-retrieved sentences dynamically. The two modules can be trained end-to-end or separately, which is flexible and extensible. Our framework coordinates the conventional retrieval-based methods with orthodox encoder-decoder methods, which can not only draw on the diverse expressions in the retrieved sentences but also generate natural and accurate content of the video. Extensive experiments on several benchmark datasets show that our proposed approach surpasses the state-of-the-art performance, indicating the effectiveness and promising of the proposed paradigm in the task of video captioning."},{"title":"Design and Analysis of Distributed State Estimation Algorithms Based on\n  Belief Propagation and Applications in Smart Grids","source":"Mirsad Cosovic","docs_id":"1811.08355","section":["cs.IT","cs.DC","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Design and Analysis of Distributed State Estimation Algorithms Based on\n  Belief Propagation and Applications in Smart Grids. We present a detailed study on application of factor graphs and the belief propagation (BP) algorithm to the power system state estimation (SE) problem. We start from the BP solution for the linear DC model, for which we provide a detailed convergence analysis. Using BP-based DC model we propose a fast real-time state estimator for the power system SE. The proposed estimator is easy to distribute and parallelize, thus alleviating computational limitations and allowing for processing measurements in real time. The presented algorithm may run as a continuous process. Using insights from the DC model, we use two different approaches to derive the BP algorithm for the non-linear model. The first method directly applies BP methodology, however, providing only approximate BP solution for the non-linear model. In the second approach, we make a key further step by providing the solution in which the BP is applied sequentially over the non-linear model, akin to what is done by the Gauss-Newton method. The resulting iterative Gauss-Newton belief propagation (GN-BP) algorithm can be interpreted as a distributed Gauss-Newton method with the same accuracy as the centralized SE."},{"title":"Discriminative Nearest Neighbor Few-Shot Intent Detection by\n  Transferring Natural Language Inference","source":"Jian-Guo Zhang, Kazuma Hashimoto, Wenhao Liu, Chien-Sheng Wu, Yao Wan,\n  Philip S. Yu, Richard Socher, Caiming Xiong","docs_id":"2010.13009","section":["cs.CL","cs.AI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Discriminative Nearest Neighbor Few-Shot Intent Detection by\n  Transferring Natural Language Inference. Intent detection is one of the core components of goal-oriented dialog systems, and detecting out-of-scope (OOS) intents is also a practically important skill. Few-shot learning is attracting much attention to mitigate data scarcity, but OOS detection becomes even more challenging. In this paper, we present a simple yet effective approach, discriminative nearest neighbor classification with deep self-attention. Unlike softmax classifiers, we leverage BERT-style pairwise encoding to train a binary classifier that estimates the best matched training example for a user input. We propose to boost the discriminative ability by transferring a natural language inference (NLI) model. Our extensive experiments on a large-scale multi-domain intent detection task show that our method achieves more stable and accurate in-domain and OOS detection accuracy than RoBERTa-based classifiers and embedding-based nearest neighbor approaches. More notably, the NLI transfer enables our 10-shot model to perform competitively with 50-shot or even full-shot classifiers, while we can keep the inference time constant by leveraging a faster embedding retrieval model."},{"title":"Base-Stations Up in the Air: Multi-UAV Trajectory Control for Min-Rate\n  Maximization in Uplink C-RAN","source":"Stefan Roth, Ali Kariminezhad and Aydin Sezgin","docs_id":"1811.10585","section":["eess.SP","cs.IT","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Base-Stations Up in the Air: Multi-UAV Trajectory Control for Min-Rate\n  Maximization in Uplink C-RAN. In this paper we study the impact of unmanned aerial vehicles (UAVs) trajectories on terrestrial users' spectral efficiency (SE). Assuming a strong line of sight path to the users, the distance from all users to all UAVs influence the outcome of an online trajectory optimization. The trajectory should be designed in a way that the fairness rate is maximized over time. That means, the UAVs travel in the directions that maximize the minimum of the users' SE. From the free-space path-loss channel model, a data-rate gradient is calculated and used to direct the UAVs in a long-term perspective towards the local optimal solution on the two-dimensional spatial grid. Therefore, a control system implementation is designed. Thereby, the UAVs follow the data-rate gradient direction while having a more smooth trajectory compared with a gradient method. The system can react to changes of the user locations online; this system design captures the interaction between multiple UAV trajectories by joint processing at the central unit, e.g., a ground base station. Because of the wide spread of user locations, the UAVs end up in optimal locations widely apart from each other. Besides, the SE expectancy is enhancing continuously while moving along this trajectory."},{"title":"Fine-Grained Prediction of Syntactic Typology: Discovering Latent\n  Structure with Supervised Learning","source":"Dingquan Wang, Jason Eisner","docs_id":"1710.03877","section":["cs.CL"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Fine-Grained Prediction of Syntactic Typology: Discovering Latent\n  Structure with Supervised Learning. We show how to predict the basic word-order facts of a novel language given only a corpus of part-of-speech (POS) sequences. We predict how often direct objects follow their verbs, how often adjectives follow their nouns, and in general the directionalities of all dependency relations. Such typological properties could be helpful in grammar induction. While such a problem is usually regarded as unsupervised learning, our innovation is to treat it as supervised learning, using a large collection of realistic synthetic languages as training data. The supervised learner must identify surface features of a language's POS sequence (hand-engineered or neural features) that correlate with the language's deeper structure (latent trees). In the experiment, we show: 1) Given a small set of real languages, it helps to add many synthetic languages to the training data. 2) Our system is robust even when the POS sequences include noise. 3) Our system on this task outperforms a grammar induction baseline by a large margin."},{"title":"Neural Monocular 3D Human Motion Capture with Physical Awareness","source":"Soshi Shimada and Vladislav Golyanik and Weipeng Xu and Patrick\n  P\\'erez and Christian Theobalt","docs_id":"2105.01057","section":["cs.CV","cs.GR","cs.HC"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Neural Monocular 3D Human Motion Capture with Physical Awareness. We present a new trainable system for physically plausible markerless 3D human motion capture, which achieves state-of-the-art results in a broad range of challenging scenarios. Unlike most neural methods for human motion capture, our approach, which we dub physionical, is aware of physical and environmental constraints. It combines in a fully differentiable way several key innovations, i.e., 1. a proportional-derivative controller, with gains predicted by a neural network, that reduces delays even in the presence of fast motions, 2. an explicit rigid body dynamics model and 3. a novel optimisation layer that prevents physically implausible foot-floor penetration as a hard constraint. The inputs to our system are 2D joint keypoints, which are canonicalised in a novel way so as to reduce the dependency on intrinsic camera parameters -- both at train and test time. This enables more accurate global translation estimation without generalisability loss. Our model can be finetuned only with 2D annotations when the 3D annotations are not available. It produces smooth and physically principled 3D motions in an interactive frame rate in a wide variety of challenging scenes, including newly recorded ones. Its advantages are especially noticeable on in-the-wild sequences that significantly differ from common 3D pose estimation benchmarks such as Human 3.6M and MPI-INF-3DHP. Qualitative results are available at http:\/\/gvv.mpi-inf.mpg.de\/projects\/PhysAware\/"},{"title":"Characterizing and Optimizing EDA Flows for the Cloud","source":"Abdelrahman Hosny and Sherief Reda","docs_id":"2102.10800","section":["cs.DC","cs.AI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Characterizing and Optimizing EDA Flows for the Cloud. Cloud computing accelerates design space exploration in logic synthesis, and parameter tuning in physical design. However, deploying EDA jobs on the cloud requires EDA teams to deeply understand the characteristics of their jobs in cloud environments. Unfortunately, there has been little to no public information on these characteristics. Thus, in this paper, we formulate the problem of migrating EDA jobs to the cloud. First, we characterize the performance of four main EDA applications, namely: synthesis, placement, routing and static timing analysis. We show that different EDA jobs require different machine configurations. Second, using observations from our characterization, we propose a novel model based on Graph Convolutional Networks to predict the total runtime of a given application on different machine configurations. Our model achieves a prediction accuracy of 87%. Third, we develop a new formulation for optimizing cloud deployments in order to reduce deployment costs while meeting deadline constraints. We present a pseudo-polynomial optimal solution using a multi-choice knapsack mapping that reduces costs by 35.29%."},{"title":"An Optimal LiDAR Configuration Approach for Self-Driving Cars","source":"Shenyu Mou, Yan Chang, Wenshuo Wang, and Ding Zhao","docs_id":"1805.07843","section":["cs.RO"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"An Optimal LiDAR Configuration Approach for Self-Driving Cars. LiDARs plays an important role in self-driving cars and its configuration such as the location placement for each LiDAR can influence object detection performance. This paper aims to investigate an optimal configuration that maximizes the utility of on-hand LiDARs. First, a perception model of LiDAR is built based on its physical attributes. Then a generalized optimization model is developed to find the optimal configuration, including the pitch angle, roll angle, and position of LiDARs. In order to fix the optimization issue with off-the-shelf solvers, we proposed a lattice-based approach by segmenting the LiDAR's range of interest into finite subspaces, thus turning the optimal configuration into a nonlinear optimization problem. A cylinder-based method is also proposed to approximate the objective function, thereby making the nonlinear optimization problem solvable. A series of simulations are conducted to validate our proposed method. This proposed approach to optimal LiDAR configuration can provide a guideline to researchers to maximize the utility of LiDARs."},{"title":"Interval-based Synthesis","source":"Angelo Montanari (Department of Mathematics and Computer Science\n  University of Udine), Pietro Sala (Department of Computer Science University\n  of Verona)","docs_id":"1408.5960","section":["cs.LO","cs.FL"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Interval-based Synthesis. We introduce the synthesis problem for Halpern and Shoham's modal logic of intervals extended with an equivalence relation over time points, abbreviated HSeq. In analogy to the case of monadic second-order logic of one successor, the considered synthesis problem receives as input an HSeq formula phi and a finite set Sigma of propositional variables and temporal requests, and it establishes whether or not, for all possible evaluations of elements in Sigma in every interval structure, there exists an evaluation of the remaining propositional variables and temporal requests such that the resulting structure is a model for phi. We focus our attention on decidability of the synthesis problem for some meaningful fragments of HSeq, whose modalities are drawn from the set A (meets), Abar (met by), B (begins), Bbar (begun by), interpreted over finite linear orders and natural numbers. We prove that the fragment ABBbareq is decidable (non-primitive recursive hard), while the fragment AAbarBBbar turns out to be undecidable. In addition, we show that even the synthesis problem for ABBbar becomes undecidable if we replace finite linear orders by natural numbers."},{"title":"The Value of Information When Deciding What to Learn","source":"Dilip Arumugam and Benjamin Van Roy","docs_id":"2110.13973","section":["cs.LG","cs.IT","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"The Value of Information When Deciding What to Learn. All sequential decision-making agents explore so as to acquire knowledge about a particular target. It is often the responsibility of the agent designer to construct this target which, in rich and complex environments, constitutes a onerous burden; without full knowledge of the environment itself, a designer may forge a sub-optimal learning target that poorly balances the amount of information an agent must acquire to identify the target against the target's associated performance shortfall. While recent work has developed a connection between learning targets and rate-distortion theory to address this challenge and empower agents that decide what to learn in an automated fashion, the proposed algorithm does not optimally tackle the equally important challenge of efficient information acquisition. In this work, building upon the seminal design principle of information-directed sampling (Russo & Van Roy, 2014), we address this shortcoming directly to couple optimal information acquisition with the optimal design of learning targets. Along the way, we offer new insights into learning targets from the literature on rate-distortion theory before turning to empirical results that confirm the value of information when deciding what to learn."},{"title":"Leveraging Structural Information to Improve Point Line Visual-Inertial\n  Odometry","source":"Bo Xu, Peng Wang, Yijia He, Yu Chen, Yongnan Chen, Ming Zhou","docs_id":"2105.04064","section":["cs.RO"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Leveraging Structural Information to Improve Point Line Visual-Inertial\n  Odometry. Leveraging line features can help to improve the localization accuracy of point-based monocular Visual-Inertial Odometry (VIO) system, as lines provide additional constraints. Moreover, in an artificial environment, some straight lines are parallel to each other. In this paper, we designed a VIO system based on points and straight lines, which divides straight lines into structural straight lines (that is, straight lines parallel to each other) and non-structural straight lines. In addition, unlike the orthogonal representation using four parameters to represent the 3D straight line, we only used two parameters to minimize the representation of the structural straight line and the non-structural straight line. Furthermore, we designed a straight line matching strategy based on sampling points to improve the efficiency and success rate of straight line matching. The effectiveness of our method is verified on both public datasets of EuRoc and TUM VI benchmark and compared with other state-of-the-art algorithms."},{"title":"Accomplice Manipulation of the Deferred Acceptance Algorithm","source":"Hadi Hosseini, Fatima Umar, Rohit Vaish","docs_id":"2012.04518","section":["cs.GT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Accomplice Manipulation of the Deferred Acceptance Algorithm. The deferred acceptance algorithm is an elegant solution to the stable matching problem that guarantees optimality and truthfulness for one side of the market. Despite these desirable guarantees, it is susceptible to strategic misreporting of preferences by the agents on the other side. We study a novel model of strategic behavior under the deferred acceptance algorithm: manipulation through an accomplice. Here, an agent on the proposed-to side (say, a woman) partners with an agent on the proposing side -- an accomplice -- to manipulate on her behalf (possibly at the expense of worsening his match). We show that the optimal manipulation strategy for an accomplice comprises of promoting exactly one woman in his true list (i.e., an inconspicuous manipulation). This structural result immediately gives a polynomial-time algorithm for computing an optimal accomplice manipulation. We also study the conditions under which the manipulated matching is stable with respect to the true preferences. Our experimental results show that accomplice manipulation outperforms self manipulation both in terms of the frequency of occurrence as well as the quality of matched partners."},{"title":"Exact solution and the multidimensional Godunov scheme for the acoustic\n  equations","source":"Wasilij Barsukow and Christian Klingenberg","docs_id":"2004.04217","section":["math.NA","cs.NA"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Exact solution and the multidimensional Godunov scheme for the acoustic\n  equations. The acoustic equations derived as a linearization of the Euler equations are a valuable system for studies of multi-dimensional solutions. Additionally they possess a low Mach number limit analogous to that of the Euler equations. Aiming at understanding the behaviour of the multi-dimensional Godunov scheme in this limit, first the exact solution of the corresponding Cauchy problem in three spatial dimensions is derived. The appearance of logarithmic singularities in the exact solution of the 4-quadrant Riemann Problem in two dimensions is discussed. The solution formulae are then used to obtain the multidimensional Godunov finite volume scheme in two dimensions. It is shown to be superior to the dimensionally split upwind\/Roe scheme concerning its domain of stability and ability to resolve multi-dimensional Riemann problems. It is shown experimentally and theoretically that despite taking into account multi-dimensional information it is, however, not able to resolve the low Mach number limit."},{"title":"Topological bifurcations in a model society of reasonable contrarians","source":"Franco Bagnoli and Raul Rechtman","docs_id":"1308.4002","section":["nlin.CG","cs.SI","nlin.CD","physics.soc-ph"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Topological bifurcations in a model society of reasonable contrarians. People are often divided into conformists and contrarians, the former tending to align to the majority opinion in their neighborhood and the latter tending to disagree with that majority. In practice, however, the contrarian tendency is rarely followed when there is an overwhelming majority with a given opinion, which denotes a social norm. Such reasonable contrarian behavior is often considered a mark of independent thought, and can be a useful strategy in financial markets. We present the opinion dynamics of a society of reasonable contrarian agents. The model is a cellular automaton of Ising type, with antiferromagnetic pair interactions modeling contrarianism and plaquette terms modeling social norms. We introduce the entropy of the collective variable as a way of comparing deterministic (mean-field) and probabilistic (simulations) bifurcation diagrams. In the mean field approximation the model exhibits bifurcations and a chaotic phase, interpreted as coherent oscillations of the whole society. However, in a one-dimensional spatial arrangement one observes incoherent oscillations and a constant average. In simulations on Watts-Strogatz networks with a small-world effect the mean field behavior is recovered, with a bifurcation diagram that resembles the mean-field one, but using the rewiring probability as the control parameter. Similar bifurcation diagrams are found for scale free networks, and we are able to compute an effective connectivity for such networks."},{"title":"Ontology-based Recommender System of Economic Articles","source":"David Werner (Le2i), Christophe Cruz (Le2i), Christophe Nicolle (Le2i)","docs_id":"1301.4781","section":["cs.IR","cs.DL"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Ontology-based Recommender System of Economic Articles. Decision makers need economical information to drive their decisions. The Company Actualis SARL is specialized in the production and distribution of a press review about French regional economic actors. This economic review represents for a client a prospecting tool on partners and competitors. To reduce the overload of useless information, the company is moving towards a customized review for each customer. Three issues appear to achieve this goal. First, how to identify the elements in the text in order to extract objects that match with the recommendation's criteria presented? Second, How to define the structure of these objects, relationships and articles in order to provide a source of knowledge usable by the extraction process to produce new knowledge from articles? The latter issue is the feedback on customer experience to identify the quality of distributed information in real-time and to improve the relevance of the recommendations. This paper presents a new type of recommendation based on the semantic description of both articles and user profile."},{"title":"Approximate Membership Query Filters with a False Positive Free Set","source":"Pedro Reviriego, Alfonso S\\'anchez-Maci\\'an, Stefan Walzer, Peter C.\n  Dillinger","docs_id":"2111.06856","section":["cs.DS"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Approximate Membership Query Filters with a False Positive Free Set. In the last decade, significant efforts have been made to reduce the false positive rate of approximate membership checking structures. This has led to the development of new structures such as cuckoo filters and xor filters. Adaptive filters that can react to false positives as they occur to avoid them for future queries to the same elements have also been recently developed. In this paper, we propose a new type of static filters that completely avoid false positives for a given set of negative elements and show how they can be efficiently implemented using xor probing filters. Several constructions of these filters with a false positive free set are proposed that minimize the memory and speed overheads introduced by avoiding false positives. The proposed filters have been extensively evaluated to validate their functionality and show that in many cases both the memory and speed overheads are negligible. We also discuss several use cases to illustrate the potential benefits of the proposed filters in practical applications."},{"title":"Neuroevolution Results in Emergence of Short-Term Memory for\n  Goal-Directed Behavior","source":"Konstantin Lakhman and Mikhail Burtsev","docs_id":"1204.3221","section":["cs.NE","cs.AI","nlin.AO"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Neuroevolution Results in Emergence of Short-Term Memory for\n  Goal-Directed Behavior. Animals behave adaptively in the environment with multiply competing goals. Understanding of the mechanisms underlying such goal-directed behavior remains a challenge for neuroscience as well for adaptive system research. To address this problem we developed an evolutionary model of adaptive behavior in the multigoal stochastic environment. Proposed neuroevolutionary algorithm is based on neuron's duplication as a basic mechanism of agent's recurrent neural network development. Results of simulation demonstrate that in the course of evolution agents acquire the ability to store the short-term memory and, therefore, use it in behavioral strategies with alternative actions. We found that evolution discovered two mechanisms for short-term memory. The first mechanism is integration of sensory signals and ongoing internal neural activity, resulting in emergence of cell groups specialized on alternative actions. And the second mechanism is slow neurodynamical processes that makes possible to code the previous behavioral choice."},{"title":"Reorganizing local image features with chaotic maps: an application to\n  texture recognition","source":"Joao Florindo","docs_id":"2007.07456","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Reorganizing local image features with chaotic maps: an application to\n  texture recognition. Despite the recent success of convolutional neural networks in texture recognition, model-based descriptors are still competitive, especially when we do not have access to large amounts of annotated data for training and the interpretation of the model is an important issue. Among the model-based approaches, fractal geometry has been one of the most popular, especially in biological applications. Nevertheless, fractals are part of a much broader family of models, which are the non-linear operators, studied in chaos theory. In this context, we propose here a chaos-based local descriptor for texture recognition. More specifically, we map the image into the three-dimensional Euclidean space, iterate a chaotic map over this three-dimensional structure and convert it back to the original image. From such chaos-transformed image at each iteration we collect local descriptors (here we use local binary patters) and those descriptors compose the feature representation of the texture. The performance of our method was verified on the classification of benchmark databases and in the identification of Brazilian plant species based on the texture of the leaf surface. The achieved results confirmed our expectation of a competitive performance, even when compared with some learning-based modern approaches in the literature."},{"title":"Effective Model Sparsification by Scheduled Grow-and-Prune Methods","source":"Xiaolong Ma, Minghai Qin, Fei Sun, Zejiang Hou, Kun Yuan, Yi Xu,\n  Yanzhi Wang, Yen-Kuang Chen, Rong Jin, Yuan Xie","docs_id":"2106.09857","section":["cs.CV","cs.AI","cs.LG","cs.NE"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Effective Model Sparsification by Scheduled Grow-and-Prune Methods. Deep neural networks (DNNs) are effective in solving many real-world problems. Larger DNN models usually exhibit better quality (e.g., accuracy) but their excessive computation results in long inference time. Model sparsification can reduce the computation and memory cost while maintaining model quality. Most existing sparsification algorithms unidirectionally remove weights, while others randomly or greedily explore a small subset of weights in each layer for pruning. The limitations of these algorithms reduce the level of achievable sparsity. In addition, many algorithms still require pre-trained dense models and thus suffer from large memory footprint. In this paper, we propose a novel scheduled grow-and-prune (GaP) methodology without having to pre-train a dense model. It addresses the shortcomings of the previous works by repeatedly growing a subset of layers to dense and then pruning them back to sparse after some training. Experiments show that the models pruned using the proposed methods match or beat the quality of the highly optimized dense models at 80% sparsity on a variety of tasks, such as image classification, objective detection, 3D object part segmentation, and translation. They also outperform other state-of-the-art (SOTA) methods for model sparsification. As an example, a 90% non-uniform sparse ResNet-50 model obtained via GaP achieves 77.9% top-1 accuracy on ImageNet, improving the previous SOTA results by 1.5%. All code will be publicly released."},{"title":"Universal Memcomputing Machines","source":"Fabio L. Traversa and Massimiliano Di Ventra","docs_id":"1405.0931","section":["cs.NE","cond-mat.mes-hall","cs.ET","cs.IT","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Universal Memcomputing Machines. We introduce the notion of universal memcomputing machines (UMMs): a class of brain-inspired general-purpose computing machines based on systems with memory, whereby processing and storing of information occur on the same physical location. We analytically prove that the memory properties of UMMs endow them with universal computing power - they are Turing-complete -, intrinsic parallelism, functional polymorphism, and information overhead, namely their collective states can support exponential data compression directly in memory. We also demonstrate that a UMM has the same computational power as a non-deterministic Turing machine, namely it can solve NP--complete problems in polynomial time. However, by virtue of its information overhead, a UMM needs only an amount of memory cells (memprocessors) that grows polynomially with the problem size. As an example we provide the polynomial-time solution of the subset-sum problem and a simple hardware implementation of the same. Even though these results do not prove the statement NP=P within the Turing paradigm, the practical realization of these UMMs would represent a paradigm shift from present von Neumann architectures bringing us closer to brain-like neural computation."},{"title":"Air-Ground Collaborative Mobile Edge Computing: Architecture,\n  Challenges, and Opportunities","source":"Zhen Qin, Hai Wang, Yuben Qu, Haipeng Dai, and Zhenhua Wei","docs_id":"2101.07930","section":["cs.NI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Air-Ground Collaborative Mobile Edge Computing: Architecture,\n  Challenges, and Opportunities. By pushing computation, cache, and network control to the edge, mobile edge computing (MEC) is expected to play a leading role in fifth generation (5G) and future sixth generation (6G). Nevertheless, facing ubiquitous fast-growing computational demands, it is impossible for a single MEC paradigm to effectively support high-quality intelligent services at end user equipments (UEs). To address this issue, we propose an air-ground collaborative MEC (AGC-MEC) architecture in this article. The proposed AGC-MEC integrates all potentially available MEC servers within air and ground in the envisioned 6G, by a variety of collaborative ways to provide computation services at their best for UEs. Firstly, we introduce the AGC-MEC architecture and elaborate three typical use cases. Then, we discuss four main challenges in the AGC-MEC as well as their potential solutions. Next, we conduct a case study of collaborative service placement for AGC-MEC to validate the effectiveness of the proposed collaborative service placement strategy. Finally, we highlight several potential research directions of the AGC-MEC."},{"title":"Programmable 3D snapshot microscopy with Fourier convolutional networks","source":"Diptodip Deb, Zhenfei Jiao, Alex B. Chen, Misha B. Ahrens, Kaspar\n  Podgorski, Srinivas C. Turaga","docs_id":"2104.10611","section":["eess.IV","cs.CV","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Programmable 3D snapshot microscopy with Fourier convolutional networks. 3D snapshot microscopy enables fast volumetric imaging by capturing a 3D volume in a single 2D camera image, and has found a variety of biological applications such as whole brain imaging of fast neural activity in larval zebrafish. The optimal microscope design for this optical 3D-to-2D encoding is both sample- and task-dependent, with no general solution known. Highly programmable optical elements create new possibilities for sample-specific computational optimization of microscope parameters, e.g. tuning the collection of light for a given sample structure. We perform such optimization with deep learning, using a differentiable wave-optics simulation of light propagation through a programmable microscope and a neural network to reconstruct volumes from the microscope image. We introduce a class of global kernel Fourier convolutional neural networks which can efficiently decode information from multiple depths in the volume, globally encoded across a 3D snapshot image. We show that our proposed networks succeed in large field of view volume reconstruction and microscope parameter optimization where traditional networks fail. We also show that our networks outperform the state-of-the-art learned reconstruction algorithms for lensless computational photography."},{"title":"ALEVS: Active Learning by Statistical Leverage Sampling","source":"Cem Orhan and \\\"Oznur Ta\\c{s}tan","docs_id":"1507.04155","section":["cs.LG","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"ALEVS: Active Learning by Statistical Leverage Sampling. Active learning aims to obtain a classifier of high accuracy by using fewer label requests in comparison to passive learning by selecting effective queries. Many active learning methods have been developed in the past two decades, which sample queries based on informativeness or representativeness of unlabeled data points. In this work, we explore a novel querying criterion based on statistical leverage scores. The statistical leverage scores of a row in a matrix are the squared row-norms of the matrix containing its (top) left singular vectors and is a measure of influence of the row on the matrix. Leverage scores have been used for detecting high influential points in regression diagnostics and have been recently shown to be useful for data analysis and randomized low-rank matrix approximation algorithms. We explore how sampling data instances with high statistical leverage scores perform in active learning. Our empirical comparison on several binary classification datasets indicate that querying high leverage points is an effective strategy."},{"title":"G-VAE, a Geometric Convolutional VAE for ProteinStructure Generation","source":"Hao Huang, Boulbaba Ben Amor, Xichan Lin, Fan Zhu, Yi Fang","docs_id":"2106.11920","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"G-VAE, a Geometric Convolutional VAE for ProteinStructure Generation. Analyzing the structure of proteins is a key part of understanding their functions and thus their role in biology at the molecular level. In addition, design new proteins in a methodical way is a major engineering challenge. In this work, we introduce a joint geometric-neural networks approach for comparing, deforming and generating 3D protein structures. Viewing protein structures as 3D open curves, we adopt the Square Root Velocity Function (SRVF) representation and leverage its suitable geometric properties along with Deep Residual Networks (ResNets) for a joint registration and comparison. Our ResNets handle better large protein deformations while being more computationally efficient. On top of the mathematical framework, we further design a Geometric Variational Auto-Encoder (G-VAE), that once trained, maps original, previously unseen structures, into a low-dimensional (latent) hyper-sphere. Motivated by the spherical structure of the pre-shape space, we naturally adopt the von Mises-Fisher (vMF) distribution to model our hidden variables. We test the effectiveness of our models by generating novel protein structures and predicting completions of corrupted protein structures. Experimental results show that our method is able to generate plausible structures, different from the structures in the training data."},{"title":"Enabling Explainable Fusion in Deep Learning with Fuzzy Integral Neural\n  Networks","source":"Muhammad Aminul Islam, Derek T. Anderson, Anthony J. Pinar, Timothy C.\n  Havens, Grant Scott, James M. Keller","docs_id":"1905.04394","section":["cs.NE","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Enabling Explainable Fusion in Deep Learning with Fuzzy Integral Neural\n  Networks. Information fusion is an essential part of numerous engineering systems and biological functions, e.g., human cognition. Fusion occurs at many levels, ranging from the low-level combination of signals to the high-level aggregation of heterogeneous decision-making processes. While the last decade has witnessed an explosion of research in deep learning, fusion in neural networks has not observed the same revolution. Specifically, most neural fusion approaches are ad hoc, are not understood, are distributed versus localized, and\/or explainability is low (if present at all). Herein, we prove that the fuzzy Choquet integral (ChI), a powerful nonlinear aggregation function, can be represented as a multi-layer network, referred to hereafter as ChIMP. We also put forth an improved ChIMP (iChIMP) that leads to a stochastic gradient descent-based optimization in light of the exponential number of ChI inequality constraints. An additional benefit of ChIMP\/iChIMP is that it enables eXplainable AI (XAI). Synthetic validation experiments are provided and iChIMP is applied to the fusion of a set of heterogeneous architecture deep models in remote sensing. We show an improvement in model accuracy and our previously established XAI indices shed light on the quality of our data, model, and its decisions."},{"title":"Multi-Task and Multi-Corpora Training Strategies to Enhance\n  Argumentative Sentence Linking Performance","source":"Jan Wira Gotama Putra and Simone Teufel and Takenobu Tokunaga","docs_id":"2109.13067","section":["cs.CL"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Multi-Task and Multi-Corpora Training Strategies to Enhance\n  Argumentative Sentence Linking Performance. Argumentative structure prediction aims to establish links between textual units and label the relationship between them, forming a structured representation for a given input text. The former task, linking, has been identified by earlier works as particularly challenging, as it requires finding the most appropriate structure out of a very large search space of possible link combinations. In this paper, we improve a state-of-the-art linking model by using multi-task and multi-corpora training strategies. Our auxiliary tasks help the model to learn the role of each sentence in the argumentative structure. Combining multi-corpora training with a selective sampling strategy increases the training data size while ensuring that the model still learns the desired target distribution well. Experiments on essays written by English-as-a-foreign-language learners show that both strategies significantly improve the model's performance; for instance, we observe a 15.8% increase in the F1-macro for individual link predictions."},{"title":"Symmetry Aware Evaluation of 3D Object Detection and Pose Estimation in\n  Scenes of Many Parts in Bulk","source":"Romain Br\\'egier (Inria), Fr\\'ed\\'eric Devernay (PRIMA, IMAGINE),\n  Laetitia Leyrit (LASMEA), James Crowley","docs_id":"1806.08129","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Symmetry Aware Evaluation of 3D Object Detection and Pose Estimation in\n  Scenes of Many Parts in Bulk. While 3D object detection and pose estimation has been studied for a long time, its evaluation is not yet completely satisfactory. Indeed, existing datasets typically consist in numerous acquisitions of only a few scenes because of the tediousness of pose annotation, and existing evaluation protocols cannot handle properly objects with symmetries. This work aims at addressing those two points. We first present automatic techniques to produce fully annotated RGBD data of many object instances in arbitrary poses, with which we produce a dataset of thousands of independent scenes of bulk parts composed of both real and synthetic images. We then propose a consistent evaluation methodology suitable for any rigid object, regardless of its symmetries. We illustrate it with two reference object detection and pose estimation methods on different objects, and show that incorporating symmetry considerations into pose estimation methods themselves can lead to significant performance gains. The proposed dataset is available at http:\/\/rbregier.github.io\/dataset2017."},{"title":"Design and Analysis Framework for Sparse FIR Channel Shortening","source":"Abubakr O. Al-Abbasi, Ridha Hamila, Waheed U. Bajwa, and Naofal\n  Al-Dhahir","docs_id":"1603.00160","section":["cs.IT","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Design and Analysis Framework for Sparse FIR Channel Shortening. A major performance and complexity limitation in broadband communications is the long channel delay spread which results in a highly-frequency-selective channel frequency response. Channel shortening equalizers (CSEs) are used to ensure that the cascade of a long channel impulse response (CIR) and the CSE is approximately equivalent to a target impulse response (TIR) with much shorter delay spread. In this paper, we propose a general framework that transforms the problems of design of sparse CSE and TIR finite impulse response (FIR) filters into the problem of sparsest-approximation of a vector in different dictionaries. In addition, we compare several choices of sparsifying dictionaries under this framework. Furthermore, the worst-case coherence of these dictionaries, which determines their sparsifying effectiveness, are analytically and\/or numerically evaluated. Finally, the usefulness of the proposed framework for the design of sparse CSE and TIR filters is validated through numerical experiments."},{"title":"The Innovative Behaviour of Software Engineers: Findings from a Pilot\n  Case Study","source":"Cleviton Monteiro, Fabio Queda Bueno da Silva, Luiz Fernando Capretz","docs_id":"1612.04648","section":["cs.SE"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"The Innovative Behaviour of Software Engineers: Findings from a Pilot\n  Case Study. Context: In the workplace, some individuals engage in the voluntary and intentional generation, promotion, and realization of new ideas for the benefit of individual performance, group effectiveness, or the organization. The literature classifies this phenomenon as innovative behaviour. Despite its importance to the development of innovation, innovative behaviour has not been fully investigated in software engineering. Objective: To understand the factors that support or inhibit innovative behaviour in software engineering practice. Method: We conducted a pilot case study in a Canadian software company using interviews and observations as data collection techniques. Using qualitative analysis, we identified relevant factors and relationships not addressed by studies from other areas. Results: Individual innovative behaviour is influenced by individual attitudes and also by situational factors such as relationships in the workplace, organizational characteristics, and project type. We built a model to express the interacting effects of these factors. Conclusions: Innovative behaviour is dependent on individual and contextual factors. Our results contribute to relevant impacts on research and practice, and to topics that deserve further study."},{"title":"Unsupervised Adaptive Re-identification in Open World Dynamic Camera\n  Networks","source":"Rameswar Panda, Amran Bhuiyan, Vittorio Murino, Amit K. Roy-Chowdhury","docs_id":"1706.03112","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Unsupervised Adaptive Re-identification in Open World Dynamic Camera\n  Networks. Person re-identification is an open and challenging problem in computer vision. Existing approaches have concentrated on either designing the best feature representation or learning optimal matching metrics in a static setting where the number of cameras are fixed in a network. Most approaches have neglected the dynamic and open world nature of the re-identification problem, where a new camera may be temporarily inserted into an existing system to get additional information. To address such a novel and very practical problem, we propose an unsupervised adaptation scheme for re-identification models in a dynamic camera network. First, we formulate a domain perceptive re-identification method based on geodesic flow kernel that can effectively find the best source camera (already installed) to adapt with a newly introduced target camera, without requiring a very expensive training phase. Second, we introduce a transitive inference algorithm for re-identification that can exploit the information from best source camera to improve the accuracy across other camera pairs in a network of multiple cameras. Extensive experiments on four benchmark datasets demonstrate that the proposed approach significantly outperforms the state-of-the-art unsupervised learning based alternatives whilst being extremely efficient to compute."},{"title":"On Controller Design for Systems on Manifolds in Euclidean Space","source":"Dong Eui Chang","docs_id":"1807.03475","section":["math.OC","cs.RO","cs.SY"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"On Controller Design for Systems on Manifolds in Euclidean Space. A new method is developed to design controllers in Euclidean space for systems defined on manifolds. The idea is to embed the state-space manifold $M$ of a given control system into some Euclidean space $\\mathbb R^n$, extend the system from $M$ to the ambient space $\\mathbb R^n$, and modify it outside $M$ to add transversal stability to $M$ in the final dynamics in $\\mathbb R^n$. Controllers are designed for the final system in the ambient space $\\mathbb R^n$. Then, their restriction to $M$ produces controllers for the original system on $M$. This method has the merit that only one single global Cartesian coordinate system in the ambient space $\\mathbb R^n$ is used for controller synthesis, and any controller design method in $\\mathbb R^n$, such as the linearization method, can be globally applied for the controller synthesis. The proposed method is successfully applied to the tracking problem for the following two benchmark systems: the fully actuated rigid body system and the quadcopter drone system."},{"title":"Temporal Second Difference Traces","source":"Mitchell Keith Bloch","docs_id":"1104.4664","section":["cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Temporal Second Difference Traces. Q-learning is a reliable but inefficient off-policy temporal-difference method, backing up reward only one step at a time. Replacing traces, using a recency heuristic, are more efficient but less reliable. In this work, we introduce model-free, off-policy temporal difference methods that make better use of experience than Watkins' Q(\\lambda). We introduce both Optimistic Q(\\lambda) and the temporal second difference trace (TSDT). TSDT is particularly powerful in deterministic domains. TSDT uses neither recency nor frequency heuristics, storing (s,a,r,s',\\delta) so that off-policy updates can be performed after apparently suboptimal actions have been taken. There are additional advantages when using state abstraction, as in MAXQ. We demonstrate that TSDT does significantly better than both Q-learning and Watkins' Q(\\lambda) in a deterministic cliff-walking domain. Results in a noisy cliff-walking domain are less advantageous for TSDT, but demonstrate the efficacy of Optimistic Q(\\lambda), a replacing trace with some of the advantages of TSDT."},{"title":"Latent fingerprint minutia extraction using fully convolutional network","source":"Yao Tang, Fei Gao, Jufu Feng","docs_id":"1609.09850","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Latent fingerprint minutia extraction using fully convolutional network. Minutiae play a major role in fingerprint identification. Extracting reliable minutiae is difficult for latent fingerprints which are usually of poor quality. As the limitation of traditional handcrafted features, a fully convolutional network (FCN) is utilized to learn features directly from data to overcome complex background noises. Raw fingerprints are mapped to a correspondingly-sized minutia-score map with a fixed stride. And thus a large number of minutiae will be extracted through a given threshold. Then small regions centering at these minutia points are entered into a convolutional neural network (CNN) to reclassify these minutiae and calculate their orientations. The CNN shares convolutional layers with the fully convolutional network to speed up. 0.45 second is used on average to detect one fingerprint on a GPU. On the NIST SD27 database, we achieve 53\\% recall rate and 53\\% precise rate that outperform many other algorithms. Our trained model is also visualized to show that we have successfully extracted features preserving ridge information of a latent fingerprint."},{"title":"Reconciling Feature-Reuse and Overfitting in DenseNet with Specialized\n  Dropout","source":"Kun Wan, Boyuan Feng, Lingwei Xie, Yufei Ding","docs_id":"1810.00091","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Reconciling Feature-Reuse and Overfitting in DenseNet with Specialized\n  Dropout. Recently convolutional neural networks (CNNs) achieve great accuracy in visual recognition tasks. DenseNet becomes one of the most popular CNN models due to its effectiveness in feature-reuse. However, like other CNN models, DenseNets also face overfitting problem if not severer. Existing dropout method can be applied but not as effective due to the introduced nonlinear connections. In particular, the property of feature-reuse in DenseNet will be impeded, and the dropout effect will be weakened by the spatial correlation inside feature maps. To address these problems, we craft the design of a specialized dropout method from three aspects, dropout location, dropout granularity, and dropout probability. The insights attained here could potentially be applied as a general approach for boosting the accuracy of other CNN models with similar nonlinear connections. Experimental results show that DenseNets with our specialized dropout method yield better accuracy compared to vanilla DenseNet and state-of-the-art CNN models, and such accuracy boost increases with the model depth."},{"title":"Query-Adaptive Hash Code Ranking for Large-Scale Multi-View Visual\n  Search","source":"Xianglong Liu, Lei Huang, Cheng Deng, Bo Lang, Dacheng Tao","docs_id":"1904.08623","section":["cs.IR","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Query-Adaptive Hash Code Ranking for Large-Scale Multi-View Visual\n  Search. Hash based nearest neighbor search has become attractive in many applications. However, the quantization in hashing usually degenerates the discriminative power when using Hamming distance ranking. Besides, for large-scale visual search, existing hashing methods cannot directly support the efficient search over the data with multiple sources, and while the literature has shown that adaptively incorporating complementary information from diverse sources or views can significantly boost the search performance. To address the problems, this paper proposes a novel and generic approach to building multiple hash tables with multiple views and generating fine-grained ranking results at bitwise and tablewise levels. For each hash table, a query-adaptive bitwise weighting is introduced to alleviate the quantization loss by simultaneously exploiting the quality of hash functions and their complement for nearest neighbor search. From the tablewise aspect, multiple hash tables are built for different data views as a joint index, over which a query-specific rank fusion is proposed to rerank all results from the bitwise ranking by diffusing in a graph. Comprehensive experiments on image search over three well-known benchmarks show that the proposed method achieves up to 17.11% and 20.28% performance gains on single and multiple table search over state-of-the-art methods."},{"title":"Generalizing to the Open World: Deep Visual Odometry with Online\n  Adaptation","source":"Shunkai Li, Xin Wu, Yingdian Cao, Hongbin Zha","docs_id":"2103.15279","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Generalizing to the Open World: Deep Visual Odometry with Online\n  Adaptation. Despite learning-based visual odometry (VO) has shown impressive results in recent years, the pretrained networks may easily collapse in unseen environments. The large domain gap between training and testing data makes them difficult to generalize to new scenes. In this paper, we propose an online adaptation framework for deep VO with the assistance of scene-agnostic geometric computations and Bayesian inference. In contrast to learning-based pose estimation, our method solves pose from optical flow and depth while the single-view depth estimation is continuously improved with new observations by online learned uncertainties. Meanwhile, an online learned photometric uncertainty is used for further depth and pose optimization by a differentiable Gauss-Newton layer. Our method enables fast adaptation of deep VO networks to unseen environments in a self-supervised manner. Extensive experiments including Cityscapes to KITTI and outdoor KITTI to indoor TUM demonstrate that our method achieves state-of-the-art generalization ability among self-supervised VO methods."},{"title":"Explicit probabilistic models for databases and networks","source":"Tijl De Bie","docs_id":"0906.5148","section":["cs.AI","cs.DB","cs.IT","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Explicit probabilistic models for databases and networks. Recent work in data mining and related areas has highlighted the importance of the statistical assessment of data mining results. Crucial to this endeavour is the choice of a non-trivial null model for the data, to which the found patterns can be contrasted. The most influential null models proposed so far are defined in terms of invariants of the null distribution. Such null models can be used by computation intensive randomization approaches in estimating the statistical significance of data mining results. Here, we introduce a methodology to construct non-trivial probabilistic models based on the maximum entropy (MaxEnt) principle. We show how MaxEnt models allow for the natural incorporation of prior information. Furthermore, they satisfy a number of desirable properties of previously introduced randomization approaches. Lastly, they also have the benefit that they can be represented explicitly. We argue that our approach can be used for a variety of data types. However, for concreteness, we have chosen to demonstrate it in particular for databases and networks."},{"title":"Adversarial Encoder-Multi-Task-Decoder for Multi-Stage Processes","source":"Andre Mendes, Julian Togelius, Leandro dos Santos Coelho","docs_id":"2003.06899","section":["cs.LG","cs.SI","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Adversarial Encoder-Multi-Task-Decoder for Multi-Stage Processes. In multi-stage processes, decisions occur in an ordered sequence of stages. Early stages usually have more observations with general information (easier\/cheaper to collect), while later stages have fewer observations but more specific data. This situation can be represented by a dual funnel structure, in which the sample size decreases from one stage to the other while the information increases. Training classifiers in this scenario is challenging since information in the early stages may not contain distinct patterns to learn (underfitting). In contrast, the small sample size in later stages can cause overfitting. We address both cases by introducing a framework that combines adversarial autoencoders (AAE), multi-task learning (MTL), and multi-label semi-supervised learning (MLSSL). We improve the decoder of the AAE with an MTL component so it can jointly reconstruct the original input and use feature nets to predict the features for the next stages. We also introduce a sequence constraint in the output of an MLSSL classifier to guarantee the sequential pattern in the predictions. Using real-world data from different domains (selection process, medical diagnosis), we show that our approach outperforms other state-of-the-art methods."},{"title":"Minimum Expected Distortion in Gaussian Source Coding with Fading Side\n  Information","source":"Chris T. K. Ng, Chao Tian, Andrea J. Goldsmith, Shlomo Shamai (Shitz)","docs_id":"0812.3709","section":["cs.IT","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Minimum Expected Distortion in Gaussian Source Coding with Fading Side\n  Information. An encoder, subject to a rate constraint, wishes to describe a Gaussian source under squared error distortion. The decoder, besides receiving the encoder's description, also observes side information consisting of uncompressed source symbol subject to slow fading and noise. The decoder knows the fading realization but the encoder knows only its distribution. The rate-distortion function that simultaneously satisfies the distortion constraints for all fading states was derived by Heegard and Berger. A layered encoding strategy is considered in which each codeword layer targets a given fading state. When the side-information channel has two discrete fading states, the expected distortion is minimized by optimally allocating the encoding rate between the two codeword layers. For multiple fading states, the minimum expected distortion is formulated as the solution of a convex optimization problem with linearly many variables and constraints. Through a limiting process on the primal and dual solutions, it is shown that single-layer rate allocation is optimal when the fading probability density function is continuous and quasiconcave (e.g., Rayleigh, Rician, Nakagami, and log-normal). In particular, under Rayleigh fading, the optimal single codeword layer targets the least favorable state as if the side information was absent."},{"title":"Bounds Preserving Temporal Integration Methods for Hyperbolic\n  Conservation Laws","source":"Tarik Dzanic, Will Trojak, and Freddie D. Witherden","docs_id":"2107.04899","section":["math.NA","cs.NA"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Bounds Preserving Temporal Integration Methods for Hyperbolic\n  Conservation Laws. In this work, we present a modification of explicit Runge-Kutta temporal integration schemes that guarantees the preservation of any locally-defined quasiconvex set of bounds for the solution. These schemes operate on the basis of a bijective mapping between an admissible set of solutions and the real domain to strictly enforce bounds. Within this framework, we show that it is possible to recover a wide range of methods independently of the spatial discretization, including positivity preserving, discrete maximum principle satisfying, entropy dissipative, and invariant domain preserving schemes. Furthermore, these schemes are proven to recover the order of accuracy of the underlying Runge-Kutta method upon which they are built. The additional computational cost is the evaluation of two nonlinear mappings which generally have closed-form solutions. We show the utility of this approach in numerical experiments using a pseudospectral spatial discretization without any explicit shock capturing schemes for nonlinear hyperbolic problems with discontinuities."},{"title":"Contextual Action Recognition with R*CNN","source":"Georgia Gkioxari, Ross Girshick, Jitendra Malik","docs_id":"1505.01197","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Contextual Action Recognition with R*CNN. There are multiple cues in an image which reveal what action a person is performing. For example, a jogger has a pose that is characteristic for jogging, but the scene (e.g. road, trail) and the presence of other joggers can be an additional source of information. In this work, we exploit the simple observation that actions are accompanied by contextual cues to build a strong action recognition system. We adapt RCNN to use more than one region for classification while still maintaining the ability to localize the action. We call our system R*CNN. The action-specific models and the feature maps are trained jointly, allowing for action specific representations to emerge. R*CNN achieves 90.2% mean AP on the PASAL VOC Action dataset, outperforming all other approaches in the field by a significant margin. Last, we show that R*CNN is not limited to action recognition. In particular, R*CNN can also be used to tackle fine-grained tasks such as attribute classification. We validate this claim by reporting state-of-the-art performance on the Berkeley Attributes of People dataset."},{"title":"Connectivity-Aware Traffic Phase Scheduling for Heterogeneously\n  Connected Vehicles","source":"Shanyu Zhou and Hulya Seferoglu","docs_id":"1608.07352","section":["cs.SY"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Connectivity-Aware Traffic Phase Scheduling for Heterogeneously\n  Connected Vehicles. We consider a transportation system of heterogeneously connected vehicles, where not all vehicles are able to communicate. Heterogeneous connectivity in transportation systems is coupled to practical constraints such that (i) not all vehicles may be equipped with devices having communication interfaces, (ii) some vehicles may not prefer to communicate due to privacy and security reasons, and (iii) communication links are not perfect and packet losses and delay occur in practice. In this context, it is crucial to develop control algorithms by taking into account the heterogeneity. In this paper, we particularly focus on making traffic phase scheduling decisions. We develop a connectivity-aware traffic phase scheduling algorithm for heterogeneously connected vehicles that increases the intersection efficiency (in terms of the average number of vehicles that are allowed to pass the intersection) by taking into account the heterogeneity. The simulation results show that our algorithm significantly improves the efficiency of intersections as compared to the baselines."},{"title":"Optimizing IoT and Web Traffic Using Selective Edge Compression","source":"Themis Melissaris, Kelly Shaw, Margaret Martonosi","docs_id":"2012.14968","section":["cs.NI","cs.PF"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Optimizing IoT and Web Traffic Using Selective Edge Compression. Internet of Things (IoT) devices and applications are generating and communicating vast quantities of data, and the rate of data collection is increasing rapidly. These high communication volumes are challenging for energy-constrained, data-capped, wireless mobile devices and networked sensors. Compression is commonly used to reduce web traffic, to save energy, and to make network transfers faster. If not used judiciously, however, compression can hurt performance. This work proposes and evaluates mechanisms that employ selective compression at the network's edge, based on data characteristics and network conditions. This approach (i) improves the performance of network transfers in IoT environments, while (ii) providing significant data savings. We demonstrate that our library speeds up web transfers by an average of 2.18x and 2.03x under fixed and dynamically changing network conditions respectively. Furthermore, it also provides consistent data savings, compacting data down to 19% of the original data size."},{"title":"TextRank Based Search Term Identification for Software Change Tasks","source":"Mohammad Masudur Rahman and Chanchal K. Roy","docs_id":"1807.02263","section":["cs.SE","cs.IR"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"TextRank Based Search Term Identification for Software Change Tasks. During maintenance, software developers deal with a number of software change requests. Each of those requests is generally written using natural language texts, and it involves one or more domain related concepts. A developer needs to map those concepts to exact source code locations within the project in order to implement the requested change. This mapping generally starts with a search within the project that requires one or more suitable search terms. Studies suggest that the developers often perform poorly in coming up with good search terms for a change task. In this paper, we propose and evaluate a novel TextRank-based technique that automatically identifies and suggests search terms for a software change task by analyzing its task description. Experiments with 349 change tasks from two subject systems and comparison with one of the latest and closely related state-of-the-art approaches show that our technique is highly promising in terms of suggestion accuracy, mean average precision and recall."},{"title":"Grid Saliency for Context Explanations of Semantic Segmentation","source":"Lukas Hoyer, Mauricio Munoz, Prateek Katiyar, Anna Khoreva, Volker\n  Fischer","docs_id":"1907.13054","section":["cs.CV","cs.AI","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Grid Saliency for Context Explanations of Semantic Segmentation. Recently, there has been a growing interest in developing saliency methods that provide visual explanations of network predictions. Still, the usability of existing methods is limited to image classification models. To overcome this limitation, we extend the existing approaches to generate grid saliencies, which provide spatially coherent visual explanations for (pixel-level) dense prediction networks. As the proposed grid saliency allows to spatially disentangle the object and its context, we specifically explore its potential to produce context explanations for semantic segmentation networks, discovering which context most influences the class predictions inside a target object area. We investigate the effectiveness of grid saliency on a synthetic dataset with an artificially induced bias between objects and their context as well as on the real-world Cityscapes dataset using state-of-the-art segmentation networks. Our results show that grid saliency can be successfully used to provide easily interpretable context explanations and, moreover, can be employed for detecting and localizing contextual biases present in the data."},{"title":"Deterministic Implementations for Reproducibility in Deep Reinforcement\n  Learning","source":"Prabhat Nagarajan, Garrett Warnell, Peter Stone","docs_id":"1809.05676","section":["cs.AI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Deterministic Implementations for Reproducibility in Deep Reinforcement\n  Learning. While deep reinforcement learning (DRL) has led to numerous successes in recent years, reproducing these successes can be extremely challenging. One reproducibility challenge particularly relevant to DRL is nondeterminism in the training process, which can substantially affect the results. Motivated by this challenge, we study the positive impacts of deterministic implementations in eliminating nondeterminism in training. To do so, we consider the particular case of the deep Q-learning algorithm, for which we produce a deterministic implementation by identifying and controlling all sources of nondeterminism in the training process. One by one, we then allow individual sources of nondeterminism to affect our otherwise deterministic implementation, and measure the impact of each source on the variance in performance. We find that individual sources of nondeterminism can substantially impact the performance of agent, illustrating the benefits of deterministic implementations. In addition, we also discuss the important role of deterministic implementations in achieving exact replicability of results."},{"title":"A novel mutation operator based on the union of fitness and design\n  spaces information for Differential Evolution","source":"H. Sharifi Noghabi, H. Rajabi Mashhadi, K. Shojaei","docs_id":"1510.02513","section":["cs.NE"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A novel mutation operator based on the union of fitness and design\n  spaces information for Differential Evolution. Differential Evolution (DE) is one of the most successful and powerful evolutionary algorithms for global optimization problem. The most important operator in this algorithm is mutation operator which parents are selected randomly to participate in it. Recently, numerous papers are tried to make this operator more intelligent by selection of parents for mutation intelligently. The intelligent selection for mutation vectors is performed by applying design space (also known as decision space) criterion or fitness space criterion, however, in both cases, half of valuable information of the problem space is disregarded. In this article, a Universal Differential Evolution (UDE) is proposed which takes advantage of both design and fitness spaces criteria for intelligent selection of mutation vectors. The experimental analysis on UDE are performed on CEC2005 benchmarks and the results stated that UDE significantly improved the performance of differential evolution in comparison with other methods that only use one criterion for intelligent selection."},{"title":"An Improved Algorithm for The $k$-Dyck Edit Distance Problem","source":"Dvir Fried, Shay Golan, Tomasz Kociumaka, Tsvi Kopelowitz, Ely Porat\n  and Tatiana Starikovskaya","docs_id":"2111.02336","section":["cs.DS"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"An Improved Algorithm for The $k$-Dyck Edit Distance Problem. A Dyck sequence is a sequence of opening and closing parentheses (of various types) that is balanced. The Dyck edit distance of a given sequence of parentheses $S$ is the smallest number of edit operations (insertions, deletions, and substitutions) needed to transform $S$ into a Dyck sequence. We consider the threshold Dyck edit distance problem, where the input is a sequence of parentheses $S$ and a positive integer $k$, and the goal is to compute the Dyck edit distance of $S$ only if the distance is at most $k$, and otherwise report that the distance is larger than $k$. Backurs and Onak [PODS'16] showed that the threshold Dyck edit distance problem can be solved in $O(n+k^{16})$ time. In this work, we design new algorithms for the threshold Dyck edit distance problem which costs $O(n+k^{4.782036})$ time with high probability or $O(n+k^{4.853059})$ deterministically. Our algorithms combine several new structural properties of the Dyck edit distance problem, a refined algorithm for fast $(\\min,+)$ matrix product, and a careful modification of ideas used in Valiant's parsing algorithm."},{"title":"A Dispersed Federated Learning Framework for 6G-Enabled Autonomous\n  Driving Cars","source":"Latif U. Khan, Yan Kyaw Tun, Madyan Alsenwi, Muhammad Imran, Zhu Han,\n  and Choong Seon Hong","docs_id":"2105.09641","section":["cs.NI","eess.SP"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A Dispersed Federated Learning Framework for 6G-Enabled Autonomous\n  Driving Cars. Sixth-Generation (6G)-based Internet of Everything applications (e.g. autonomous driving cars) have witnessed a remarkable interest. Autonomous driving cars using federated learning (FL) has the ability to enable different smart services. Although FL implements distributed machine learning model training without the requirement to move the data of devices to a centralized server, it its own implementation challenges such as robustness, centralized server security, communication resources constraints, and privacy leakage due to the capability of a malicious aggregation server to infer sensitive information of end-devices. To address the aforementioned limitations, a dispersed federated learning (DFL) framework for autonomous driving cars is proposed to offer robust, communication resource-efficient, and privacy-aware learning. A mixed-integer non-linear (MINLP) optimization problem is formulated to jointly minimize the loss in federated learning model accuracy due to packet errors and transmission latency. Due to the NP-hard and non-convex nature of the formulated MINLP problem, we propose the Block Successive Upper-bound Minimization (BSUM) based solution. Furthermore, the performance comparison of the proposed scheme with three baseline schemes has been carried out. Extensive numerical results are provided to show the validity of the proposed BSUM-based scheme."},{"title":"Multiscale dynamical embeddings of complex networks","source":"Michael T. Schaub and Jean-Charles Delvenne and Renaud Lambiotte and\n  Mauricio Barahona","docs_id":"1804.03733","section":["cs.SI","cs.SY","physics.soc-ph"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Multiscale dynamical embeddings of complex networks. Complex systems and relational data are often abstracted as dynamical processes on networks. To understand, predict and control their behavior, a crucial step is to extract reduced descriptions of such networks. Inspired by notions from Control Theory, we propose a time-dependent dynamical similarity measure between nodes, which quantifies the effect a node-input has on the network. This dynamical similarity induces an embedding that can be employed for several analysis tasks. Here we focus on (i)~dimensionality reduction, i.e., projecting nodes onto a low dimensional space that captures dynamic similarity at different time scales, and (ii)~how to exploit our embeddings to uncover functional modules. We exemplify our ideas through case studies focusing on directed networks without strong connectivity, and signed networks. We further highlight how certain ideas from community detection can be generalized and linked to Control Theory, by using the here developed dynamical perspective."},{"title":"Parametrized Invariance for Infinite State Processes","source":"Alejandro S\\'anchez, C\\'esar S\\'anchez","docs_id":"1312.4043","section":["cs.LO"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Parametrized Invariance for Infinite State Processes. We study the uniform verification problem for infinite state processes, which consists of proving that the parallel composition of an arbitrary number of processes satisfies a temporal property. Our practical motivation is to build a general framework for the temporal verification of concurrent datatypes. The contribution of this paper is a general method for the verification of safety properties of parametrized programs that manipulate complex local and global data, including mutable state in the heap. This method is based on the separation between two concerns: (1) the interaction between executing threads---handled by novel parametrized invariance rules---,and the data being manipulated---handled by specialized decision procedures. The proof rules discharge automatically a finite collection of verification conditions (VCs), the number depending only on the size of the program description and the specification, but not on the number of processes in any given instance or on the kind of data manipulated. Moreover, all VCs are quantifier free, which eases the development of decision procedures for complex data-types on top of off-the-shelf SMT solvers. We discuss the practical verification (of shape and also functional correctness properties) of a concurrent list implementation based on the method presented in this paper. Our tool also all VCs using a decision procedure for a theory of list layouts in the heap built on top of state-of-the-art SMT solvers."},{"title":"Detecting unseen visual relations using analogies","source":"Julia Peyre, Ivan Laptev, Cordelia Schmid, Josef Sivic","docs_id":"1812.05736","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Detecting unseen visual relations using analogies. We seek to detect visual relations in images of the form of triplets t = (subject, predicate, object), such as \"person riding dog\", where training examples of the individual entities are available but their combinations are unseen at training. This is an important set-up due to the combinatorial nature of visual relations : collecting sufficient training data for all possible triplets would be very hard. The contributions of this work are three-fold. First, we learn a representation of visual relations that combines (i) individual embeddings for subject, object and predicate together with (ii) a visual phrase embedding that represents the relation triplet. Second, we learn how to transfer visual phrase embeddings from existing training triplets to unseen test triplets using analogies between relations that involve similar objects. Third, we demonstrate the benefits of our approach on three challenging datasets : on HICO-DET, our model achieves significant improvement over a strong baseline for both frequent and unseen triplets, and we observe similar improvement for the retrieval of unseen triplets with out-of-vocabulary predicates on the COCO-a dataset as well as the challenging unusual triplets in the UnRel dataset."},{"title":"Budget-Constrained Multi-Armed Bandits with Multiple Plays","source":"Datong P. Zhou, Claire J. Tomlin","docs_id":"1711.05928","section":["cs.LG","cs.AI","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Budget-Constrained Multi-Armed Bandits with Multiple Plays. We study the multi-armed bandit problem with multiple plays and a budget constraint for both the stochastic and the adversarial setting. At each round, exactly $K$ out of $N$ possible arms have to be played (with $1\\leq K \\leq N$). In addition to observing the individual rewards for each arm played, the player also learns a vector of costs which has to be covered with an a-priori defined budget $B$. The game ends when the sum of current costs associated with the played arms exceeds the remaining budget. Firstly, we analyze this setting for the stochastic case, for which we assume each arm to have an underlying cost and reward distribution with support $[c_{\\min}, 1]$ and $[0, 1]$, respectively. We derive an Upper Confidence Bound (UCB) algorithm which achieves $O(NK^4 \\log B)$ regret. Secondly, for the adversarial case in which the entire sequence of rewards and costs is fixed in advance, we derive an upper bound on the regret of order $O(\\sqrt{NB\\log(N\/K)})$ utilizing an extension of the well-known $\\texttt{Exp3}$ algorithm. We also provide upper bounds that hold with high probability and a lower bound of order $\\Omega((1 - K\/N)^2 \\sqrt{NB\/K})$."},{"title":"HepML, an XML-based format for describing simulated data in high energy\n  physics","source":"S. Belov, L. Dudko, D. Kekelidze, A. Sherstnev","docs_id":"1001.2576","section":["hep-ph","cs.DL","cs.SE"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"HepML, an XML-based format for describing simulated data in high energy\n  physics. In this paper we describe a HepML format and a corresponding C++ library developed for keeping complete description of parton level events in a unified and flexible form. HepML tags contain enough information to understand what kind of physics the simulated events describe and how the events have been prepared. A HepML block can be included into event files in the LHEF format. The structure of the HepML block is described by means of several XML Schemas. The Schemas define necessary information for the HepML block and how this information should be located within the block. The library libhepml is a C++ library intended for parsing and serialization of HepML tags, and representing the HepML block in computer memory. The library is an API for external software. For example, Matrix Element Monte Carlo event generators can use the library for preparing and writing a header of a LHEF file in the form of HepML tags. In turn, Showering and Hadronization event generators can parse the HepML header and get the information in the form of C++ classes. libhepml can be used in C++, C, and Fortran programs. All necessary parts of HepML have been prepared and we present the project to the HEP community."},{"title":"Two-Stream Video Classification with Cross-Modality Attention","source":"Lu Chi and Guiyu Tian and Yadong Mu and Qi Tian","docs_id":"1908.00497","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Two-Stream Video Classification with Cross-Modality Attention. Fusing multi-modality information is known to be able to effectively bring significant improvement in video classification. However, the most popular method up to now is still simply fusing each stream's prediction scores at the last stage. A valid question is whether there exists a more effective method to fuse information cross modality. With the development of attention mechanism in natural language processing, there emerge many successful applications of attention in the field of computer vision. In this paper, we propose a cross-modality attention operation, which can obtain information from other modality in a more effective way than two-stream. Correspondingly we implement a compatible block named CMA block, which is a wrapper of our proposed attention operation. CMA can be plugged into many existing architectures. In the experiments, we comprehensively compare our method with two-stream and non-local models widely used in video classification. All experiments clearly demonstrate strong performance superiority by our proposed method. We also analyze the advantages of the CMA block by visualizing the attention map, which intuitively shows how the block helps the final prediction."},{"title":"Strategy Synthesis for Partially-known Switched Stochastic Systems","source":"John Jackson (1), Luca Laurenti (2), Eric Frew (1), Morteza Lahijanian\n  (1) ((1) University of Colorado Boulder, (2) TU Delft)","docs_id":"2104.02172","section":["eess.SY","cs.SY"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Strategy Synthesis for Partially-known Switched Stochastic Systems. We present a data-driven framework for strategy synthesis for partially-known switched stochastic systems. The properties of the system are specified using linear temporal logic (LTL) over finite traces (LTLf), which is as expressive as LTL and enables interpretations over finite behaviors. The framework first learns the unknown dynamics via Gaussian process regression. Then, it builds a formal abstraction of the switched system in terms of an uncertain Markov model, namely an Interval Markov Decision Process (IMDP), by accounting for both the stochastic behavior of the system and the uncertainty in the learning step. Then, we synthesize a strategy on the resulting IMDP that maximizes the satisfaction probability of the LTLf specification and is robust against all the uncertainties in the abstraction. This strategy is then refined into a switching strategy for the original stochastic system. We show that this strategy is near-optimal and provide a bound on its distance (error) to the optimal strategy. We experimentally validate our framework on various case studies, including both linear and non-linear switched stochastic systems."},{"title":"Leader-Contention-Based User Matching for 802.11 Multiuser MIMO Networks","source":"Tung-Wei Kuo, Kuang-Che Lee, Kate Ching-Ju Lin and Ming-Jer Tsai","docs_id":"1404.6041","section":["cs.NI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Leader-Contention-Based User Matching for 802.11 Multiuser MIMO Networks. In multiuser MIMO (MU-MIMO) LANs, the achievable throughput of a client depends on who are transmitting concurrently with it. Existing MU-MIMO MAC protocols however enable clients to use the traditional 802.11 contention to contend for concurrent transmission opportunities on the uplink. Such a contention-based protocol not only wastes lots of channel time on multiple rounds of contention, but also fails to maximally deliver the gain of MU-MIMO because users randomly join concurrent transmissions without considering their channel characteristics. To address such inefficiency, this paper introduces MIMOMate, a leader-contention-based MU-MIMO MAC protocol that matches clients as concurrent transmitters according to their channel characteristics to maximally deliver the MU-MIMO gain, while ensuring all users to fairly share concurrent transmission opportunities. Furthermore, MIMOMate elects the leader of the matched users to contend for transmission opportunities using traditional 802.11 CSMA\/CA. It hence requires only a single contention overhead for concurrent streams, and can be compatible with legacy 802.11 devices. A prototype implementation in USRP-N200 shows that MIMOMate achieves an average throughput gain of 1.42x and 1.52x over the traditional contention-based protocol for 2-antenna and 3-antenna AP scenarios, respectively, and also provides fairness for clients."},{"title":"Advances in Scaling Community Discovery Methods for Large Signed Graph\n  Networks","source":"Maria Tomasso and Lucas Rusnak and Jelena Te\\v{s}i\\'c","docs_id":"2110.07514","section":["cs.SI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Advances in Scaling Community Discovery Methods for Large Signed Graph\n  Networks. Community detection is a common task in social network analysis (SNA) with applications in a variety of fields including medicine, criminology, and business. Despite the popularity of community detection, there is no clear consensus on the most effective methodology for signed networks. In this paper, we summarize the development of community detection in signed networks and evaluate current state-of-the-art techniques on several real-world data sets. First, we give a comprehensive background of community detection in signed graphs. Next, we compare various adaptations of the Laplacian matrix in recovering ground-truth community labels via spectral clustering in small signed graph data sets. Then, we evaluate the scalability of leading algorithms on small, large, dense, and sparse real-world signed graph networks. We conclude with a discussion of our novel findings and recommendations for extensions and improvements in state-of-the-art techniques for signed graph community discovery in large, sparse, real-world signed graphs."},{"title":"An entropy stable spectral vanishing viscosity for discontinuous\n  Galerkin schemes: application to shock capturing and LES models","source":"Andr\\'es Mateo-Gab\\'in, Juan Manzanero, Eusebio Valero","docs_id":"2109.06653","section":["math.NA","cs.NA","physics.flu-dyn"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"An entropy stable spectral vanishing viscosity for discontinuous\n  Galerkin schemes: application to shock capturing and LES models. We present a stable spectral vanishing viscosity for discontinuous Galerkin schemes, with applications to turbulent and supersonic flows. The idea behind the SVV is to spatially filter the dissipative fluxes, such that it concentrates in higher wavenumbers, where the flow is typically under-resolved, leaving low wavenumbers dissipation-free. Moreover, we derive a stable approximation of the Guermond-Popov fluxes with the Bassi-Rebay 1 scheme, used to introduce density regularization in shock capturing simulations. This filtering uses a Cholesky decomposition of the fluxes that ensures the entropy stability of the scheme, which also includes a stable approximation of boundary conditions for adiabatic walls. For turbulent flows, we test the method with the three-dimensional Taylor-Green vortex and show that energy is correctly dissipated, and the scheme is stable when a kinetic energy preserving split-form is used in combination with a low dissipation Riemann solver. Finally, we test the shock capturing capabilities of our method with the Shu-Osher and the supersonic forward facing step cases, obtaining good results without spurious oscillations even with coarse meshes."},{"title":"Tuneful: An Online Significance-Aware Configuration Tuner for Big Data\n  Analytics","source":"Ayat Fekry, Lucian Carata, Thomas Pasquier, Andrew Rice, Andy Hopper","docs_id":"2001.08002","section":["cs.DC","cs.SY","eess.SY"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Tuneful: An Online Significance-Aware Configuration Tuner for Big Data\n  Analytics. Distributed analytics engines such as Spark are a common choice for processing extremely large datasets. However, finding good configurations for these systems remains challenging, with each workload potentially requiring a different setup to run optimally. Using suboptimal configurations incurs significant extra runtime costs. %Furthermore, Spark and similar platforms are gaining traction within data-scientists communities where awareness of such issues is relatively low. We propose Tuneful, an approach that efficiently tunes the configuration of in-memory cluster computing systems. Tuneful combines incremental Sensitivity Analysis and Bayesian optimization to identify near-optimal configurations from a high-dimensional search space, using a small number of executions. This setup allows the tuning to be done online, without any previous training. Our experimental results show that Tuneful reduces the search time for finding close-to-optimal configurations by 62\\% (at the median) when compared to existing state-of-the-art techniques. This means that the amortization of the tuning cost happens significantly faster, enabling practical tuning for new classes of workloads."},{"title":"Active Access: A Mechanism for High-Performance Distributed Data-Centric\n  Computations","source":"Maciej Besta, Torsten Hoefler","docs_id":"1910.12897","section":["cs.DC","cs.AR","cs.NI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Active Access: A Mechanism for High-Performance Distributed Data-Centric\n  Computations. Remote memory access (RMA) is an emerging high-performance programming model that uses RDMA hardware directly. Yet, accessing remote memories cannot invoke activities at the target which complicates implementation and limits performance of data-centric algorithms. We propose Active Access (AA), a mechanism that integrates well-known active messaging (AM) semantics with RMA to enable high-performance distributed data-centric computations. AA supports a new programming model where the user specifies handlers that are triggered when incoming puts and gets reference designated addresses. AA is based on a set of extensions to the Input\/Output Memory Management Unit (IOMMU), a unit that provides high-performance hardware support for remapping I\/O accesses to memory. We illustrate that AA outperforms existing AM and RMA designs, accelerates various codes such as distributed hashtables or logging schemes, and enables new protocols such as incremental checkpointing for RMA.We also discuss how extended IOMMUs can support a virtualized global address space in a distributed system that offers features known from on-node memory virtualization. We expect that AA can enhance the design of HPC operating and runtime systems in large computing centers."},{"title":"Flow Motifs in Interaction Networks","source":"Chrysanthi Kosyfaki, Nikos Mamoulis, Evaggelia Pitoura, Panayiotis\n  Tsaparas","docs_id":"1810.08408","section":["cs.SI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Flow Motifs in Interaction Networks. Many real-world phenomena are best represented as interaction networks with dynamic structures (e.g., transaction networks, social networks, traffic networks). Interaction networks capture flow of data which is transferred between their vertices along a timeline. Analyzing such networks is crucial toward comprehend- ing processes in them. A typical analysis task is the finding of motifs, which are small subgraph patterns that repeat themselves in the network. In this paper, we introduce network flow motifs, a novel type of motifs that model significant flow transfer among a set of vertices within a constrained time window. We design an algorithm for identifying flow motif instances in a large graph. Our algorithm can be easily adapted to find the top-k instances of maximal flow. In addition, we design a dynamic programming module that finds the instance with the maximum flow. We evaluate the performance of the algorithm on three real datasets and identify flow motifs which are significant for these graphs. Our results show that our algorithm is scalable and that the real networks indeed include interesting motifs, which appear much more frequently than in randomly generated networks having similar characteristics."},{"title":"A Learning and Masking Approach to Secure Learning","source":"Linh Nguyen, Sky Wang, Arunesh Sinha","docs_id":"1709.04447","section":["cs.CR","cs.CV","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A Learning and Masking Approach to Secure Learning. Deep Neural Networks (DNNs) have been shown to be vulnerable against adversarial examples, which are data points cleverly constructed to fool the classifier. Such attacks can be devastating in practice, especially as DNNs are being applied to ever increasing critical tasks like image recognition in autonomous driving. In this paper, we introduce a new perspective on the problem. We do so by first defining robustness of a classifier to adversarial exploitation. Next, we show that the problem of adversarial example generation can be posed as learning problem. We also categorize attacks in literature into high and low perturbation attacks; well-known attacks like fast-gradient sign method (FGSM) and our attack produce higher perturbation adversarial examples while the more potent but computationally inefficient Carlini-Wagner (CW) attack is low perturbation. Next, we show that the dual approach of the attack learning problem can be used as a defensive technique that is effective against high perturbation attacks. Finally, we show that a classifier masking method achieved by adding noise to the a neural network's logit output protects against low distortion attacks such as the CW attack. We also show that both our learning and masking defense can work simultaneously to protect against multiple attacks. We demonstrate the efficacy of our techniques by experimenting with the MNIST and CIFAR-10 datasets."},{"title":"Inexpensive surface electromyography sleeve with consistent electrode\n  placement enables dexterous and stable prosthetic control through deep\n  learning","source":"Jacob A. George, Anna Neibling, Michael D. Paskett, Gregory A. Clark","docs_id":"2003.00070","section":["cs.RO","cs.CV","q-bio.NC"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Inexpensive surface electromyography sleeve with consistent electrode\n  placement enables dexterous and stable prosthetic control through deep\n  learning. The dexterity of conventional myoelectric prostheses is limited in part by the small datasets used to train the control algorithms. Variations in surface electrode positioning make it difficult to collect consistent data and to estimate motor intent reliably over time. To address these challenges, we developed an inexpensive, easy-to-don sleeve that can record robust and repeatable surface electromyography from 32 embedded monopolar electrodes. Embedded grommets are used to consistently align the sleeve with natural skin markings (e.g., moles, freckles, scars). The sleeve can be manufactured in a few hours for less than $60. Data from seven intact participants show the sleeve provides a signal-to-noise ratio of 14, a don-time under 11 seconds, and sub-centimeter precision for electrode placement. Furthermore, in a case study with one intact participant, we use the sleeve to demonstrate that neural networks can provide simultaneous and proportional control of six degrees of freedom, even 263 days after initial algorithm training. We also highlight that consistent recordings, accumulated over time to establish a large dataset, significantly improve dexterity. These results suggest that deep learning with a 74-layer neural network can substantially improve the dexterity and stability of myoelectric prosthetic control, and that deep-learning techniques can be readily instantiated and further validated through inexpensive sleeves\/sockets with consistent recording locations."},{"title":"Applying Dynamic Model for Multiple Manoeuvring Target Tracking Using\n  Particle Filtering","source":"Mohammad Javad Parseh and Saeid Pashazadeh","docs_id":"1211.4524","section":["cs.CV","cs.AI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Applying Dynamic Model for Multiple Manoeuvring Target Tracking Using\n  Particle Filtering. In this paper, we applied a dynamic model for manoeuvring targets in SIR particle filter algorithm for improving tracking accuracy of multiple manoeuvring targets. In our proposed approach, a color distribution model is used to detect changes of target's model . Our proposed approach controls deformation of target's model. If deformation of target's model is larger than a predetermined threshold, then the model will be updated. Global Nearest Neighbor (GNN) algorithm is used as data association algorithm. We named our proposed method as Deformation Detection Particle Filter (DDPF) . DDPF approach is compared with basic SIR-PF algorithm on real airshow videos. Comparisons results show that, the basic SIR-PF algorithm is not able to track the manoeuvring targets when the rotation or scaling is occurred in target' s model. However, DDPF approach updates target's model when the rotation or scaling is occurred. Thus, the proposed approach is able to track the manoeuvring targets more efficiently and accurately."},{"title":"Leveraging Trust and Distrust in Recommender Systems via Deep Learning","source":"Dimitrios Rafailidis","docs_id":"1905.13612","section":["cs.LG","cs.IR","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Leveraging Trust and Distrust in Recommender Systems via Deep Learning. The data scarcity of user preferences and the cold-start problem often appear in real-world applications and limit the recommendation accuracy of collaborative filtering strategies. Leveraging the selections of social friends and foes can efficiently face both problems. In this study, we propose a strategy that performs social deep pairwise learning. Firstly, we design a ranking loss function incorporating multiple ranking criteria based on the choice in users, and the choice in their friends and foes to improve the accuracy in the top-k recommendation task. We capture the nonlinear correlations between user preferences and the social information of trust and distrust relationships via a deep learning strategy. In each backpropagation step, we follow a social negative sampling strategy to meet the multiple ranking criteria of our ranking loss function. We conduct comprehensive experiments on a benchmark dataset from Epinions, among the largest publicly available that has been reported in the relevant literature. The experimental results demonstrate that the proposed model beats other state-of-the art methods, attaining an 11.49% average improvement over the most competitive model. We show that our deep learning strategy plays an important role in capturing the nonlinear correlations between user preferences and the social information of trust and distrust relationships, and demonstrate the importance of our social negative sampling strategy on the proposed model."},{"title":"Material-separating regularizer for multi-energy X-ray tomography","source":"Jacek Gondzio, Matti Lassas, Salla-Maaria Latva-\\\"Aij\\\"o, Samuli\n  Siltanen, Filippo Zanetti","docs_id":"2107.03535","section":["math.NA","cs.NA","physics.med-ph"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Material-separating regularizer for multi-energy X-ray tomography. Dual-energy X-ray tomography is considered in a context where the target under imaging consists of two distinct materials. The materials are assumed to be possibly intertwined in space, but at any given location there is only one material present. Further, two X-ray energies are chosen so that there is a clear difference in the spectral dependence of the attenuation coefficients of the two materials. A novel regularizer is presented for the inverse problem of reconstructing separate tomographic images for the two materials. A combination of two things, (a) non-negativity constraint, and (b) penalty term containing the inner product between the two material images, promotes the presence of at most one material in a given pixel. A preconditioned interior point method is derived for the minimization of the regularization functional. Numerical tests with digital phantoms suggest that the new algorithm outperforms the baseline method, Joint Total Variation regularization, in terms of correctly material-characterized pixels. While the method is tested only in a two-dimensional setting with two materials and two energies, the approach readily generalizes to three dimensions and more materials. The number of materials just needs to match the number of energies used in imaging."},{"title":"Side-Channel Inference Attacks on Mobile Keypads using Smartwatches","source":"Anindya Maiti, Murtuza Jadliwala, Jibo He, Igor Bilogrevic","docs_id":"1710.03656","section":["cs.CR"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Side-Channel Inference Attacks on Mobile Keypads using Smartwatches. Smartwatches enable many novel applications and are fast gaining popularity. However, the presence of a diverse set of on-board sensors provides an additional attack surface to malicious software and services on these devices. In this paper, we investigate the feasibility of key press inference attacks on handheld numeric touchpads by using smartwatch motion sensors as a side-channel. We consider different typing scenarios, and propose multiple attack approaches to exploit the characteristics of the observed wrist movements for inferring individual key presses. Experimental evaluation using commercial off-the-shelf smartwatches and smartphones show that key press inference using smartwatch motion sensors is not only fairly accurate, but also comparable with similar attacks using smartphone motion sensors. Additionally, hand movements captured by a combination of both smartwatch and smartphone motion sensors yields better inference accuracy than either device considered individually."},{"title":"Multimodal Matching Transformer for Live Commenting","source":"Chaoqun Duan and Lei Cui and Shuming Ma and Furu Wei and Conghui Zhu\n  and Tiejun Zhao","docs_id":"2002.02649","section":["cs.CL"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Multimodal Matching Transformer for Live Commenting. Automatic live commenting aims to provide real-time comments on videos for viewers. It encourages users engagement on online video sites, and is also a good benchmark for video-to-text generation. Recent work on this task adopts encoder-decoder models to generate comments. However, these methods do not model the interaction between videos and comments explicitly, so they tend to generate popular comments that are often irrelevant to the videos. In this work, we aim to improve the relevance between live comments and videos by modeling the cross-modal interactions among different modalities. To this end, we propose a multimodal matching transformer to capture the relationships among comments, vision, and audio. The proposed model is based on the transformer framework and can iteratively learn the attention-aware representations for each modality. We evaluate the model on a publicly available live commenting dataset. Experiments show that the multimodal matching transformer model outperforms the state-of-the-art methods."},{"title":"Secrecy and Energy Efficiency in Massive MIMO Aided Heterogeneous C-RAN:\n  A New Look at Interference","source":"Lifeng Wang, Kai-Kit Wong, Maged Elkashlan, Arumugam Nallanathan, and\n  Sangarapillai Lambotharan","docs_id":"1607.03344","section":["cs.NI","cs.IT","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Secrecy and Energy Efficiency in Massive MIMO Aided Heterogeneous C-RAN:\n  A New Look at Interference. In this paper, we investigate the potential benefits of the massive multiple-input multiple-output (MIMO) enabled heterogeneous cloud radio access network (C-RAN) in terms of the secrecy and energy efficiency (EE). In this network, both remote radio heads (RRHs) and massive MIMO macrocell base stations (BSs) are deployed and soft fractional frequency reuse (S-FFR) is adopted to mitigate the inter-tier interference. We first examine the physical layer security by deriving the area ergodic secrecy rate and secrecy outage probability. Our results reveal that the use of massive MIMO and C-RAN can greatly improve the secrecy performance. For C-RAN, a large number of RRHs achieves high area ergodic secrecy rate and low secrecy outage probability, due to its powerful interference management. We find that for massive MIMO aided macrocells, having more antennas and serving more users improves secrecy performance. Then we derive the EE of the heterogeneous C-RAN, illustrating that increasing the number of RRHs significantly enhances the network EE. Furthermore, it is indicated that allocating more radio resources to the RRHs can linearly increase the EE of RRH tier and improve the network EE without affecting the EE of the macrocells."},{"title":"Gaussian Process-based Min-norm Stabilizing Controller for\n  Control-Affine Systems with Uncertain Input Effects and Dynamics","source":"Fernando Casta\\~neda, Jason J. Choi, Bike Zhang, Claire J. Tomlin and\n  Koushil Sreenath","docs_id":"2011.07183","section":["eess.SY","cs.LG","cs.SY","math.OC"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Gaussian Process-based Min-norm Stabilizing Controller for\n  Control-Affine Systems with Uncertain Input Effects and Dynamics. This paper presents a method to design a min-norm Control Lyapunov Function (CLF)-based stabilizing controller for a control-affine system with uncertain dynamics using Gaussian Process (GP) regression. In order to estimate both state and input-dependent model uncertainty, we propose a novel compound kernel that captures the control-affine nature of the problem. Furthermore, by the use of GP Upper Confidence Bound analysis, we provide probabilistic bounds of the regression error, leading to the formulation of a CLF-based stability chance constraint which can be incorporated in a min-norm optimization problem. We show that this resulting optimization problem is convex, and we call it Gaussian Process-based Control Lyapunov Function Second-Order Cone Program (GP-CLF-SOCP). The data-collection process and the training of the GP regression model are carried out in an episodic learning fashion. We validate the proposed algorithm and controller in numerical simulations of an inverted pendulum and a kinematic bicycle model, resulting in stable trajectories which are very similar to the ones obtained if we actually knew the true plant dynamics."},{"title":"Fooling Detection Alone is Not Enough: First Adversarial Attack against\n  Multiple Object Tracking","source":"Yunhan Jia, Yantao Lu, Junjie Shen, Qi Alfred Chen, Zhenyu Zhong, Tao\n  Wei","docs_id":"1905.11026","section":["cs.CV","cs.CR"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Fooling Detection Alone is Not Enough: First Adversarial Attack against\n  Multiple Object Tracking. Recent work in adversarial machine learning started to focus on the visual perception in autonomous driving and studied Adversarial Examples (AEs) for object detection models. However, in such visual perception pipeline the detected objects must also be tracked, in a process called Multiple Object Tracking (MOT), to build the moving trajectories of surrounding obstacles. Since MOT is designed to be robust against errors in object detection, it poses a general challenge to existing attack techniques that blindly target objection detection: we find that a success rate of over 98% is needed for them to actually affect the tracking results, a requirement that no existing attack technique can satisfy. In this paper, we are the first to study adversarial machine learning attacks against the complete visual perception pipeline in autonomous driving, and discover a novel attack technique, tracker hijacking, that can effectively fool MOT using AEs on object detection. Using our technique, successful AEs on as few as one single frame can move an existing object in to or out of the headway of an autonomous vehicle to cause potential safety hazards. We perform evaluation using the Berkeley Deep Drive dataset and find that on average when 3 frames are attacked, our attack can have a nearly 100% success rate while attacks that blindly target object detection only have up to 25%."},{"title":"Data-Driven Ensembles for Deep and Hard-Decision Hybrid Decoding","source":"Tomer Raviv, Nir Raviv, Yair Be'ery","docs_id":"2001.06247","section":["cs.IT","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Data-Driven Ensembles for Deep and Hard-Decision Hybrid Decoding. Ensemble models are widely used to solve complex tasks by their decomposition into multiple simpler tasks, each one solved locally by a single member of the ensemble. Decoding of error-correction codes is a hard problem due to the curse of dimensionality, leading one to consider ensembles-of-decoders as a possible solution. Nonetheless, one must take complexity into account, especially in decoding. We suggest a low-complexity scheme where a single member participates in the decoding of each word. First, the distribution of feasible words is partitioned into non-overlapping regions. Thereafter, specialized experts are formed by independently training each member on a single region. A classical hard-decision decoder (HDD) is employed to map every word to a single expert in an injective manner. FER gains of up to 0.4dB at the waterfall region, and of 1.25dB at the error floor region are achieved for two BCH(63,36) and (63,45) codes with cycle-reduced parity-check matrices, compared to the previous best result of the paper \"Active Deep Decoding of Linear Codes\"."},{"title":"Individual Explanations in Machine Learning Models: A Survey for\n  Practitioners","source":"Alfredo Carrillo, Luis F. Cant\\'u and Alejandro Noriega","docs_id":"2104.04144","section":["cs.LG","cs.AI","stat.AP"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Individual Explanations in Machine Learning Models: A Survey for\n  Practitioners. In recent years, the use of sophisticated statistical models that influence decisions in domains of high societal relevance is on the rise. Although these models can often bring substantial improvements in the accuracy and efficiency of organizations, many governments, institutions, and companies are reluctant to their adoption as their output is often difficult to explain in human-interpretable ways. Hence, these models are often regarded as black-boxes, in the sense that their internal mechanisms can be opaque to human audit. In real-world applications, particularly in domains where decisions can have a sensitive impact--e.g., criminal justice, estimating credit scores, insurance risk, health risks, etc.--model interpretability is desired. Recently, the academic literature has proposed a substantial amount of methods for providing interpretable explanations to machine learning models. This survey reviews the most relevant and novel methods that form the state-of-the-art for addressing the particular problem of explaining individual instances in machine learning. It seeks to provide a succinct review that can guide data science and machine learning practitioners in the search for appropriate methods to their problem domain."},{"title":"FLRA: A Reference Architecture for Federated Learning Systems","source":"Sin Kit Lo, Qinghua Lu, Hye-Young Paik, and Liming Zhu","docs_id":"2106.11570","section":["cs.LG","cs.DC","cs.SE"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"FLRA: A Reference Architecture for Federated Learning Systems. Federated learning is an emerging machine learning paradigm that enables multiple devices to train models locally and formulate a global model, without sharing the clients' local data. A federated learning system can be viewed as a large-scale distributed system, involving different components and stakeholders with diverse requirements and constraints. Hence, developing a federated learning system requires both software system design thinking and machine learning knowledge. Although much effort has been put into federated learning from the machine learning perspectives, our previous systematic literature review on the area shows that there is a distinct lack of considerations for software architecture design for federated learning. In this paper, we propose FLRA, a reference architecture for federated learning systems, which provides a template design for federated learning-based solutions. The proposed FLRA reference architecture is based on an extensive review of existing patterns of federated learning systems found in the literature and existing industrial implementation. The FLRA reference architecture consists of a pool of architectural patterns that could address the frequently recurring design problems in federated learning architectures. The FLRA reference architecture can serve as a design guideline to assist architects and developers with practical solutions for their problems, which can be further customised."},{"title":"Bayesian optimisation of large-scale photonic reservoir computers","source":"Piotr Antonik, Nicolas Marsal, Daniel Brunner, Damien Rontani","docs_id":"2004.02535","section":["cs.NE"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Bayesian optimisation of large-scale photonic reservoir computers. Introduction. Reservoir computing is a growing paradigm for simplified training of recurrent neural networks, with a high potential for hardware implementations. Numerous experiments in optics and electronics yield comparable performance to digital state-of-the-art algorithms. Many of the most recent works in the field focus on large-scale photonic systems, with tens of thousands of physical nodes and arbitrary interconnections. While this trend significantly expands the potential applications of photonic reservoir computing, it also complicates the optimisation of the high number of hyper-parameters of the system. Methods. In this work, we propose the use of Bayesian optimisation for efficient exploration of the hyper-parameter space in a minimum number of iteration. Results. We test this approach on a previously reported large-scale experimental system, compare it to the commonly used grid search, and report notable improvements in performance and the number of experimental iterations required to optimise the hyper-parameters. Conclusion. Bayesian optimisation thus has the potential to become the standard method for tuning the hyper-parameters in photonic reservoir computing."},{"title":"The MBPEP: a deep ensemble pruning algorithm providing high quality\n  uncertainty prediction","source":"Ruihan Hu, Qijun Huang, Sheng Chang, Hao Wang and Jin He","docs_id":"1902.09238","section":["cs.LG","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"The MBPEP: a deep ensemble pruning algorithm providing high quality\n  uncertainty prediction. Machine learning algorithms have been effectively applied into various real world tasks. However, it is difficult to provide high-quality machine learning solutions to accommodate an unknown distribution of input datasets; this difficulty is called the uncertainty prediction problems. In this paper, a margin-based Pareto deep ensemble pruning (MBPEP) model is proposed. It achieves the high-quality uncertainty estimation with a small value of the prediction interval width (MPIW) and a high confidence of prediction interval coverage probability (PICP) by using deep ensemble networks. In addition to these networks, unique loss functions are proposed, and these functions make the sub-learners available for standard gradient descent learning. Furthermore, the margin criterion fine-tuning-based Pareto pruning method is introduced to optimize the ensembles. Several experiments including predicting uncertainties of classification and regression are conducted to analyze the performance of MBPEP. The experimental results show that MBPEP achieves a small interval width and a low learning error with an optimal number of ensembles. For the real-world problems, MBPEP performs well on input datasets with unknown distributions datasets incomings and improves learning performance on a multi task problem when compared to that of each single model."},{"title":"Cerebellar-Inspired Learning Rule for Gain Adaptation of Feedback\n  Controllers","source":"Ivan Herreros, Xerxes D. Arsiwalla, Cosimo Della Santina, Jordi-Ysard\n  Puigbo, Antonio Bicchi, Paul Verschure","docs_id":"1707.01484","section":["q-bio.NC","cond-mat.dis-nn","cs.SY","nlin.AO","physics.bio-ph"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Cerebellar-Inspired Learning Rule for Gain Adaptation of Feedback\n  Controllers. How does our nervous system successfully acquire feedback control strategies in spite of a wide spectrum of response dynamics from different musculo-skeletal systems? The cerebellum is a crucial brain structure in enabling precise motor control in animals. Recent advances suggest that synaptic plasticity of cerebellar Purkinje cells involves molecular mechanisms that mimic the dynamics of the efferent motor system that they control allowing them to match the timing of their learning rule to behavior. Counter-Factual Predictive Control (CFPC) is a cerebellum-based feed-forward control scheme that exploits that principle for acquiring anticipatory actions. CFPC extends the classical Widrow-Hoff\/Least Mean Squares by inserting a forward model of the downstream closed-loop system in its learning rule. Here we apply that same insight to the problem of learning the gains of a feedback controller. To that end, we frame a Model-Reference Adaptive Control (MRAC) problem and derive an adaptive control scheme treating the gains of a feedback controller as if they were the weights of an adaptive linear unit. Our results demonstrate that rather than being exclusively confined to cerebellar learning, the approach of controlling plasticity with a forward model of the subsystem controlled, an approach that we term as Model-Enhanced Least Mean Squares (ME-LMS), can provide a solution to wide set of adaptive control problems."},{"title":"Learning Discrete Bayesian Networks from Continuous Data","source":"Yi-Chun Chen, Tim Allan Wheeler, Mykel John Kochenderfer","docs_id":"1512.02406","section":["cs.AI","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Learning Discrete Bayesian Networks from Continuous Data. Learning Bayesian networks from raw data can help provide insights into the relationships between variables. While real data often contains a mixture of discrete and continuous-valued variables, many Bayesian network structure learning algorithms assume all random variables are discrete. Thus, continuous variables are often discretized when learning a Bayesian network. However, the choice of discretization policy has significant impact on the accuracy, speed, and interpretability of the resulting models. This paper introduces a principled Bayesian discretization method for continuous variables in Bayesian networks with quadratic complexity instead of the cubic complexity of other standard techniques. Empirical demonstrations show that the proposed method is superior to the established minimum description length algorithm. In addition, this paper shows how to incorporate existing methods into the structure learning process to discretize all continuous variables and simultaneously learn Bayesian network structures."},{"title":"Point cloud ridge-valley feature enhancement based on position and\n  normal guidance","source":"Jianhui Nie, Zhaochen Zhang, Ye Liu, Hao Gao, Feng Xu, WenKai Shi","docs_id":"1910.04942","section":["cs.GR","cs.CG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Point cloud ridge-valley feature enhancement based on position and\n  normal guidance. Ridge-valley features are important elements of point clouds, as they contain rich surface information. To recognize these features from point clouds, this paper introduces an extreme point distance (EPD) criterion with scale independence. Compared with traditional methods, the EPD greatly reduces the number of potential feature points and improves the robustness of multiscale feature point recognition. On this basis, a feature enhancement algorithm based on user priori guidance is proposed that adjusts the coordinates of the feature area by solving an objective equation containing the expected position and normal constraints. Since the expected normal can be expressed as a function of neighborhood point coordinates, the above objective equation can be converted into linear sparse equations with enhanced feature positions as variables, and thus, the closed solution can be obtained. In addition, a parameterization method for scattered point clouds based on feature line guidance is proposed, which reduces the number of unknowns by 2\/3 and eliminates lateral sliding in the direction perpendicular to feature lines. Finally, the application of the algorithm in multiscale ridge-valley feature recognition, freeform surface feature enhancement and computer-aided design (CAD) workpiece sharp feature restoration verifies its effectiveness."},{"title":"Perceptual Losses for Real-Time Style Transfer and Super-Resolution","source":"Justin Johnson, Alexandre Alahi, Li Fei-Fei","docs_id":"1603.08155","section":["cs.CV","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Perceptual Losses for Real-Time Style Transfer and Super-Resolution. We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks using a \\emph{per-pixel} loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by defining and optimizing \\emph{perceptual} loss functions based on high-level features extracted from pretrained networks. We combine the benefits of both approaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results."},{"title":"Convolutional Generative Adversarial Networks with Binary Neurons for\n  Polyphonic Music Generation","source":"Hao-Wen Dong and Yi-Hsuan Yang","docs_id":"1804.09399","section":["cs.LG","cs.AI","cs.SD","eess.AS","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Convolutional Generative Adversarial Networks with Binary Neurons for\n  Polyphonic Music Generation. It has been shown recently that deep convolutional generative adversarial networks (GANs) can learn to generate music in the form of piano-rolls, which represent music by binary-valued time-pitch matrices. However, existing models can only generate real-valued piano-rolls and require further post-processing, such as hard thresholding (HT) or Bernoulli sampling (BS), to obtain the final binary-valued results. In this paper, we study whether we can have a convolutional GAN model that directly creates binary-valued piano-rolls by using binary neurons. Specifically, we propose to append to the generator an additional refiner network, which uses binary neurons at the output layer. The whole network is trained in two stages. Firstly, the generator and the discriminator are pretrained. Then, the refiner network is trained along with the discriminator to learn to binarize the real-valued piano-rolls the pretrained generator creates. Experimental results show that using binary neurons instead of HT or BS indeed leads to better results in a number of objective measures. Moreover, deterministic binary neurons perform better than stochastic ones in both objective measures and a subjective test. The source code, training data and audio examples of the generated results can be found at https:\/\/salu133445.github.io\/bmusegan\/ ."},{"title":"A Semantic approach for effective document clustering using WordNet","source":"Leena H. Patil, Mohammed Atique","docs_id":"1303.0489","section":["cs.CL","cs.IR"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A Semantic approach for effective document clustering using WordNet. Now a days, the text document is spontaneously increasing over the internet, e-mail and web pages and they are stored in the electronic database format. To arrange and browse the document it becomes difficult. To overcome such problem the document preprocessing, term selection, attribute reduction and maintaining the relationship between the important terms using background knowledge, WordNet, becomes an important parameters in data mining. In these paper the different stages are formed, firstly the document preprocessing is done by removing stop words, stemming is performed using porter stemmer algorithm, word net thesaurus is applied for maintaining relationship between the important terms, global unique words, and frequent word sets get generated, Secondly, data matrix is formed, and thirdly terms are extracted from the documents by using term selection approaches tf-idf, tf-df, and tf2 based on their minimum threshold value. Further each and every document terms gets preprocessed, where the frequency of each term within the document is counted for representation. The purpose of this approach is to reduce the attributes and find the effective term selection method using WordNet for better clustering accuracy. Experiments are evaluated on Reuters Transcription Subsets, wheat, trade, money grain, and ship, Reuters 21578, Classic 30, 20 News group (atheism), 20 News group (Hardware), 20 News group (Computer Graphics) etc."},{"title":"Comparing the notions of optimality in CP-nets, strategic games and soft\n  constraints","source":"Krzysztof R. Apt, Francesca Rossi, Kristen Brent Venable","docs_id":"0711.2909","section":["cs.AI","cs.GT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Comparing the notions of optimality in CP-nets, strategic games and soft\n  constraints. The notion of optimality naturally arises in many areas of applied mathematics and computer science concerned with decision making. Here we consider this notion in the context of three formalisms used for different purposes in reasoning about multi-agent systems: strategic games, CP-nets, and soft constraints. To relate the notions of optimality in these formalisms we introduce a natural qualitative modification of the notion of a strategic game. We show then that the optimal outcomes of a CP-net are exactly the Nash equilibria of such games. This allows us to use the techniques of game theory to search for optimal outcomes of CP-nets and vice-versa, to use techniques developed for CP-nets to search for Nash equilibria of the considered games. Then, we relate the notion of optimality used in the area of soft constraints to that used in a generalization of strategic games, called graphical games. In particular we prove that for a natural class of soft constraints that includes weighted constraints every optimal solution is both a Nash equilibrium and Pareto efficient joint strategy. For a natural mapping in the other direction we show that Pareto efficient joint strategies coincide with the optimal solutions of soft constraints."},{"title":"Awareness Logic: Kripke Lattices as a Middle Ground between Syntactic\n  and Semantic Models","source":"Gaia Belardinelli and Rasmus K. Rendsvig","docs_id":"2106.12868","section":["cs.AI","cs.LO","cs.MA","econ.TH","math.LO"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Awareness Logic: Kripke Lattices as a Middle Ground between Syntactic\n  and Semantic Models. The literature on awareness modeling includes both syntax-free and syntax-based frameworks. Heifetz, Meier \\& Schipper (HMS) propose a lattice model of awareness that is syntax-free. While their lattice approach is elegant and intuitive, it precludes the simple option of relying on formal language to induce lattices, and does not explicitly distinguish uncertainty from unawareness. Contra this, the most prominent syntax-based solution, the Fagin-Halpern (FH) model, accounts for this distinction and offers a simple representation of awareness, but lacks the intuitiveness of the lattice structure. Here, we combine these two approaches by providing a lattice of Kripke models, induced by atom subset inclusion, in which uncertainty and unawareness are separate. We show our model equivalent to both HMS and FH models by defining transformations between them which preserve satisfaction of formulas of a language for explicit knowledge, and obtain completeness through our and HMS' results. Lastly, we prove that the Kripke lattice model can be shown equivalent to the FH model (when awareness is propositionally determined) also with respect to the language of the Logic of General Awareness, for which the FH model where originally proposed."},{"title":"Algorithmic Principles of Camera-based Respiratory Motion Extraction","source":"Wenjin Wang, Albertus C. den Brinker","docs_id":"2105.07537","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Algorithmic Principles of Camera-based Respiratory Motion Extraction. Measuring the respiratory signal from a video based on body motion has been proposed and recently matured in products for video health monitoring. The core algorithm for this measurement is the estimation of tiny chest\/abdominal motions induced by respiration, and the fundamental challenge is motion sensitivity. Though prior arts reported on the validation with real human subjects, there is no thorough\/rigorous benchmark to quantify the sensitivities and boundary conditions of motion-based core respiratory algorithms that measure sub-pixel displacement between video frames. In this paper, we designed a setup with a fully-controllable physical phantom to investigate the essence of core algorithms, together with a mathematical model incorporating two motion estimation strategies and three spatial representations, leading to six algorithmic combinations for respiratory signal extraction. Their promises and limitations are discussed and clarified via the phantom benchmark. The insights gained in this paper are intended to improve the understanding and applications of camera-based respiration measurement in health monitoring."},{"title":"A Theory of Usable Information Under Computational Constraints","source":"Yilun Xu, Shengjia Zhao, Jiaming Song, Russell Stewart, Stefano Ermon","docs_id":"2002.10689","section":["cs.LG","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A Theory of Usable Information Under Computational Constraints. We propose a new framework for reasoning about information in complex systems. Our foundation is based on a variational extension of Shannon's information theory that takes into account the modeling power and computational constraints of the observer. The resulting \\emph{predictive $\\mathcal{V}$-information} encompasses mutual information and other notions of informativeness such as the coefficient of determination. Unlike Shannon's mutual information and in violation of the data processing inequality, $\\mathcal{V}$-information can be created through computation. This is consistent with deep neural networks extracting hierarchies of progressively more informative features in representation learning. Additionally, we show that by incorporating computational constraints, $\\mathcal{V}$-information can be reliably estimated from data even in high dimensions with PAC-style guarantees. Empirically, we demonstrate predictive $\\mathcal{V}$-information is more effective than mutual information for structure learning and fair representation learning."},{"title":"ReGenMorph: Visibly Realistic GAN Generated Face Morphing Attacks by\n  Attack Re-generation","source":"Naser Damer, Kiran Raja, Marius S\\\"u{\\ss}milch, Sushma Venkatesh, Fadi\n  Boutros, Meiling Fang, Florian Kirchbuchner, Raghavendra Ramachandra, Arjan\n  Kuijper","docs_id":"2108.09130","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"ReGenMorph: Visibly Realistic GAN Generated Face Morphing Attacks by\n  Attack Re-generation. Face morphing attacks aim at creating face images that are verifiable to be the face of multiple identities, which can lead to building faulty identity links in operations like border checks. While creating a morphed face detector (MFD), training on all possible attack types is essential to achieve good detection performance. Therefore, investigating new methods of creating morphing attacks drives the generalizability of MADs. Creating morphing attacks was performed on the image level, by landmark interpolation, or on the latent-space level, by manipulating latent vectors in a generative adversarial network. The earlier results in varying blending artifacts and the latter results in synthetic-like striping artifacts. This work presents the novel morphing pipeline, ReGenMorph, to eliminate the LMA blending artifacts by using a GAN-based generation, as well as, eliminate the manipulation in the latent space, resulting in visibly realistic morphed images compared to previous works. The generated ReGenMorph appearance is compared to recent morphing approaches and evaluated for face recognition vulnerability and attack detectability, whether as known or unknown attacks."},{"title":"Bootstrapping Monte Carlo Tree Search with an Imperfect Heuristic","source":"Truong-Huy Dinh Nguyen, Wee-Sun Lee, and Tze-Yun Leong","docs_id":"1206.5940","section":["cs.AI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Bootstrapping Monte Carlo Tree Search with an Imperfect Heuristic. We consider the problem of using a heuristic policy to improve the value approximation by the Upper Confidence Bound applied in Trees (UCT) algorithm in non-adversarial settings such as planning with large-state space Markov Decision Processes. Current improvements to UCT focus on either changing the action selection formula at the internal nodes or the rollout policy at the leaf nodes of the search tree. In this work, we propose to add an auxiliary arm to each of the internal nodes, and always use the heuristic policy to roll out simulations at the auxiliary arms. The method aims to get fast convergence to optimal values at states where the heuristic policy is optimal, while retaining similar approximation as the original UCT in other states. We show that bootstrapping with the proposed method in the new algorithm, UCT-Aux, performs better compared to the original UCT algorithm and its variants in two benchmark experiment settings. We also examine conditions under which UCT-Aux works well."},{"title":"Learning transition times in event sequences: the Event-Based Hidden\n  Markov Model of disease progression","source":"Peter A. Wijeratne and Daniel C. Alexander","docs_id":"2011.01023","section":["cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Learning transition times in event sequences: the Event-Based Hidden\n  Markov Model of disease progression. Progressive diseases worsen over time and are characterised by monotonic change in features that track disease progression. Here we connect ideas from two formerly separate methodologies -- event-based and hidden Markov modelling -- to derive a new generative model of disease progression. Our model can uniquely infer the most likely group-level sequence and timing of events (natural history) from limited datasets. Moreover, it can infer and predict individual-level trajectories (prognosis) even when data are missing, giving it high clinical utility. Here we derive the model and provide an inference scheme based on the expectation maximisation algorithm. We use clinical, imaging and biofluid data from the Alzheimer's Disease Neuroimaging Initiative to demonstrate the validity and utility of our model. First, we train our model to uncover a new group-level sequence of feature changes in Alzheimer's disease over a period of ${\\sim}17.3$ years. Next, we demonstrate that our model provides improved utility over a continuous time hidden Markov model by area under the receiver operator characteristic curve ${\\sim}0.23$. Finally, we demonstrate that our model maintains predictive accuracy with up to $50\\%$ missing data. These results support the clinical validity of our model and its broader utility in resource-limited medical applications."},{"title":"Non-Bayesian Quickest Detection with Stochastic Sample Right Constraints","source":"Jun Geng and Lifeng Lai","docs_id":"1302.3834","section":["cs.IT","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Non-Bayesian Quickest Detection with Stochastic Sample Right Constraints. In this paper, we study the design and analysis of optimal detection scheme for sensors that are deployed to monitor the change in the environment and are powered by the energy harvested from the environment. In this type of applications, detection delay is of paramount importance. We model this problem as quickest change detection problem with a stochastic energy constraint. In particular, a wireless sensor powered by renewable energy takes observations from a random sequence, whose distribution will change at a certain unknown time. Such a change implies events of interest. The energy in the sensor is consumed by taking observations and is replenished randomly. The sensor cannot take observations if there is no energy left in the battery. Our goal is to design a power allocation scheme and a detection strategy to minimize the worst case detection delay, which is the difference between the time when an alarm is raised and the time when the change occurs. Two types of average run length (ARL) constraint, namely an algorithm level ARL constraint and an system level ARL constraint, are considered. We propose a low complexity scheme in which the energy allocation rule is to spend energy to take observations as long as the battery is not empty and the detection scheme is the Cumulative Sum test. We show that this scheme is optimal for the formulation with the algorithm level ARL constraint and is asymptotically optimal for the formulations with the system level ARL constraint."},{"title":"Chittron: An Automatic Bangla Image Captioning System","source":"Motiur Rahman, Nabeel Mohammed, Nafees Mansoor, Sifat Momen","docs_id":"1809.00339","section":["cs.CL","cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Chittron: An Automatic Bangla Image Captioning System. Automatic image caption generation aims to produce an accurate description of an image in natural language automatically. However, Bangla, the fifth most widely spoken language in the world, is lagging considerably in the research and development of such domain. Besides, while there are many established data sets to related to image annotation in English, no such resource exists for Bangla yet. Hence, this paper outlines the development of \"Chittron\", an automatic image captioning system in Bangla. Moreover, to address the data set availability issue, a collection of 16,000 Bangladeshi contextual images has been accumulated and manually annotated in Bangla. This data set is then used to train a model which integrates a pre-trained VGG16 image embedding model with stacked LSTM layers. The model is trained to predict the caption when the input is an image, one word at a time. The results show that the model has successfully been able to learn a working language model and to generate captions of images quite accurately in many cases. The results are evaluated mainly qualitatively. However, BLEU scores are also reported. It is expected that a better result can be obtained with a bigger and more varied data set."},{"title":"Cellular Controlled Cooperative Unmanned Aerial Vehicle Networks with\n  Sense-and-Send Protocol","source":"Shuhang Zhang, Hongliang Zhang, Boya Di, and Lingyang Song","docs_id":"1805.11779","section":["cs.SY","eess.SP"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Cellular Controlled Cooperative Unmanned Aerial Vehicle Networks with\n  Sense-and-Send Protocol. In this paper, we consider a cellular controlled unmanned aerial vehicle (UAV) sensing network in which multiple UAVs cooperatively complete each sensing task. We first propose a sense-and-send protocol where the UAVs collect sensory data of the tasks and transmit the collected data to the base station. We then formulate a joint trajectory, sensing location, and UAV scheduling optimization problem that minimizes the completion time for all the sensing tasks in the network. To solve this NP-hard problem efficiently, we decouple it into three sub-problems: trajectory optimization, sensing location optimization, and UAV scheduling. An iterative trajectory, sensing, and scheduling optimization (ITSSO) algorithm is proposed to solve these sub-problems jointly. The convergence and complexity of the ITSSO algorithm, together with the system performance are analysed. Simulation results show that the proposed ITSSO algorithm saves the task completion time by 15% compared to the non-cooperative scheme."},{"title":"Submodular Order Functions and Assortment Optimization","source":"Rajan Udwani","docs_id":"2107.02743","section":["math.OC","cs.DS"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Submodular Order Functions and Assortment Optimization. We define a new class of set functions that in addition to being monotone and subadditive, also admit a very limited form of submodularity defined over a permutation of the ground set. We refer to this permutation as a submodular order. This class of functions includes monotone submodular functions as a sub-family. To understand the importance of this structure in optimization problems we consider the problem of maximizing function value under various types of constraints. To demonstrate the modeling power of submodular order functions we show applications in two different settings. First, we apply our results to the extensively studied problem of assortment optimization. While the objectives in assortment optimization are known to be non-submodular (and non-monotone) even for simple choice models, we show that they are compatible with the notion of submodular order. Consequently, we obtain new and in some cases the first constant factor guarantee for constrained assortment optimization in fundamental choice models. As a second application of submodular order functions, we show an intriguing connection to the maximization of monotone submodular functions in the streaming model. We recover some best known guarantees for this problem as a corollary of our results."},{"title":"Consensus Based Sampling","source":"J. A. Carrillo and F. Hoffmann and A. M. Stuart and U. Vaes","docs_id":"2106.02519","section":["math.DS","cs.NA","math.NA"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Consensus Based Sampling. We propose a novel method for sampling and optimization tasks based on a stochastic interacting particle system. We explain how this method can be used for the following two goals: (i) generating approximate samples from a given target distribution; (ii) optimizing a given objective function. The approach is derivative-free and affine invariant, and is therefore well-suited for solving inverse problems defined by complex forward models: (i) allows generation of samples from the Bayesian posterior and (ii) allows determination of the maximum a posteriori estimator. We investigate the properties of the proposed family of methods in terms of various parameter choices, both analytically and by means of numerical simulations. The analysis and numerical simulation establish that the method has potential for general purpose optimization tasks over Euclidean space; contraction properties of the algorithm are established under suitable conditions, and computational experiments demonstrate wide basins of attraction for various specific problems. The analysis and experiments also demonstrate the potential for the sampling methodology in regimes in which the target distribution is unimodal and close to Gaussian; indeed we prove that the method recovers a Laplace approximation to the measure in certain parametric regimes and provide numerical evidence that this Laplace approximation attracts a large set of initial conditions in a number of examples."},{"title":"Information Update: TDMA or FDMA?","source":"Haoyuan Pan, Soung Chang Liew","docs_id":"1911.02241","section":["cs.IT","eess.SP","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Information Update: TDMA or FDMA?. This paper studies information freshness in information update systems operated with TDMA and FDMA. Information freshness is characterized by a recently introduced metric, age of information (AoI), defined as the time elapsed since the generation of the last successfully received update. In an update system with multiple users sharing the same wireless channel to send updates to a common receiver, how to divide the channel among users affects information freshness. We investigate the AoI performances of two fundamental multiple access schemes, TDMA and FDMA. We first derive the time-averaged AoI by estimating the packet error rate of short update packets based on Gallager's random coding bound. For time-critical systems, we further define a new AoI metric, termed bounded AoI, which corresponds to an AoI threshold for the instantaneous AoI. Specifically, the instantaneous AoI is below the bounded AoI a large percentage of the time. We give a theoretical upper bound for bounded AoI. Our simulation results are consistent with our theoretical analysis. Although TDMA outperforms FDMA in terms of average AoI, FDMA is more robust against varying channel conditions since it gives a more stable bounded AoI across different received powers. Overall, our findings give insight to the design of practical multiple access systems with AoI requirements."},{"title":"Sinkhorn Natural Gradient for Generative Models","source":"Zebang Shen and Zhenfu Wang and Alejandro Ribeiro and Hamed Hassani","docs_id":"2011.04162","section":["stat.ML","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Sinkhorn Natural Gradient for Generative Models. We consider the problem of minimizing a functional over a parametric family of probability measures, where the parameterization is characterized via a push-forward structure. An important application of this problem is in training generative adversarial networks. In this regard, we propose a novel Sinkhorn Natural Gradient (SiNG) algorithm which acts as a steepest descent method on the probability space endowed with the Sinkhorn divergence. We show that the Sinkhorn information matrix (SIM), a key component of SiNG, has an explicit expression and can be evaluated accurately in complexity that scales logarithmically with respect to the desired accuracy. This is in sharp contrast to existing natural gradient methods that can only be carried out approximately. Moreover, in practical applications when only Monte-Carlo type integration is available, we design an empirical estimator for SIM and provide the stability analysis. In our experiments, we quantitatively compare SiNG with state-of-the-art SGD-type solvers on generative tasks to demonstrate its efficiency and efficacy of our method."},{"title":"A Non-Cooperative Method for Path Loss Estimation in Femtocell Networks","source":"Qinliang su, Aiping Huang, Zhaoyang Zhang, Kai Xu, Jin Yang","docs_id":"1008.0270","section":["cs.NI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A Non-Cooperative Method for Path Loss Estimation in Femtocell Networks. A macrocell superposed by indoor deployed femtocells forms a geography-overlapped and spectrum-shared two tier network, which can efficiently improve coverage and enhance system capacity. It is important for reducing inter-tier co-channel interference that any femtocell user (FU) can select suitable access channel according to the path losses between itself and the macrocell users (MUs). Path loss should be estimated non-cooperatively since information exchange is difficult between macrocell and femtocells. In this paper, a novel method is proposed for FU to estimate the path loss between itself and any MU independently. According to the adaptive modulation and coding (AMC) mode information broadcasted by the macrocell base station (BS), FU first estimates the path loss between BS and a MU by using Maximum a Posteriori (MAP) method. The probability distribution function (PDF) and statistics of the transmission power of the MU is then derived. According to the sequence of received powers from the MU, FU estimates the path loss between itself and the MU by using minimum mean square error (MMSE) method. Simulation results show that the proposed method can efficiently estimate the path loss between any FU and any MU in all kinds of conditions."},{"title":"Gift Contagion in Online Groups: Evidence From WeChat Red Packets","source":"Yuan Yuan, Tracy Liu, Chenhao Tan, Qian Chen, Alex Pentland, Jie Tang","docs_id":"1906.09698","section":["econ.GN","cs.HC","cs.SI","q-fin.EC"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Gift Contagion in Online Groups: Evidence From WeChat Red Packets. Gifts are important instruments for forming bonds in interpersonal relationships. Our study analyzes the phenomenon of gift contagion in online groups. Gift contagion encourages social bonds of prompting further gifts; it may also promote group interaction and solidarity. Using data on 36 million online red packet gifts on China's social site WeChat, we leverage a natural experimental design to identify the social contagion of gift giving in online groups. Our natural experiment is enabled by the randomization of the gift amount allocation algorithm on WeChat, which addresses the common challenge of causal identifications in observational data. Our study provides evidence of gift contagion: on average, receiving one additional dollar causes a recipient to send 18 cents back to the group within the subsequent 24 hours. Decomposing this effect, we find that it is mainly driven by the extensive margin -- more recipients are triggered to send red packets. Moreover, we find that this effect is stronger for \"luckiest draw\" recipients, suggesting the presence of a group norm regarding the next red packet sender. Finally, we investigate the moderating effects of group- and individual-level social network characteristics on gift contagion as well as the causal impact of receiving gifts on group network structure. Our study has implications for promoting group dynamics and designing marketing strategies for product adoption."},{"title":"The Role of Conditional Independence in the Evolution of Intelligent\n  Systems","source":"Jory Schossau, Larissa Albantakis, Arend Hintze","docs_id":"1801.05462","section":["cs.AI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"The Role of Conditional Independence in the Evolution of Intelligent\n  Systems. Systems are typically made from simple components regardless of their complexity. While the function of each part is easily understood, higher order functions are emergent properties and are notoriously difficult to explain. In networked systems, both digital and biological, each component receives inputs, performs a simple computation, and creates an output. When these components have multiple outputs, we intuitively assume that the outputs are causally dependent on the inputs but are themselves independent of each other given the state of their shared input. However, this intuition can be violated for components with probabilistic logic, as these typically cannot be decomposed into separate logic gates with one output each. This violation of conditional independence on the past system state is equivalent to instantaneous interaction --- the idea is that some information between the outputs is not coming from the inputs and thus must have been created instantaneously. Here we compare evolved artificial neural systems with and without instantaneous interaction across several task environments. We show that systems without instantaneous interactions evolve faster, to higher final levels of performance, and require fewer logic components to create a densely connected cognitive machinery."},{"title":"Efficient and Accurate In-Database Machine Learning with SQL Code\n  Generation in Python","source":"Michael Kaufmann, Gabriel Stechschulte, Anna Huber","docs_id":"2104.03224","section":["cs.DB","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Efficient and Accurate In-Database Machine Learning with SQL Code\n  Generation in Python. Following an analysis of the advantages of SQL-based Machine Learning (ML) and a short literature survey of the field, we describe a novel method for In-Database Machine Learning (IDBML). We contribute a process for SQL-code generation in Python using template macros in Jinja2 as well as the prototype implementation of the process. We describe our implementation of the process to compute multidimensional histogram (MDH) probability estimation in SQL. For this, we contribute and implement a novel discretization method called equal quantized rank binning (EQRB) and equal-width binning (EWB). Based on this, we provide data gathered in a benchmarking experiment for the quantitative empirical evaluation of our method and system using the Covertype dataset. We measured accuracy and computation time and compared it to Scikit Learn state of the art classification algorithms. Using EWB, our multidimensional probability estimation was the fastest of all tested algorithms, while being only 1-2% less accurate than the best state of the art methods found (decision trees and random forests). Our method was significantly more accurate than Naive Bayes, which assumes independent one-dimensional probabilities and\/or densities. Also, our method was significantly more accurate and faster than logistic regression. This motivates for further research in accuracy improvement and in IDBML with SQL code generation for big data and larger-than-memory datasets."},{"title":"Failure-Resilient Coverage Maximization with Multiple Robots","source":"Ishat E Rabban, Pratap Tokekar","docs_id":"2007.02204","section":["cs.RO"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Failure-Resilient Coverage Maximization with Multiple Robots. The task of maximizing coverage using multiple robots has several applications such as surveillance, exploration, and environmental monitoring. A major challenge of deploying such multi-robot systems in a practical scenario is to ensure resilience against robot failures. A recent work introduced the Resilient Coverage Maximization (RCM) problem where the goal is to maximize a submodular coverage utility when the robots are subject to adversarial attacks or failures. The RCM problem is known to be NP-hard. In this paper, we propose two approximation algorithms for the RCM problem, namely, the Ordered Greedy (OrG) and the Local Search (LS) algorithm. Both algorithms empirically outperform the state-of-the-art solution in terms of accuracy and running time. To demonstrate the effectiveness of our proposed solution, we empirically compare our proposed algorithms with the existing solution and a brute force optimal algorithm. We also perform a case study on the persistent monitoring problem to show the applicability of our proposed algorithms in a practical setting."},{"title":"Tracking Triadic Cardinality Distributions for Burst Detection in Social\n  Activity Streams","source":"Junzhou Zhao, John C.S. Lui, Don Towsley, Pinghui Wang, Xiaohong Guan","docs_id":"1411.3808","section":["cs.SI","physics.soc-ph"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Tracking Triadic Cardinality Distributions for Burst Detection in Social\n  Activity Streams. In everyday life, we often observe unusually frequent interactions among people before or during important events, e.g., we receive\/send more greetings from\/to our friends on Christmas Day, than usual. We also observe that some videos suddenly go viral through people's sharing in online social networks (OSNs). Do these seemingly different phenomena share a common structure? All these phenomena are associated with sudden surges of user activities in networks, which we call \"bursts\" in this work. We find that the emergence of a burst is accompanied with the formation of triangles in networks. This finding motivates us to propose a new method to detect bursts in OSNs. We first introduce a new measure, \"triadic cardinality distribution\", corresponding to the fractions of nodes with different numbers of triangles, i.e., triadic cardinalities, within a network. We demonstrate that this distribution changes when a burst occurs, and is naturally immunized against spamming social-bot attacks. Hence, by tracking triadic cardinality distributions, we can reliably detect bursts in OSNs. To avoid handling massive activity data generated by OSN users, we design an efficient sample-estimate solution to estimate the triadic cardinality distribution from sampled data. Extensive experiments conducted on real data demonstrate the usefulness of this triadic cardinality distribution and the effectiveness of our sample-estimate solution."},{"title":"Identifying Object States in Cooking-Related Images","source":"Ahmad Babaeian Jelodar, Md Sirajus Salekin, Yu Sun","docs_id":"1805.06956","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Identifying Object States in Cooking-Related Images. Understanding object states is as important as object recognition for robotic task planning and manipulation. To our knowledge, this paper explicitly introduces and addresses the state identification problem in cooking related images for the first time. In this paper, objects and ingredients in cooking videos are explored and the most frequent objects are analyzed. Eleven states from the most frequent cooking objects are examined and a dataset of images containing those objects and their states is created. As a solution to the state identification problem, a Resnet based deep model is proposed. The model is initialized with Imagenet weights and trained on the dataset of eleven classes. The trained state identification model is evaluated on a subset of the Imagenet dataset and state labels are provided using a combination of the model with manual checking. Moreover, an individual model is fine-tuned for each object in the dataset using the weights from the initially trained model and object-specific images, where significant improvement is demonstrated."},{"title":"Message passing-based link configuration in short range millimeter wave\n  systems","source":"Nitin Jonathan Myers, Jarkko Kaleva, Antti T\\\"olli, Robert W. Heath Jr","docs_id":"1907.05009","section":["eess.SP","cs.IT","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Message passing-based link configuration in short range millimeter wave\n  systems. Millimeter wave (mmWave) communication in typical wearable and data center settings is short range. As the distance between the transmitter and the receiver in short range scenarios can be comparable to the length of the antenna arrays, the common far field approximation for the channel may not be applicable. As a result, dictionaries that result in a sparse channel representation in the far field setting may not be appropriate for short distances. In this paper, we develop a novel framework to exploit the structure in short range mmWave channels. The proposed method splits the channel into several subchannels for which the far field approximation can be applied. Then, the structure within and across different subchannels is leveraged using message passing. We show how information about the antenna array geometry can be used to design message passing factors that incorporate structure across successive subchannels. Simulation results indicate that our framework can be used to achieve better beam alignment with fewer channel measurements when compared to standard compressed sensing-based techniques that do not exploit structure across subchannels."},{"title":"Discovering Picturesque Highlights from Egocentric Vacation Videos","source":"Vinay Bettadapura, Daniel Castro, Irfan Essa","docs_id":"1601.04406","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Discovering Picturesque Highlights from Egocentric Vacation Videos. We present an approach for identifying picturesque highlights from large amounts of egocentric video data. Given a set of egocentric videos captured over the course of a vacation, our method analyzes the videos and looks for images that have good picturesque and artistic properties. We introduce novel techniques to automatically determine aesthetic features such as composition, symmetry and color vibrancy in egocentric videos and rank the video frames based on their photographic qualities to generate highlights. Our approach also uses contextual information such as GPS, when available, to assess the relative importance of each geographic location where the vacation videos were shot. Furthermore, we specifically leverage the properties of egocentric videos to improve our highlight detection. We demonstrate results on a new egocentric vacation dataset which includes 26.5 hours of videos taken over a 14 day vacation that spans many famous tourist destinations and also provide results from a user-study to access our results."},{"title":"Extreme Memorization via Scale of Initialization","source":"Harsh Mehta, Ashok Cutkosky, Behnam Neyshabur","docs_id":"2008.13363","section":["cs.LG","cs.CV","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Extreme Memorization via Scale of Initialization. We construct an experimental setup in which changing the scale of initialization strongly impacts the implicit regularization induced by SGD, interpolating from good generalization performance to completely memorizing the training set while making little progress on the test set. Moreover, we find that the extent and manner in which generalization ability is affected depends on the activation and loss function used, with $\\sin$ activation demonstrating extreme memorization. In the case of the homogeneous ReLU activation, we show that this behavior can be attributed to the loss function. Our empirical investigation reveals that increasing the scale of initialization correlates with misalignment of representations and gradients across examples in the same class. This insight allows us to devise an alignment measure over gradients and representations which can capture this phenomenon. We demonstrate that our alignment measure correlates with generalization of deep models trained on image classification tasks."},{"title":"Assessing Performance Implications of Deep Copy Operations via\n  Microbenchmarking","source":"Millad Ghane, Sunita Chandrasekaran, Margaret S. Cheung","docs_id":"1906.01128","section":["cs.DC","cs.PF","cs.PL"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Assessing Performance Implications of Deep Copy Operations via\n  Microbenchmarking. As scientific frameworks become sophisticated, so do their data structures. Current data structures are no longer simple in design and they have been progressively complicated. The typical trend in designing data structures in scientific applications are basically nested data structures: pointing to a data structure within another one. Managing nested data structures on a modern heterogeneous system requires tremendous effort due to the separate memory space design. In this paper, we will discuss the implications of deep copy on data transfers on current heterogeneous. Then, we will discuss the two options that are currently available to perform the memory copy operations on complex structures and will introduce pointerchain directive that we proposed. Afterwards, we will introduce a set of extensive benchmarks to compare the available approaches. Our goal is to make our proposed benchmarks a basis to examine the efficiency of upcoming approaches that address the challenge of deep copy operations."},{"title":"A coarse-to-fine framework for unsupervised multi-contrast MR image\n  deformable registration with dual consistency constraint","source":"Weijian Huang, Hao Yang, Xinfeng Liu, Cheng Li, Ian Zhang, Rongpin\n  Wang, Hairong Zheng, Shanshan Wang","docs_id":"2008.01896","section":["eess.IV","cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A coarse-to-fine framework for unsupervised multi-contrast MR image\n  deformable registration with dual consistency constraint. Multi-contrast magnetic resonance (MR) image registration is useful in the clinic to achieve fast and accurate imaging-based disease diagnosis and treatment planning. Nevertheless, the efficiency and performance of the existing registration algorithms can still be improved. In this paper, we propose a novel unsupervised learning-based framework to achieve accurate and efficient multi-contrast MR image registrations. Specifically, an end-to-end coarse-to-fine network architecture consisting of affine and deformable transformations is designed to improve the robustness and achieve end-to-end registration. Furthermore, a dual consistency constraint and a new prior knowledge-based loss function are developed to enhance the registration performances. The proposed method has been evaluated on a clinical dataset containing 555 cases, and encouraging performances have been achieved. Compared to the commonly utilized registration methods, including VoxelMorph, SyN, and LT-Net, the proposed method achieves better registration performance with a Dice score of 0.8397 in identifying stroke lesions. With regards to the registration speed, our method is about 10 times faster than the most competitive method of SyN (Affine) when testing on a CPU. Moreover, we prove that our method can still perform well on more challenging tasks with lacking scanning information data, showing high robustness for the clinical application."},{"title":"Self-Imitation Learning for Robot Tasks with Sparse and Delayed Rewards","source":"Zhixin Chen, Mengxiang Lin","docs_id":"2010.06962","section":["cs.LG","cs.AI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Self-Imitation Learning for Robot Tasks with Sparse and Delayed Rewards. The application of reinforcement learning (RL) in robotic control is still limited in the environments with sparse and delayed rewards. In this paper, we propose a practical self-imitation learning method named Self-Imitation Learning with Constant Reward (SILCR). Instead of requiring hand-defined immediate rewards from environments, our method assigns the immediate rewards at each timestep with constant values according to their final episodic rewards. In this way, even if the dense rewards from environments are unavailable, every action taken by the agents would be guided properly. We demonstrate the effectiveness of our method in some challenging continuous robotics control tasks in MuJoCo simulation and the results show that our method significantly outperforms the alternative methods in tasks with sparse and delayed rewards. Even compared with alternatives with dense rewards available, our method achieves competitive performance. The ablation experiments also show the stability and reproducibility of our method."},{"title":"Learning Discriminative Shrinkage Deep Networks for Image Deconvolution","source":"Pin-Hung Kuo, Jinshan Pan, Shao-Yi Chien and Ming-Hsuan Yang","docs_id":"2111.13876","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Learning Discriminative Shrinkage Deep Networks for Image Deconvolution. Non-blind deconvolution is an ill-posed problem. Most existing methods usually formulate this problem into a maximum-a-posteriori framework and address it by designing kinds of regularization terms and data terms of the latent clear images. In this paper, we propose an effective non-blind deconvolution approach by learning discriminative shrinkage functions to implicitly model these terms. In contrast to most existing methods that use deep convolutional neural networks (CNNs) or radial basis functions to simply learn the regularization term, we formulate both the data term and regularization term and split the deconvolution model into data-related and regularization-related sub-problems according to the alternating direction method of multipliers. We explore the properties of the Maxout function and develop a deep CNN model with a Maxout layer to learn discriminative shrinkage functions to directly approximate the solutions of these two sub-problems. Moreover, given the fast Fourier transform based image restoration usually leads to ringing artifacts while conjugate gradient-based image restoration is time-consuming, we develop the conjugate gradient network to restore the latent clear images effectively and efficiently. Experimental results show that the proposed method performs favorably against the state-of-the-art ones in terms of efficiency and accuracy."},{"title":"Detection of Consonant Errors in Disordered Speech Based on\n  Consonant-vowel Segment Embedding","source":"Si-Ioi Ng, Cymie Wing-Yee Ng, Jingyu Li, Tan Lee","docs_id":"2106.08536","section":["eess.AS","cs.SD"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Detection of Consonant Errors in Disordered Speech Based on\n  Consonant-vowel Segment Embedding. Speech sound disorder (SSD) refers to a type of developmental disorder in young children who encounter persistent difficulties in producing certain speech sounds at the expected age. Consonant errors are the major indicator of SSD in clinical assessment. Previous studies on automatic assessment of SSD revealed that detection of speech errors concerning short and transitory consonants is less satisfactory. This paper investigates a neural network based approach to detecting consonant errors in disordered speech using consonant-vowel (CV) diphone segment in comparison to using consonant monophone segment. The underlying assumption is that the vowel part of a CV segment carries important information of co-articulation from the consonant. Speech embeddings are extracted from CV segments by a recurrent neural network model. The similarity scores between the embeddings of the test segment and the reference segments are computed to determine if the test segment is the expected consonant or not. Experimental results show that using CV segments achieves improved performance on detecting speech errors concerning those \"difficult\" consonants reported in the previous studies."},{"title":"Forensics Analysis of Xbox One Game Console","source":"Ali M. Al-Haj","docs_id":"1904.00734","section":["cs.CR","cs.CY"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Forensics Analysis of Xbox One Game Console. Games console devices have been designed to be an entertainment system. However, the 8th generation games console have new features that can support criminal activities and investigators need to be aware of them. This paper highlights the forensics value of the Microsoft game console Xbox One, the latest version of their Xbox series. The Xbox One game console provides many features including web browsing, social networking, and chat functionality. From a forensic perspective, all those features will be a place of interest in forensic examinations. However, the available published literature focused on examining the physical hard drive artefacts, which are encrypted and cannot provide deep analysis of the user's usage of the console. In this paper, we carried out an investigation of the Xbox One games console by using two approaches: a physical investigation of the hard drive to identify the valuable file timestamp information and logical examination via the graphical user interface. Furthermore, this paper identifies potential valuable forensic data sources within the Xbox One and provides best practices guidance for collecting data in a forensically sound manner."},{"title":"Reliable Over-the-Air Computation by Amplify-and-Forward Based Relay","source":"Suhua Tang, Huarui Yin, Chao Zhang, Sadao Obana","docs_id":"2010.12146","section":["cs.NI","eess.SP"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Reliable Over-the-Air Computation by Amplify-and-Forward Based Relay. In typical sensor networks, data collection and processing are separated. A sink collects data from all nodes sequentially, which is very time consuming. Over-the-air computation, as a new diagram of sensor networks, integrates data collection and processing in one slot: all nodes transmit their signals simultaneously in the analog wave and the processing is done in the air. This method, although efficient, requires that signals from all nodes arrive at the sink, aligned in signal magnitude so as to enable an unbiased estimation. For nodes far away from the sink with a low channel gain, misalignment in signal magnitude is unavoidable. To solve this problem, in this paper, we investigate the amplify-and-forward based relay, in which a relay node amplifies signals from many nodes at the same time. We first discuss the general relay model and a simple relay policy. Then, a coherent relay policy is proposed to reduce relay transmission power. Directly minimizing the computation error tends to over-increase node transmission power. Therefore, the two relay policies are further refined with a new metric, and the transmission power is reduced while the computation error is kept low. In addition, the coherent relay policy helps to reduce the relay transmission power by half, to below the limit, which makes it one step ahead towards practical applications."},{"title":"PoseTrack: A Benchmark for Human Pose Estimation and Tracking","source":"Mykhaylo Andriluka, Umar Iqbal, Eldar Insafutdinov, Leonid Pishchulin,\n  Anton Milan, Juergen Gall and Bernt Schiele","docs_id":"1710.10000","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"PoseTrack: A Benchmark for Human Pose Estimation and Tracking. Human poses and motions are important cues for analysis of videos with people and there is strong evidence that representations based on body pose are highly effective for a variety of tasks such as activity recognition, content retrieval and social signal processing. In this work, we aim to further advance the state of the art by establishing \"PoseTrack\", a new large-scale benchmark for video-based human pose estimation and articulated tracking, and bringing together the community of researchers working on visual human analysis. The benchmark encompasses three competition tracks focusing on i) single-frame multi-person pose estimation, ii) multi-person pose estimation in videos, and iii) multi-person articulated tracking. To facilitate the benchmark and challenge we collect, annotate and release a new %large-scale benchmark dataset that features videos with multiple people labeled with person tracks and articulated pose. A centralized evaluation server is provided to allow participants to evaluate on a held-out test set. We envision that the proposed benchmark will stimulate productive research both by providing a large and representative training dataset as well as providing a platform to objectively evaluate and compare the proposed methods. The benchmark is freely accessible at https:\/\/posetrack.net."},{"title":"dalex: Responsible Machine Learning with Interactive Explainability and\n  Fairness in Python","source":"Hubert Baniecki, Wojciech Kretowicz, Piotr Piatyszek, Jakub\n  Wisniewski, Przemyslaw Biecek","docs_id":"2012.14406","section":["cs.LG","cs.HC","cs.SE","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"dalex: Responsible Machine Learning with Interactive Explainability and\n  Fairness in Python. The increasing amount of available data, computing power, and the constant pursuit for higher performance results in the growing complexity of predictive models. Their black-box nature leads to opaqueness debt phenomenon inflicting increased risks of discrimination, lack of reproducibility, and deflated performance due to data drift. To manage these risks, good MLOps practices ask for better validation of model performance and fairness, higher explainability, and continuous monitoring. The necessity of deeper model transparency appears not only from scientific and social domains, but also emerging laws and regulations on artificial intelligence. To facilitate the development of responsible machine learning models, we showcase dalex, a Python package which implements the model-agnostic interface for interactive model exploration. It adopts the design crafted through the development of various tools for responsible machine learning; thus, it aims at the unification of the existing solutions. This library's source code and documentation are available under open license at https:\/\/python.drwhy.ai\/."},{"title":"Revised Progressive-Hedging-Algorithm Based Two-layer Solution Scheme\n  for Bayesian Reinforcement Learning","source":"Xin Huang, Duan Li, Daniel Zhuoyu Long","docs_id":"1906.09035","section":["eess.SY","cs.LG","cs.SY"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Revised Progressive-Hedging-Algorithm Based Two-layer Solution Scheme\n  for Bayesian Reinforcement Learning. Stochastic control with both inherent random system noise and lack of knowledge on system parameters constitutes the core and fundamental topic in reinforcement learning (RL), especially under non-episodic situations where online learning is much more demanding. This challenge has been notably addressed in Bayesian RL recently where some approximation techniques have been developed to find suboptimal policies. While existing approaches mainly focus on approximating the value function, or on involving Thompson sampling, we propose a novel two-layer solution scheme in this paper to approximate the optimal policy directly, by combining the time-decomposition based dynamic programming (DP) at the lower layer and the scenario-decomposition based revised progressive hedging algorithm (PHA) at the upper layer, for a type of Bayesian RL problem. The key feature of our approach is to separate reducible system uncertainty from irreducible one at two different layers, thus decomposing and conquering. We demonstrate our solution framework more especially via the linear-quadratic-Gaussian problem with unknown gain, which, although seemingly simple, has been a notorious subject over more than half century in dual control."},{"title":"Unveiling the Hyper-Rayleigh Regime of the Fluctuating Two-Ray Fading\n  Model","source":"Celia Garcia-Corrales, Unai Fernandez-Plazaola, Francisco J. Ca\\~nete,\n  Jos\\'e F. Paris and F. Javier Lopez-Martinez","docs_id":"1905.00065","section":["cs.IT","eess.SP","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Unveiling the Hyper-Rayleigh Regime of the Fluctuating Two-Ray Fading\n  Model. The recently proposed Fluctuating Two-Ray (FTR) model is gaining momentum as a reference fading model in scenarios where two dominant specular waves are present. Despite the numerous research works devoted to the performance analysis under FTR fading, little attention has been paid to effectively understanding the interplay between the fading model parameters and the fading severity. According to a new scale defined in this work, which measures the hyper-Rayleigh character of a fading channel in terms of the Amount of Fading, the outage probability and the average capacity, we see that the FTR fading model exhibits a full hyper-Rayleigh behavior. However, the Two-Wave with Diffuse Power fading model from which the former is derived has only strong hyper-Rayleigh behavior, which constitutes an interesting new insight. We also identify that the random fluctuations in the dominant specular waves are ultimately responsible for the full hyper-Rayleigh behavior of this class of fading channels."},{"title":"Data-Driven Extract Method Recommendations: A Study at ING","source":"David van der Leij and Jasper Binda and Robbert van Dalen and Pieter\n  Vallen and Yaping Luo and Maur\\'icio Aniche","docs_id":"2107.05396","section":["cs.SE"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Data-Driven Extract Method Recommendations: A Study at ING. The sound identification of refactoring opportunities is still an open problem in software engineering. Recent studies have shown the effectiveness of machine learning models in recommending methods that should undergo different refactoring operations. In this work, we experiment with such approaches to identify methods that should undergo an Extract Method refactoring, in the context of ING, a large financial organization. More specifically, we (i) compare the code metrics distributions, which are used as features by the models, between open-source and ING systems, (ii) measure the accuracy of different machine learning models in recommending Extract Method refactorings, (iii) compare the recommendations given by the models with the opinions of ING experts. Our results show that the feature distributions of ING systems and open-source systems are somewhat different, that machine learning models can recommend Extract Method refactorings with high accuracy, and that experts tend to agree with most of the recommendations of the model."},{"title":"Reachability Analysis of Reversal-bounded Automata on Series-Parallel\n  Graphs","source":"Rayna Dimitrova, Rupak Majumdar","docs_id":"1509.07202","section":["cs.FL"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Reachability Analysis of Reversal-bounded Automata on Series-Parallel\n  Graphs. Extensions to finite-state automata on strings, such as multi-head automata or multi-counter automata, have been successfully used to encode many infinite-state non-regular verification problems. In this paper, we consider a generalization of automata-theoretic infinite-state verification from strings to labeled series-parallel graphs. We define a model of non-deterministic, 2-way, concurrent automata working on series-parallel graphs and communicating through shared registers on the nodes of the graph. We consider the following verification problem: given a family of series-parallel graphs described by a context-free graph transformation system (GTS), and a concurrent automaton over series-parallel graphs, is some graph generated by the GTS accepted by the automaton? The general problem is undecidable already for (one-way) multi-head automata over strings. We show that a bounded version, where the automata make a fixed number of reversals along the graph and use a fixed number of shared registers is decidable, even though there is no bound on the sizes of series-parallel graphs generated by the GTS. Our decidability result is based on establishing that the number of context switches is bounded and on an encoding of the computation of bounded concurrent automata to reduce the emptiness problem to the emptiness problem for pushdown automata."},{"title":"LPOS: Location Privacy for Optimal Sensing in Cognitive Radio Networks","source":"Mohamed Grissa, Attila Yavuz and Bechir Hamdaoui","docs_id":"1806.03572","section":["cs.NI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"LPOS: Location Privacy for Optimal Sensing in Cognitive Radio Networks. Cognitive Radio Networks (CRNs) enable opportunistic access to the licensed channel resources by allowing unlicensed users to exploit vacant channel opportunities. One effective technique through which unlicensed users, often referred to as Secondary Users (SUs), acquire whether a channel is vacant is cooperative spectrum sensing. Despite its effectiveness in enabling CRN access, cooperative sensing suffers from location privacy threats, merely because the sensing reports that need to be exchanged among the SUs to perform the sensing task are highly correlated to the SUs' locations. In this paper, we develop a new Location Privacy for Optimal Sensing (LPOS) scheme that preserves the location privacy of SUs while achieving optimal sensing performance through voting-based sensing. In addition, LPOS is the only alternative among existing CRN location privacy preserving schemes (to the best of our knowledge) that ensures high privacy, achieves fault tolerance, and is robust against the highly dynamic and wireless nature of CRNs."},{"title":"Locality-Sensitive Hashing for f-Divergences: Mutual Information Loss\n  and Beyond","source":"Lin Chen, Hossein Esfandiari, Thomas Fu, Vahab S. Mirrokni","docs_id":"1910.12414","section":["cs.LG","cs.DB","cs.DS","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Locality-Sensitive Hashing for f-Divergences: Mutual Information Loss\n  and Beyond. Computing approximate nearest neighbors in high dimensional spaces is a central problem in large-scale data mining with a wide range of applications in machine learning and data science. A popular and effective technique in computing nearest neighbors approximately is the locality-sensitive hashing (LSH) scheme. In this paper, we aim to develop LSH schemes for distance functions that measure the distance between two probability distributions, particularly for f-divergences as well as a generalization to capture mutual information loss. First, we provide a general framework to design LHS schemes for f-divergence distance functions and develop LSH schemes for the generalized Jensen-Shannon divergence and triangular discrimination in this framework. We show a two-sided approximation result for approximation of the generalized Jensen-Shannon divergence by the Hellinger distance, which may be of independent interest. Next, we show a general method of reducing the problem of designing an LSH scheme for a Krein kernel (which can be expressed as the difference of two positive definite kernels) to the problem of maximum inner product search. We exemplify this method by applying it to the mutual information loss, due to its several important applications such as model compression."},{"title":"Planar Prior Assisted PatchMatch Multi-View Stereo","source":"Qingshan Xu and Wenbing Tao","docs_id":"1912.11744","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Planar Prior Assisted PatchMatch Multi-View Stereo. The completeness of 3D models is still a challenging problem in multi-view stereo (MVS) due to the unreliable photometric consistency in low-textured areas. Since low-textured areas usually exhibit strong planarity, planar models are advantageous to the depth estimation of low-textured areas. On the other hand, PatchMatch multi-view stereo is very efficient for its sampling and propagation scheme. By taking advantage of planar models and PatchMatch multi-view stereo, we propose a planar prior assisted PatchMatch multi-view stereo framework in this paper. In detail, we utilize a probabilistic graphical model to embed planar models into PatchMatch multi-view stereo and contribute a novel multi-view aggregated matching cost. This novel cost takes both photometric consistency and planar compatibility into consideration, making it suited for the depth estimation of both non-planar and planar regions. Experimental results demonstrate that our method can efficiently recover the depth information of extremely low-textured areas, thus obtaining high complete 3D models and achieving state-of-the-art performance."},{"title":"Uniform bounds for invariant subspace perturbations","source":"Anil Damle and Yuekai Sun","docs_id":"1905.07865","section":["math.NA","cs.NA","math.ST","stat.TH"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Uniform bounds for invariant subspace perturbations. For a fixed symmetric matrix A and symmetric perturbation E we develop purely deterministic bounds on how invariant subspaces of A and A+E can differ when measured by a suitable \"row-wise\" metric rather than via traditional measures of subspace distance. Understanding perturbations of invariant subspaces with respect to such metrics is becoming increasingly important across a wide variety of applications and therefore necessitates new theoretical developments. Under minimal assumptions we develop new bounds on subspace perturbations under the two-to-infinity matrix norm and show in what settings these row-wise differences in the invariant subspaces can be significantly smaller than the analogous two or Frobenius norm differences. We also demonstrate that the constitutive pieces of our bounds are necessary absent additional assumptions and, therefore, our results provide a natural starting point for further analysis of specific problems. Lastly, we briefly discuss extensions of our bounds to scenarios where A and\/or E are non-normal matrices."},{"title":"Few-Shot Scene Adaptive Crowd Counting Using Meta-Learning","source":"Mahesh Kumar Krishna Reddy, Mohammad Hossain, Mrigank Rochan and Yang\n  Wang","docs_id":"2002.00264","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Few-Shot Scene Adaptive Crowd Counting Using Meta-Learning. We consider the problem of few-shot scene adaptive crowd counting. Given a target camera scene, our goal is to adapt a model to this specific scene with only a few labeled images of that scene. The solution to this problem has potential applications in numerous real-world scenarios, where we ideally like to deploy a crowd counting model specially adapted to a target camera. We accomplish this challenge by taking inspiration from the recently introduced learning-to-learn paradigm in the context of few-shot regime. In training, our method learns the model parameters in a way that facilitates the fast adaptation to the target scene. At test time, given a target scene with a small number of labeled data, our method quickly adapts to that scene with a few gradient updates to the learned parameters. Our extensive experimental results show that the proposed approach outperforms other alternatives in few-shot scene adaptive crowd counting. Code is available at https:\/\/github.com\/maheshkkumar\/fscc."},{"title":"On the Linear convergence of Natural Policy Gradient Algorithm","source":"Sajad Khodadadian, Prakirt Raj Jhunjhunwala, Sushil Mahavir Varma,\n  Siva Theja Maguluri","docs_id":"2105.01424","section":["cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"On the Linear convergence of Natural Policy Gradient Algorithm. Markov Decision Processes are classically solved using Value Iteration and Policy Iteration algorithms. Recent interest in Reinforcement Learning has motivated the study of methods inspired by optimization, such as gradient ascent. Among these, a popular algorithm is the Natural Policy Gradient, which is a mirror descent variant for MDPs. This algorithm forms the basis of several popular Reinforcement Learning algorithms such as Natural actor-critic, TRPO, PPO, etc, and so is being studied with growing interest. It has been shown that Natural Policy Gradient with constant step size converges with a sublinear rate of O(1\/k) to the global optimal. In this paper, we present improved finite time convergence bounds, and show that this algorithm has geometric (also known as linear) asymptotic convergence rate. We further improve this convergence result by introducing a variant of Natural Policy Gradient with adaptive step sizes. Finally, we compare different variants of policy gradient methods experimentally."},{"title":"Cortical oscillations implement a backbone for sampling-based\n  computation in spiking neural networks","source":"Agnes Korcsak-Gorzo, Michael G. M\\\"uller, Andreas Baumbach, Luziwei\n  Leng, Oliver Julien Breitwieser, Sacha J. van Albada, Walter Senn, Karlheinz\n  Meier, Robert Legenstein, Mihai A. Petrovici","docs_id":"2006.11099","section":["q-bio.NC","cs.NE"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Cortical oscillations implement a backbone for sampling-based\n  computation in spiking neural networks. Brains need to deal with an uncertain world. Often, this requires visiting multiple interpretations of the available information or multiple solutions to an encountered problem. This gives rise to the so-called mixing problem: since all of these \"valid\" states represent powerful attractors, but between themselves can be very dissimilar, switching between such states can be difficult. We propose that cortical oscillations can be effectively used to overcome this challenge. By acting as an effective temperature, background spiking activity modulates exploration. Rhythmic changes induced by cortical oscillations can then be interpreted as a form of simulated tempering. We provide a rigorous mathematical discussion of this link and study some of its phenomenological implications in computer simulations. This identifies a new computational role of cortical oscillations and connects them to various phenomena in the brain, such as sampling-based probabilistic inference, memory replay, multisensory cue combination and place cell flickering."},{"title":"Distributed Scalar Quantization for Computing: High-Resolution Analysis\n  and Extensions","source":"Vinith Misra, Vivek K Goyal, Lav R. Varshney","docs_id":"0811.3617","section":["cs.IT","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Distributed Scalar Quantization for Computing: High-Resolution Analysis\n  and Extensions. Communication of quantized information is frequently followed by a computation. We consider situations of \\emph{distributed functional scalar quantization}: distributed scalar quantization of (possibly correlated) sources followed by centralized computation of a function. Under smoothness conditions on the sources and function, companding scalar quantizer designs are developed to minimize mean-squared error (MSE) of the computed function as the quantizer resolution is allowed to grow. Striking improvements over quantizers designed without consideration of the function are possible and are larger in the entropy-constrained setting than in the fixed-rate setting. As extensions to the basic analysis, we characterize a large class of functions for which regular quantization suffices, consider certain functions for which asymptotic optimality is achieved without arbitrarily fine quantization, and allow limited collaboration between source encoders. In the entropy-constrained setting, a single bit per sample communicated between encoders can have an arbitrarily-large effect on functional distortion. In contrast, such communication has very little effect in the fixed-rate setting."},{"title":"Fast Topological Clustering with Wasserstein Distance","source":"Tananun Songdechakraiwut, Bryan M. Krause, Matthew I. Banks, Kirill V.\n  Nourski and Barry D. Van Veen","docs_id":"2112.00101","section":["cs.LG","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Fast Topological Clustering with Wasserstein Distance. The topological patterns exhibited by many real-world networks motivate the development of topology-based methods for assessing the similarity of networks. However, extracting topological structure is difficult, especially for large and dense networks whose node degrees range over multiple orders of magnitude. In this paper, we propose a novel and computationally practical topological clustering method that clusters complex networks with intricate topology using principled theory from persistent homology and optimal transport. Such networks are aggregated into clusters through a centroid-based clustering strategy based on both their topological and geometric structure, preserving correspondence between nodes in different networks. The notions of topological proximity and centroid are characterized using a novel and efficient approach to computation of the Wasserstein distance and barycenter for persistence barcodes associated with connected components and cycles. The proposed method is demonstrated to be effective using both simulated networks and measured functional brain networks."},{"title":"A Robotic Line Scan System with Adaptive ROI for Inspection of Defects\n  over Convex Free-form Specular Surfaces","source":"Shengzeng Huo, David Navarro-Alarcon, David Chik","docs_id":"2008.10816","section":["cs.RO"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A Robotic Line Scan System with Adaptive ROI for Inspection of Defects\n  over Convex Free-form Specular Surfaces. In this paper, we present a new robotic system to perform defect inspection tasks over free-form specular surfaces. The autonomous procedure is achieved by a six-DOF manipulator, equipped with a line scan camera and a high-intensity lighting system. Our method first uses the object's CAD mesh model to implement a K-means unsupervised learning algorithm that segments the object's surface into areas with similar curvature. Then, the scanning path is computed by using an adaptive algorithm that adjusts the camera's ROI to observe regions with irregular shapes properly. A novel iterative closest point-based projection registration method that robustly localizes the object in the robot's coordinate frame system is proposed to deal with the blind spot problem of specular objects captured by depth sensors. Finally, an image processing pipeline automatically detects surface defects in the captured high-resolution images. A detailed experimental study with a vision-guided robotic scanning system is reported to validate the proposed methodology."},{"title":"EE-AE: An Exclusivity Enhanced Unsupervised Feature Learning Approach","source":"Jingcai Guo, Song Guo","docs_id":"1904.00172","section":["cs.LG","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"EE-AE: An Exclusivity Enhanced Unsupervised Feature Learning Approach. Unsupervised learning is becoming more and more important recently. As one of its key components, the autoencoder (AE) aims to learn a latent feature representation of data which is more robust and discriminative. However, most AE based methods only focus on the reconstruction within the encoder-decoder phase, which ignores the inherent relation of data, i.e., statistical and geometrical dependence, and easily causes overfitting. In order to deal with this issue, we propose an Exclusivity Enhanced (EE) unsupervised feature learning approach to improve the conventional AE. To the best of our knowledge, our research is the first to utilize such exclusivity concept to cooperate with feature extraction within AE. Moreover, in this paper we also make some improvements to the stacked AE structure especially for the connection of different layers from decoders, this could be regarded as a weight initialization trial. The experimental results show that our proposed approach can achieve remarkable performance compared with other related methods."},{"title":"A Comprehensive Study of Commonly Practiced Heavy and Light Weight\n  Software Methodologies","source":"Asif Irshad Khan, Rizwan Jameel Qurashi, Usman Ali Khan","docs_id":"1111.3001","section":["cs.SE"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A Comprehensive Study of Commonly Practiced Heavy and Light Weight\n  Software Methodologies. Software has been playing a key role in the development of modern society. Software industry has an option to choose suitable methodology\/process model for its current needs to provide solutions to give problems. Though some companies have their own customized methodology for developing their software but majority agrees that software methodologies fall under two categories that are heavyweight and lightweight. Heavyweight methodologies (Waterfall Model, Spiral Model) are also known as the traditional methodologies, and their focuses are detailed documentation, inclusive planning, and extroverted design. Lightweight methodologies (XP, SCRUM) are, referred as agile methodologies. Light weight methodologies focused mainly on short iterative cycles, and rely on the knowledge within a team. The aim of this paper is to describe the characteristics of popular heavyweight and lightweight methodologies that are widely practiced in software industries. We have discussed the strengths and weakness of the selected models. Further we have discussed the strengths and weakness between the two opponent methodologies and some criteria is also illustrated that help project managers for the selection of suitable model for their projects."},{"title":"Learning Markov State Abstractions for Deep Reinforcement Learning","source":"Cameron Allen, Neev Parikh, Omer Gottesman, George Konidaris","docs_id":"2106.04379","section":["cs.LG","cs.AI","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Learning Markov State Abstractions for Deep Reinforcement Learning. A fundamental assumption of reinforcement learning in Markov decision processes (MDPs) is that the relevant decision process is, in fact, Markov. However, when MDPs have rich observations, agents typically learn by way of an abstract state representation, and such representations are not guaranteed to preserve the Markov property. We introduce a novel set of conditions and prove that they are sufficient for learning a Markov abstract state representation. We then describe a practical training procedure that combines inverse model estimation and temporal contrastive learning to learn an abstraction that approximately satisfies these conditions. Our novel training objective is compatible with both online and offline training: it does not require a reward signal, but agents can capitalize on reward information when available. We empirically evaluate our approach on a visual gridworld domain and a set of continuous control benchmarks. Our approach learns representations that capture the underlying structure of the domain and lead to improved sample efficiency over state-of-the-art deep reinforcement learning with visual features -- often matching or exceeding the performance achieved with hand-designed compact state information."},{"title":"Asymptotically Tight Steady-State Queue Length Bounds Implied By Drift\n  Conditions","source":"Atilla Eryilmaz and R. Srikant","docs_id":"1104.0327","section":["math.PR","cs.NI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Asymptotically Tight Steady-State Queue Length Bounds Implied By Drift\n  Conditions. The Foster-Lyapunov theorem and its variants serve as the primary tools for studying the stability of queueing systems. In addition, it is well known that setting the drift of the Lyapunov function equal to zero in steady-state provides bounds on the expected queue lengths. However, such bounds are often very loose due to the fact that they fail to capture resource pooling effects. The main contribution of this paper is to show that the approach of \"setting the drift of a Lyapunov function equal to zero\" can be used to obtain bounds on the steady-state queue lengths which are tight in the heavy-traffic limit. The key is to establish an appropriate notion of state-space collapse in terms of steady-state moments of weighted queue length differences, and use this state-space collapse result when setting the Lyapunov drift equal to zero. As an application of the methodology, we prove the steady-state equivalent of the heavy-traffic optimality result of Stolyar for wireless networks operating under the MaxWeight scheduling policy."},{"title":"Assessing Threat of Adversarial Examples on Deep Neural Networks","source":"Abigail Graese, Andras Rozsa, Terrance E. Boult","docs_id":"1610.04256","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Assessing Threat of Adversarial Examples on Deep Neural Networks. Deep neural networks are facing a potential security threat from adversarial examples, inputs that look normal but cause an incorrect classification by the deep neural network. For example, the proposed threat could result in hand-written digits on a scanned check being incorrectly classified but looking normal when humans see them. This research assesses the extent to which adversarial examples pose a security threat, when one considers the normal image acquisition process. This process is mimicked by simulating the transformations that normally occur in acquiring the image in a real world application, such as using a scanner to acquire digits for a check amount or using a camera in an autonomous car. These small transformations negate the effect of the carefully crafted perturbations of adversarial examples, resulting in a correct classification by the deep neural network. Thus just acquiring the image decreases the potential impact of the proposed security threat. We also show that the already widely used process of averaging over multiple crops neutralizes most adversarial examples. Normal preprocessing, such as text binarization, almost completely neutralizes adversarial examples. This is the first paper to show that for text driven classification, adversarial examples are an academic curiosity, not a security threat."},{"title":"Where Is the Normative Proof? Assumptions and Contradictions in ML\n  Fairness Research","source":"A. Feder Cooper","docs_id":"2010.10407","section":["cs.CY","cs.AI","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Where Is the Normative Proof? Assumptions and Contradictions in ML\n  Fairness Research. Across machine learning (ML) sub-disciplines researchers make mathematical assumptions to facilitate proof-writing. While such assumptions are necessary for providing mathematical guarantees for how algorithms behave, they also necessarily limit the applicability of these algorithms to different problem settings. This practice is known--in fact, obvious--and accepted in ML research. However, similar attention is not paid to the normative assumptions that ground this work. I argue such assumptions are equally as important, especially in areas of ML with clear social impact, such as fairness. This is because, similar to how mathematical assumptions constrain applicability, normative assumptions also limit algorithm applicability to certain problem domains. I show that, in existing papers published in top venues, once normative assumptions are clarified, it is often possible to get unclear or contradictory results. While the mathematical assumptions and results are sound, the implicit normative assumptions and accompanying normative results contraindicate using these methods in practical fairness applications."},{"title":"MigrantStore: Leveraging Virtual Memory in DRAM-PCM Memory Architecture","source":"Hamza Bin Sohail, Balajee Vamanan, T. N. Vijaykumar","docs_id":"1504.04297","section":["cs.AR"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"MigrantStore: Leveraging Virtual Memory in DRAM-PCM Memory Architecture. With the imminent slowing down of DRAM scaling, Phase Change Memory (PCM) is emerging as a lead alternative for main memory technology. While PCM achieves low energy due to various technology-specific advantages, PCM is significantly slower than DRAM (especially for writes) and can endure far fewer writes before wearing out. Previous work has proposed to use a large, DRAM-based hardware cache to absorb writes and provide faster access. However, due to ineffectual caching where blocks are evicted before sufficient number of accesses, hardware caches incur significant overheads in energy and bandwidth, two key but scarce resources in modern multicores. Because using hardware for detecting and removing such ineffectual caching would incur additional hardware cost and complexity, we leverage the OS virtual memory support for this purpose. We propose a DRAM-PCM hybrid memory architecture where the OS migrates pages on demand from the PCM to DRAM. We call the DRAM part of our memory as MigrantStore which includes two ideas. First, to reduce the energy, bandwidth, and wear overhead of ineffectual migrations, we propose migration hysteresis. Second, to reduce the software overhead of good replacement policies, we propose recently- accessed-page-id (RAPid) buffer, a hardware buffer to track the addresses of recently-accessed MigrantStore pages."},{"title":"Security and Privacy Issues in Cloud Computing","source":"Jaydip Sen","docs_id":"1303.4814","section":["cs.CR","cs.NI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Security and Privacy Issues in Cloud Computing. Cloud computing transforms the way information technology (IT) is consumed and managed, promising improved cost efficiencies, accelerated innovation, faster time-to-market, and the ability to scale applications on demand (Leighton, 2009). According to Gartner, while the hype grew exponentially during 2008 and continued since, it is clear that there is a major shift towards the cloud computing model and that the benefits may be substantial (Gartner Hype-Cycle, 2012). However, as the shape of the cloud computing is emerging and developing rapidly both conceptually and in reality, the legal\/contractual, economic, service quality, interoperability, security and privacy issues still pose significant challenges. In this chapter, we describe various service and deployment models of cloud computing and identify major challenges. In particular, we discuss three critical challenges: regulatory, security and privacy issues in cloud computing. Some solutions to mitigate these challenges are also proposed along with a brief presentation on the future trends in cloud computing deployment."},{"title":"A Relational Gradient Descent Algorithm For Support Vector Machine\n  Training","source":"Mahmoud Abo-Khamis, Sungjin Im, Benjamin Moseley, Kirk Pruhs, Alireza\n  Samadian","docs_id":"2005.05325","section":["cs.DS","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A Relational Gradient Descent Algorithm For Support Vector Machine\n  Training. We consider gradient descent like algorithms for Support Vector Machine (SVM) training when the data is in relational form. The gradient of the SVM objective can not be efficiently computed by known techniques as it suffers from the ``subtraction problem''. We first show that the subtraction problem can not be surmounted by showing that computing any constant approximation of the gradient of the SVM objective function is $\\#P$-hard, even for acyclic joins. We, however, circumvent the subtraction problem by restricting our attention to stable instances, which intuitively are instances where a nearly optimal solution remains nearly optimal if the points are perturbed slightly. We give an efficient algorithm that computes a ``pseudo-gradient'' that guarantees convergence for stable instances at a rate comparable to that achieved by using the actual gradient. We believe that our results suggest that this sort of stability the analysis would likely yield useful insight in the context of designing algorithms on relational data for other learning problems in which the subtraction problem arises."},{"title":"Channel Estimation for Opportunistic Spectrum Access: Uniform and Random\n  Sensing","source":"Quanquan Liang, Mingyan Liu","docs_id":"1005.2544","section":["cs.IT","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Channel Estimation for Opportunistic Spectrum Access: Uniform and Random\n  Sensing. The knowledge of channel statistics can be very helpful in making sound opportunistic spectrum access decisions. It is therefore desirable to be able to efficiently and accurately estimate channel statistics. In this paper we study the problem of optimally placing sensing times over a time window so as to get the best estimate on the parameters of an on-off renewal channel. We are particularly interested in a sparse sensing regime with a small number of samples relative to the time window size. Using Fisher information as a measure, we analytically derive the best and worst sensing sequences under a sparsity condition. We also present a way to derive the best\/worst sequences without this condition using a dynamic programming approach. In both cases the worst turns out to be the uniform sensing sequence, where sensing times are evenly spaced within the window. With these results we argue that without a priori knowledge, a robust sensing strategy should be a randomized strategy. We then compare different random schemes using a family of distributions generated by the circular $\\beta$ ensemble, and propose an adaptive sensing scheme to effectively track time-varying channel parameters. We further discuss the applicability of compressive sensing for this problem."},{"title":"Classification of URL bitstreams using Bag of Bytes","source":"Keiichi Shima, Daisuke Miyamoto, Hiroshi Abe, Tomohiro Ishihara,\n  Kazuya Okada, Yuji Sekiya, Hirochika Asai, Yusuke Doi","docs_id":"2111.06087","section":["cs.NI","cs.AI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Classification of URL bitstreams using Bag of Bytes. Protecting users from accessing malicious web sites is one of the important management tasks for network operators. There are many open-source and commercial products to control web sites users can access. The most traditional approach is blacklist-based filtering. This mechanism is simple but not scalable, though there are some enhanced approaches utilizing fuzzy matching technologies. Other approaches try to use machine learning (ML) techniques by extracting features from URL strings. This approach can cover a wider area of Internet web sites, but finding good features requires deep knowledge of trends of web site design. Recently, another approach using deep learning (DL) has appeared. The DL approach will help to extract features automatically by investigating a lot of existing sample data. Using this technique, we can build a flexible filtering decision module by keep teaching the neural network module about recent trends, without any specific expert knowledge of the URL domain. In this paper, we apply a mechanical approach to generate feature vectors from URL strings. We implemented our approach and tested with realistic URL access history data taken from a research organization and data from the famous archive site of phishing site information, PhishTank.com. Our approach achieved 2~3% better accuracy compared to the existing DL-based approach."},{"title":"Foot anthropometry device and single object image thresholding","source":"Amir Mohammad Esmaieeli Sikaroudi, Sasan Ghaffari, Ali Yousefi, Hassan\n  Sadeghi Naeini","docs_id":"1707.03004","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Foot anthropometry device and single object image thresholding. This paper introduces a device, algorithm and graphical user interface to obtain anthropometric measurements of foot. Presented device facilitates obtaining scale of image and image processing by taking one image from side foot and underfoot simultaneously. Introduced image processing algorithm minimizes a noise criterion, which is suitable for object detection in single object images and outperforms famous image thresholding methods when lighting condition is poor. Performance of image-based method is compared to manual method. Image-based measurements of underfoot in average was 4mm less than actual measures. Mean absolute error of underfoot length was 1.6mm, however length obtained from side foot had 4.4mm mean absolute error. Furthermore, based on t-test and f-test results, no significant difference between manual and image-based anthropometry observed. In order to maintain anthropometry process performance in different situations user interface designed for handling changes in light conditions and altering speed of the algorithm."},{"title":"Transfer Learning in Electronic Health Records through Clinical Concept\n  Embedding","source":"Jose Roberto Ayala Solares, Yajie Zhu, Abdelaali Hassaine, Shishir\n  Rao, Yikuan Li, Mohammad Mamouei, Dexter Canoy, Kazem Rahimi, Gholamreza\n  Salimi-Khorshidi","docs_id":"2107.12919","section":["cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Transfer Learning in Electronic Health Records through Clinical Concept\n  Embedding. Deep learning models have shown tremendous potential in learning representations, which are able to capture some key properties of the data. This makes them great candidates for transfer learning: Exploiting commonalities between different learning tasks to transfer knowledge from one task to another. Electronic health records (EHR) research is one of the domains that has witnessed a growing number of deep learning techniques employed for learning clinically-meaningful representations of medical concepts (such as diseases and medications). Despite this growth, the approaches to benchmark and assess such learned representations (or, embeddings) is under-investigated; this can be a big issue when such embeddings are shared to facilitate transfer learning. In this study, we aim to (1) train some of the most prominent disease embedding techniques on a comprehensive EHR data from 3.1 million patients, (2) employ qualitative and quantitative evaluation techniques to assess these embeddings, and (3) provide pre-trained disease embeddings for transfer learning. This study can be the first comprehensive approach for clinical concept embedding evaluation and can be applied to any embedding techniques and for any EHR concept."},{"title":"BuildingNet: Learning to Label 3D Buildings","source":"Pratheba Selvaraju, Mohamed Nabail, Marios Loizou, Maria Maslioukova,\n  Melinos Averkiou, Andreas Andreou, Siddhartha Chaudhuri, Evangelos\n  Kalogerakis","docs_id":"2110.04955","section":["cs.CV","cs.GR"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"BuildingNet: Learning to Label 3D Buildings. We introduce BuildingNet: (a) a large-scale dataset of 3D building models whose exteriors are consistently labeled, (b) a graph neural network that labels building meshes by analyzing spatial and structural relations of their geometric primitives. To create our dataset, we used crowdsourcing combined with expert guidance, resulting in 513K annotated mesh primitives, grouped into 292K semantic part components across 2K building models. The dataset covers several building categories, such as houses, churches, skyscrapers, town halls, libraries, and castles. We include a benchmark for evaluating mesh and point cloud labeling. Buildings have more challenging structural complexity compared to objects in existing benchmarks (e.g., ShapeNet, PartNet), thus, we hope that our dataset can nurture the development of algorithms that are able to cope with such large-scale geometric data for both vision and graphics tasks e.g., 3D semantic segmentation, part-based generative models, correspondences, texturing, and analysis of point cloud data acquired from real-world buildings. Finally, we show that our mesh-based graph neural network significantly improves performance over several baselines for labeling 3D meshes."},{"title":"Types for Information Flow Control: Labeling Granularity and Semantic\n  Models","source":"Vineet Rajani, Deepak Garg","docs_id":"1805.00120","section":["cs.CR","cs.PL"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Types for Information Flow Control: Labeling Granularity and Semantic\n  Models. Language-based information flow control (IFC) tracks dependencies within a program using sensitivity labels and prohibits public outputs from depending on secret inputs. In particular, literature has proposed several type systems for tracking these dependencies. On one extreme, there are fine-grained type systems (like Flow Caml) that label all values individually and track dependence at the level of individual values. On the other extreme are coarse-grained type systems (like HLIO) that track dependence coarsely, by associating a single label with an entire computation context and not labeling all values individually. In this paper, we show that, despite their glaring differences, both these styles are, in fact, equally expressive. To do this, we show a semantics- and type-preserving translation from a coarse-grained type system to a fine-grained one and vice-versa. The forward translation isn't surprising, but the backward translation is: It requires a construct to arbitrarily limit the scope of a context label in the coarse-grained type system (e.g., HLIO's \"toLabeled\" construct). As a separate contribution, we show how to extend work on logical relation models of IFC types to higher-order state. We build such logical relations for both the fine-grained type system and the coarse-grained type system. We use these relations to prove the two type systems and our translations between them sound."},{"title":"Learnable Fourier Features for Multi-Dimensional Spatial Positional\n  Encoding","source":"Yang Li, Si Si, Gang Li, Cho-Jui Hsieh, Samy Bengio","docs_id":"2106.02795","section":["cs.LG","cs.AI","cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Learnable Fourier Features for Multi-Dimensional Spatial Positional\n  Encoding. Attentional mechanisms are order-invariant. Positional encoding is a crucial component to allow attention-based deep model architectures such as Transformer to address sequences or images where the position of information matters. In this paper, we propose a novel positional encoding method based on learnable Fourier features. Instead of hard-coding each position as a token or a vector, we represent each position, which can be multi-dimensional, as a trainable encoding based on learnable Fourier feature mapping, modulated with a multi-layer perceptron. The representation is particularly advantageous for a spatial multi-dimensional position, e.g., pixel positions on an image, where $L_2$ distances or more complex positional relationships need to be captured. Our experiments based on several public benchmark tasks show that our learnable Fourier feature representation for multi-dimensional positional encoding outperforms existing methods by both improving the accuracy and allowing faster convergence."},{"title":"Higher-dimension Tensor Completion via Low-rank Tensor Ring\n  Decomposition","source":"Longhao Yuan, Jianting Cao, Qiang Wu and Qibin Zhao","docs_id":"1807.01589","section":["cs.NA"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Higher-dimension Tensor Completion via Low-rank Tensor Ring\n  Decomposition. The problem of incomplete data is common in signal processing and machine learning. Tensor completion algorithms aim to recover the incomplete data from its partially observed entries. In this paper, taking advantages of high compressibility and flexibility of recently proposed tensor ring (TR) decomposition, we propose a new tensor completion approach named tensor ring weighted optimization (TR-WOPT). It finds the latent factors of the incomplete tensor by gradient descent algorithm, then the latent factors are employed to predict the missing entries of the tensor. We conduct various tensor completion experiments on synthetic data and real-world data. The simulation results show that TR-WOPT performs well in various high-dimension tensors. Furthermore, image completion results show that our proposed algorithm outperforms the state-of-the-art algorithms in many situations. Especially when the missing rate of the test images is high (e.g., over 0.9), the performance of our TR-WOPT is significantly better than the compared algorithms."},{"title":"Multimodal Multipart Learning for Action Recognition in Depth Videos","source":"Amir Shahroudy, Gang Wang, Tian-Tsong Ng, Qingxiong Yang","docs_id":"1507.08761","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Multimodal Multipart Learning for Action Recognition in Depth Videos. The articulated and complex nature of human actions makes the task of action recognition difficult. One approach to handle this complexity is dividing it to the kinetics of body parts and analyzing the actions based on these partial descriptors. We propose a joint sparse regression based learning method which utilizes the structured sparsity to model each action as a combination of multimodal features from a sparse set of body parts. To represent dynamics and appearance of parts, we employ a heterogeneous set of depth and skeleton based features. The proper structure of multimodal multipart features are formulated into the learning framework via the proposed hierarchical mixed norm, to regularize the structured features of each part and to apply sparsity between them, in favor of a group feature selection. Our experimental results expose the effectiveness of the proposed learning method in which it outperforms other methods in all three tested datasets while saturating one of them by achieving perfect accuracy."},{"title":"Cross-Domain Perceptual Reward Functions","source":"Ashley D. Edwards, Srijan Sood, and Charles L. Isbell Jr","docs_id":"1705.09045","section":["cs.AI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Cross-Domain Perceptual Reward Functions. In reinforcement learning, we often define goals by specifying rewards within desirable states. One problem with this approach is that we typically need to redefine the rewards each time the goal changes, which often requires some understanding of the solution in the agents environment. When humans are learning to complete tasks, we regularly utilize alternative sources that guide our understanding of the problem. Such task representations allow one to specify goals on their own terms, thus providing specifications that can be appropriately interpreted across various environments. This motivates our own work, in which we represent goals in environments that are different from the agents. We introduce Cross-Domain Perceptual Reward (CDPR) functions, learned rewards that represent the visual similarity between an agents state and a cross-domain goal image. We report results for learning the CDPRs with a deep neural network and using them to solve two tasks with deep reinforcement learning."},{"title":"Uncertainty Guided Multi-Scale Residual Learning-using a Cycle Spinning\n  CNN for Single Image De-Raining","source":"Rajeev Yasarla and Vishal M. Patel","docs_id":"1906.11129","section":["cs.CV","cs.LG","eess.IV","stat.ML"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Uncertainty Guided Multi-Scale Residual Learning-using a Cycle Spinning\n  CNN for Single Image De-Raining. Single image de-raining is an extremely challenging problem since the rainy image may contain rain streaks which may vary in size, direction and density. Previous approaches have attempted to address this problem by leveraging some prior information to remove rain streaks from a single image. One of the major limitations of these approaches is that they do not consider the location information of rain drops in the image. The proposed Uncertainty guided Multi-scale Residual Learning (UMRL) network attempts to address this issue by learning the rain content at different scales and using them to estimate the final de-rained output. In addition, we introduce a technique which guides the network to learn the network weights based on the confidence measure about the estimate. Furthermore, we introduce a new training and testing procedure based on the notion of cycle spinning to improve the final de-raining performance. Extensive experiments on synthetic and real datasets to demonstrate that the proposed method achieves significant improvements over the recent state-of-the-art methods. Code is available at: https:\/\/github.com\/rajeevyasarla\/UMRL--using-Cycle-Spinning"},{"title":"Text-Independent Speaker Verification Using 3D Convolutional Neural\n  Networks","source":"Amirsina Torfi, Jeremy Dawson, Nasser M. Nasrabadi","docs_id":"1705.09422","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Text-Independent Speaker Verification Using 3D Convolutional Neural\n  Networks. In this paper, a novel method using 3D Convolutional Neural Network (3D-CNN) architecture has been proposed for speaker verification in the text-independent setting. One of the main challenges is the creation of the speaker models. Most of the previously-reported approaches create speaker models based on averaging the extracted features from utterances of the speaker, which is known as the d-vector system. In our paper, we propose an adaptive feature learning by utilizing the 3D-CNNs for direct speaker model creation in which, for both development and enrollment phases, an identical number of spoken utterances per speaker is fed to the network for representing the speakers' utterances and creation of the speaker model. This leads to simultaneously capturing the speaker-related information and building a more robust system to cope with within-speaker variation. We demonstrate that the proposed method significantly outperforms the traditional d-vector verification system. Moreover, the proposed system can also be an alternative to the traditional d-vector system which is a one-shot speaker modeling system by utilizing 3D-CNNs."},{"title":"Explanations based on the Missing: Towards Contrastive Explanations with\n  Pertinent Negatives","source":"Amit Dhurandhar, Pin-Yu Chen, Ronny Luss, Chun-Chen Tu, Paishun Ting,\n  Karthikeyan Shanmugam and Payel Das","docs_id":"1802.07623","section":["cs.AI","cs.CV","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Explanations based on the Missing: Towards Contrastive Explanations with\n  Pertinent Negatives. In this paper we propose a novel method that provides contrastive explanations justifying the classification of an input by a black box classifier such as a deep neural network. Given an input we find what should be %necessarily and minimally and sufficiently present (viz. important object pixels in an image) to justify its classification and analogously what should be minimally and necessarily \\emph{absent} (viz. certain background pixels). We argue that such explanations are natural for humans and are used commonly in domains such as health care and criminology. What is minimally but critically \\emph{absent} is an important part of an explanation, which to the best of our knowledge, has not been explicitly identified by current explanation methods that explain predictions of neural networks. We validate our approach on three real datasets obtained from diverse domains; namely, a handwritten digits dataset MNIST, a large procurement fraud dataset and a brain activity strength dataset. In all three cases, we witness the power of our approach in generating precise explanations that are also easy for human experts to understand and evaluate."},{"title":"Optimization-friendly generic mechanisms without money","source":"Mark Braverman","docs_id":"2106.07752","section":["cs.GT","cs.DS","cs.LG","math.OC"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Optimization-friendly generic mechanisms without money. The goal of this paper is to develop a generic framework for converting modern optimization algorithms into mechanisms where inputs come from self-interested agents. We focus on aggregating preferences from $n$ players in a context without money. Special cases of this setting include voting, allocation of items by lottery, and matching. Our key technical contribution is a new meta-algorithm we call \\apex (Adaptive Pricing Equalizing Externalities). The framework is sufficiently general to be combined with any optimization algorithm that is based on local search. We outline an agenda for studying the algorithm's properties and its applications. As a special case of applying the framework to the problem of one-sided assignment with lotteries, we obtain a strengthening of the 1979 result by Hylland and Zeckhauser on allocation via a competitive equilibrium from equal incomes (CEEI). The [HZ79] result posits that there is a (fractional) allocation and a set of item prices such that the allocation is a competitive equilibrium given prices. We further show that there is always a reweighing of the players' utility values such that running unit-demand VCG with reweighed utilities leads to a HZ-equilibrium prices. Interestingly, not all HZ competitive equilibria come from VCG prices. As part of our proof, we re-prove the [HZ79] result using only Brouwer's fixed point theorem (and not the more general Kakutani's theorem). This may be of independent interest."},{"title":"Repeated Matching Pennies with Limited Randomness","source":"Michele Budinich and Lance Fortnow","docs_id":"1102.1096","section":["cs.GT","cs.CC"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Repeated Matching Pennies with Limited Randomness. We consider a repeated Matching Pennies game in which players have limited access to randomness. Playing the (unique) Nash equilibrium in this n-stage game requires n random bits. Can there be Nash equilibria that use less than n random coins? Our main results are as follows: We give a full characterization of approximate equilibria, showing that, for any e in [0, 1], the game has a e-Nash equilibrium if and only if both players have (1 - e)n random coins. When players are bound to run in polynomial time, Nash equilibria can exist if and only if one-way functions exist. It is possible to trade-off randomness for running time. In particular, under reasonable assumptions, if we give one player only O(log n) random coins but allow him to run in arbitrary polynomial time and we restrict his opponent to run in time n^k, for some fixed k, then we can sustain an Nash equilibrium. When the game is played for an infinite amount of rounds with time discounted utilities, under reasonable assumptions, we can reduce the amount of randomness required to achieve a e-Nash equilibrium to n, where n is the number of random coins necessary to achieve an approximate Nash equilibrium in the general case."},{"title":"Events-to-Video: Bringing Modern Computer Vision to Event Cameras","source":"Henri Rebecq, Ren\\'e Ranftl, Vladlen Koltun, Davide Scaramuzza","docs_id":"1904.08298","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Events-to-Video: Bringing Modern Computer Vision to Event Cameras. Event cameras are novel sensors that report brightness changes in the form of asynchronous \"events\" instead of intensity frames. They have significant advantages over conventional cameras: high temporal resolution, high dynamic range, and no motion blur. Since the output of event cameras is fundamentally different from conventional cameras, it is commonly accepted that they require the development of specialized algorithms to accommodate the particular nature of events. In this work, we take a different view and propose to apply existing, mature computer vision techniques to videos reconstructed from event data. We propose a novel recurrent network to reconstruct videos from a stream of events, and train it on a large amount of simulated event data. Our experiments show that our approach surpasses state-of-the-art reconstruction methods by a large margin (> 20%) in terms of image quality. We further apply off-the-shelf computer vision algorithms to videos reconstructed from event data on tasks such as object classification and visual-inertial odometry, and show that this strategy consistently outperforms algorithms that were specifically designed for event data. We believe that our approach opens the door to bringing the outstanding properties of event cameras to an entirely new range of tasks. A video of the experiments is available at https:\/\/youtu.be\/IdYrC4cUO0I"},{"title":"Motion Planning With Gamma-Harmonic Potential Fields","source":"Ahmad A. Masoud","docs_id":"1606.09278","section":["cs.RO","cs.SY"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Motion Planning With Gamma-Harmonic Potential Fields. This paper extends the capabilities of the harmonic potential field (HPF) approach to planning. The extension covers the situation where the workspace of a robot cannot be segmented into geometrical subregions where each region has an attribute of its own. The suggested approach uses a task-centered, probabilistic descriptor of the workspace as an input to the planner. This descriptor is processed, along with a goal point, to yield the navigation policy needed to steer the agent from any point in its workspace to the target. The approach is easily adaptable to planning in a cluttered environment containing a vector drift field. The extension of the HPF approach is based on the physical analogy with an electric current flowing in a nonhomogeneous conducting medium. The resulting potential field is known as the gamma-harmonic potential (GHPF). Proofs of the ability of the modified approach to avoid zero-probability (definite threat) regions and to converge to the goal are provided. The capabilities of the planer are demonstrated using simulation."},{"title":"Efficient Distributed Medium Access","source":"Devavrat Shah, Jinwoo Shin and Prasad Tetali","docs_id":"1104.2380","section":["cs.NI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Efficient Distributed Medium Access. Consider a wireless network of n nodes represented by a graph G=(V, E) where an edge (i,j) models the fact that transmissions of i and j interfere with each other, i.e. simultaneous transmissions of i and j become unsuccessful. Hence it is required that at each time instance a set of non-interfering nodes (corresponding to an independent set in G) access the wireless medium. To utilize wireless resources efficiently, it is required to arbitrate the access of medium among interfering nodes properly. Moreover, to be of practical use, such a mechanism is required to be totally distributed as well as simple. As the main result of this paper, we provide such a medium access algorithm. It is randomized, totally distributed and simple: each node attempts to access medium at each time with probability that is a function of its local information. We establish efficiency of the algorithm by showing that the corresponding network Markov chain is positive recurrent as long as the demand imposed on the network can be supported by the wireless network (using any algorithm). In that sense, the proposed algorithm is optimal in terms of utilizing wireless resources. The algorithm is oblivious to the network graph structure, in contrast with the so-called `polynomial back-off' algorithm by Hastad-Leighton-Rogoff (STOC '87, SICOMP '96) that is established to be optimal for the complete graph and bipartite graphs (by Goldberg-MacKenzie (SODA '96, JCSS '99))."},{"title":"Agile Ways of Working: A Team Maturity Perspective","source":"Lucas Gren, Alfredo Goldman and Christian Jacobsson","docs_id":"1911.09064","section":["cs.SE"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Agile Ways of Working: A Team Maturity Perspective. With the agile approach to managing software development projects comes an increased dependability on well functioning teams, since many of the practices are built on teamwork. The objective of this study was to investigate if, and how, team development from a group psychological perspective is related to some work practices of agile teams. Data were collected from 34 agile teams (200 individuals) from six software development organizations and one university in both Brazil and Sweden using the Group Development Questionnaire (Scale IV) and the Perceptive Agile Measurement (PAM). The result indicates a strong correlation between levels of group maturity and the two agile practices \\emph{iterative development} and \\emph{retrospectives}. We, therefore, conclude that agile teams at different group development stages adopt parts of team agility differently, thus confirming previous studies but with more data and by investigating concrete and applied agile practices. We thereby add evidence to the hypothesis that an agile implementation and management of agile projects need to be adapted to the group maturity levels of the agile teams."},{"title":"Bandits with Switching Costs: T^{2\/3} Regret","source":"Ofer Dekel, Jian Ding, Tomer Koren, Yuval Peres","docs_id":"1310.2997","section":["cs.LG","math.PR"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Bandits with Switching Costs: T^{2\/3} Regret. We study the adversarial multi-armed bandit problem in a setting where the player incurs a unit cost each time he switches actions. We prove that the player's $T$-round minimax regret in this setting is $\\widetilde{\\Theta}(T^{2\/3})$, thereby closing a fundamental gap in our understanding of learning with bandit feedback. In the corresponding full-information version of the problem, the minimax regret is known to grow at a much slower rate of $\\Theta(\\sqrt{T})$. The difference between these two rates provides the \\emph{first} indication that learning with bandit feedback can be significantly harder than learning with full-information feedback (previous results only showed a different dependence on the number of actions, but not on $T$.) In addition to characterizing the inherent difficulty of the multi-armed bandit problem with switching costs, our results also resolve several other open problems in online learning. One direct implication is that learning with bandit feedback against bounded-memory adaptive adversaries has a minimax regret of $\\widetilde{\\Theta}(T^{2\/3})$. Another implication is that the minimax regret of online learning in adversarial Markov decision processes (MDPs) is $\\widetilde{\\Theta}(T^{2\/3})$. The key to all of our results is a new randomized construction of a multi-scale random walk, which is of independent interest and likely to prove useful in additional settings."},{"title":"Round-Robin Streaming with Generations","source":"Yao Li and P\\'eter Vingelmann and Morten Videb{\\ae}k Pedersen and\n  Emina Soljanin","docs_id":"1206.3014","section":["cs.IT","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Round-Robin Streaming with Generations. We consider three types of application layer coding for streaming over lossy links: random linear coding, systematic random linear coding, and structured coding. The file being streamed is divided into sub-blocks (generations). Code symbols are formed by combining data belonging to the same generation, and transmitted in a round-robin fashion. We compare the schemes based on delivery packet count, net throughput, and energy consumption for a range of generation sizes. We determine these performance measures both analytically and in an experimental configuration. We find our analytical predictions to match the experimental results. We show that coding at the application layer brings about a significant increase in net data throughput, and thereby reduction in energy consumption due to reduced communication time. On the other hand, on devices with constrained computing resources, heavy coding operations cause packet drops in higher layers and negatively affect the net throughput. We find from our experimental results that low-rate MDS codes are best for small generation sizes, whereas systematic random linear coding has the best net throughput and lowest energy consumption for larger generation sizes due to its low decoding complexity."},{"title":"SAF- BAGE: Salient Approach for Facial Soft-Biometric Classification -\n  Age, Gender, and Facial Expression","source":"Ayesha Gurnani, Kenil Shah, Vandit Gajjar, Viraj Mavani, Yash\n  Khandhediya","docs_id":"1803.05719","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"SAF- BAGE: Salient Approach for Facial Soft-Biometric Classification -\n  Age, Gender, and Facial Expression. How can we improve the facial soft-biometric classification with help of the human visual system? This paper explores the use of saliency which is equivalent to the human visual system to classify Age, Gender and Facial Expression soft-biometric for facial images. Using the Deep Multi-level Network (ML-Net) [1] and off-the-shelf face detector [2], we propose our approach - SAF-BAGE, which first detects the face in the test image, increases the Bounding Box (B-Box) margin by 30%, finds the saliency map using ML-Net, with 30% reweighted ratio of saliency map, it multiplies with the input cropped face and extracts the Convolutional Neural Networks (CNN) predictions on the multiplied reweighted salient face. Our CNN uses the model AlexNet [3], which is pre-trained on ImageNet. The proposed approach surpasses the performance of other approaches, increasing the state-of-the-art by approximately 0.8% on the widely-used Adience [28] dataset for Age and Gender classification and by nearly 3% on the recent AffectNet [36] dataset for Facial Expression classification. We hope our simple, reproducible and effective approach will help ease future research in facial soft-biometric classification using saliency."},{"title":"STEAM: A Hierarchical Co-Simulation Framework for Superconducting\n  Accelerator Magnet Circuits","source":"Lorenzo Bortot and Bernhard Auchmann and Idoia Cortes Garcia and\n  Alejando M. Fernando Navarro and Micha{\\l} Maciejewski and Matthias Mentink\n  and Marco Prioli and Emmanuele Ravaioli and Sebastian Sch\\\"ops and Arjan\n  Verweij","docs_id":"1801.08957","section":["physics.acc-ph","cs.CE"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"STEAM: A Hierarchical Co-Simulation Framework for Superconducting\n  Accelerator Magnet Circuits. Simulating the transient effects occurring in superconducting accelerator magnet circuits requires including the mutual electro-thermo-dynamic interaction among the circuit elements, such as power converters, magnets, and protection systems. Nevertheless, the numerical analysis is traditionally done separately for each element in the circuit, leading to possible non-consistent results. We present STEAM, a hierarchical co-simulation framework featuring the waveform relaxation method. The framework simulates a complex system as a composition of simpler, independent models that exchange information. The convergence of the coupling algorithm ensures the consistency of the solution. The modularity of the framework allows integrating models developed with both proprietary and in-house tools. The framework implements a user-customizable hierarchical algorithm to schedule how models participate to the co-simulation, for the purpose of using computational resources efficiently. As a case study, a quench scenario is co-simulated for the inner triplet circuit for the High Luminosity upgrade of the LHC at CERN."},{"title":"Toward Robust Long Range Policy Transfer","source":"Wei-Cheng Tseng, Jin-Siang Lin, Yao-Min Feng, Min Sun","docs_id":"2103.02957","section":["cs.LG","cs.AI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Toward Robust Long Range Policy Transfer. Humans can master a new task within a few trials by drawing upon skills acquired through prior experience. To mimic this capability, hierarchical models combining primitive policies learned from prior tasks have been proposed. However, these methods fall short comparing to the human's range of transferability. We propose a method, which leverages the hierarchical structure to train the combination function and adapt the set of diverse primitive polices alternatively, to efficiently produce a range of complex behaviors on challenging new tasks. We also design two regularization terms to improve the diversity and utilization rate of the primitives in the pre-training phase. We demonstrate that our method outperforms other recent policy transfer methods by combining and adapting these reusable primitives in tasks with continuous action space. The experiment results further show that our approach provides a broader transferring range. The ablation study also shows the regularization terms are critical for long range policy transfer. Finally, we show that our method consistently outperforms other methods when the quality of the primitives varies."},{"title":"Optimal Filtering of Malicious IP Sources","source":"Fabio Soldo, Athina Markopoulou, Katerina Argyraki","docs_id":"0811.3828","section":["cs.NI"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Optimal Filtering of Malicious IP Sources. How can we protect the network infrastructure from malicious traffic, such as scanning, malicious code propagation, and distributed denial-of-service (DDoS) attacks? One mechanism for blocking malicious traffic is filtering: access control lists (ACLs) can selectively block traffic based on fields of the IP header. Filters (ACLs) are already available in the routers today but are a scarce resource because they are stored in the expensive ternary content addressable memory (TCAM). In this paper, we develop, for the first time, a framework for studying filter selection as a resource allocation problem. Within this framework, we study five practical cases of source address\/prefix filtering, which correspond to different attack scenarios and operator's policies. We show that filter selection optimization leads to novel variations of the multidimensional knapsack problem and we design optimal, yet computationally efficient, algorithms to solve them. We also evaluate our approach using data from Dshield.org and demonstrate that it brings significant benefits in practice. Our set of algorithms is a building block that can be immediately used by operators and manufacturers to block malicious traffic in a cost-efficient way."},{"title":"Saliency Guided Self-attention Network for Weakly and Semi-supervised\n  Semantic Segmentation","source":"Qi Yao, Xiaojin Gong","docs_id":"1910.05475","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Saliency Guided Self-attention Network for Weakly and Semi-supervised\n  Semantic Segmentation. Weakly supervised semantic segmentation (WSSS) using only image-level labels can greatly reduce the annotation cost and therefore has attracted considerable research interest. However, its performance is still inferior to the fully supervised counterparts. To mitigate the performance gap, we propose a saliency guided self-attention network (SGAN) to address the WSSS problem. The introduced self-attention mechanism is able to capture rich and extensive contextual information but may mis-spread attentions to unexpected regions. In order to enable this mechanism to work effectively under weak supervision, we integrate class-agnostic saliency priors into the self-attention mechanism and utilize class-specific attention cues as an additional supervision for SGAN. Our SGAN is able to produce dense and accurate localization cues so that the segmentation performance is boosted. Moreover, by simply replacing the additional supervisions with partially labeled ground-truth, SGAN works effectively for semi-supervised semantic segmentation as well. Experiments on the PASCAL VOC 2012 and COCO datasets show that our approach outperforms all other state-of-the-art methods in both weakly and semi-supervised settings."},{"title":"A twin error gauge for Kaczmarz's iterations","source":"Bart S. van Lith, Per Christian Hansen, Michiel E. Hochstenbach","docs_id":"1906.07470","section":["math.NA","cs.NA"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A twin error gauge for Kaczmarz's iterations. We propose two new algebraic reconstruction techniques based on Kaczmarz's method that produce a regularized solution to noisy tomography problems. Tomography problems exhibit semi-convergence when iterative methods are employed, and the aim is therefore to stop near the semi-convergence point. Our approach is based on an error gauge that is constructed by pairing standard down-sweep Kaczmarz's method with its up-sweep version; we stop the iterations when this error gauge is minimal. The reconstructions of the new methods differ from standard Kaczmarz iterates in that our final result is the average of the stopped up- and down-sweeps. Even when Kaczmarz's method is supplied with an oracle that provides the exact error -- and is thereby able to stop at the best possible iterate -- our methods have a lower two-norm error in the vast majority of our test cases. In terms of computational cost, our methods are a little cheaper than standard Kaczmarz equipped with a statistical stopping rule."},{"title":"METEOR: A Massive Dense & Heterogeneous Behavior Dataset for Autonomous\n  Driving","source":"Rohan Chandra, Mridul Mahajan, Rahul Kala, Rishitha Palugulla,\n  Chandrababu Naidu, Alok Jain, and Dinesh Manocha","docs_id":"2109.07648","section":["cs.CV","cs.AI","cs.RO"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"METEOR: A Massive Dense & Heterogeneous Behavior Dataset for Autonomous\n  Driving. We present a new and complex traffic dataset, METEOR, which captures traffic patterns in unstructured scenarios in India. METEOR consists of more than 1000 one-minute video clips, over 2 million annotated frames with ego-vehicle trajectories, and more than 13 million bounding boxes for surrounding vehicles or traffic agents. METEOR is a unique dataset in terms of capturing the heterogeneity of microscopic and macroscopic traffic characteristics. Furthermore, we provide annotations for rare and interesting driving behaviors such as cut-ins, yielding, overtaking, overspeeding, zigzagging, sudden lane changing, running traffic signals, driving in the wrong lanes, taking wrong turns, lack of right-of-way rules at intersections, etc. We also present diverse traffic scenarios corresponding to rainy weather, nighttime driving, driving in rural areas with unmarked roads, and high-density traffic scenarios. We use our novel dataset to evaluate the performance of object detection and behavior prediction algorithms. We show that state-of-the-art object detectors fail in these challenging conditions and also propose a new benchmark test: action-behavior prediction with a baseline mAP score of 70.74."},{"title":"Apache VXQuery: A Scalable XQuery Implementation","source":"E. Preston Carman Jr. (1), Till Westmann (2), Vinayak R. Borkar (3),\n  Michael J. Carey (3) and Vassilis J. Tsotras (1) ((1) UC Riverside, (2)\n  Oracle Labs, (3) UC Irvine)","docs_id":"1504.00331","section":["cs.DB"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Apache VXQuery: A Scalable XQuery Implementation. The wide use of XML for document management and data exchange has created the need to query large repositories of XML data. To efficiently query such large data collections and take advantage of parallelism, we have implemented Apache VXQuery, an open-source scalable XQuery processor. The system builds upon two other open-source frameworks -- Hyracks, a parallel execution engine, and Algebricks, a language agnostic compiler toolbox. Apache VXQuery extends these two frameworks and provides an implementation of the XQuery specifics (data model, data-model dependent functions and optimizations, and a parser). We describe the architecture of Apache VXQuery, its integration with Hyracks and Algebricks, and the XQuery optimization rules applied to the query plan to improve path expression efficiency and to enable query parallelism. An experimental evaluation using a real 500GB dataset with various selection, aggregation and join XML queries shows that Apache VXQuery performs well both in terms of scale-up and speed-up. Our experiments show that it is about 3x faster than Saxon (an open-source and commercial XQuery processor) on a 4-core, single node implementation, and around 2.5x faster than Apache MRQL (a MapReduce-based parallel query processor) on an eight (4-core) node cluster."},{"title":"Determining Optimal Rates for Communication for Omniscience","source":"Ni Ding, Chung Chan, Qiaoqiao Zhou, Rodney A. Kennedy and Parastoo\n  Sadeghi","docs_id":"1611.08367","section":["cs.IT","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Determining Optimal Rates for Communication for Omniscience. This paper considers the communication for omniscience (CO) problem: A set of users observe a discrete memoryless multiple source and want to recover the entire multiple source via noise-free broadcast communications. We study the problem of how to determine an optimal rate vector that attains omniscience with the minimum sum-rate, the total number of communications. The results cover both asymptotic and non-asymptotic models where the transmission rates are real and integral, respectively. We propose a modified decomposition algorithm (MDA) and a sum-rate increment algorithm (SIA) for the asymptotic and non-asymptotic models, respectively, both of which determine the value of the minimum sum-rate and a corresponding optimal rate vector in polynomial time. For the coordinate saturation capacity (CoordSatCap) algorithm, a nesting algorithm in MDA and SIA, we propose to implement it by a fusion method and show by experimental results that this fusion method contributes to a reduction in computation complexity. Finally, we show that the separable convex minimization problem over the optimal rate vector set in the asymptotic model can be decomposed by the fundamental partition, the optimal partition of the user set that determines the minimum sum-rate, so that the problem can be solved more efficiently."},{"title":"A breakthrough in Speech emotion recognition using Deep Retinal\n  Convolution Neural Networks","source":"Yafeng Niu, Dongsheng Zou, Yadong Niu, Zhongshi He, Hua Tan","docs_id":"1707.09917","section":["cs.SD","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A breakthrough in Speech emotion recognition using Deep Retinal\n  Convolution Neural Networks. Speech emotion recognition (SER) is to study the formation and change of speaker's emotional state from the speech signal perspective, so as to make the interaction between human and computer more intelligent. SER is a challenging task that has encountered the problem of less training data and low prediction accuracy. Here we propose a data augmentation algorithm based on the imaging principle of the retina and convex lens, to acquire the different sizes of spectrogram and increase the amount of training data by changing the distance between the spectrogram and the convex lens. Meanwhile, with the help of deep learning to get the high-level features, we propose the Deep Retinal Convolution Neural Networks (DRCNNs) for SER and achieve the average accuracy over 99%. The experimental results indicate that DRCNNs outperforms the previous studies in terms of both the number of emotions and the accuracy of recognition. Predictably, our results will dramatically improve human-computer interaction."},{"title":"Dynamic Data Structures for Document Collections and Graphs","source":"J. Ian Munro and Yakov Nekrich and Jeffrey Scott Vitter","docs_id":"1503.05977","section":["cs.DS"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Dynamic Data Structures for Document Collections and Graphs. In the dynamic indexing problem, we must maintain a changing collection of text documents so that we can efficiently support insertions, deletions, and pattern matching queries. We are especially interested in developing efficient data structures that store and query the documents in compressed form. All previous compressed solutions to this problem rely on answering rank and select queries on a dynamic sequence of symbols. Because of the lower bound in [Fredman and Saks, 1989], answering rank queries presents a bottleneck in compressed dynamic indexing. In this paper we show how this lower bound can be circumvented using our new framework. We demonstrate that the gap between static and dynamic variants of the indexing problem can be almost closed. Our method is based on a novel framework for adding dynamism to static compressed data structures. Our framework also applies more generally to dynamizing other problems. We show, for example, how our framework can be applied to develop compressed representations of dynamic graphs and binary relations."},{"title":"Deterministic Pilot Design and Channel Estimation for Downlink Massive\n  MIMO-OTFS Systems in Presence of the Fractional Doppler","source":"Ding Shi, Wenjin Wang, Li You, Xiaohang Song, Yi Hong, Xiqi Gao,\n  Gerhard Fettweis","docs_id":"2105.09628","section":["cs.IT","math.IT"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Deterministic Pilot Design and Channel Estimation for Downlink Massive\n  MIMO-OTFS Systems in Presence of the Fractional Doppler. Although the combination of the orthogonal time frequency space (OTFS) modulation and the massive multiple-input multiple-output (MIMO) technology can make communication systems perform better in high-mobility scenarios, there are still many challenges in downlink channel estimation owing to inaccurate modeling and high pilot overhead in practical systems. In this paper, we propose a channel state information (CSI) acquisition scheme for downlink massive MIMO-OTFS in presence of the fractional Doppler, including deterministic pilot design and channel estimation algorithm. First, we analyze the input-output relationship of the single-input single-output (SISO) OTFS based on the orthogonal frequency division multiplexing (OFDM) modem and extend it to massive MIMO-OTFS. Moreover, we formulate an accurate model for the practical system in which the fractional Doppler is considered and the influence of subpaths is revealed. A deterministic pilot design is then proposed based on the model and the structure of the pilot matrix to reduce pilot overhead and save memory consumption. Since channel geometry changes very slowly relative to the communication timescale, we put forward a modified sensing matrix based channel estimation (MSMCE) algorithm to acquire the downlink CSI. Simulation results demonstrate that the proposed downlink CSI acquisition scheme has significant advantages over traditional algorithms."},{"title":"Dialogue Enhancement in Object-based Audio -- Evaluating the Benefit on\n  People above 65","source":"Davide Straninger","docs_id":"2006.14282","section":["eess.AS","cs.SD"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Dialogue Enhancement in Object-based Audio -- Evaluating the Benefit on\n  People above 65. Due to age-related hearing loss, elderly people often struggle with following the language on TV. Because they form an increasing part of the audience, this problem will become even more important in the future and needs to be addressed by research and development. Object-based audio is a promising approach to solve this issue as it offers the possibility of customizable dialogue enhancement (DE). For this thesis an Adjustment \/ Satisfaction Test (A\/ST) was conducted to evaluate the preferred loudness difference (LD) between speech and background in people above 65. Two different types of DE were tested: DE with separately available audio components (speech and background) and DE with components created by blind source separation (BSS). The preferred LDs compared to the original, differences of the preferred LDs between the two DE methods and the listener satisfaction were tested. It was observed that the preferred LDs were larger than the original LDs, that customizable DE increases listener satisfaction and that the two DE methods performed comparably well in terms of preferred LD and listener satisfaction. Based on the results, it can be assumed that elderly viewers above 65 will benefit equally from user-adjustable DE by available components and by dialogue separation."},{"title":"A Fast General Methodology for Information-Theoretically Optimal\n  Encodings of Graphs","source":"Xin He, Ming-Yang Kao, Hsueh-I Lu","docs_id":"cs\/0101021","section":["cs.DS","cs.GR"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"A Fast General Methodology for Information-Theoretically Optimal\n  Encodings of Graphs. We propose a fast methodology for encoding graphs with information-theoretically minimum numbers of bits. Specifically, a graph with property pi is called a pi-graph. If pi satisfies certain properties, then an n-node m-edge pi-graph G can be encoded by a binary string X such that (1) G and X can be obtained from each other in O(n log n) time, and (2) X has at most beta(n)+o(beta(n)) bits for any continuous super-additive function beta(n) so that there are at most 2^{beta(n)+o(beta(n))} distinct n-node pi-graphs. The methodology is applicable to general classes of graphs; this paper focuses on planar graphs. Examples of such pi include all conjunctions over the following groups of properties: (1) G is a planar graph or a plane graph; (2) G is directed or undirected; (3) G is triangulated, triconnected, biconnected, merely connected, or not required to be connected; (4) the nodes of G are labeled with labels from {1, ..., ell_1} for ell_1 <= n; (5) the edges of G are labeled with labels from {1, ..., ell_2} for ell_2 <= m; and (6) each node (respectively, edge) of G has at most ell_3 = O(1) self-loops (respectively, ell_4 = O(1) multiple edges). Moreover, ell_3 and ell_4 are not required to be O(1) for the cases of pi being a plane triangulation. These examples are novel applications of small cycle separators of planar graphs and are the only nontrivial classes of graphs, other than rooted trees, with known polynomial-time information-theoretically optimal coding schemes."},{"title":"Finite-Sample Concentration of the Multinomial in Relative Entropy","source":"Rohit Agrawal","docs_id":"1904.02291","section":["cs.IT","math.IT","math.PR","math.ST","stat.TH"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Finite-Sample Concentration of the Multinomial in Relative Entropy. We show that the moment generating function of the Kullback-Leibler divergence (relative entropy) between the empirical distribution of $n$ independent samples from a distribution $P$ over a finite alphabet of size $k$ (i.e. a multinomial distribution) and $P$ itself is no more than that of a gamma distribution with shape $k - 1$ and rate $n$. The resulting exponential concentration inequality becomes meaningful (less than 1) when the divergence $\\varepsilon$ is larger than $(k-1)\/n$, whereas the standard method of types bound requires $\\varepsilon > \\frac{1}{n} \\cdot \\log{\\binom{n+k-1}{k-1}} \\geq (k-1)\/n \\cdot \\log(1 + n\/(k-1))$, thus saving a factor of order $\\log(n\/k)$ in the standard regime of parameters where $n\\gg k$. As a consequence, we also obtain finite-sample bounds on all the moments of the empirical divergence (equivalently, the discrete likelihood-ratio statistic), which are within constant factors (depending on the moment) of their asymptotic values. Our proof proceeds via a simple reduction to the case $k = 2$ of a binary alphabet (i.e. a binomial distribution), and has the property that improvements in the case of $k = 2$ directly translate to improvements for general $k$. In particular, we conjecture a bound on the binomial moment generating function that would almost close the quadratic gap between our finite-sample bound and the asymptotic moment generating function bound from Wilks' theorem (which does not hold for finite samples)."},{"title":"VeriSmart: A Highly Precise Safety Verifier for Ethereum Smart Contracts","source":"Sunbeom So, Myungho Lee, Jisu Park, Heejo Lee, Hakjoo Oh","docs_id":"1908.11227","section":["cs.PL","cs.CR"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"VeriSmart: A Highly Precise Safety Verifier for Ethereum Smart Contracts. We present VeriSmart, a highly precise verifier for ensuring arithmetic safety of Ethereum smart contracts. Writing safe smart contracts without unintended behavior is critically important because smart contracts are immutable and even a single flaw can cause huge financial damage. In particular, ensuring that arithmetic operations are safe is one of the most important and common security concerns of Ethereum smart contracts nowadays. In response, several safety analyzers have been proposed over the past few years, but state-of-the-art is still unsatisfactory; no existing tools achieve high precision and recall at the same time, inherently limited to producing annoying false alarms or missing critical bugs. By contrast, VeriSmart aims for an uncompromising analyzer that performs exhaustive verification without compromising precision or scalability, thereby greatly reducing the burden of manually checking undiscovered or incorrectly-reported issues. To achieve this goal, we present a new domain-specific algorithm for verifying smart contracts, which is able to automatically discover and leverage transaction invariants that are essential for precisely analyzing smart contracts. Evaluation with real-world smart contracts shows that VeriSmart can detect all arithmetic bugs with a negligible number of false alarms, far outperforming existing analyzers."},{"title":"ClarQ: A large-scale and diverse dataset for Clarification Question\n  Generation","source":"Vaibhav Kumar and Alan W. black","docs_id":"2006.05986","section":["cs.CL","cs.AI","cs.LG"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"ClarQ: A large-scale and diverse dataset for Clarification Question\n  Generation. Question answering and conversational systems are often baffled and need help clarifying certain ambiguities. However, limitations of existing datasets hinder the development of large-scale models capable of generating and utilising clarification questions. In order to overcome these limitations, we devise a novel bootstrapping framework (based on self-supervision) that assists in the creation of a diverse, large-scale dataset of clarification questions based on post-comment tuples extracted from stackexchange. The framework utilises a neural network based architecture for classifying clarification questions. It is a two-step method where the first aims to increase the precision of the classifier and second aims to increase its recall. We quantitatively demonstrate the utility of the newly created dataset by applying it to the downstream task of question-answering. The final dataset, ClarQ, consists of ~2M examples distributed across 173 domains of stackexchange. We release this dataset in order to foster research into the field of clarification question generation with the larger goal of enhancing dialog and question answering systems."},{"title":"Wavelet Based QRS Complex Detection of ECG Signal","source":"Sayantan Mukhopadhyay, Shouvik Biswas, Anamitra Bardhan Roy, Nilanjan\n  Dey","docs_id":"1209.1563","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Wavelet Based QRS Complex Detection of ECG Signal. The Electrocardiogram (ECG) is a sensitive diagnostic tool that is used to detect various cardiovascular diseases by measuring and recording the electrical activity of the heart in exquisite detail. A wide range of heart condition is determined by thorough examination of the features of the ECG report. Automatic extraction of time plane features is important for identification of vital cardiac diseases. This paper presents a multi-resolution wavelet transform based system for detection 'P', 'Q', 'R', 'S', 'T' peaks complex from original ECG signal. 'R-R' time lapse is an important minutia of the ECG signal that corresponds to the heartbeat of the concerned person. Abrupt increase in height of the 'R' wave or changes in the measurement of the 'R-R' denote various anomalies of human heart. Similarly 'P-P', 'Q-Q', 'S-S', 'T-T' also corresponds to different anomalies of heart and their peak amplitude also envisages other cardiac diseases. In this proposed method the 'PQRST' peaks are marked and stored over the entire signal and the time interval between two consecutive 'R' peaks and other peaks interval are measured to detect anomalies in behavior of heart, if any. The peaks are achieved by the composition of Daubeheissub bands wavelet of original ECG signal. The accuracy of the 'PQRST' complex detection and interval measurement is achieved up to 100% with high exactitude by processing and thresholding the original ECG signal."},{"title":"Inconsistency-aware Uncertainty Estimation for Semi-supervised Medical\n  Image Segmentation","source":"Yinghuan Shi, Jian Zhang, Tong Ling, Jiwen Lu, Yefeng Zheng, Qian Yu,\n  Lei Qi, Yang Gao","docs_id":"2110.08762","section":["cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Inconsistency-aware Uncertainty Estimation for Semi-supervised Medical\n  Image Segmentation. In semi-supervised medical image segmentation, most previous works draw on the common assumption that higher entropy means higher uncertainty. In this paper, we investigate a novel method of estimating uncertainty. We observe that, when assigned different misclassification costs in a certain degree, if the segmentation result of a pixel becomes inconsistent, this pixel shows a relative uncertainty in its segmentation. Therefore, we present a new semi-supervised segmentation model, namely, conservative-radical network (CoraNet in short) based on our uncertainty estimation and separate self-training strategy. In particular, our CoraNet model consists of three major components: a conservative-radical module (CRM), a certain region segmentation network (C-SN), and an uncertain region segmentation network (UC-SN) that could be alternatively trained in an end-to-end manner. We have extensively evaluated our method on various segmentation tasks with publicly available benchmark datasets, including CT pancreas, MR endocardium, and MR multi-structures segmentation on the ACDC dataset. Compared with the current state of the art, our CoraNet has demonstrated superior performance. In addition, we have also analyzed its connection with and difference from conventional methods of uncertainty estimation in semi-supervised medical image segmentation."},{"title":"Generation of 3D Brain MRI Using Auto-Encoding Generative Adversarial\n  Networks","source":"Gihyun Kwon, Chihye Han, Dae-shik Kim","docs_id":"1908.02498","section":["eess.IV","cs.CV"],"start_character":"N\/A","end_character":"N\/A","date":"N\/A","text":"Generation of 3D Brain MRI Using Auto-Encoding Generative Adversarial\n  Networks. As deep learning is showing unprecedented success in medical image analysis tasks, the lack of sufficient medical data is emerging as a critical problem. While recent attempts to solve the limited data problem using Generative Adversarial Networks (GAN) have been successful in generating realistic images with diversity, most of them are based on image-to-image translation and thus require extensive datasets from different domains. Here, we propose a novel model that can successfully generate 3D brain MRI data from random vectors by learning the data distribution. Our 3D GAN model solves both image blurriness and mode collapse problems by leveraging alpha-GAN that combines the advantages of Variational Auto-Encoder (VAE) and GAN with an additional code discriminator network. We also use the Wasserstein GAN with Gradient Penalty (WGAN-GP) loss to lower the training instability. To demonstrate the effectiveness of our model, we generate new images of normal brain MRI and show that our model outperforms baseline models in both quantitative and qualitative measurements. We also train the model to synthesize brain disorder MRI data to demonstrate the wide applicability of our model. Our results suggest that the proposed model can successfully generate various types and modalities of 3D whole brain volumes from a small set of training data."}]
