{"0": {"documentation": {"title": "Additive Tree-Structured Conditional Parameter Spaces in Bayesian\n  Optimization: A Novel Covariance Function and a Fast Implementation", "source": "Xingchen Ma, Matthew B. Blaschko", "docs_id": "2010.03171", "section": ["stat.ML", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Additive Tree-Structured Conditional Parameter Spaces in Bayesian\n  Optimization: A Novel Covariance Function and a Fast Implementation. Bayesian optimization (BO) is a sample-efficient global optimization algorithm for black-box functions which are expensive to evaluate. Existing literature on model based optimization in conditional parameter spaces are usually built on trees. In this work, we generalize the additive assumption to tree-structured functions and propose an additive tree-structured covariance function, showing improved sample-efficiency, wider applicability and greater flexibility. Furthermore, by incorporating the structure information of parameter spaces and the additive assumption in the BO loop, we develop a parallel algorithm to optimize the acquisition function and this optimization can be performed in a low dimensional space. We demonstrate our method on an optimization benchmark function, on a neural network compression problem, on pruning pre-trained VGG16 and ResNet50 models as well as on searching activation functions of ResNet20. Experimental results show our approach significantly outperforms the current state of the art for conditional parameter optimization including SMAC, TPE and Jenatton et al. (2017)."}, "answer": " Question: Which of the following statements is true about the proposed additive tree-structured covariance function in the paper?\n\nA) It shows worse sample-efficiency compared to existing methods\n\nB) It cannot be applied to neural network compression problems\n\nC) It incorporates the additive assumption and tree structure of the parameter space\n\nD) It does not allow for parallel optimization of the acquisition function\n\nCorrect Answer: C\n\nExplanation: The paper proposes an additive tree-structured covariance function that incorporates the additive assumption and tree structure of the parameter space. This results in improved sample-efficiency, wider applicability, and greater flexibility compared to existing methods. It can be applied to problems like neural network compression. The additive structure also enables parallel optimization of the acquisition function in a low dimensional space."}, "1": {"documentation": {"title": "Detecting Algebraic Manipulation in Leaky Storage Systems", "source": "Fuchun Lin, Reihaneh Safavi-Naini, Pengwei Wang", "docs_id": "1607.00089", "section": ["cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Detecting Algebraic Manipulation in Leaky Storage Systems. Algebraic Manipulation Detection (AMD) Codes detect adversarial noise that is added to a coded message and stored in a storage that is opaque to the adversary. We study AMD codes when the storage can leak up to \\rho\\log|G| bits of information about the stored codeword, where G is the group in which the stored codeword lives and \\rho is a constant. We propose \\rho-AMD codes that provide protection in this new setting, and define weak and strong \\rho-AMD codes that provide security for a random and an arbitrary message, respectively. We derive concrete and asymptotic bounds for the efficiency of these codes featuring a rate upper bound of 1-\\rho for the strong codes. We also define the class of \\rho^{LV}-AMD codes that provide protection when leakage is in the form of a number of codeword components, and give constructions featuring a strong \\rho^{LV}-AMD codes that asymptotically achieve the rate 1-\\rho. We describe applications of \\rho-AMD codes to, (i) robust ramp secret sharing scheme and, (ii) wiretap II channel when the adversary can eavesdrop a \\rho fraction of codeword components and tamper with all components of the codeword."}, "answer": " Here is a multi-form question on the given documentation:\n\nQuestion: Which of the following statements is true about Algebraic Manipulation Detection (AMD) Codes according to the documentation?\n\nA) AMD Codes can detect adversarial noise added to a coded message stored in a system that leaks information to the adversary. \n\nB) AMD Codes provide protection when the storage system leaks more than \u03c1log|G| bits of information about the stored codeword.\n\nC) Strong \u03c1-AMD codes provide security for a random message but not for an arbitrary message. \n\nD) \u03c1LV-AMD codes allow leakage of a number of codeword components and can asymptotically achieve rate 1-\u03c1.\n\nCorrect Answer: D\n\nExplanation: According to the documentation, \u03c1LV-AMD codes allow leakage of a number of codeword components and can asymptotically achieve rate 1-\u03c1 for strong codes. Option D is correct. Options A, B and C describe other aspects of AMD codes not directly related to \u03c1LV-AMD codes."}, "2": {"documentation": {"title": "Restoration of Non-rigidly Distorted Underwater Images using a\n  Combination of Compressive Sensing and Local Polynomial Image Representations", "source": "Jerin Geo James, Pranay Agrawal, Ajit Rajwade", "docs_id": "1908.01940", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Restoration of Non-rigidly Distorted Underwater Images using a\n  Combination of Compressive Sensing and Local Polynomial Image Representations. Images of static scenes submerged beneath a wavy water surface exhibit severe non-rigid distortions. The physics of water flow suggests that water surfaces possess spatio-temporal smoothness and temporal periodicity. Hence they possess a sparse representation in the 3D discrete Fourier (DFT) basis. Motivated by this, we pose the task of restoration of such video sequences as a compressed sensing (CS) problem. We begin by tracking a few salient feature points across the frames of a video sequence of the submerged scene. Using these point trajectories, we show that the motion fields at all other (non-tracked) points can be effectively estimated using a typical CS solver. This by itself is a novel contribution in the field of non-rigid motion estimation. We show that this method outperforms state of the art algorithms for underwater image restoration. We further consider a simple optical flow algorithm based on local polynomial expansion of the image frames (PEOF). Surprisingly, we demonstrate that PEOF is more efficient and often outperforms all the state of the art methods in terms of numerical measures. Finally, we demonstrate that a two-stage approach consisting of the CS step followed by PEOF much more accurately preserves the image structure and improves the (visual as well as numerical) video quality as compared to just the PEOF stage."}, "answer": " Here is a multi-form question on the document:\n\nQuestion: Which of the following statements is true about the proposed approach for restoring non-rigidly distorted underwater images in the paper?\n\nA) It uses only compressive sensing on the 3D DFT coefficients of the video sequence. \n\nB) It first estimates motion fields using compressive sensing, then refines them using local polynomial expansion of optical flow.\n\nC) It relies solely on local polynomial expansion of optical flow without using compressive sensing.\n\nD) It first uses local polynomial expansion of optical flow, then refine the results with compressive sensing.\n\nCorrect Answer: B\n\nExplanation: The paper proposes a two-stage approach - first estimating motion fields using compressive sensing on the 3D DFT coefficients, then refining the results using local polynomial expansion of optical flow (PEOF). So B is the correct statement. A is incorrect as it does not use PEOF. C is incorrect as it does not use compressive sensing. D inverts the order of the two steps."}, "3": {"documentation": {"title": "Grant-free Radio Access IoT Networks: Scalability Analysis in\n  Coexistence Scenarios", "source": "Meysam Masoudi, Amin Azari, Emre Altug Yavuz, and Cicek Cavdar", "docs_id": "1711.00581", "section": ["cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Grant-free Radio Access IoT Networks: Scalability Analysis in\n  Coexistence Scenarios. IoT networks with grant-free radio access, like SigFox and LoRa, offer low-cost durable communications over unlicensed band. These networks are becoming more and more popular due to the ever-increasing need for ultra durable, in terms of battery lifetime, IoT networks. Most studies evaluate the system performance assuming single radio access technology deployment. In this paper, we study the impact of coexisting competing radio access technologies on the system performance. Considering $\\mathpzc K$ technologies, defined by time and frequency activity factors, bandwidth, and power, which share a set of radio resources, we derive closed-form expressions for the successful transmission probability, expected battery lifetime, and experienced delay as a function of distance to the serving access point. Our analytical model, which is validated by simulation results, provides a tool to evaluate the coexistence scenarios and analyze how introduction of a new coexisting technology may degrade the system performance in terms of success probability and battery lifetime. We further investigate solutions in which this destructive effect could be compensated, e.g., by densifying the network to a certain extent and utilizing joint reception."}, "answer": " Question: Consider a scenario with K grant-free radio access IoT networks coexisting and sharing radio resources. What is one way to compensate for the destructive effect that introducing a new coexisting technology can have on success probability and battery lifetime?\n\nA) Increasing the bandwidth used by the new technology\nB) Reducing the transmit power of the new technology \nC) Densifying the network of the new technology to a certain extent\nD) Utilizing joint reception by the new technology\n\nCorrect Answer: C) Densifying the network of the new technology to a certain extent\n\nExplanation: The paper states that densifying the network to a certain extent is one potential solution to compensate for the degraded system performance caused by introducing a new coexisting technology. Increasing bandwidth or reducing transmit power of the new technology does not directly address the issue. While joint reception could help, densification is specifically mentioned in the paper as a way to compensate."}, "4": {"documentation": {"title": "Value of peripheral nodes in controlling multilayer networks", "source": "Yan Zhang, Antonios Garas, Frank Schweitzer", "docs_id": "1506.02963", "section": ["physics.soc-ph", "cs.SI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Value of peripheral nodes in controlling multilayer networks. We analyze the controllability of a two-layer network, where driver nodes can be chosen randomly only from one layer. Each layer contains a scale-free network with directed links and the node dynamics depends on the incoming links from other nodes. We combine the in-degree and out-degree values to assign an importance value $w$ to each node, and distinguish between peripheral nodes with low $w$ and central nodes with high $w$. Based on numerical simulations, we find that the controllable part of the network is larger when choosing low $w$ nodes to connect the two layers. The control is as efficient when peripheral nodes are driver nodes as it is for the case of more central nodes. However, if we assume a cost to utilize nodes that is proportional to their overall degree, utilizing peripheral nodes to connect the two layers or to act as driver nodes is not only the most cost-efficient solution, it is also the one that performs best in controlling the two-layer network among the different interconnecting strategies we have tested."}, "answer": " Question: Based on the research described, which of the following statements is true about controlling a two-layer network where driver nodes can only be chosen from one layer?\n\nA) Choosing low importance nodes to connect the two layers results in less controllable parts of the network compared to choosing high importance nodes.\n\nB) The control is less efficient when peripheral nodes are used as driver nodes compared to more central nodes. \n\nC) If node utilization cost is proportional to degree, using peripheral nodes to connect layers or as drivers is the most cost-efficient but does not maximize controllability.\n\nD) Choosing peripheral nodes to connect layers or act as drivers provides efficient, low-cost control that performs the best among strategies tested.\n\nCorrect Answer: D"}, "5": {"documentation": {"title": "Enabling Incremental Training with Forward Pass for Edge Devices", "source": "Dana AbdulQader, Shoba Krishnan, Claudionor N. Coelho Jr", "docs_id": "2103.14007", "section": ["cs.LG", "cs.AR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Enabling Incremental Training with Forward Pass for Edge Devices. Deep Neural Networks (DNNs) are commonly deployed on end devices that exist in constantly changing environments. In order for the system to maintain it's accuracy, it is critical that it is able to adapt to changes and recover by retraining parts of the network. However, end devices have limited resources making it challenging to train on the same device. Moreover, training deep neural networks is both memory and compute intensive due to the backpropagation algorithm. In this paper we introduce a method using evolutionary strategy (ES) that can partially retrain the network enabling it to adapt to changes and recover after an error has occurred. This technique enables training on an inference-only hardware without the need to use backpropagation and with minimal resource overhead. We demonstrate the ability of our technique to retrain a quantized MNIST neural network after injecting noise to the input. Furthermore, we present the micro-architecture required to enable training on HLS4ML (an inference hardware architecture) and implement it in Verilog. We synthesize our implementation for a Xilinx Kintex Ultrascale Field Programmable Gate Array (FPGA) resulting in less than 1% resource utilization required to implement the incremental training."}, "answer": " Here is a 4-option multiple choice question on the paper:\n\nQuestion: Which of the following statements is true about the incremental training method proposed in the paper?\n\nA) It uses evolutionary strategy to retrain parts of the network without needing backpropagation.\n\nB) It requires using the full training set to adapt the network weights after changes occur. \n\nC) It can only be applied to neural networks deployed on servers with abundant compute resources.\n\nD) It relies on storing all previous network parameters to recover from errors.\n\nCorrect Answer: A\n\nExplanation: \nOption A is correct. The paper proposes using evolutionary strategy rather than backpropagation to enable incremental retraining on resource-constrained edge devices after changes occur, without needing the full training set.\n\nOption B is incorrect. The method does not require the full training set for incremental adaptation. \n\nOption C is incorrect. It is designed for edge devices with limited resources, not servers. \n\nOption D is incorrect. It does not store previous parameters, only retrains parts of the current network."}, "6": {"documentation": {"title": "Eignets for function approximation on manifolds", "source": "H. N. Mhaskar", "docs_id": "0909.5000", "section": ["cs.LG", "cs.NA", "cs.NE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Eignets for function approximation on manifolds. Let $\\XX$ be a compact, smooth, connected, Riemannian manifold without boundary, $G:\\XX\\times\\XX\\to \\RR$ be a kernel. Analogous to a radial basis function network, an eignet is an expression of the form $\\sum_{j=1}^M a_jG(\\circ,y_j)$, where $a_j\\in\\RR$, $y_j\\in\\XX$, $1\\le j\\le M$. We describe a deterministic, universal algorithm for constructing an eignet for approximating functions in $L^p(\\mu;\\XX)$ for a general class of measures $\\mu$ and kernels $G$. Our algorithm yields linear operators. Using the minimal separation amongst the centers $y_j$ as the cost of approximation, we give modulus of smoothness estimates for the degree of approximation by our eignets, and show by means of a converse theorem that these are the best possible for every \\emph{individual function}. We also give estimates on the coefficients $a_j$ in terms of the norm of the eignet. Finally, we demonstrate that if any sequence of eignets satisfies the optimal estimates for the degree of approximation of a smooth function, measured in terms of the minimal separation, then the derivatives of the eignets also approximate the corresponding derivatives of the target function in an optimal manner."}, "answer": " Here is a multi-form question on the eignets material:\n\nQuestion: Let $\\XX$ be a compact Riemannian manifold and $G:\\XX\\times\\XX\\to\\RR$ a kernel. Which of the following statements about eignets of the form $\\sum_{j=1}^M a_j G(x,y_j)$ approximating functions in $L^p(\\mu;\\XX)$ is true?\n\nA) The approximation algorithm yields nonlinear operators. \n\nB) The minimal separation between centers $y_j$ gives modulus of continuity estimates.\n\nC) The estimates on coefficients $a_j$ depend on the norm of the target function. \n\nD) Derivatives of eignets approximate derivatives of the target function suboptimally.\n\nCorrect Answer: B\n\nExplanation: \nA) Is incorrect - the approximation algorithm yields linear operators. \nB) Is correct - minimal separation between centers gives modulus of smoothness estimates.\nC) Is incorrect - estimates on $a_j$ depend on the norm of the eignet, not the target function.\nD) Is incorrect - derivatives of eignets optimally approximate derivatives of the target function."}, "7": {"documentation": {"title": "The Dynamics of Group Codes: Dual Abelian Group Codes and Systems", "source": "G. David Forney Jr. and Mitchell D. Trott", "docs_id": "cs/0408038", "section": ["cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Dynamics of Group Codes: Dual Abelian Group Codes and Systems. Fundamental results concerning the dynamics of abelian group codes (behaviors) and their duals are developed. Duals of sequence spaces over locally compact abelian groups may be defined via Pontryagin duality; dual group codes are orthogonal subgroups of dual sequence spaces. The dual of a complete code or system is finite, and the dual of a Laurent code or system is (anti-)Laurent. If C and C^\\perp are dual codes, then the state spaces of C act as the character groups of the state spaces of C^\\perp. The controllability properties of C are the observability properties of C^\\perp. In particular, C is (strongly) controllable if and only if C^\\perp is (strongly) observable, and the controller memory of C is the observer memory of C^\\perp. The controller granules of C act as the character groups of the observer granules of C^\\perp. Examples of minimal observer-form encoder and syndrome-former constructions are given. Finally, every observer granule of C is an \"end-around\" controller granule of C."}, "answer": " Here is a 4-option multiple choice question based on the given text:\n\nQuestion: According to the text, which of the following statements about the relationship between a group code C and its dual C^\\perp is true?\n\nA) The controller granules of C act as the character groups of the observer granules of C^\\perp.\n\nB) C is observable if and only if C^\\perp is controllable. \n\nC) The state spaces of C are the character groups of the state spaces of C^\\perp.\n\nD) C and C^\\perp have the same controller and observer memories.\n\nCorrect Answer: C\n\nExplanation: The text states that \"If C and C^\\perp are dual codes, then the state spaces of C act as the character groups of the state spaces of C^\\perp.\" Therefore, option C is correct. Options A, B, and D are incorrect based on the information provided in the text."}, "8": {"documentation": {"title": "A Skeleton-Driven Neural Occupancy Representation for Articulated Hands", "source": "Korrawe Karunratanakul, Adrian Spurr, Zicong Fan, Otmar Hilliges, Siyu\n  Tang", "docs_id": "2109.11399", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Skeleton-Driven Neural Occupancy Representation for Articulated Hands. We present Hand ArticuLated Occupancy (HALO), a novel representation of articulated hands that bridges the advantages of 3D keypoints and neural implicit surfaces and can be used in end-to-end trainable architectures. Unlike existing statistical parametric hand models (e.g.~MANO), HALO directly leverages 3D joint skeleton as input and produces a neural occupancy volume representing the posed hand surface. The key benefits of HALO are (1) it is driven by 3D key points, which have benefits in terms of accuracy and are easier to learn for neural networks than the latent hand-model parameters; (2) it provides a differentiable volumetric occupancy representation of the posed hand; (3) it can be trained end-to-end, allowing the formulation of losses on the hand surface that benefit the learning of 3D keypoints. We demonstrate the applicability of HALO to the task of conditional generation of hands that grasp 3D objects. The differentiable nature of HALO is shown to improve the quality of the synthesized hands both in terms of physical plausibility and user preference."}, "answer": " Question: What are the key benefits of the HALO hand representation presented in the paper?\n\nA) It is driven by 3D keypoints, provides a volumetric occupancy representation, and can be trained end-to-end.\n\nB) It uses a statistical model to represent hand articulation and surface geometry. \n\nC) It represents hands with a mesh surface model optimized for real-time rendering.\n\nD) It encodes hand pose and shape in a low-dimensional latent space learned by a VAE.\n\nCorrect Answer: A\n\nExplanation: According to the paper, the key benefits of HALO are 1) it is driven by 3D keypoints, 2) it provides a differentiable volumetric occupancy representation, and 3) it can be trained end-to-end."}, "9": {"documentation": {"title": "Efficient Exact Paths For Dyck and semi-Dyck Labeled Path Reachability", "source": "Phillip G. Bradford", "docs_id": "1802.05239", "section": ["cs.DS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Efficient Exact Paths For Dyck and semi-Dyck Labeled Path Reachability. The exact path length problem is to determine if there is a path of a given fixed cost between two vertices. This paper focuses on the exact path problem for costs $-1,0$ or $+1$ between all pairs of vertices in an edge-weighted digraph. The edge weights are from $\\{ -1, +1 \\}$. In this case, this paper gives an $\\widetilde{O}(n^{\\omega})$ exact path solution. Here $\\omega$ is the best exponent for matrix multiplication and $\\widetilde{O}$ is the asymptotic upper-bound mod polylog factors. Variations of this algorithm determine which pairs of digraph nodes have Dyck or semi-Dyck labeled paths between them, assuming two parenthesis. Therefore, determining digraph reachability for Dyck or semi-Dyck labeled paths costs $\\widetilde{O}(n^{\\omega})$. A path label is made by concatenating all symbols along the path's edges. The exact path length problem has many applications. These applications include the labeled path problems given here, which in turn, also have numerous applications."}, "answer": " Here is a multi-form question on the material:\n\nQuestion: Which of the following statements accurately summarizes a key contribution of the paper \"Efficient Exact Paths For Dyck and semi-Dyck Labeled Path Reachability\"?\n\nA) The paper gives an efficient algorithm for finding shortest paths in general graphs.\n\nB) The paper gives an $\\widetilde{O}(n^{\\omega})$ algorithm for finding paths of fixed cost between vertices when edge weights are -1 or +1. \n\nC) The paper shows that reachability for arbitrary edge-labeled digraphs can be solved in $\\widetilde{O}(n^{\\omega})$ time.\n\nD) The paper presents an $\\widetilde{O}(n^{\\omega})$ time algorithm for determining if Dyck or semi-Dyck labeled paths exist between digraph vertices.\n\nCorrect Answer: D\n\nExplanation: The paper focuses on an $\\widetilde{O}(n^{\\omega})$ time algorithm for determining reachability between vertices in digraphs when considering Dyck or semi-Dyck path labels. It does not give a general shortest path algorithm, handle arbitrary edge labels, or prove fixed cost path reachability for all graphs."}, "10": {"documentation": {"title": "Topological bifurcations in a model society of reasonable contrarians", "source": "Franco Bagnoli and Raul Rechtman", "docs_id": "1308.4002", "section": ["nlin.CG", "cs.SI", "nlin.CD", "physics.soc-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Topological bifurcations in a model society of reasonable contrarians. People are often divided into conformists and contrarians, the former tending to align to the majority opinion in their neighborhood and the latter tending to disagree with that majority. In practice, however, the contrarian tendency is rarely followed when there is an overwhelming majority with a given opinion, which denotes a social norm. Such reasonable contrarian behavior is often considered a mark of independent thought, and can be a useful strategy in financial markets. We present the opinion dynamics of a society of reasonable contrarian agents. The model is a cellular automaton of Ising type, with antiferromagnetic pair interactions modeling contrarianism and plaquette terms modeling social norms. We introduce the entropy of the collective variable as a way of comparing deterministic (mean-field) and probabilistic (simulations) bifurcation diagrams. In the mean field approximation the model exhibits bifurcations and a chaotic phase, interpreted as coherent oscillations of the whole society. However, in a one-dimensional spatial arrangement one observes incoherent oscillations and a constant average. In simulations on Watts-Strogatz networks with a small-world effect the mean field behavior is recovered, with a bifurcation diagram that resembles the mean-field one, but using the rewiring probability as the control parameter. Similar bifurcation diagrams are found for scale free networks, and we are able to compute an effective connectivity for such networks."}, "answer": " Here is a 4-option multiple choice question on the paper:\n\nQuestion: Which of the following statements is true about the model society of reasonable contrarians studied in the paper?\n\nA) The model exhibits coherent oscillations and a chaotic phase in a one-dimensional spatial arrangement.\n\nB) The model shows incoherent oscillations and a constant average opinion in simulations on Watts-Strogatz small-world networks. \n\nC) The bifurcation diagram on scale-free networks resembles the mean-field bifurcation diagram when using the rewiring probability as the control parameter.\n\nD) The entropy of the collective variable cannot be used to compare deterministic and probabilistic bifurcation diagrams.\n\nCorrect Answer: C\n\nExplanation: The paper states that in a one-dimensional spatial arrangement, the model shows incoherent oscillations and a constant average opinion. On small-world and scale-free networks, the mean-field behavior is recovered and the bifurcation diagram resembles the mean-field one when using rewiring probability as the control parameter. The entropy of the collective variable is introduced as a way of comparing deterministic and probabilistic bifurcation diagrams."}, "11": {"documentation": {"title": "A Polynomial-Time Algorithm for Solving the Minimal Observability\n  Problem in Conjunctive Boolean Networks", "source": "Eyal Weiss and Michael Margaliot", "docs_id": "1706.04072", "section": ["math.OC", "cs.SY", "math.DS", "q-bio.QM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Polynomial-Time Algorithm for Solving the Minimal Observability\n  Problem in Conjunctive Boolean Networks. Many complex systems in biology, physics, and engineering include a large number of state-variables, and measuring the full state of the system is often impossible. Typically, a set of sensors is used to measure part of the state-variables. A system is called observable if these measurements allow to reconstruct the entire state of the system. When the system is not observable, an important and practical problem is how to add a \\emph{minimal} number of sensors so that the system becomes observable. This minimal observability problem is practically useful and theoretically interesting, as it pinpoints the most informative nodes in the system. We consider the minimal observability problem for an important special class of Boolean networks, called conjunctive Boolean networks (CBNs). Using a graph-theoretic approach, we provide a necessary and sufficient condition for observability of a CBN with $n$ state-variables, and an efficient~$O(n^2)$-time algorithm for solving the minimal observability problem. We demonstrate the usefulness of these results by studying the properties of a class of random CBNs."}, "answer": " Here is a multi-form question on the paper:\n\nQuestion: Which of the following statements about the minimal observability problem for conjunctive Boolean networks (CBNs) is true according to the paper?\n\nA) The paper provides a necessary but not sufficient condition for observability of a CBN.\n\nB) The paper gives an O(n^3)-time algorithm for solving the minimal observability problem. \n\nC) The paper shows that random CBNs always require adding the maximal number of sensors for observability.\n\nD) The paper presents an efficient O(n^2)-time algorithm for the minimal observability problem.\n\nCorrect Answer: D) The paper presents an efficient O(n^2)-time algorithm for the minimal observability problem.\n\nThe paper provides both necessary and sufficient conditions for observability of a CBN, gives an O(n^2) algorithm, not O(n^3), and does not make claims about random CBNs requiring maximal sensors."}, "12": {"documentation": {"title": "Infinite Shift-invariant Grouped Multi-task Learning for Gaussian\n  Processes", "source": "Yuyang Wang, Roni Khardon, Pavlos Protopapas", "docs_id": "1203.0970", "section": ["cs.LG", "astro-ph.IM", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Infinite Shift-invariant Grouped Multi-task Learning for Gaussian\n  Processes. Multi-task learning leverages shared information among data sets to improve the learning performance of individual tasks. The paper applies this framework for data where each task is a phase-shifted periodic time series. In particular, we develop a novel Bayesian nonparametric model capturing a mixture of Gaussian processes where each task is a sum of a group-specific function and a component capturing individual variation, in addition to each task being phase shifted. We develop an efficient \\textsc{em} algorithm to learn the parameters of the model. As a special case we obtain the Gaussian mixture model and \\textsc{em} algorithm for phased-shifted periodic time series. Furthermore, we extend the proposed model by using a Dirichlet Process prior and thereby leading to an infinite mixture model that is capable of doing automatic model selection. A Variational Bayesian approach is developed for inference in this model. Experiments in regression, classification and class discovery demonstrate the performance of the proposed models using both synthetic data and real-world time series data from astrophysics. Our methods are particularly useful when the time series are sparsely and non-synchronously sampled."}, "answer": " Here is a 4-option multiple choice question on the paper:\n\nQuestion: Which of the following statements is true about the multi-task learning framework proposed in the paper?\n\nA) It uses a Dirichlet process prior to model the mixture of Gaussian processes, enabling automatic model selection.\n\nB) It models each task as a Gaussian process with a phase shift component and an individual variation component. \n\nC) It extends standard Gaussian process regression to classification and class discovery problems.\n\nD) It uses an EM algorithm for inference but does not have a Bayesian nonparametric formulation.\n\nCorrect Answer: B\n\nExplanation: The paper proposes a multi-task learning framework where each task is modeled as a Gaussian process with two components - a group-specific function and a component capturing individual variation, along with a phase shift. Options A, C, and D refer to other aspects of the model and inference, but option B correctly summarizes the core modeling framework."}, "13": {"documentation": {"title": "Equivalence of Systematic Linear Data Structures and Matrix Rigidity", "source": "Sivaramakrishnan Natarajan Ramamoorthy, Cyrus Rashtchian", "docs_id": "1910.11921", "section": ["cs.CC", "cs.DS", "math.CO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Equivalence of Systematic Linear Data Structures and Matrix Rigidity. Recently, Dvir, Golovnev, and Weinstein have shown that sufficiently strong lower bounds for linear data structures would imply new bounds for rigid matrices. However, their result utilizes an algorithm that requires an $NP$ oracle, and hence, the rigid matrices are not explicit. In this work, we derive an equivalence between rigidity and the systematic linear model of data structures. For the $n$-dimensional inner product problem with $m$ queries, we prove that lower bounds on the query time imply rigidity lower bounds for the query set itself. In particular, an explicit lower bound of $\\omega\\left(\\frac{n}{r}\\log m\\right)$ for $r$ redundant storage bits would yield better rigidity parameters than the best bounds due to Alon, Panigrahy, and Yekhanin. We also prove a converse result, showing that rigid matrices directly correspond to hard query sets for the systematic linear model. As an application, we prove that the set of vectors obtained from rank one binary matrices is rigid with parameters matching the known results for explicit sets. This implies that the vector-matrix-vector problem requires query time $\\Omega(n^{3/2}/r)$ for redundancy $r \\geq \\sqrt{n}$ in the systematic linear model, improving a result of Chakraborty, Kamma, and Larsen. Finally, we prove a cell probe lower bound for the vector-matrix-vector problem in the high error regime, improving a result of Chattopadhyay, Kouck\\'{y}, Loff, and Mukhopadhyay."}, "answer": " Here is a multi-form question on the document:\n\nQuestion: What is the main result shown in the paper regarding the relationship between linear data structures and matrix rigidity?\n\nA) The paper shows that lower bounds for linear data structures imply new bounds for rigid matrices, using an algorithm requiring an NP oracle.\n\nB) The paper proves an equivalence between rigidity and the systematic linear model of data structures, showing lower bounds for linear data structures correspond to rigid matrices. \n\nC) The paper gives a new construction of rigid matrices from rank one binary matrices, implying an \u03a9(n^{3/2}/r) lower bound for the vector-matrix-vector problem.\n\nD) The paper shows how to derive explicit rigid matrices from lower bounds on the query time in the systematic linear model of data structures.\n\nCorrect Answer: B\n\nExplanation: The paper proves an equivalence between rigidity in the systematic linear model of data structures and lower bounds on query time. Specifically, it shows that a lower bound of \u03c9(n/r log m) for query time implies better rigidity parameters, and rigid matrices imply hard query sets in the systematic linear model."}, "14": {"documentation": {"title": "The SensorCloud Protocol: Securely Outsourcing Sensor Data to the Cloud", "source": "Martin Henze, Ren\\'e Hummen, Roman Matzutt, Klaus Wehrle", "docs_id": "1607.03239", "section": ["cs.NI", "cs.CR", "cs.DC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The SensorCloud Protocol: Securely Outsourcing Sensor Data to the Cloud. The increasing deployment of sensor networks, ranging from home networks to industrial automation, leads to a similarly growing demand for storing and processing the collected sensor data. To satisfy this demand, the most promising approach to date is the utilization of the dynamically scalable, on-demand resources made available via the cloud computing paradigm. However, prevalent security and privacy concerns are a huge obstacle for the outsourcing of sensor data to the cloud. Hence, sensor data needs to be secured properly before it can be outsourced to the cloud. When securing the outsourcing of sensor data to the cloud, one important challenge lies in the representation of sensor data and the choice of security measures applied to it. In this paper, we present the SensorCloud protocol, which enables the representation of sensor data and actuator commands using JSON as well as the encoding of the object security mechanisms applied to a given sensor data item. Notably, we solely utilize mechanisms that have been or currently are in the process of being standardized at the IETF to aid the wide applicability of our approach."}, "answer": " Question: What protocol enables the secure representation and outsourcing of sensor data to the cloud using standardized mechanisms?\n\nA) The SensorCloud protocol, which represents sensor data and actuator commands in JSON format and applies IETF-standardized object security mechanisms to sensor data before outsourcing it to the cloud.\n\nB) The JSON Sensor protocol, which represents sensor data in JSON format and uses proprietary security mechanisms when outsourcing data to the cloud. \n\nC) The Cloud Sensor protocol, which outsources sensor data to the cloud without any standardized data representation or security mechanisms.\n\nD) The IETF Sensor Security protocol, which applies standardized security mechanisms to sensor data before outsourcing but does not specify a data representation format.\n\nCorrect Answer: A"}, "15": {"documentation": {"title": "Reconciling Feature-Reuse and Overfitting in DenseNet with Specialized\n  Dropout", "source": "Kun Wan, Boyuan Feng, Lingwei Xie, Yufei Ding", "docs_id": "1810.00091", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Reconciling Feature-Reuse and Overfitting in DenseNet with Specialized\n  Dropout. Recently convolutional neural networks (CNNs) achieve great accuracy in visual recognition tasks. DenseNet becomes one of the most popular CNN models due to its effectiveness in feature-reuse. However, like other CNN models, DenseNets also face overfitting problem if not severer. Existing dropout method can be applied but not as effective due to the introduced nonlinear connections. In particular, the property of feature-reuse in DenseNet will be impeded, and the dropout effect will be weakened by the spatial correlation inside feature maps. To address these problems, we craft the design of a specialized dropout method from three aspects, dropout location, dropout granularity, and dropout probability. The insights attained here could potentially be applied as a general approach for boosting the accuracy of other CNN models with similar nonlinear connections. Experimental results show that DenseNets with our specialized dropout method yield better accuracy compared to vanilla DenseNet and state-of-the-art CNN models, and such accuracy boost increases with the model depth."}, "answer": " Question: Which of the following statements best summarizes the key idea presented in the given text?\n\nA) DenseNets achieve high accuracy for visual recognition tasks but suffer from overfitting issues more severely than other CNN models. The existing dropout method is not very effective for DenseNets due to the introduced nonlinear connections which impede feature reuse.\n\nB) The specialized dropout method proposed in the paper addresses the overfitting problem in DenseNets by modifying three aspects - dropout location, dropout granularity and dropout probability. This boosts the accuracy of DenseNets to be better than vanilla DenseNets and other state-of-the-art CNN models.\n\nC) Spatial correlation inside feature maps in DenseNets weakens the effect of existing dropout methods. The proposed specialized dropout method with changes in dropout probability, location and granularity helps DenseNets achieve state-of-the-art accuracy by retaining feature reuse property.\n\nD) DenseNets are more prone to overfitting compared to other CNN models. The paper proposes a novel dropout method customized for DenseNets which surpasses the accuracy of all other CNN models by strengthening feature reuse and mitigating spatial correlation.\n\nCorrect Answer: C\n\nExplanation: Option C best summarizes the key idea that the specialized dropout method proposed in the paper helps address the limitations of applying existing dropout to DenseNets by modifying dropout location, granularity and probability. This allows DenseNets to retain the feature reuse property while reducing overfitting caused by spatial correlation in feature maps."}, "16": {"documentation": {"title": "A Data Quality Metric (DQM): How to Estimate The Number of Undetected\n  Errors in Data Sets", "source": "Yeounoh Chung, Sanjay Krishnan, Tim Kraska", "docs_id": "1611.04878", "section": ["cs.DB"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Data Quality Metric (DQM): How to Estimate The Number of Undetected\n  Errors in Data Sets. Data cleaning, whether manual or algorithmic, is rarely perfect leaving a dataset with an unknown number of false positives and false negatives after cleaning. In many scenarios, quantifying the number of remaining errors is challenging because our data integrity rules themselves may be incomplete, or the available gold-standard datasets may be too small to extrapolate. As the use of inherently fallible crowds becomes more prevalent in data cleaning problems, it is important to have estimators to quantify the extent of such errors. We propose novel species estimators to estimate the number of distinct remaining errors in a dataset after it has been cleaned by a set of crowd workers -- essentially, quantifying the utility of hiring additional workers to clean the dataset. This problem requires new estimators that are robust to false positives and false negatives, and we empirically show on three real-world datasets that existing species estimators are unstable for this problem, while our proposed techniques quickly converge."}, "answer": " Here is a multi-form question based on the given documentation:\n\nQuestion: After a dataset has been cleaned by crowd workers, what metric can be used to estimate the number of distinct remaining errors?\n\nA) Precision \nB) Recall\nC) A novel species estimator\nD) F1 score\n\nCorrect Answer: C) A novel species estimator\n\nExplanation: The documentation states that novel species estimators can be used to estimate the number of distinct remaining errors in a dataset after it has been cleaned by crowd workers. This helps quantify the utility of hiring additional workers to further clean the dataset."}, "17": {"documentation": {"title": "An Information-Theoretic Framework for Identifying Age-Related Genes\n  Using Human Dermal Fibroblast Transcriptome Data", "source": "Salman Mohamadi, Donald Adjeroh", "docs_id": "2111.02595", "section": ["q-bio.GN", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "An Information-Theoretic Framework for Identifying Age-Related Genes\n  Using Human Dermal Fibroblast Transcriptome Data. Investigation of age-related genes is of great importance for multiple purposes, for instance, improving our understanding of the mechanism of ageing, increasing life expectancy, age prediction, and other healthcare applications. In his work, starting with a set of 27,142 genes, we develop an information-theoretic framework for identifying genes that are associated with aging by applying unsupervised and semi-supervised learning techniques on human dermal fibroblast gene expression data. First, we use unsupervised learning and apply information-theoretic measures to identify key features for effective representation of gene expression values in the transcriptome data. Using the identified features, we perform clustering on the data. Finally, we apply semi-supervised learning on the clusters using different distance measures to identify novel genes that are potentially associated with aging. Performance assessment for both unsupervised and semi-supervised methods show the effectiveness of the framework."}, "answer": " Question: What information-theoretic measure was used in the initial unsupervised learning step to identify key features for effective representation of the gene expression values in the transcriptome data?\n\nA) Mutual information \n\nB) Conditional entropy\n\nC) Joint entropy\n\nD) Kullback-Leibler divergence\n\nCorrect Answer: A) Mutual information\n\nExplanation: The documentation states that information-theoretic measures were used in the initial unsupervised learning step to identify key features. Mutual information is a common information-theoretic measure used for feature selection, as it quantifies the dependence between variables. The other options are also information-theoretic measures but are not typically used for feature selection."}, "18": {"documentation": {"title": "Outlier detection at the parcel-level in wheat and rapeseed crops using\n  multispectral and SAR time series", "source": "Florian Mouret and Mohanad Albughdadi and Sylvie Duthoit and Denis\n  Kouam\\'e and Guillaume Rieu and Jean-Yves Tourneret", "docs_id": "2004.08431", "section": ["eess.IV", "cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Outlier detection at the parcel-level in wheat and rapeseed crops using\n  multispectral and SAR time series. This paper studies the detection of anomalous crop development at the parcel-level based on an unsupervised outlier detection technique. The experimental validation is conducted on rapeseed and wheat parcels located in Beauce (France). The proposed methodology consists of four sequential steps: 1) preprocessing of synthetic aperture radar (SAR) and multispectral images acquired using Sentinel-1 and Sentinel-2 satellites, 2) extraction of SAR and multispectral pixel-level features, 3) computation of parcel-level features using zonal statistics and 4) outlier detection. The different types of anomalies that can affect the studied crops are analyzed and described. The different factors that can influence the outlier detection results are investigated with a particular attention devoted to the synergy between Sentinel-1 and Sentinel-2 data. Overall, the best performance is obtained when using jointly a selection of Sentinel-1 and Sentinel-2 features with the isolation forest algorithm. The selected features are VV and VH backscattering coefficients for Sentinel-1 and 5 Vegetation Indexes for Sentinel-2 (among us, the Normalized Difference Vegetation Index and two variants of the Normalized Difference Water). When using these features with an outlier ratio of 10%, the percentage of detected true positives (i.e., crop anomalies) is equal to 94.1% for rapeseed parcels and 95.5% for wheat parcels."}, "answer": " Here is a multi-form question based on the given text:\n\nQuestion: Which of the following statements is true about the outlier detection methodology presented in the paper?\n\nA) It uses only Sentinel-1 SAR data for detecting anomalies in crops.\n\nB) It extracts features at the pixel-level before computing parcel-level statistics. \n\nC) The best results are obtained using only Sentinel-2 multispectral data.\n\nD) Vegetation indexes from Sentinel-2 are not useful for detecting crop anomalies.\n\nCorrect Answer: B\n\nExplanation: According to the text, the methodology extracts features at the pixel-level from both Sentinel-1 SAR and Sentinel-2 multispectral data, before computing parcel-level statistics. It found that using a combination of Sentinel-1 and Sentinel-2 features worked best, including vegetation indexes from Sentinel-2."}, "19": {"documentation": {"title": "Determining Optimal Rates for Communication for Omniscience", "source": "Ni Ding, Chung Chan, Qiaoqiao Zhou, Rodney A. Kennedy and Parastoo\n  Sadeghi", "docs_id": "1611.08367", "section": ["cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Determining Optimal Rates for Communication for Omniscience. This paper considers the communication for omniscience (CO) problem: A set of users observe a discrete memoryless multiple source and want to recover the entire multiple source via noise-free broadcast communications. We study the problem of how to determine an optimal rate vector that attains omniscience with the minimum sum-rate, the total number of communications. The results cover both asymptotic and non-asymptotic models where the transmission rates are real and integral, respectively. We propose a modified decomposition algorithm (MDA) and a sum-rate increment algorithm (SIA) for the asymptotic and non-asymptotic models, respectively, both of which determine the value of the minimum sum-rate and a corresponding optimal rate vector in polynomial time. For the coordinate saturation capacity (CoordSatCap) algorithm, a nesting algorithm in MDA and SIA, we propose to implement it by a fusion method and show by experimental results that this fusion method contributes to a reduction in computation complexity. Finally, we show that the separable convex minimization problem over the optimal rate vector set in the asymptotic model can be decomposed by the fundamental partition, the optimal partition of the user set that determines the minimum sum-rate, so that the problem can be solved more efficiently."}, "answer": " Here is a 4-option multiple choice question on the key concepts from the paper:\n\nQuestion: Which of the following statements is true about the communication for omniscience (CO) problem studied in the paper?\n\nA) The goal is for a set of users to recover a discrete memoryless multiple source via noisy broadcast communications.\n\nB) The modified decomposition algorithm (MDA) determines the minimum sum-rate and optimal rate vector in exponential time. \n\nC) The coordinate saturation capacity (CoordSatCap) algorithm is implemented using a fusion method to reduce computation complexity.\n\nD) The fundamental partition refers to the optimal partitioning of the source to minimize sum-rate.\n\nCorrect Answer: C\n\nExplanation: \nOption A is incorrect - the paper considers noise-free broadcast communications. Option B is incorrect - MDA determines the minimum sum-rate and optimal rate vector in polynomial time. Option C is correct - the paper proposes implementing CoordSatCap using a fusion method to reduce complexity. Option D is incorrect - the fundamental partition refers to the optimal partitioning of the user set, not the source."}, "20": {"documentation": {"title": "Planning and Operations of Mixed Fleets in Mobility-on-Demand Systems", "source": "Kaidi Yang, Matthew W. Tsao, Xin Xu, Marco Pavone", "docs_id": "2008.08131", "section": ["eess.SY", "cs.SY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Planning and Operations of Mixed Fleets in Mobility-on-Demand Systems. Automated vehicles (AVs) are expected to be beneficial for Mobility-on-Demand (MoD), thanks to their ability of being globally coordinated. To facilitate the steady transition towards full autonomy, we consider the transition period of AV deployment, whereby an MoD system operates a mixed fleet of automated vehicles (AVs) and human-driven vehicles (HVs). In such systems, AVs are centrally coordinated by the operator, and the HVs might strategically respond to the coordination of AVs. We devise computationally tractable strategies to coordinate mixed fleets in MoD systems. Specifically, we model an MoD system with a mixed fleet using a Stackelberg framework where the MoD operator serves as the leader and human-driven vehicles serve as the followers. We develop two models: 1) a steady-state model to analyze the properties of the problem and determine the planning variables (e.g., compensations, prices, and the fleet size of AVs), and 2) a time-varying model to design a real-time coordination algorithm for AVs. The proposed models are validated using a case study inspired by real operational data of a MoD service in Singapore. Results show that the proposed algorithms can significantly improve system performance."}, "answer": " Question: Which of the following statements is true about the mixed fleet Mobility-on-Demand (MoD) system described in the passage?\n\nA) The system uses a fleet of only automated vehicles (AVs) that are centrally coordinated by the operator.\n\nB) The system uses a mixed fleet of AVs that are centrally coordinated and human-driven vehicles (HVs) that respond strategically. \n\nC) The operator serves as a follower and the HVs serve as leaders in the Stackelberg framework used to model the system.\n\nD) The steady-state model is used to design a real-time coordination algorithm, while the time-varying model analyzes properties of the system.\n\nCorrect Answer: B\n\nExplanation: The passage states that the MoD system operates a mixed fleet of AVs that are centrally coordinated by the operator, and HVs that might strategically respond to the AV coordination. So B is correct. A is incorrect as the fleet is mixed, not only AVs. C is incorrect because the operator serves as the leader, not follower, in the Stackelberg framework. D is incorrect because the roles of the steady-state and time-varying models are reversed."}, "21": {"documentation": {"title": "RoboChain: A Secure Data-Sharing Framework for Human-Robot Interaction", "source": "Eduardo Castell\\'o Ferrer, Ognjen Rudovic, Thomas Hardjono, Alex\n  Pentland", "docs_id": "1802.04480", "section": ["cs.RO", "cs.HC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "RoboChain: A Secure Data-Sharing Framework for Human-Robot Interaction. Robots have potential to revolutionize the way we interact with the world around us. One of their largest potentials is in the domain of mobile health where they can be used to facilitate clinical interventions. However, to accomplish this, robots need to have access to our private data in order to learn from these data and improve their interaction capabilities. Furthermore, to enhance this learning process, the knowledge sharing among multiple robot units is the natural step forward. However, to date, there is no well-established framework which allows for such data sharing while preserving the privacy of the users (e.g., the hospital patients). To this end, we introduce RoboChain - the first learning framework for secure, decentralized and computationally efficient data and model sharing among multiple robot units installed at multiple sites (e.g., hospitals). RoboChain builds upon and combines the latest advances in open data access and blockchain technologies, as well as machine learning. We illustrate this framework using the example of a clinical intervention conducted in a private network of hospitals. Specifically, we lay down the system architecture that allows multiple robot units, conducting the interventions at different hospitals, to perform efficient learning without compromising the data privacy."}, "answer": " Question: What is the main goal of the RoboChain framework introduced in the paper?\n\nA) To allow multiple robot units to securely share private user data and learned models across different sites like hospitals.\n\nB) To develop a new blockchain technology specifically for use in human-robot interaction systems. \n\nC) To create a decentralized network of robots that can operate autonomously without human oversight.\n\nD) To enhance the interaction capabilities of a single robot unit by allowing it to access large amounts of private user data.\n\nCorrect Answer: A\n\nExplanation: According to the documentation, the main goal of RoboChain is to allow multiple robot units installed at different sites like hospitals to securely share private user data and learned models in order to enhance the learning process, while preserving user privacy."}, "22": {"documentation": {"title": "Theoretic Shaping Bounds for Single Letter Constraints and Mismatched\n  Decoding", "source": "Stella Achtenberg and Dan Raphaeli", "docs_id": "1308.5938", "section": ["cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Theoretic Shaping Bounds for Single Letter Constraints and Mismatched\n  Decoding. Shaping gain is attained in schemes where a shaped subcode is chosen from a larger codebook by a codeword selection process. This includes the popular method of Trellis Shaping (TS), originally proposed by Forney for average power reduction. The decoding process of such schemes is mismatched, since it is aware of only the large codebook. This study models such schemes by a random code construction and derives achievable bounds on the transmission rate under matched and mismatched decoding. For matched decoding the bound is obtained using a modified asymptotic equipartition property (AEP) theorem derived to suit this particular code construction. For mismatched decoding, relying on the large codebook performance is generally wrong, since the performance of the non-typical codewords within the large codebook may differ substantially from the typical ones. Hence, we present two novel lower bounds on the capacity under mismatched decoding. The first is based upon Gallager's random exponent, whereas the second on a modified version of the joint-typicality decoder."}, "answer": " Here is a 4-option multiple choice question on the abstract:\n\nQuestion: Which of the following statements is true about the study described in the abstract?\n\nA) It derives an achievable rate bound for shaping schemes using a modified AEP theorem under matched decoding.\n\nB) It shows that relying on large codebook performance gives accurate results under mismatched decoding. \n\nC) It presents a joint-typicality decoder that outperforms Gallager's random coding exponent bound.\n\nD) It models shaping schemes using a codebook construction different from Trellis Shaping.\n\nCorrect Answer: A\n\nExplanation: \nOption A is correct according to the abstract, which states that \"For matched decoding the bound is obtained using a modified asymptotic equipartition property (AEP) theorem derived to suit this particular code construction.\"\n\nOption B is incorrect, as the abstract notes that \"relying on the large codebook performance is generally wrong\" under mismatched decoding.\n\nOption C is incorrect, as the abstract presents two novel lower bounds, one based on Gallager's exponent and one using a modified joint-typicality decoder. It does not claim the joint-typicality bound outperforms Gallager's.\n\nOption D is incorrect, as the abstract states the study models shaping schemes \"by a random code construction\", not a different construction than Trellis Shaping."}, "23": {"documentation": {"title": "Relightable 3D Head Portraits from a Smartphone Video", "source": "Artem Sevastopolsky, Savva Ignatiev, Gonzalo Ferrer, Evgeny Burnaev,\n  Victor Lempitsky", "docs_id": "2012.09963", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Relightable 3D Head Portraits from a Smartphone Video. In this work, a system for creating a relightable 3D portrait of a human head is presented. Our neural pipeline operates on a sequence of frames captured by a smartphone camera with the flash blinking (flash-no flash sequence). A coarse point cloud reconstructed via structure-from-motion software and multi-view denoising is then used as a geometric proxy. Afterwards, a deep rendering network is trained to regress dense albedo, normals, and environmental lighting maps for arbitrary new viewpoints. Effectively, the proxy geometry and the rendering network constitute a relightable 3D portrait model, that can be synthesized from an arbitrary viewpoint and under arbitrary lighting, e.g. directional light, point light, or an environment map. The model is fitted to the sequence of frames with human face-specific priors that enforce the plausibility of albedo-lighting decomposition and operates at the interactive frame rate. We evaluate the performance of the method under varying lighting conditions and at the extrapolated viewpoints and compare with existing relighting methods."}, "answer": " Here is a multi-form question based on the given documentation:\n\nQuestion: Which of the following best describes the input data used to create the relightable 3D portrait model in the presented system?\n\nA) A single image captured by a smartphone camera with the flash on. \n\nB) A sequence of images captured by a smartphone camera from different viewpoints without a flash.\n\nC) A video captured by a smartphone camera with the flash blinking in a flash-no flash sequence.\n\nD) Multiple images captured by a DSLR camera under different lighting conditions.\n\nCorrect Answer: C\n\nExplanation: According to the documentation, the input to the system is \"a sequence of frames captured by a smartphone camera with the flash blinking (flash-no flash sequence)\". So a video captured with alternating flash and no flash frames is the correct input data."}, "24": {"documentation": {"title": "Jiffy: A Lock-free Skip List with Batch Updates and Snapshots", "source": "Tadeusz Kobus, Maciej Kokoci\\'nski, Pawe{\\l} T. Wojciechowski", "docs_id": "2102.01044", "section": ["cs.DS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Jiffy: A Lock-free Skip List with Batch Updates and Snapshots. In this paper we introduce Jiffy, the first lock-free, linearizable ordered key-value index that offers both (1) batch updates, which are put and remove operations that are executed atomically, and (2) consistent snapshots used by, e.g., range scan operations. Jiffy is built as a multiversioned lock-free skip list and relies on CPU's Time Stamp Counter register to generate version numbers at minimal cost. For faster skip list traversals and better utilization of the CPU caches, key-value entries are grouped into immutable objects called revisions. Moreover, by changing the size of revisions and thus modifying the synchronization granularity, our index can adapt to varying contentions levels (smaller revisions are more suited for write-heavy workloads whereas large revisions benefit read-dominated workloads, especially when they feature many range scan operations). Structure modifications to the index, which result in changing the size of revisions, happen through (lock-free) skip list node split and merge operations that are carefully coordinated with the update operations. Despite rich semantics, Jiffy offers highly scalable performance, which is comparable or exceeds the performance of the state-of-the-art lock-free ordered indices that feature linearizable range scan operations. Compared to its (lock-based) rivals that also support batch updates, Jiffy can execute large batch updates up to 7.4x more efficiently."}, "answer": " Here is a multi-form question on the Jiffy paper:\n\nQuestion: Which of the following statements is true about Jiffy, the lock-free skip list described in the paper?\n\nA) It uses locks to coordinate structure modifications with update operations.\n\nB) It relies on logical timestamps instead of physical timestamps to generate version numbers. \n\nC) It groups key-value entries into immutable objects called revisions to improve cache utilization.\n\nD) Its performance is worse than existing lock-free ordered indices that support linearizable range scans.\n\nCorrect Answer: C\n\nExplanations:\n\nA) Incorrect. Jiffy is lock-free and uses coordination of node splits/merges with updates instead of locks.\n\nB) Incorrect. Jiffy uses the CPU's Time Stamp Counter register to efficiently generate version numbers. \n\nC) Correct. Jiffy groups entries into immutable revisions to improve skip list traversal and cache utilization.\n\nD) Incorrect. The paper states that Jiffy offers highly scalable performance comparable or better than existing lock-free ordered indices."}, "25": {"documentation": {"title": "Ranking Catamorphisms and Unranking Anamorphisms on Hereditarily Finite\n  Datatypes", "source": "Paul Tarau", "docs_id": "0808.0753", "section": ["cs.SC", "cs.DM", "cs.DS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Ranking Catamorphisms and Unranking Anamorphisms on Hereditarily Finite\n  Datatypes. Using specializations of unfold and fold on a generic tree data type we derive unranking and ranking functions providing natural number encodings for various Hereditarily Finite datatypes. In this context, we interpret unranking operations as instances of a generic anamorphism and ranking operations as instances of the corresponding catamorphism. Starting with Ackerman's Encoding from Hereditarily Finite Sets to Natural Numbers we define pairings and tuple encodings that provide building blocks for a theory of Hereditarily Finite Functions. The more difficult problem of ranking and unranking Hereditarily Finite Permutations is then tackled using Lehmer codes and factoradics. The self-contained source code of the paper, as generated from a literate Haskell program, is available at \\url{http://logic.csci.unt.edu/tarau/research/2008/fFUN.zip}. Keywords: ranking/unranking, pairing/tupling functions, Ackermann encoding, hereditarily finite sets, hereditarily finite functions, permutations and factoradics, computational mathematics, Haskell data representations"}, "answer": " Here is a multi-form question on the documentation:\n\nQuestion: Which of the following best summarizes the key ideas presented in the Arxiv documentation?\n\nA) The documentation presents ranking and unranking functions for encoding hereditarily finite sets and permutations into natural numbers using factoradics. It implements these functions in Haskell.\n\nB) The documentation introduces ranking and unranking operations as instances of catamorphisms and anamorphisms on generic tree data types. It focuses on encodings for hereditarily finite sets, functions, and permutations.\n\nC) The documentation proposes a new theory of pairing and tuple encodings to provide natural number representations of hereditarily finite datatypes. It emphasizes applications to computational mathematics. \n\nD) The documentation describes how to derive unranking and ranking algorithms from unfold and fold operations on an abstract tree type. It applies these to hereditarily finite sets, functions, and permutations.\n\nCorrect Answer: D\n\nExplanation: The documentation focuses on using unfold and fold on a generic tree datatype to implement ranking and unranking functions for hereditarily finite datatypes like sets, functions, and permutations. While it touches on topics like factoradics, Haskell, and computational math, the core ideas are deriving these algorithms from catamorphisms/anamorphisms on a tree type."}, "26": {"documentation": {"title": "Verifying Response Times in Networked Automation Systems Using Jitter\n  Bounds", "source": "Seshadhri Srinivasan, Furio Buonopane, Srini Ramaswamy, Juri Vain", "docs_id": "1507.04300", "section": ["cs.SY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Verifying Response Times in Networked Automation Systems Using Jitter\n  Bounds. Networked Automation Systems (NAS) have to meet stringent response time during operation. Verifying response time of automation is an important step during design phase before deployment. Timing discrepancies due to hardware, software and communication components of NAS affect the response time. This investigation uses model templates for verifying the response time in NAS. First, jitter bounds model the timing fluctuations of NAS components. These jitter bounds are the inputs to model templates that are formal models of timing fluctuations. The model templates are atomic action patterns composed of three composition operators- sequential, alternative, and parallel and embedded in time wrapper that specifies clock driven activation conditions. Model templates in conjunction with formal model of technical process offer an easier way to verify the response time. The investigation demonstrates the proposed verification method using an industrial steam boiler with typical NAS components in plant floor."}, "answer": " Here is a 4-option multiple choice question on the document:\n\nQuestion: Which of the following statements is true about the proposed verification method for response times in Networked Automation Systems (NAS)?\n\nA) It uses jitter bounds to model timing fluctuations in NAS components as inputs to formal models composed of sequential, alternative and parallel operators.\n\nB) It verifies response times by directly measuring them after deployment of the NAS. \n\nC) It models the technical process but does not use model templates with composition operators.\n\nD) It relies solely on jitter bounds of NAS components and does not use any model templates.\n\nCorrect Answer: A\n\nExplanation: The proposed verification method uses jitter bounds to model timing fluctuations of NAS components. These are fed as inputs to model templates composed of sequential, alternative and parallel operators to formally model the timing variations. The model templates along with models of technical process are used to verify response times before deployment."}, "27": {"documentation": {"title": "A heuristic scheme for the Cooperative Team Orienteering Problem with\n  Time Windows", "source": "Iman Roozbeh, Melih Ozlen, John W. Hearne", "docs_id": "1608.05485", "section": ["cs.AI", "math.OC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A heuristic scheme for the Cooperative Team Orienteering Problem with\n  Time Windows. The Cooperative Orienteering Problem with Time Windows (COPTW)is a class of problems with some important applications and yet has received relatively little attention. In the COPTW a certain number of team members are required to collect the associated reward from each customer simultaneously and cooperatively. This requirement to have one or more team members simultaneously available at a vertex to collect the reward, poses a challenging OR task. Exact methods are not able to handle large scale instances of the COPTW and no heuristic schemes have been developed for this problem so far. In this paper, a new modification to the classical Clarke and Wright saving heuristic is proposed to handle this problem. A new benchmark set generated by adding the resource requirement attribute to the existing benchmarks. The heuristic algorithm followed by boosting operators achieves optimal solutions for 64.5% of instances for which the optimal results are known. The proposed solution approach attains an optimality gap of 2.61% for the same instances and solves benchmarks with realistic size within short computational times."}, "answer": " Here is a 4-option multiple choice question with explanations for the given text:\n\nQuestion: Which of the following statements is true about the Cooperative Orienteering Problem with Time Windows (COPTW) according to the text?\n\nA) The COPTW has received significant attention and many heuristic schemes have been developed for it. \n\nB) In the COPTW, team members are not required to be simultaneously present at a vertex to collect the reward.\n\nC) Exact methods are able to optimally solve large-scale instances of the COPTW within reasonable time.\n\nD) A new heuristic scheme based on Clarke and Wright savings is proposed in the paper to solve the COPTW.\n\nCorrect Answer: D\n\nExplanations: \nA) Is incorrect - the text states that COPTW has received relatively little attention. No heuristic schemes existed prior to this work.\nB) Is incorrect - the text states team members ARE required to be simultaneously present to collect rewards in COPTW.  \nC) Is incorrect - the text states exact methods cannot handle large scale COPTW instances.\nD) Is correct - the text proposes a new heuristic scheme based on Clarke and Wright savings to handle the COPTW."}, "28": {"documentation": {"title": "Analyzing Linear Communication Networks using the Ribosome Flow Model", "source": "Yoram Zarai and Oz Mendel and Michael Margaliot", "docs_id": "1508.06038", "section": ["cs.NI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Analyzing Linear Communication Networks using the Ribosome Flow Model. The Ribosome Flow Model (RFM) describes the unidirectional movement of interacting particles along a one-dimensional chain of sites. As a site becomes fuller, the effective entry rate into this site decreases. The RFM has been used to model and analyze mRNA translation, a biological process in which ribosomes (the particles) move along the mRNA molecule (the chain), and decode the genetic information into proteins. Here we propose the RFM as an analytical framework for modeling and analyzing linear communication networks. In this context, the moving particles are data-packets, the chain of sites is a one dimensional set of ordered buffers, and the decreasing entry rate to a fuller buffer represents a kind of decentralized backpressure flow control. For an RFM with homogeneous link capacities, we provide closed-form expressions for important network metrics including the throughput and end-to-end delay. We use these results to analyze the hop length and the transmission probability (in a contention access mode) that minimize the end-to-end delay in a multihop linear network, and provide closed-form expressions for the optimal parameter values."}, "answer": " Here is a multi-form question on the Ribosome Flow Model for analyzing linear communication networks:\n\nQuestion: Which of the following statements is true about the Ribosome Flow Model (RFM) for analyzing linear communication networks?\n\nA) The RFM models the movement of ribosomes along an mRNA molecule during protein translation. It cannot be used to model data packet flow in communication networks.\n\nB) The RFM can model data packet flow in linear communication networks as particles moving along a chain of buffers. As buffers get fuller, the packet entry rate decreases, implementing decentralized backpressure flow control. \n\nC) The RFM assumes heterogeneous link capacities in linear networks. It provides closed-form expressions for throughput and delay under this assumption.\n\nD) For an RFM with homogeneous link capacities, closed-form solutions exist for throughput but not for end-to-end delay.\n\nCorrect Answer: B\n\nExplanation: \nOption B is correct according to the given information. The RFM can model data packets as particles and network buffers as sites. Backpressure flow control is implemented by decreasing entry rate into fuller buffers. Closed-form solutions exist for both throughput and delay under homogeneous capacities.\n\nOption A is incorrect because the RFM is proposed for modeling communication networks, not just biological systems. \n\nOption C is incorrect because the given information states the RFM analysis is for homogeneous, not heterogeneous capacities. \n\nOption D is incorrect because closed-form delay expressions are provided under the homogeneous capacity assumption."}, "29": {"documentation": {"title": "Rare-Allele Detection Using Compressed Se(que)nsing", "source": "Noam Shental, Amnon Amir and Or Zuk", "docs_id": "0909.0400", "section": ["q-bio.GN", "cs.IT", "cs.LG", "math.IT", "q-bio.QM", "stat.AP", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Rare-Allele Detection Using Compressed Se(que)nsing. Detection of rare variants by resequencing is important for the identification of individuals carrying disease variants. Rapid sequencing by new technologies enables low-cost resequencing of target regions, although it is still prohibitive to test more than a few individuals. In order to improve cost trade-offs, it has recently been suggested to apply pooling designs which enable the detection of carriers of rare alleles in groups of individuals. However, this was shown to hold only for a relatively low number of individuals in a pool, and requires the design of pooling schemes for particular cases. We propose a novel pooling design, based on a compressed sensing approach, which is both general, simple and efficient. We model the experimental procedure and show via computer simulations that it enables the recovery of rare allele carriers out of larger groups than were possible before, especially in situations where high coverage is obtained for each individual. Our approach can also be combined with barcoding techniques to enhance performance and provide a feasible solution based on current resequencing costs. For example, when targeting a small enough genomic region (~100 base-pairs) and using only ~10 sequencing lanes and ~10 distinct barcodes, one can recover the identity of 4 rare allele carriers out of a population of over 4000 individuals."}, "answer": " Here is a 4-option multiple choice question on the document:\n\nQuestion: Which of the following statements is true about the compressed sensing approach for rare-allele detection proposed in the document?\n\nA) It enables detection of rare alleles in larger pools of individuals compared to previous methods by using higher sequencing coverage per individual.\n\nB) It relies on designing specific pooling schemes tailored to each case rather than a general approach. \n\nC) It can only be applied when targeting large genomic regions of over 1000 base pairs.\n\nD) It requires using a very large number of sequencing lanes and barcodes to be effective.\n\nCorrect Answer: A\n\nExplanation: The document states that the proposed compressed sensing approach enables recovery of rare allele carriers from larger groups than previous methods, especially when high sequencing coverage is obtained per individual. It is described as a general, simple and efficient approach not requiring case-specific pooling schemes. The example shows it can work for small target regions of ~100 base pairs, not just large regions. And it shows good performance with small numbers of lanes and barcodes, not requiring very large numbers."}, "30": {"documentation": {"title": "Algorithmic Principles of Camera-based Respiratory Motion Extraction", "source": "Wenjin Wang, Albertus C. den Brinker", "docs_id": "2105.07537", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Algorithmic Principles of Camera-based Respiratory Motion Extraction. Measuring the respiratory signal from a video based on body motion has been proposed and recently matured in products for video health monitoring. The core algorithm for this measurement is the estimation of tiny chest/abdominal motions induced by respiration, and the fundamental challenge is motion sensitivity. Though prior arts reported on the validation with real human subjects, there is no thorough/rigorous benchmark to quantify the sensitivities and boundary conditions of motion-based core respiratory algorithms that measure sub-pixel displacement between video frames. In this paper, we designed a setup with a fully-controllable physical phantom to investigate the essence of core algorithms, together with a mathematical model incorporating two motion estimation strategies and three spatial representations, leading to six algorithmic combinations for respiratory signal extraction. Their promises and limitations are discussed and clarified via the phantom benchmark. The insights gained in this paper are intended to improve the understanding and applications of camera-based respiration measurement in health monitoring."}, "answer": " Question: What is the fundamental challenge in estimating respiratory motion from video according to the paper?\n\nA) Estimating chest expansion and contraction over time\nB) Achieving sufficient motion sensitivity to detect small displacements \nC) Tracking inherent biases in camera sensors over time\nD) Separating respiratory motion from other body motions\n\nCorrect Answer: B) Achieving sufficient motion sensitivity to detect small displacements\n\nExplanations:\nThe paper states that \"the fundamental challenge is motion sensitivity\" in estimating respiratory motion from video. Respiration induces small chest and abdominal motions that must be detected through sub-pixel displacement between video frames. Choice B matches this key point from the paper. Choices A, C, and D refer to other potential challenges but not the core one called out in the given summary."}, "31": {"documentation": {"title": "Flexible User Mapping for Radio Resource Assignment in Advanced\n  Satellite Payloads", "source": "Tom\\'as Ram\\'irez (1), Carlos Mosquera (1), Nader Alagha (2) ((1)\n  atlanTTic Research Center, Universidade de Vigo, Galicia, Spain, (2) European\n  Space Agency Technical Research Center (ESTEC), Noordwijk, The Netherlands)", "docs_id": "2109.09385", "section": ["cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Flexible User Mapping for Radio Resource Assignment in Advanced\n  Satellite Payloads. This work explores the flexible assignment of users to beams in order to match the non-uniform traffic demand in satellite systems, breaking the conventional cell boundaries and serving users potentially by non-dominant beams. The additional degree of freedom from the flexible beam-user mapping brings about a user-centric framework, with a two-step optimization process as a practical strategy. The smart beam-user mapping is jointly explored with adjustable bandwidth allocation per beam, and tested against different techniques for payloads with flexible radio resource allocation. Numerical results are obtained for various non-uniform traffic distributions to evaluate the performance of the solutions. The traffic profile across beams is shaped by the Dirichlet distribution, which can be conveniently parameterized, and makes simulations easily reproducible. Even with ideal conditions for the power allocation, both flexible beam-user mapping and adjustable power allocation similarly enhance the flexible assignment of the bandwidth on average. Results show that a smart pairing of users and beams provides significant advantages in highly asymmetric demand scenarios, with improvements up to 10\\% and 30\\% in terms of the offered and the minimum user rates, respectively, in hot-spot like cases with no impact on the user equipment."}, "answer": " Question: Which of the following statements is true about the flexible user mapping approach explored in the paper?\n\nA) It breaks conventional cell boundaries but serves users only by dominant beams.\n\nB) It provides a user-centric framework with a two-step optimization process. \n\nC) It jointly optimizes beam-user mapping and power allocation but not bandwidth allocation.\n\nD) It is tested against techniques with fixed radio resource allocation across beams.\n\nCorrect Answer: B\n\nExplanation: The paper explores a flexible approach to assigning users to beams, breaking conventional cell boundaries and potentially serving users by non-dominant beams. It proposes a user-centric framework with a two-step optimization process for beam-user mapping and bandwidth allocation. The approach is tested against techniques with fixed resource allocation, not flexible as in option D. It considers joint optimization of beam-user mapping and bandwidth allocation per beam, not just power allocation as in C. Users can be served by non-dominant beams, not just dominant beams as in A."}, "32": {"documentation": {"title": "Pursuing Open-Source Development of Predictive Algorithms: The Case of\n  Criminal Sentencing Algorithms", "source": "Philip D. Waggoner, Alec Macmillen", "docs_id": "2011.06422", "section": ["stat.AP", "cs.CY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Pursuing Open-Source Development of Predictive Algorithms: The Case of\n  Criminal Sentencing Algorithms. Currently, there is uncertainty surrounding the merits of open-source versus proprietary algorithm development. Though justification in favor of each exists, we argue that open-source algorithm development should be the standard in highly consequential contexts that affect people's lives for reasons of transparency and collaboration, which contribute to greater predictive accuracy and enjoy the additional advantage of cost-effectiveness. To make this case, we focus on criminal sentencing algorithms, as criminal sentencing is highly consequential, and impacts society and individual people. Further, the popularity of this topic has surged in the wake of recent studies uncovering racial bias in proprietary sentencing algorithms among other issues of over-fitting and model complexity. We suggest these issues are exacerbated by the proprietary and expensive nature of virtually all widely used criminal sentencing algorithms. Upon replicating a major algorithm using real criminal profiles, we fit three penalized regressions and demonstrate an increase in predictive power of these open-source and relatively computationally inexpensive options. The result is a data-driven suggestion that if judges who are making sentencing decisions want to craft appropriate sentences based on a high degree of accuracy and at low costs, then they should be pursuing open-source options."}, "answer": " Question: What does the paper argue is the best approach for developing criminal sentencing algorithms?\n\nA) Proprietary algorithms developed by private companies, as they have more resources to invest in improving accuracy.\n\nB) Open-source algorithms developed collaboratively, for greater transparency, accuracy, and cost-effectiveness. \n\nC) Algorithms should not be used at all for criminal sentencing due to concerns about bias.\n\nD) Simple regression models rather than more complex machine learning algorithms to avoid overfitting.\n\nCorrect Answer: B\n\nThe paper argues that open-source development should be the standard for criminal sentencing algorithms. This allows for greater collaboration and transparency, which contributes to improved accuracy. Open-source development is also more cost-effective. The paper demonstrates that simple penalized regression models developed openly can outperform proprietary algorithms."}, "33": {"documentation": {"title": "Exploiting Unlabeled Data in CNNs by Self-supervised Learning to Rank", "source": "Xialei Liu, Joost van de Weijer, Andrew D. Bagdanov", "docs_id": "1902.06285", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Exploiting Unlabeled Data in CNNs by Self-supervised Learning to Rank. For many applications the collection of labeled data is expensive laborious. Exploitation of unlabeled data during training is thus a long pursued objective of machine learning. Self-supervised learning addresses this by positing an auxiliary task (different, but related to the supervised task) for which data is abundantly available. In this paper, we show how ranking can be used as a proxy task for some regression problems. As another contribution, we propose an efficient backpropagation technique for Siamese networks which prevents the redundant computation introduced by the multi-branch network architecture. We apply our framework to two regression problems: Image Quality Assessment (IQA) and Crowd Counting. For both we show how to automatically generate ranked image sets from unlabeled data. Our results show that networks trained to regress to the ground truth targets for labeled data and to simultaneously learn to rank unlabeled data obtain significantly better, state-of-the-art results for both IQA and crowd counting. In addition, we show that measuring network uncertainty on the self-supervised proxy task is a good measure of informativeness of unlabeled data. This can be used to drive an algorithm for active learning and we show that this reduces labeling effort by up to 50%."}, "answer": " Here is a 4-option multiple choice question on the document:\n\nQuestion: Which of the following statements is true about the proposed method in the paper?\n\nA) It uses ranking as an auxiliary task for classification problems.\n\nB) It shows how to generate ranked image sets only for the crowd counting task. \n\nC) It obtains state-of-the-art results on both image quality assessment and crowd counting by using ranking as a proxy task.\n\nD) It uses a standard backpropagation technique without any modification for the Siamese network.\n\nCorrect Answer: C\n\nExplanation: According to the document, the proposed method shows how ranking can be used as an auxiliary/proxy task for some regression problems like image quality assessment and crowd counting. It obtains state-of-the-art results on both tasks by training the network to regress to ground truth targets on labeled data and simultaneously learn to rank unlabeled data."}, "34": {"documentation": {"title": "A Novel Multi-task Deep Learning Model for Skin Lesion Segmentation and\n  Classification", "source": "Xulei Yang, Zeng Zeng, Si Yong Yeo, Colin Tan, Hong Liang Tey, Yi Su", "docs_id": "1703.01025", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Novel Multi-task Deep Learning Model for Skin Lesion Segmentation and\n  Classification. In this study, a multi-task deep neural network is proposed for skin lesion analysis. The proposed multi-task learning model solves different tasks (e.g., lesion segmentation and two independent binary lesion classifications) at the same time by exploiting commonalities and differences across tasks. This results in improved learning efficiency and potential prediction accuracy for the task-specific models, when compared to training the individual models separately. The proposed multi-task deep learning model is trained and evaluated on the dermoscopic image sets from the International Skin Imaging Collaboration (ISIC) 2017 Challenge - Skin Lesion Analysis towards Melanoma Detection, which consists of 2000 training samples and 150 evaluation samples. The experimental results show that the proposed multi-task deep learning model achieves promising performances on skin lesion segmentation and classification. The average value of Jaccard index for lesion segmentation is 0.724, while the average values of area under the receiver operating characteristic curve (AUC) on two individual lesion classifications are 0.880 and 0.972, respectively."}, "answer": " Question: Which of the following statements is true about the multi-task deep learning model proposed for skin lesion analysis in the study?\n\nA) It performs worse than individual task-specific models trained separately.\n\nB) It exploits commonalities and differences across three tasks - segmentation, classification 1 and classification 2. \n\nC) It achieves average Jaccard index of 0.880 for lesion segmentation.\n\nD) It is trained and evaluated on ISIC 2018 Challenge dataset.\n\nCorrect Answer: B\n\nExplanations:\n\nA) Incorrect. The study states that the proposed multi-task model results in improved efficiency and potential accuracy compared to individual task-specific models.\n\nB) Correct. The study proposes a multi-task model that solves segmentation, classification 1 and classification 2 by exploiting commonalities and differences. \n\nC) Incorrect. The average Jaccard index achieved for segmentation is 0.724, not 0.880.\n\nD) Incorrect. The model is trained and evaluated on ISIC 2017 challenge dataset, not 2018."}, "35": {"documentation": {"title": "Learning to predict synchronization of coupled oscillators on\n  heterogeneous graphs", "source": "Hardeep Bassi, Richard Yim, Rohith Kodukula, Joshua Vendrow, Cherlin\n  Zhu, Hanbaek Lyu", "docs_id": "2012.14048", "section": ["math.DS", "cs.LG", "nlin.AO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Learning to predict synchronization of coupled oscillators on\n  heterogeneous graphs. Suppose we are given a system of coupled oscillators on an arbitrary graph along with the trajectory of the system during some period. Can we predict whether the system will eventually synchronize? This is an important but analytically intractable question especially when the structure of the underlying graph is highly varied. In this work, we take an entirely different approach that we call \"learning to predict synchronization\" (L2PSync), by viewing it as a classification problem for sets of graphs paired with initial dynamics into two classes: `synchronizing' or `non-synchronizing'. Our conclusion is that, once trained on large enough datasets of synchronizing and non-synchronizing dynamics on heterogeneous sets of graphs, a number of binary classification algorithms can successfully predict the future of an unknown system with surprising accuracy. We also propose an \"ensemble prediction\" algorithm that scales up our method to large graphs by training on dynamics observed from multiple random subgraphs. We find that in many instances, the first few iterations of the dynamics are far more important than the static features of the graphs. We demonstrate our method on three models of continuous and discrete coupled oscillators -- The Kuramoto model, the Firefly Cellular Automata, and the Greenberg-Hastings model."}, "answer": " Question: Which of the following statements is true about the paper \"Learning to predict synchronization of coupled oscillators on heterogeneous graphs\"?\n\nA) The paper proposes a new analytical method to predict synchronization of coupled oscillators that works for any graph structure.\n\nB) The paper views predicting synchronization as a classification problem and shows machine learning models can predict synchronization with high accuracy after training on enough data. \n\nC) The paper finds static graph features are more important than early dynamics for predicting synchronization.\n\nD) The paper shows the Kuramoto model always synchronizes regardless of the graph structure.\n\nCorrect Answer: B\n\nThe paper views predicting synchronization as a classification problem and trains machine learning models to predict synchronization. It does not propose an analytical method, find static features most important, or show the Kuramoto model always synchronizes."}, "36": {"documentation": {"title": "Social Graph Restoration via Random Walk Sampling", "source": "Kazuki Nakajima, Kazuyuki Shudo", "docs_id": "2111.11966", "section": ["cs.SI", "physics.soc-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Social Graph Restoration via Random Walk Sampling. Analyzing social graphs with limited data access is challenging for third-party researchers. To address this challenge, a number of algorithms that estimate the structural properties via a random walk have been developed. However, most existing algorithms are limited to the estimation of local structural properties. Here we propose a method for restoring the original social graph from the small sample obtained by a random walk. The proposed method generates a graph that preserves the estimates of local structural properties and the structure of the subgraph sampled by a random walk. We compare the proposed method with subgraph sampling using a crawling method and the existing method for generating a graph that structurally resembles the original graph via a random walk. Our experimental results show that the proposed method more accurately reproduces local and global structural properties on average and provides a better visual representation of the original graph than the compared methods. We expect that our method will lead to exhaustive analyses of social graphs with limited data access."}, "answer": " Here is a 4-option multiple choice question with explanations for the given documentation:\n\nQuestion: Which of the following statements is true about the proposed social graph restoration method described in the documentation?\n\nA) It estimates global structural properties of the graph directly from the random walk sample.\n\nB) It reproduces local structural properties less accurately than subgraph sampling methods on average. \n\nC) It generates a graph that preserves both local structure and global structure of the original graph.\n\nD) It provides a worse visual representation of the original graph compared to existing methods.\n\nCorrect Answer: C\n\nExplanations:\n\nA) Incorrect. The documentation states that most existing algorithms using random walks are limited to estimating local structural properties. The proposed method aims to restore the original graph structure, including both local and global properties.\n\nB) Incorrect. The documentation states that experimental results show the proposed method more accurately reproduces local structural properties on average compared to other methods. \n\nC) Correct. The documentation states that the proposed method generates a graph that preserves estimates of local structural properties and the global structure sampled by the random walk.\n\nD) Incorrect. The documentation states that experimental results show the proposed method provides a better visual representation of the original graph compared to existing methods."}, "37": {"documentation": {"title": "Epidemiological data challenges: planning for a more robust future\n  through data standards", "source": "Geoffrey Fairchild (1), Byron Tasseff (1), Hari Khalsa (1), Nicholas\n  Generous (2), Ashlynn R. Daughton (1), Nileena Velappan (3), Reid Priedhorsky\n  (4), Alina Deshpande (3) ((1) Analytics, Intelligence, and Technology\n  Division, Los Alamos National Laboratory, Los Alamos, New Mexico, USA, (2)\n  Intelligence and Emerging Threats Program Office, Los Alamos National\n  Laboratory, Los Alamos, New Mexico, USA, (3) Bioscience Division, Los Alamos\n  National Laboratory, Los Alamos, New Mexico, USA, (4) High Performance\n  Computing Division, Los Alamos National Laboratory, Los Alamos, New Mexico,\n  USA)", "docs_id": "1805.00445", "section": ["cs.CY", "cs.IR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Epidemiological data challenges: planning for a more robust future\n  through data standards. Accessible epidemiological data are of great value for emergency preparedness and response, understanding disease progression through a population, and building statistical and mechanistic disease models that enable forecasting. The status quo, however, renders acquiring and using such data difficult in practice. In many cases, a primary way of obtaining epidemiological data is through the internet, but the methods by which the data are presented to the public often differ drastically among institutions. As a result, there is a strong need for better data sharing practices. This paper identifies, in detail and with examples, the three key challenges one encounters when attempting to acquire and use epidemiological data: 1) interfaces, 2) data formatting, and 3) reporting. These challenges are used to provide suggestions and guidance for improvement as these systems evolve in the future. If these suggested data and interface recommendations were adhered to, epidemiological and public health analysis, modeling, and informatics work would be significantly streamlined, which can in turn yield better public health decision-making capabilities."}, "answer": " Here is a 4-option multiple choice question based on the given text:\n\nQuestion: Which of the following is NOT identified as one of the three key challenges encountered when attempting to acquire and use epidemiological data?\n\nA) Reporting inconsistencies \n\nB) Interface design issues\n\nC) Data security protocols\n\nD) Data formatting differences\n\nCorrect Answer: C) Data security protocols\n\nExplanation: The passage identifies the three key challenges as interfaces, data formatting, and reporting. Data security protocols are not mentioned as one of the three main challenges, so C) is the correct answer. The passage states the challenges are interfaces, data formatting, and reporting."}, "38": {"documentation": {"title": "Reweighted Proximal Pruning for Large-Scale Language Representation", "source": "Fu-Ming Guo, Sijia Liu, Finlay S. Mungall, Xue Lin and Yanzhi Wang", "docs_id": "1909.12486", "section": ["cs.LG", "cs.CL", "cs.NE", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Reweighted Proximal Pruning for Large-Scale Language Representation. Recently, pre-trained language representation flourishes as the mainstay of the natural language understanding community, e.g., BERT. These pre-trained language representations can create state-of-the-art results on a wide range of downstream tasks. Along with continuous significant performance improvement, the size and complexity of these pre-trained neural models continue to increase rapidly. Is it possible to compress these large-scale language representation models? How will the pruned language representation affect the downstream multi-task transfer learning objectives? In this paper, we propose Reweighted Proximal Pruning (RPP), a new pruning method specifically designed for a large-scale language representation model. Through experiments on SQuAD and the GLUE benchmark suite, we show that proximal pruned BERT keeps high accuracy for both the pre-training task and the downstream multiple fine-tuning tasks at high prune ratio. RPP provides a new perspective to help us analyze what large-scale language representation might learn. Additionally, RPP makes it possible to deploy a large state-of-the-art language representation model such as BERT on a series of distinct devices (e.g., online servers, mobile phones, and edge devices)."}, "answer": " Question: What method is proposed in the paper to compress large-scale language representation models while maintaining high accuracy on downstream tasks?\n\nA) Weighted Proximal Retraining \nB) Reweighted Proximal Pruning\nC) Quantized Proximal Compression\nD) Layerwise Magnitude Pruning\n\nCorrect Answer: B) Reweighted Proximal Pruning\n\nExplanation: The paper proposes Reweighted Proximal Pruning (RPP), a new pruning method designed specifically for compressing large-scale language representation models like BERT. Through experiments, RPP is shown to keep high accuracy on downstream tasks like SQuAD and GLUE even at high prune ratios."}, "39": {"documentation": {"title": "A Novel Paradigm for Calculating Ramsey Number via Artificial Bee Colony\n  Algorithm", "source": "Wei-Hao Mao, Fei Gao, Yi-Jin Dong, Wen-Ming Li", "docs_id": "1512.01613", "section": ["cs.AI", "cs.NE", "math.CO", "math.OC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Novel Paradigm for Calculating Ramsey Number via Artificial Bee Colony\n  Algorithm. The Ramsey number is of vital importance in Ramsey's theorem. This paper proposed a novel methodology for constructing Ramsey graphs about R(3,10), which uses Artificial Bee Colony optimization(ABC) to raise the lower bound of Ramsey number R(3,10). The r(3,10)-graph contains two limitations, that is, neither complete graphs of order 3 nor independent sets of order 10. To resolve these limitations, a special mathematical model is put in the paradigm to convert the problems into discrete optimization whose smaller minimizers are correspondent to bigger lower bound as approximation of inf R(3,10). To demonstrate the potential of the proposed method, simulations are done to to minimize the amount of these two types of graphs. For the first time, four r(3,9,39) graphs with best approximation for inf R(3,10) are reported in simulations to support the current lower bound for R(3,10). The experiments' results show that the proposed paradigm for Ramsey number's calculation driven by ABC is a successful method with the advantages of high precision and robustness."}, "answer": " Here is a multi-form question based on the given text:\n\nQuestion: Which of the following statements is true about the novel paradigm proposed in the paper for calculating Ramsey numbers using Artificial Bee Colony optimization?\n\nA) It uses ABC optimization to find the exact value of R(3,10).\n\nB) It converts the problem of avoiding complete graphs of order 3 and independent sets of order 10 into a discrete optimization problem. \n\nC) It achieves the current best lower bound for R(3,9,39).\n\nD) It finds the optimal r(3,10)-graphs containing the most number of complete graphs of order 3.\n\nCorrect Answer: B\n\nExplanation: The paper proposes using ABC optimization to minimize the number of complete graphs of order 3 and independent sets of order 10 in r(3,10)-graphs, in order to raise the lower bound approximation for R(3,10). It does not find the exact Ramsey number or optimal graphs, but converts the constraints into a discrete optimization problem solved by ABC."}, "40": {"documentation": {"title": "Risk-Based Optimization of Virtual Reality over Terahertz Reconfigurable\n  Intelligent Surfaces", "source": "Christina Chaccour, Mehdi Naderi Soorki, Walid Saad, Mehdi Bennis,\n  Petar Popovski", "docs_id": "2002.09052", "section": ["cs.IT", "eess.SP", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Risk-Based Optimization of Virtual Reality over Terahertz Reconfigurable\n  Intelligent Surfaces. In this paper, the problem of associating reconfigurable intelligent surfaces (RISs) to virtual reality (VR) users is studied for a wireless VR network. In particular, this problem is considered within a cellular network that employs terahertz (THz) operated RISs acting as base stations. To provide a seamless VR experience, high data rates and reliable low latency need to be continuously guaranteed. To address these challenges, a novel risk-based framework based on the entropic value-at-risk is proposed for rate optimization and reliability performance. Furthermore, a Lyapunov optimization technique is used to reformulate the problem as a linear weighted function, while ensuring that higher order statistics of the queue length are maintained under a threshold. To address this problem, given the stochastic nature of the channel, a policy-based reinforcement learning (RL) algorithm is proposed. Since the state space is extremely large, the policy is learned through a deep-RL algorithm. In particular, a recurrent neural network (RNN) RL framework is proposed to capture the dynamic channel behavior and improve the speed of conventional RL policy-search algorithms. Simulation results demonstrate that the maximal queue length resulting from the proposed approach is only within 1% of the optimal solution. The results show a high accuracy and fast convergence for the RNN with a validation accuracy of 91.92%."}, "answer": " Here is a 4-option multiple choice question on the paper:\n\nQuestion: Which of the following statements is true about the proposed framework in the paper?\n\nA) It uses a model-free reinforcement learning algorithm based on policy gradients to learn the RIS association policy.\n\nB) It formulates the rate optimization problem as a linear program by using Lyapunov optimization. \n\nC) It employs a value-at-risk constraint to ensure queue stability by bounding the 99.99th percentile of queue lengths.\n\nD) It achieves near-optimal performance but takes a long time to converge due to the large state space.\n\nCorrect Answer: C\n\nExplanation: \nThe paper proposes a risk-based optimization framework using the entropic value-at-risk to constrain the queue length statistics. Lyapunov optimization is used to reformulate the problem into a tractable form. A recurrent neural network based reinforcement learning algorithm is proposed to learn the RIS association policy to deal with the large state space."}, "41": {"documentation": {"title": "Fault Tolerance for Remote Memory Access Programming Models", "source": "Maciej Besta, Torsten Hoefler", "docs_id": "2010.09025", "section": ["cs.DC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Fault Tolerance for Remote Memory Access Programming Models. Remote Memory Access (RMA) is an emerging mechanism for programming high-performance computers and datacenters. However, little work exists on resilience schemes for RMA-based applications and systems. In this paper we analyze fault tolerance for RMA and show that it is fundamentally different from resilience mechanisms targeting the message passing (MP) model. We design a model for reasoning about fault tolerance for RMA, addressing both flat and hierarchical hardware. We use this model to construct several highly-scalable mechanisms that provide efficient low-overhead in-memory checkpointing, transparent logging of remote memory accesses, and a scheme for transparent recovery of failed processes. Our protocols take into account diminishing amounts of memory per core, one of major features of future exascale machines. The implementation of our fault-tolerance scheme entails negligible additional overheads. Our reliability model shows that in-memory checkpointing and logging provide high resilience. This study enables highly-scalable resilience mechanisms for RMA and fills a research gap between fault tolerance and emerging RMA programming models."}, "answer": " Question: Which of the following statements is true about the paper \"Fault Tolerance for Remote Memory Access Programming Models\"?\n\nA) The paper proposes new mechanisms for fault tolerance targeting the message passing programming model.\n\nB) The paper fills a research gap by providing the first analysis of fault tolerance for RMA programming models. \n\nC) The paper shows that existing fault tolerance mechanisms for message passing can be directly applied to RMA.\n\nD) The paper focuses on implementing transparent checkpointing and logging for MPI applications.\n\nCorrect Answer: B\n\nExplanation: The paper analyzes fault tolerance specifically for RMA programming models and proposes new mechanisms tailored for RMA. It aims to fill the research gap in this area, as previous fault tolerance work targeted message passing models. The paper does not simply apply existing MP mechanisms or focus on MPI applications."}, "42": {"documentation": {"title": "Multimodal Recurrent Neural Networks with Information Transfer Layers\n  for Indoor Scene Labeling", "source": "Abrar H. Abdulnabi, Bing Shuai, Zhen Zuo, Lap-Pui Chau, Gang Wang", "docs_id": "1803.04687", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Multimodal Recurrent Neural Networks with Information Transfer Layers\n  for Indoor Scene Labeling. This paper proposes a new method called Multimodal RNNs for RGB-D scene semantic segmentation. It is optimized to classify image pixels given two input sources: RGB color channels and Depth maps. It simultaneously performs training of two recurrent neural networks (RNNs) that are crossly connected through information transfer layers, which are learnt to adaptively extract relevant cross-modality features. Each RNN model learns its representations from its own previous hidden states and transferred patterns from the other RNNs previous hidden states; thus, both model-specific and crossmodality features are retained. We exploit the structure of quad-directional 2D-RNNs to model the short and long range contextual information in the 2D input image. We carefully designed various baselines to efficiently examine our proposed model structure. We test our Multimodal RNNs method on popular RGB-D benchmarks and show how it outperforms previous methods significantly and achieves competitive results with other state-of-the-art works."}, "answer": " Here is a 4-option multiple choice question on the paper:\n\nQuestion: Which of the following statements is true about the Multimodal RNNs method proposed in the paper?\n\nA) It uses convolutional neural networks to extract features from the RGB and depth input.\n\nB) It consists of a single RNN that takes as input the concatenated RGB and depth data. \n\nC) It simultaneously trains two separate RNNs for RGB and depth that are connected via information transfer layers.\n\nD) It uses a standard RNN architecture without any modifications to model long-range dependencies.\n\nCorrect Answer: C\n\nExplanation: The paper proposes Multimodal RNNs that simultaneously train two RNNs for RGB and depth input. These RNNs are connected via information transfer layers that allow them to share information. The method does not use CNNs for feature extraction and does not concatenate the modalities into a single RNN. It also uses quad-directional RNNs to capture long-range context."}, "43": {"documentation": {"title": "MoEVC: A Mixture-of-experts Voice Conversion System with Sparse Gating\n  Mechanism for Accelerating Online Computation", "source": "Yu-Tao Chang, Yuan-Hong Yang, Yu-Huai Peng, Syu-Siang Wang, Tai-Shih\n  Chi, Yu Tsao, Hsin-Min Wang", "docs_id": "1912.11984", "section": ["cs.SD", "eess.AS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "MoEVC: A Mixture-of-experts Voice Conversion System with Sparse Gating\n  Mechanism for Accelerating Online Computation. With the recent advancements of deep learning technologies, the performance of voice conversion (VC) in terms of quality and similarity has been significantly improved. However, heavy computations are generally required for deep-learning-based VC systems, which can cause notable latency and thus confine their deployments in real-world applications. Therefore, increasing online computation efficiency has become an important task. In this study, we propose a novel mixture-of-experts (MoE) based VC system. The MoE model uses a gating mechanism to specify optimal weights to feature maps to increase VC performance. In addition, assigning sparse constraints on the gating mechanism can accelerate online computation by skipping the convolution process by zeroing out redundant feature maps. Experimental results show that by specifying suitable sparse constraints, we can effectively increase the online computation efficiency with a notable 70% FLOPs (floating-point operations per second) reduction while improving the VC performance in both objective evaluations and human listening tests."}, "answer": " Question: Which of the following statements is true about the proposed MoEVC voice conversion system described in the passage?\n\nA) It uses a dense gating mechanism that increases computation time but improves voice conversion performance.\n\nB) It assigns sparse constraints to the gating mechanism to skip redundant convolutions and accelerate online computation. \n\nC) It confines the use of deep learning technologies to improve voice conversion quality and similarity.\n\nD) It focuses only on improving objective evaluations and does not conduct human listening tests.\n\nCorrect Answer: B\n\nExplanation: The passage states that the MoEVC voice conversion system uses a sparse gating mechanism to specify weights and accelerate online computation by skipping redundant convolutions. This allows a 70% reduction in FLOPs while improving performance. Human listening tests were also conducted to evaluate the system."}, "44": {"documentation": {"title": "Efficient $\\mathbb{Z}_2$ synchronization on $\\mathbb{Z}^d$ under\n  symmetry-preserving side information", "source": "Ahmed El Alaoui", "docs_id": "2106.02111", "section": ["math.PR", "cs.IT", "math.IT", "math.ST", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Efficient $\\mathbb{Z}_2$ synchronization on $\\mathbb{Z}^d$ under\n  symmetry-preserving side information. We consider $\\mathbb{Z}_2$-synchronization on the Euclidean lattice. Every vertex of $\\mathbb{Z}^d$ is assigned an independent symmetric random sign $\\theta_u$, and for every edge $(u,v)$ of the lattice, one observes the product $\\theta_u\\theta_v$ flipped independently with probability $p$. The task is to reconstruct products $\\theta_u\\theta_v$ for pairs of vertices $u$ and $v$ which are arbitrarily far apart. Abb\\'e, Massouli\\'e, Montanari, Sly and Srivastava (2018) showed that synchronization is possible if and only if $p$ is below a critical threshold $\\tilde{p}_c(d)$, and efficiently so for $p$ small enough. We augment this synchronization setting with a model of side information preserving the sign symmetry of $\\theta$, and propose an \\emph{efficient} algorithm which synchronizes a randomly chosen pair of far away vertices on average, up to a differently defined critical threshold $p_c(d)$. We conjecture that $ p_c(d)=\\tilde{p}_c(d)$ for all $d \\ge 2$. Our strategy is to \\emph{renormalize} the synchronization model in order to reduce the effective noise parameter, and then apply a variant of the multiscale algorithm of AMMSS. The success of the renormalization procedure is conditional on a plausible but unproved assumption about the regularity of the free energy of an Ising spin glass model on $\\mathbb{Z}^d$."}, "answer": " Here is a multi-form question based on the given text:\n\nQuestion: Which of the following statements is true about the results presented in the paper by Abb\u00e9 et al. (2018)?\n\nA) They showed that synchronization is possible if and only if $p$ is above a critical threshold $\\tilde{p}_c(d)$.\n\nB) They proposed an efficient algorithm that synchronizes vertices up to a threshold $p_c(d) \\neq \\tilde{p}_c(d)$. \n\nC) They conjectured that $p_c(d) = \\tilde{p}_c(d)$ for all $d \\geq 2$.\n\nD) Their algorithm works for all values of the noise parameter $p$.\n\nCorrect Answer: C\n\nExplanation: The text states that Abb\u00e9 et al. showed synchronization is possible below a threshold $\\tilde{p}_c(d)$. The new paper proposes an algorithm that works up to a different threshold $p_c(d)$ and conjectures the two thresholds are equal for all $d \\geq 2$."}, "45": {"documentation": {"title": "Optimal Sensor Gain Control for Minimum-Information Estimation of\n  Continuous-Time Gauss-Markov Processes", "source": "Vrushabh Zinage, Takashi Tanaka and Valeri Ugrinovskii", "docs_id": "2109.13854", "section": ["eess.SY", "cs.SY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Optimal Sensor Gain Control for Minimum-Information Estimation of\n  Continuous-Time Gauss-Markov Processes. We consider the scenario in which a continuous-time Gauss-Markov process is estimated by the Kalman-Bucy filter over a Gaussian channel (sensor) with a variable sensor gain. The problem of scheduling the sensor gain over a finite time interval to minimize the weighted sum of the data rate (the mutual information between the sensor output and the underlying Gauss-Markov process) and the distortion (the mean-square estimation error) is formulated as an optimal control problem. A necessary optimality condition for a scheduled sensor gain is derived based on Pontryagin's minimum principle. For a scalar problem, we show that an optimal sensor gain control is of bang-bang type, except the possibility of taking an intermediate value when there exists a stationary point on the switching surface in the phase space of canonical dynamics. Furthermore, we show that the number of switches is at most two and the time instants at which the optimal gain must be switched can be computed from the analytical solutions to the canonical equations."}, "answer": " Here is a 4-option multiple choice question on the paper:\n\nQuestion: Which of the following statements is true about the optimal sensor gain control derived in the paper for a scalar estimation problem?\n\nA) The optimal gain is constant over the entire time interval.\n\nB) The optimal gain switches between two values at most twice over the time interval. \n\nC) The optimal gain can switch between multiple values more than twice over the time interval.\n\nD) The time instants when the optimal gain switches can be determined without solving the canonical equations.\n\nCorrect Answer: B\n\nExplanation: According to the paper, for a scalar estimation problem, the optimal sensor gain control is of bang-bang type, with at most two switches. The time instants when the optimal gain must switch can be computed by solving the analytical canonical equations."}, "46": {"documentation": {"title": "Coverage in mmWave Cellular Networks with Base station Cooperation", "source": "Diana Maamari, Natasha Devroye, Daniela Tuninetti", "docs_id": "1503.05269", "section": ["cs.IT", "cs.NI", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Coverage in mmWave Cellular Networks with Base station Cooperation. The presence of signal outage, due to shadowing and blockage, is expected to be the main bottleneck in millimeter wave (mmWave) networks. Moreover, with the anticipated vision that mmWave networks would have a dense deployment of base stations, interference from strong line-of-sight base stations increases too, thus further increasing the probability of outage. To address the issue of reducing outage, this paper explores the possibility of base station cooperation in the downlink of a mmWave heterogenous network. The main focus of this work is showing that, in a stochastic geometry framework, cooperation from randomly located base stations decreases outage probability. With the presumed vision that less severe fading will be experienced due to highly directional transmissions, one might expect that cooperation would increase the coverage probability; our numerical examples suggest that is in fact the case. Coverage probabilities are derived accounting for: different fading distributions, antenna directionality and blockage. Numerical results suggest that coverage with base station cooperation in dense mmWave systems and with no small scale fading considerably exceeds coverage with no cooperation. In contrast, an insignificant increase is reported when mmWave networks are less dense with a high probability of signal blockage and with Rayleigh fading."}, "answer": " Here is a 4-option multiple choice question on the key findings from the Arxiv paper on mmWave cellular networks with base station cooperation:\n\nQuestion: According to the paper, which of the following statements is true about the impact of base station cooperation on coverage probability in mmWave cellular networks?\n\nA) Cooperation provides negligible coverage gains in dense networks with no small-scale fading.\nB) Cooperation significantly improves coverage in sparse networks with high blockage probability.  \nC) Cooperation reduces coverage probability in all network scenarios considered.\nD) Cooperation increases coverage probability especially in dense networks with low small-scale fading.\n\nCorrect Answer: D\n\nExplanation: The paper finds that base station cooperation provides significant coverage probability gains in dense mmWave networks with directional transmissions and negligible small-scale fading. However, the gains are insignificant in sparse networks with high blockage probability and Rayleigh fading."}, "47": {"documentation": {"title": "Attention Based Real Image Restoration", "source": "Saeed Anwar, Nick Barnes, and Lars Petersson", "docs_id": "2004.13524", "section": ["cs.CV", "cs.LG", "eess.IV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Attention Based Real Image Restoration. Deep convolutional neural networks perform better on images containing spatially invariant degradations, also known as synthetic degradations; however, their performance is limited on real-degraded photographs and requires multiple-stage network modeling. To advance the practicability of restoration algorithms, this paper proposes a novel single-stage blind real image restoration network (R$^2$Net) by employing a modular architecture. We use a residual on the residual structure to ease the flow of low-frequency information and apply feature attention to exploit the channel dependencies. Furthermore, the evaluation in terms of quantitative metrics and visual quality for four restoration tasks i.e. Denoising, Super-resolution, Raindrop Removal, and JPEG Compression on 11 real degraded datasets against more than 30 state-of-the-art algorithms demonstrate the superiority of our R$^2$Net. We also present the comparison on three synthetically generated degraded datasets for denoising to showcase the capability of our method on synthetics denoising. The codes, trained models, and results are available on https://github.com/saeed-anwar/R2Net."}, "answer": " Question: What type of neural networks does the paper propose to improve performance on real-degraded photographs compared to synthetic degradations?\n\nA) Convolutional neural networks with multiple stages\nB) Single-stage blind real image restoration networks \nC) Residual neural networks with attention mechanisms\nD) Generative adversarial networks for image restoration\n\nCorrect Answer: B) Single-stage blind real image restoration networks\n\nExplanation: The paper proposes a novel single-stage blind real image restoration network (R^2Net) to improve performance on real-degraded photographs compared to synthetic degradations, for which deep convolutional neural networks perform better. The other options are not mentioned in the summary."}, "48": {"documentation": {"title": "Gaussian Process-based Min-norm Stabilizing Controller for\n  Control-Affine Systems with Uncertain Input Effects and Dynamics", "source": "Fernando Casta\\~neda, Jason J. Choi, Bike Zhang, Claire J. Tomlin and\n  Koushil Sreenath", "docs_id": "2011.07183", "section": ["eess.SY", "cs.LG", "cs.SY", "math.OC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Gaussian Process-based Min-norm Stabilizing Controller for\n  Control-Affine Systems with Uncertain Input Effects and Dynamics. This paper presents a method to design a min-norm Control Lyapunov Function (CLF)-based stabilizing controller for a control-affine system with uncertain dynamics using Gaussian Process (GP) regression. In order to estimate both state and input-dependent model uncertainty, we propose a novel compound kernel that captures the control-affine nature of the problem. Furthermore, by the use of GP Upper Confidence Bound analysis, we provide probabilistic bounds of the regression error, leading to the formulation of a CLF-based stability chance constraint which can be incorporated in a min-norm optimization problem. We show that this resulting optimization problem is convex, and we call it Gaussian Process-based Control Lyapunov Function Second-Order Cone Program (GP-CLF-SOCP). The data-collection process and the training of the GP regression model are carried out in an episodic learning fashion. We validate the proposed algorithm and controller in numerical simulations of an inverted pendulum and a kinematic bicycle model, resulting in stable trajectories which are very similar to the ones obtained if we actually knew the true plant dynamics."}, "answer": " Here is a multi-form question on the paper:\n\nQuestion: Which of the following statements is true about the Gaussian Process-based min-norm stabilizing controller presented in the paper?\n\nA) It uses a novel exponential kernel to capture the control-affine nature of the system.\n\nB) It provides probabilistic bounds on the regression error using Gaussian Process Lower Confidence Bound analysis. \n\nC) The resulting optimization problem is non-convex and solved using sequential quadratic programming.\n\nD) Data collection and GP model training are done simultaneously with the control policy execution.\n\nCorrect Answer: B\n\nExplanation: \nThe paper proposes a compound kernel to capture the control-affine structure, not an exponential kernel. It uses Gaussian Process Upper Confidence Bound analysis to provide probabilistic bounds on regression error. The resulting GP-CLF-SOCP optimization problem is convex. The data collection and GP training are done episodically, not simultaneously with control execution."}, "49": {"documentation": {"title": "Expander Datacenters: From Theory to Practice", "source": "Vipul Harsh, Sangeetha Abdu Jyothi, Inderdeep Singh, P. Brighten\n  Godfrey", "docs_id": "1811.00212", "section": ["cs.NI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Expander Datacenters: From Theory to Practice. Recent work has shown that expander-based data center topologies are robust and can yield superior performance over Clos topologies. However, to achieve these benefits, previous proposals use routing and transport schemes that impede quick industry adoption. In this paper, we examine if expanders can be effective for the technology and environments practical in today's data centers, including the use of traditional protocols, at both small and large scale while complying with common practices such as over-subscription. We study bandwidth, latency and burst tolerance of topologies, highlighting pitfalls of previous topology comparisons. We consider several other metrics of interest: packet loss during failures, queue occupancy and topology degradation. Our experiments show that expanders can realize 3x more throughput than an equivalent fat tree, and 1.5x more throughput than an equivalent leaf-spine topology, for a wide range of scenarios, with only traditional protocols. We observe that expanders achieve lower flow completion times, are more resilient to bursty load conditions like incast and outcast and degrade more gracefully with increasing load. Our results are based on extensive simulations and experiments on a hardware testbed with realistic topologies and real traffic patterns."}, "answer": " Here is a 4-option multiple choice question on the paper:\n\nQuestion: Which of the following statements is supported by the results in the paper on expander data center topologies?\n\nA) Expanders achieve worse flow completion times compared to fat trees.\n\nB) Expanders can yield 3x more throughput than equivalent fat trees under realistic conditions. \n\nC) Expanders are not resilient to bursty traffic patterns like incast.\n\nD) Routing and transport schemes proposed enable quick industry adoption.\n\nCorrect Answer: B) Expanders can yield 3x more throughput than equivalent fat trees under realistic conditions.\n\nThe paper shows through simulations and testbed experiments that expander topologies can achieve significantly higher throughput compared to fat trees and leaf-spine topologies using traditional protocols. It also finds expanders to be more resilient to bursty traffic than other topologies. However, it mentions the routing schemes used impede quick industry adoption."}, "50": {"documentation": {"title": "Topic Modeling on Health Journals with Regularized Variational Inference", "source": "Robert Giaquinto and Arindam Banerjee", "docs_id": "1801.04958", "section": ["cs.CL", "cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Topic Modeling on Health Journals with Regularized Variational Inference. Topic modeling enables exploration and compact representation of a corpus. The CaringBridge (CB) dataset is a massive collection of journals written by patients and caregivers during a health crisis. Topic modeling on the CB dataset, however, is challenging due to the asynchronous nature of multiple authors writing about their health journeys. To overcome this challenge we introduce the Dynamic Author-Persona topic model (DAP), a probabilistic graphical model designed for temporal corpora with multiple authors. The novelty of the DAP model lies in its representation of authors by a persona --- where personas capture the propensity to write about certain topics over time. Further, we present a regularized variational inference algorithm, which we use to encourage the DAP model's personas to be distinct. Our results show significant improvements over competing topic models --- particularly after regularization, and highlight the DAP model's unique ability to capture common journeys shared by different authors."}, "answer": " Question: What is the key novelty of the Dynamic Author-Persona (DAP) topic model presented in the paper?\n\nA) It uses a persona to represent an author's propensity to write about certain topics over time.\n\nB) It uses regularized variational inference to encourage distinct personas. \n\nC) It is designed specifically for the CaringBridge dataset.\n\nD) It outperforms competing topic models on temporal corpora.\n\nCorrect Answer: A\n\nExplanation: The key novelty of the DAP model is its use of a \"persona\" to represent an author's tendency to write about certain topics over time, as stated in the passage. While B, C, and D are also true based on the passage, they are not the key novelty of the model."}, "51": {"documentation": {"title": "A Sparse Linear Model and Significance Test for Individual Consumption\n  Prediction", "source": "Pan Li, Baosen Zhang, Yang Weng, Ram Rajagopal", "docs_id": "1511.01853", "section": ["stat.ML", "cs.SY", "math.OC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Sparse Linear Model and Significance Test for Individual Consumption\n  Prediction. Accurate prediction of user consumption is a key part not only in understanding consumer flexibility and behavior patterns, but in the design of robust and efficient energy saving programs as well. Existing prediction methods usually have high relative errors that can be larger than 30% and have difficulties accounting for heterogeneity between individual users. In this paper, we propose a method to improve prediction accuracy of individual users by adaptively exploring sparsity in historical data and leveraging predictive relationship between different users. Sparsity is captured by popular least absolute shrinkage and selection estimator, while user selection is formulated as an optimal hypothesis testing problem and solved via a covariance test. Using real world data from PG&E, we provide extensive simulation validation of the proposed method against well-known techniques such as support vector machine, principle component analysis combined with linear regression, and random forest. The results demonstrate that our proposed methods are operationally efficient because of linear nature, and achieve optimal prediction performance."}, "answer": " Question: What method is proposed in the paper to improve prediction accuracy of individual users?\n\nA) Using support vector machine, principle component analysis combined with linear regression, and random forest\nB) Leveraging sparsity in historical data and predictive relationship between different users\nC) Applying least absolute shrinkage and selection estimator and solving an optimal hypothesis testing problem \nD) Exploring heterogeneity between individual users with relative errors larger than 30%\n\nCorrect Answer: C\n\nExplanation: The paper proposes a method to improve prediction accuracy of individual users by adaptively exploring sparsity in historical data and leveraging predictive relationship between different users. Sparsity is captured by the least absolute shrinkage and selection estimator, while user selection is formulated as an optimal hypothesis testing problem and solved via a covariance test."}, "52": {"documentation": {"title": "Sometimes, Convex Separable Optimization Is Much Harder than Linear\n  Optimization, and Other Surprises", "source": "Cornelius Brand, Martin Kouteck\\'y, Alexandra Lassota, Sebastian\n  Ordyniak", "docs_id": "2111.08048", "section": ["cs.DM", "math.OC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Sometimes, Convex Separable Optimization Is Much Harder than Linear\n  Optimization, and Other Surprises. An influential 1990 paper of Hochbaum and Shanthikumar made it common wisdom that \"convex separable optimization is not much harder than linear optimization\" [JACM 1990]. We exhibit two fundamental classes of mixed integer (linear) programs that run counter this intuition. Namely those whose constraint matrices have small coefficients and small primal or dual treedepth: While linear optimization is easy [Brand, Kouteck\\'y, Ordyniak, AAAI 2021], we prove that separable convex optimization IS much harder. Moreover, in the pure integer and mixed integer linear cases, these two classes have the same parameterized complexity. We show that they yet behave quite differently in the separable convex mixed integer case. Our approach employs the mixed Graver basis introduced by Hemmecke [Math. Prog. 2003]. We give the first non-trivial lower and upper bounds on the norm of mixed Graver basis elements. In previous works involving the integer Graver basis, such upper bounds have consistently resulted in efficient algorithms for integer programming. Curiously, this does not happen in our case. In fact, we even rule out such an algorithm."}, "answer": " Here is a 4-option multiple choice question on the given Arxiv documentation:\n\nQuestion: Which of the following statements is true about the findings in the paper \"Sometimes, Convex Separable Optimization Is Much Harder than Linear Optimization, and Other Surprises\"?\n\nA) The paper shows that convex separable optimization is not much harder than linear optimization, confirming the intuition from a 1990 paper by Hochbaum and Shanthikumar.\n\nB) The paper introduces two classes of mixed integer linear programs with small coefficients and treedepth where linear optimization is easy but convex separable optimization is much harder. \n\nC) The paper proves that convex separable mixed integer optimization has the same parameterized complexity as pure integer and mixed integer linear optimization for all classes of problems.\n\nD) The paper shows how to use mixed Graver bases to obtain efficient algorithms for convex separable mixed integer optimization, paralleling previous successes with the integer Graver basis.\n\nCorrect Answer: B\n\nExplanation: The paper introduces two specific classes of mixed integer linear programs where linear optimization is easy but convex separable optimization is much harder, contradicting the prior intuition that the two are similarly easy. The other statements are not accurate based on the given documentation."}, "53": {"documentation": {"title": "A Semantic approach for effective document clustering using WordNet", "source": "Leena H. Patil, Mohammed Atique", "docs_id": "1303.0489", "section": ["cs.CL", "cs.IR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Semantic approach for effective document clustering using WordNet. Now a days, the text document is spontaneously increasing over the internet, e-mail and web pages and they are stored in the electronic database format. To arrange and browse the document it becomes difficult. To overcome such problem the document preprocessing, term selection, attribute reduction and maintaining the relationship between the important terms using background knowledge, WordNet, becomes an important parameters in data mining. In these paper the different stages are formed, firstly the document preprocessing is done by removing stop words, stemming is performed using porter stemmer algorithm, word net thesaurus is applied for maintaining relationship between the important terms, global unique words, and frequent word sets get generated, Secondly, data matrix is formed, and thirdly terms are extracted from the documents by using term selection approaches tf-idf, tf-df, and tf2 based on their minimum threshold value. Further each and every document terms gets preprocessed, where the frequency of each term within the document is counted for representation. The purpose of this approach is to reduce the attributes and find the effective term selection method using WordNet for better clustering accuracy. Experiments are evaluated on Reuters Transcription Subsets, wheat, trade, money grain, and ship, Reuters 21578, Classic 30, 20 News group (atheism), 20 News group (Hardware), 20 News group (Computer Graphics) etc."}, "answer": " Here is a multi-form question based on the given text:\n\nQuestion: Which of the following is NOT a step in the document preprocessing and clustering approach described in the text?\n\nA) Removing stop words and stemming words using the Porter stemmer algorithm. \n\nB) Forming a data matrix and extracting terms using TF-IDF, TF-DF, and TF2 term selection.\n\nC) Using WordNet to find relationships between words and generate global unique words.\n\nD) Randomly shuffling the order of documents before clustering.\n\nCorrect Answer: D\n\nExplanation: The text describes a 3-step approach of 1) document preprocessing by removing stop words and stemming, 2) forming a data matrix and term extraction using TF-IDF, TF-DF, and TF2, and 3) using WordNet to relate words and find unique words. Randomly shuffling documents is not mentioned as a step."}, "54": {"documentation": {"title": "Unveiling the Hyper-Rayleigh Regime of the Fluctuating Two-Ray Fading\n  Model", "source": "Celia Garcia-Corrales, Unai Fernandez-Plazaola, Francisco J. Ca\\~nete,\n  Jos\\'e F. Paris and F. Javier Lopez-Martinez", "docs_id": "1905.00065", "section": ["cs.IT", "eess.SP", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Unveiling the Hyper-Rayleigh Regime of the Fluctuating Two-Ray Fading\n  Model. The recently proposed Fluctuating Two-Ray (FTR) model is gaining momentum as a reference fading model in scenarios where two dominant specular waves are present. Despite the numerous research works devoted to the performance analysis under FTR fading, little attention has been paid to effectively understanding the interplay between the fading model parameters and the fading severity. According to a new scale defined in this work, which measures the hyper-Rayleigh character of a fading channel in terms of the Amount of Fading, the outage probability and the average capacity, we see that the FTR fading model exhibits a full hyper-Rayleigh behavior. However, the Two-Wave with Diffuse Power fading model from which the former is derived has only strong hyper-Rayleigh behavior, which constitutes an interesting new insight. We also identify that the random fluctuations in the dominant specular waves are ultimately responsible for the full hyper-Rayleigh behavior of this class of fading channels."}, "answer": " Question: According to the document, which of the following statements is true about the Fluctuating Two-Ray (FTR) fading model?\n\nA) The FTR fading model exhibits only a strong hyper-Rayleigh behavior, not a full hyper-Rayleigh behavior. \n\nB) The Two-Wave with Diffuse Power fading model exhibits a fuller hyper-Rayleigh behavior than the FTR fading model.\n\nC) The FTR fading model exhibits a full hyper-Rayleigh behavior across all parameters.\n\nD) The random fluctuations in the dominant specular waves are responsible for the full hyper-Rayleigh behavior of the FTR fading model.\n\nCorrect Answer: D\n\nExplanation: The document states that the FTR fading model exhibits a full hyper-Rayleigh behavior, while the Two-Wave with Diffuse Power fading model from which it is derived only has strong hyper-Rayleigh behavior. It also identifies that the random fluctuations in the dominant specular waves are responsible for the full hyper-Rayleigh behavior of the FTR fading model."}, "55": {"documentation": {"title": "A Deep Reinforced Model for Zero-Shot Cross-Lingual Summarization with\n  Bilingual Semantic Similarity Rewards", "source": "Zi-Yi Dou, Sachin Kumar, Yulia Tsvetkov", "docs_id": "2006.15454", "section": ["cs.CL"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Deep Reinforced Model for Zero-Shot Cross-Lingual Summarization with\n  Bilingual Semantic Similarity Rewards. Cross-lingual text summarization aims at generating a document summary in one language given input in another language. It is a practically important but under-explored task, primarily due to the dearth of available data. Existing methods resort to machine translation to synthesize training data, but such pipeline approaches suffer from error propagation. In this work, we propose an end-to-end cross-lingual text summarization model. The model uses reinforcement learning to directly optimize a bilingual semantic similarity metric between the summaries generated in a target language and gold summaries in a source language. We also introduce techniques to pre-train the model leveraging monolingual summarization and machine translation objectives. Experimental results in both English--Chinese and English--German cross-lingual summarization settings demonstrate the effectiveness of our methods. In addition, we find that reinforcement learning models with bilingual semantic similarity as rewards generate more fluent sentences than strong baselines."}, "answer": " Question: What is the main goal of the proposed cross-lingual text summarization model described in the paper?\n\nA) To generate summaries in multiple languages given an input document in one language.\n\nB) To translate documents from one language to another and then generate summaries. \n\nC) To optimize semantic similarity between generated summaries and gold summaries in different languages.\n\nD) To leverage both monolingual summarization and machine translation objectives for training.\n\nCorrect Answer: C\n\nExplanation: The paper proposes an end-to-end cross-lingual text summarization model that uses reinforcement learning to directly optimize a bilingual semantic similarity metric between the summaries generated in a target language and gold summaries in a source language."}, "56": {"documentation": {"title": "Dialogue Enhancement in Object-based Audio -- Evaluating the Benefit on\n  People above 65", "source": "Davide Straninger", "docs_id": "2006.14282", "section": ["eess.AS", "cs.SD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Dialogue Enhancement in Object-based Audio -- Evaluating the Benefit on\n  People above 65. Due to age-related hearing loss, elderly people often struggle with following the language on TV. Because they form an increasing part of the audience, this problem will become even more important in the future and needs to be addressed by research and development. Object-based audio is a promising approach to solve this issue as it offers the possibility of customizable dialogue enhancement (DE). For this thesis an Adjustment / Satisfaction Test (A/ST) was conducted to evaluate the preferred loudness difference (LD) between speech and background in people above 65. Two different types of DE were tested: DE with separately available audio components (speech and background) and DE with components created by blind source separation (BSS). The preferred LDs compared to the original, differences of the preferred LDs between the two DE methods and the listener satisfaction were tested. It was observed that the preferred LDs were larger than the original LDs, that customizable DE increases listener satisfaction and that the two DE methods performed comparably well in terms of preferred LD and listener satisfaction. Based on the results, it can be assumed that elderly viewers above 65 will benefit equally from user-adjustable DE by available components and by dialogue separation."}, "answer": " Question: What was the main purpose of the Adjustment / Satisfaction Test (A/ST) conducted for the thesis described in the passage?\n\nA) To evaluate the preferred loudness difference between speech and background for people above 65 using two types of dialogue enhancement.\n\nB) To test whether elderly people above 65 struggle to follow dialogue on TV due to age-related hearing loss. \n\nC) To compare the performance of the two dialogue enhancement methods in terms of listener satisfaction.\n\nD) To determine if object-based audio can provide customizable dialogue enhancement to help elderly viewers.\n\nCorrect Answer: A\n\nExplanation: The passage states that the A/ST was conducted \"to evaluate the preferred loudness difference (LD) between speech and background in people above 65\" using two types of dialogue enhancement - one with separately available components and one using blind source separation. So the main purpose was to evaluate the preferred LDs using the two DE methods."}, "57": {"documentation": {"title": "Projected Stochastic Gradient Langevin Algorithms for Constrained\n  Sampling and Non-Convex Learning", "source": "Andrew Lamperski", "docs_id": "2012.12137", "section": ["cs.LG", "math.OC", "math.PR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Projected Stochastic Gradient Langevin Algorithms for Constrained\n  Sampling and Non-Convex Learning. Langevin algorithms are gradient descent methods with additive noise. They have been used for decades in Markov chain Monte Carlo (MCMC) sampling, optimization, and learning. Their convergence properties for unconstrained non-convex optimization and learning problems have been studied widely in the last few years. Other work has examined projected Langevin algorithms for sampling from log-concave distributions restricted to convex compact sets. For learning and optimization, log-concave distributions correspond to convex losses. In this paper, we analyze the case of non-convex losses with compact convex constraint sets and IID external data variables. We term the resulting method the projected stochastic gradient Langevin algorithm (PSGLA). We show the algorithm achieves a deviation of $O(T^{-1/4}(\\log T)^{1/2})$ from its target distribution in 1-Wasserstein distance. For optimization and learning, we show that the algorithm achieves $\\epsilon$-suboptimal solutions, on average, provided that it is run for a time that is polynomial in $\\epsilon^{-1}$ and slightly super-exponential in the problem dimension."}, "answer": " Here is a multi-form question on the paper:\n\nQuestion: Which of the following statements about the Projected Stochastic Gradient Langevin Algorithm (PSGLA) described in the paper is true?\n\nA) It achieves a deviation of O(T^(-1/4)(log T)^(1/2)) from its target distribution in 1-Wasserstein distance. \n\nB) It uses deterministic gradient descent with noise added.\n\nC) It has only been analyzed for log-concave distributions corresponding to convex losses.\n\nD) It achieves epsilon-suboptimal solutions in time polynomial in epsilon^(-1) and the problem dimension.\n\nCorrect Answer: A\n\nExplanation: \nA) Correct. The paper shows PSGLA achieves this deviation bound.\nB) Incorrect. Langevin algorithms use stochastic gradient descent with noise. \nC) Incorrect. The paper analyzes PSGLA for non-convex losses.\nD) Incorrect. The time complexity is slightly super-exponential in the problem dimension."}, "58": {"documentation": {"title": "The Effectiveness of Supervised Machine Learning Algorithms in\n  Predicting Software Refactoring", "source": "Maur\\'icio Aniche, Erick Maziero, Rafael Durelli, Vinicius Durelli", "docs_id": "2001.03338", "section": ["cs.SE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Effectiveness of Supervised Machine Learning Algorithms in\n  Predicting Software Refactoring. Refactoring is the process of changing the internal structure of software to improve its quality without modifying its external behavior. Empirical studies have repeatedly shown that refactoring has a positive impact on the understandability and maintainability of software systems. However, before carrying out refactoring activities, developers need to identify refactoring opportunities. Currently, refactoring opportunity identification heavily relies on developers' expertise and intuition. In this paper, we investigate the effectiveness of machine learning algorithms in predicting software refactorings. More specifically, we train six different machine learning algorithms (i.e., Logistic Regression, Naive Bayes, Support Vector Machine, Decision Trees, Random Forest, and Neural Network) with a dataset comprising over two million refactorings from 11,149 real-world projects from the Apache, F-Droid, and GitHub ecosystems. The resulting models predict 20 different refactorings at class, method, and variable-levels with an accuracy often higher than 90%. Our results show that (i) Random Forests are the best models for predicting software refactoring, (ii) process and ownership metrics seem to play a crucial role in the creation of better models, and (iii) models generalize well in different contexts."}, "answer": " Here is a 4-option multiple choice question on the document:\n\nQuestion: Which of the following statements is true about the machine learning models developed in the study for predicting software refactorings?\n\nA) The models achieved an accuracy of at least 90% in predicting all 20 refactorings studied.\n\nB) Ownership metrics were found to be more important than process metrics in building better predictive models. \n\nC) The Random Forest algorithm was found to be the best for predicting refactorings at the class level only.\n\nD) The models were evaluated on projects only from the Apache ecosystem and did not generalize well to other contexts.\n\nCorrect Answer: B\n\nExplanation: According to the document, the study found that Random Forests were the best models overall for predicting software refactoring across class, method and variable levels. While accuracy was often higher than 90%, the document does not state that this was achieved for all 20 refactorings. It also mentions that process and ownership metrics were important for better models, not just ownership metrics. Finally, it states that the models generalized well when evaluated on projects from Apache, F-Droid and GitHub ecosystems, not just Apache."}, "59": {"documentation": {"title": "CalQNet -- Detection of Calibration Quality for Life-Long Stereo Camera\n  Setups", "source": "Jiapeng Zhong, Zheyu Ye, Andrei Cramariuc, Florian Tschopp, Jen Jen\n  Chung, Roland Siegwart, Cesar Cadena", "docs_id": "2104.04837", "section": ["cs.RO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "CalQNet -- Detection of Calibration Quality for Life-Long Stereo Camera\n  Setups. Many mobile robotic platforms rely on an accurate knowledge of the extrinsic calibration parameters, especially systems performing visual stereo matching. Although a number of accurate stereo camera calibration methods have been developed, which provide good initial \"factory\" calibrations, the determined parameters can lose their validity over time as the sensors are exposed to environmental conditions and external effects. Thus, on autonomous platforms on-board diagnostic methods for an early detection of the need to repeat calibration procedures have the potential to prevent critical failures of crucial systems, such as state estimation or obstacle detection. In this work, we present a novel data-driven method to estimate the calibration quality and detect discrepancies between the original calibration and the current system state for stereo camera systems. The framework consists of a novel dataset generation pipeline to train CalQNet, a deep convolutional neural network. CalQNet can estimate the calibration quality using a new metric that approximates the degree of miscalibration in stereo setups. We show the framework's ability to predict from a single stereo frame if a state-of-the-art stereo-visual odometry system will diverge due to a degraded calibration in two real-world experiments."}, "answer": " Question: What is the main contribution of the CalQNet paper?\n\nA) A new dataset generation pipeline to train deep neural networks for estimating calibration quality of stereo camera systems.\n\nB) A novel deep convolutional neural network called CalQNet that can detect calibration discrepancies from a single stereo frame. \n\nC) A new metric to quantify the degree of miscalibration in stereo camera setups.\n\nD) A method to predict if stereo visual odometry will fail due to poor calibration.\n\nCorrect Answer: B\n\nExplanation: The main contribution of the CalQNet paper is a novel deep convolutional neural network called CalQNet that can estimate the calibration quality and detect discrepancies between the original and current calibration from a single stereo frame. The other choices are additional contributions or applications of CalQNet."}}