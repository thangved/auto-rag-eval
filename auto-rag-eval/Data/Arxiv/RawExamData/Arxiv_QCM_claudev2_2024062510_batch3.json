{"0": {"documentation": {"title": "HACS: Human Action Clips and Segments Dataset for Recognition and\n  Temporal Localization", "source": "Hang Zhao, Antonio Torralba, Lorenzo Torresani, Zhicheng Yan", "docs_id": "1712.09374", "section": ["cs.CV", "cs.AI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "HACS: Human Action Clips and Segments Dataset for Recognition and\n  Temporal Localization. This paper presents a new large-scale dataset for recognition and temporal localization of human actions collected from Web videos. We refer to it as HACS (Human Action Clips and Segments). We leverage both consensus and disagreement among visual classifiers to automatically mine candidate short clips from unlabeled videos, which are subsequently validated by human annotators. The resulting dataset is dubbed HACS Clips. Through a separate process we also collect annotations defining action segment boundaries. This resulting dataset is called HACS Segments. Overall, HACS Clips consists of 1.5M annotated clips sampled from 504K untrimmed videos, and HACS Seg-ments contains 139K action segments densely annotatedin 50K untrimmed videos spanning 200 action categories. HACS Clips contains more labeled examples than any existing video benchmark. This renders our dataset both a large scale action recognition benchmark and an excellent source for spatiotemporal feature learning. In our transferlearning experiments on three target datasets, HACS Clips outperforms Kinetics-600, Moments-In-Time and Sports1Mas a pretraining source. On HACS Segments, we evaluate state-of-the-art methods of action proposal generation and action localization, and highlight the new challenges posed by our dense temporal annotations."}, "answer": " Here is a multi-form question on the HACS dataset paper:\n\nQuestion: Which of the following statements is true about the HACS dataset?\n\nA) HACS Clips contains 139K action segments while HACS Segments contains 1.5M clips.\n\nB) HACS leverages agreement among classifiers to mine candidate clips which are validated by humans. \n\nC) HACS Segments contains more labeled examples than any existing video benchmark.\n\nD) HACS Clips consists of 50K untrimmed videos with dense temporal annotations.\n\nCorrect Answer: B\n\nExplanation: According to the paper, HACS Clips contains 1.5M annotated clips while HACS Segments contains 139K action segments. HACS leverages both consensus and disagreement among classifiers to mine candidate clips which are then validated by human annotators. HACS Clips is stated to have more labeled examples than existing benchmarks. HACS Segments contains 50K untrimmed videos with dense temporal annotations, not HACS Clips."}, "1": {"documentation": {"title": "RFBNet: Deep Multimodal Networks with Residual Fusion Blocks for RGB-D\n  Semantic Segmentation", "source": "Liuyuan Deng, Ming Yang, Tianyi Li, Yuesheng He, and Chunxiang Wang", "docs_id": "1907.00135", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "RFBNet: Deep Multimodal Networks with Residual Fusion Blocks for RGB-D\n  Semantic Segmentation. RGB-D semantic segmentation methods conventionally use two independent encoders to extract features from the RGB and depth data. However, there lacks an effective fusion mechanism to bridge the encoders, for the purpose of fully exploiting the complementary information from multiple modalities. This paper proposes a novel bottom-up interactive fusion structure to model the interdependencies between the encoders. The structure introduces an interaction stream to interconnect the encoders. The interaction stream not only progressively aggregates modality-specific features from the encoders but also computes complementary features for them. To instantiate this structure, the paper proposes a residual fusion block (RFB) to formulate the interdependences of the encoders. The RFB consists of two residual units and one fusion unit with gate mechanism. It learns complementary features for the modality-specific encoders and extracts modality-specific features as well as cross-modal features. Based on the RFB, the paper presents the deep multimodal networks for RGB-D semantic segmentation called RFBNet. The experiments on two datasets demonstrate the effectiveness of modeling the interdependencies and that the RFBNet achieved state-of-the-art performance."}, "answer": " Question: What is the key innovation proposed in the RFBNet paper for RGB-D semantic segmentation?\n\nA) Using two separate encoders for RGB and depth data without any interaction between them.\n\nB) Introducing a fusion mechanism to model interdependencies between RGB and depth encoders. \n\nC) Removing the depth encoder completely and using only RGB data.\n\nD) Adding a third encoder for other modalities beyond RGB and depth.\n\nCorrect Answer: B\n\nExplanation: The key innovation of RFBNet is introducing an interaction stream to interconnect the RGB and depth encoders, allowing them to exchange information and model interdependencies. This is in contrast to prior works that used separate encoders without interaction."}, "2": {"documentation": {"title": "Multilingual Speech Recognition for Low-Resource Indian Languages using\n  Multi-Task conformer", "source": "Krishna D N", "docs_id": "2109.03969", "section": ["cs.CL", "cs.SD", "eess.AS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Multilingual Speech Recognition for Low-Resource Indian Languages using\n  Multi-Task conformer. Transformers have recently become very popular for sequence-to-sequence applications such as machine translation and speech recognition. In this work, we propose a multi-task learning-based transformer model for low-resource multilingual speech recognition for Indian languages. Our proposed model consists of a conformer [1] encoder and two parallel transformer decoders. We use a phoneme decoder (PHN-DEC) for the phoneme recognition task and a grapheme decoder (GRP-DEC) to predict grapheme sequence. We consider the phoneme recognition task as an auxiliary task for our multi-task learning framework. We jointly optimize the network for both phoneme and grapheme recognition tasks using Joint CTC-Attention [2] training. We use a conditional decoding scheme to inject the language information into the model before predicting the grapheme sequence. Our experiments show that our proposed approach can obtain significant improvement over previous approaches [4]. We also show that our conformer-based dual-decoder approach outperforms both the transformer-based dual-decoder approach and single decoder approach. Finally, We compare monolingual ASR models with our proposed multilingual ASR approach."}, "answer": " Question: Which of the following statements is true about the proposed multi-task learning transformer model for low-resource multilingual speech recognition?\n\nA) It uses a transformer encoder and two parallel LSTM decoders for phoneme and grapheme recognition.\n\nB) It consists of a transformer encoder and two parallel transformer decoders for phoneme and grapheme recognition. \n\nC) It uses CTC training only for the phoneme recognition task and attention-based training for the grapheme recognition task.\n\nD) It uses a shared transformer encoder for both tasks and two separate decoders for phoneme and grapheme recognition.\n\nCorrect Answer: B\n\nExplanation: According to the documentation, the proposed model consists of a conformer encoder and two parallel transformer decoders - one for phoneme recognition and one for grapheme recognition. Both tasks use joint CTC-Attention training. Option B correctly states that it uses transformer decoders for both tasks."}, "3": {"documentation": {"title": "Boundary IoU: Improving Object-Centric Image Segmentation Evaluation", "source": "Bowen Cheng and Ross Girshick and Piotr Doll\\'ar and Alexander C. Berg\n  and Alexander Kirillov", "docs_id": "2103.16562", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Boundary IoU: Improving Object-Centric Image Segmentation Evaluation. We present Boundary IoU (Intersection-over-Union), a new segmentation evaluation measure focused on boundary quality. We perform an extensive analysis across different error types and object sizes and show that Boundary IoU is significantly more sensitive than the standard Mask IoU measure to boundary errors for large objects and does not over-penalize errors on smaller objects. The new quality measure displays several desirable characteristics like symmetry w.r.t. prediction/ground truth pairs and balanced responsiveness across scales, which makes it more suitable for segmentation evaluation than other boundary-focused measures like Trimap IoU and F-measure. Based on Boundary IoU, we update the standard evaluation protocols for instance and panoptic segmentation tasks by proposing the Boundary AP (Average Precision) and Boundary PQ (Panoptic Quality) metrics, respectively. Our experiments show that the new evaluation metrics track boundary quality improvements that are generally overlooked by current Mask IoU-based evaluation metrics. We hope that the adoption of the new boundary-sensitive evaluation metrics will lead to rapid progress in segmentation methods that improve boundary quality."}, "answer": " Question: Which of the following statements is true about Boundary IoU according to the given documentation?\n\nA) Boundary IoU focuses on evaluating the quality of object boundaries in image segmentation. \n\nB) Boundary IoU penalizes errors on small objects more than Mask IoU.\n\nC) Boundary IoU is less sensitive to boundary errors on large objects compared to Mask IoU.\n\nD) Boundary IoU displays asymmetry with respect to prediction/ground truth pairs.\n\nCorrect Answer: A\n\nExplanation: The documentation states that Boundary IoU is a new segmentation evaluation measure focused on boundary quality. It is more sensitive to boundary errors on large objects compared to Mask IoU and does not over-penalize errors on smaller objects. It also displays symmetry with respect to prediction/ground truth pairs."}, "4": {"documentation": {"title": "An Analytical Model for CBAP Allocations in IEEE 802.11ad", "source": "Chiara Pielli, Tanguy Ropitault, Nada Golmie, Michele Zorzi", "docs_id": "1906.07097", "section": ["cs.NI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "An Analytical Model for CBAP Allocations in IEEE 802.11ad. The IEEE 802.11ad standard extends WiFi operation to the millimeter wave frequencies, and introduces novel features concerning both the physical (PHY) and Medium Access Control (MAC) layers. However, while there are extensive research efforts to develop mechanisms for establishing and maintaining directional links for mmWave communications, fewer works deal with transmission scheduling and the hybrid MAC introduced by the standard. The hybrid MAC layer provides for two different kinds of resource allocations: Contention Based Access Periods (CBAPs) and contention free Service Periods (SPs). In this paper, we propose a Markov Chain model to represent CBAPs, which takes into account operation interruptions due to scheduled SPs and the deafness and hidden node problems that directional communication exacerbates. We also propose a mathematical analysis to assess interference among stations. We derive analytical expressions to assess the impact of various transmission parameters and of the Data Transmission Interval configuration on some key performance metrics such as throughput, delay and packet dropping rate. This information may be used to efficiently design a transmission scheduler that allocates contention-based and contention-free periods based on the application requirements."}, "answer": " Question: What mechanism does the IEEE 802.11ad standard introduce for transmission scheduling and dealing with the deafness and hidden node problems that directional communication exacerbates?\n\nA) Contention-free Service Periods only\nB) Contention-based Access Periods only  \nC) Both Contention-free Service Periods and Contention-based Access Periods\nD) Neither Contention-free Service Periods nor Contention-based Access Periods\n\nCorrect Answer: C\n\nExplanation: The IEEE 802.11ad standard introduces a hybrid MAC layer that provides for two different kinds of resource allocations - Contention-Based Access Periods (CBAPs) and contention-free Service Periods (SPs). The paper proposes a model to represent CBAPs and analyze interference, throughput, delay etc. to efficiently design a scheduler that allocates both contention-based and contention-free periods."}, "5": {"documentation": {"title": "Solving Power System Differential Algebraic Equations Using Differential\n  Transformation", "source": "Yang Liu, Kai Sun", "docs_id": "1903.00935", "section": ["eess.SY", "cs.SY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Solving Power System Differential Algebraic Equations Using Differential\n  Transformation. This paper proposes a novel non-iterative method to solve power system differential algebraic equations (DAEs) using the differential transformation, a mathematical tool that can obtain power series coefficients by transformation rules instead of calculating high order derivatives and has proved to be effective in solving state variables of nonlinear differential equations in our previous study. This paper further solves non-state variables, e.g. current injections and bus voltages, directly with a realistic DAE model of power grids. These non-state variables, nonlinearly coupled in network equations, are conventionally solved by numerical methods with time-consuming iterations, but their differential transformations are proved to satisfy formally linear equations in this paper. Thus, a non-iterative algorithm is designed to analytically solve all variables of a power system DAE model with ZIP loads. From test results on a Polish 2383-bus system, the proposed method demonstrates fast and reliable time performance compared to traditional numerical methods."}, "answer": " Here is a 4-option multiple choice question on the paper:\n\nQuestion: Which of the following statements is true about the proposed method in the paper for solving power system differential algebraic equations (DAEs)?\n\nA) It uses an iterative approach based on numerical methods like Newton-Raphson.\n\nB) It can only solve for the state variables in the DAEs, not the non-state variables. \n\nC) It transforms the nonlinearly coupled network equations into linear differential equations through differential transformation.\n\nD) It requires calculating high order derivatives of the DAEs.\n\nCorrect Answer: C\n\nExplanation: The paper proposes a non-iterative method using differential transformation to solve power system DAEs. It shows that the differential transformations of non-state variables like currents and voltages satisfy linear equations, allowing them to be solved directly without iterations. So option C is correct. Options A, B and D contradict what the paper states about the proposed method."}, "6": {"documentation": {"title": "Constructions of Binary Optimal Locally Repairable Codes via\n  Intersection Subspaces", "source": "Wenqin Zhang, Deng Tang, Yuan Luo", "docs_id": "2105.11271", "section": ["cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Constructions of Binary Optimal Locally Repairable Codes via\n  Intersection Subspaces. Locally repairable codes (LRCs), which can recover any symbol of a codeword by reading only a small number of other symbols, have been widely used in real-world distributed storage systems, such as Microsoft Azure Storage and Ceph Storage Cluster. Since the binary linear LRCs can significantly reduce the coding and decoding complexity, the construction of binary LRCs is of particular interest. To date, all the known optimal binary linear LRCs with the locality $2^b$ ($b\\geq 3$) are based on the so-called partial spread which is a collection of the same dimensional subspaces with pairwise trivial, i.e., zero-dimensional intersection. In this paper, we concentrate on binary linear LRCs with disjoint local repair groups. We construct dimensional optimal binary linear LRCs with locality $2^b$ ($b\\geq 3$) and minimum distance $d\\geq 6$ by employing intersection subspaces deduced from the direct sum vs. the traditional partial spread construction. This method will increase the number of possible repair groups of LRCs as many as possible, and thus efficiently enlarge the range of the construction parameters while keeping the largest code rates compared with all known binary linear LRCs with minimum distance $d\\geq 6$ and locality $2^b$ ($b\\geq 3$)."}, "answer": " Here is a 4-option multiple choice question on the documentation:\n\nQuestion: Which of the following statements is true about the constructions of binary optimal locally repairable codes (LRCs) presented in the paper?\n\nA) The constructions are based on partial spreads, which consist of equal dimensional subspaces with non-zero intersections.\n\nB) The constructions employ intersection subspaces formed by taking direct sums rather than partial spreads. \n\nC) The new constructions can only achieve a minimum distance of d=4.\n\nD) The constructions can only achieve locality 2^b for b<=2.\n\nCorrect Answer: B\n\nExplanation: The paper introduces new constructions of optimal binary LRCs using intersection subspaces formed by direct sums rather than traditional partial spreads. This allows for increased flexibility in repair group choices while achieving minimum distance d>=6 and locality 2^b for b>=3."}, "7": {"documentation": {"title": "An Interactive Control Approach to 3D Shape Reconstruction", "source": "Bipul Islam, Ji Liu, Anthony Yezzi, Romeil Sandhu", "docs_id": "1910.02738", "section": ["cs.CV", "cs.RO", "cs.SY", "eess.SY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "An Interactive Control Approach to 3D Shape Reconstruction. The ability to accurately reconstruct the 3D facets of a scene is one of the key problems in robotic vision. However, even with recent advances with machine learning, there is no high-fidelity universal 3D reconstruction method for this optimization problem as schemes often cater to specific image modalities and are often biased by scene abnormalities. Simply put, there always remains an information gap due to the dynamic nature of real-world scenarios. To this end, we demonstrate a feedback control framework which invokes operator inputs (also prone to errors) in order to augment existing reconstruction schemes. For proof-of-concept, we choose a classical region-based stereoscopic reconstruction approach and show how an ill-posed model can be augmented with operator input to be much more robust to scene artifacts. We provide necessary conditions for stability via Lyapunov analysis and perhaps more importantly, we show that the stability depends on a notion of absolute curvature. Mathematically, this aligns with previous work that has shown Ricci curvature as proxy for functional robustness of dynamical networked systems. We conclude with results that show how our method can improve standalone reconstruction schemes."}, "answer": " Here is a 4-option multiple choice question on the paper:\n\nQuestion: Which of the following statements is true about the feedback control framework presented in the paper for 3D shape reconstruction?\n\nA) It relies solely on operator inputs to reconstruct 3D shapes without using any existing reconstruction schemes.\n\nB) It combines operator inputs with outputs from existing stereoscopic reconstruction methods to make the overall process more robust. \n\nC) It shows that Ricci curvature is necessary for stability of the control framework through Lyapunov analysis.\n\nD) It demonstrates a universal 3D reconstruction method that works for all image modalities without any biases.\n\nCorrect Answer: B\n\nExplanation: The paper presents a feedback control framework that combines operator inputs with existing stereoscopic reconstruction methods to make the overall 3D shape reconstruction more robust to scene artifacts and abnormalities. It does not rely solely on operator inputs or present a universal reconstruction method. While stability analysis using Lyapunov methods is performed, the notion of absolute curvature rather than Ricci curvature is shown to be related to stability."}, "8": {"documentation": {"title": "MemTorch: An Open-source Simulation Framework for Memristive Deep\n  Learning Systems", "source": "Corey Lammie, Wei Xiang, Bernab\\'e Linares-Barranco, Mostafa Rahimi\n  Azghadi", "docs_id": "2004.10971", "section": ["cs.ET"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "MemTorch: An Open-source Simulation Framework for Memristive Deep\n  Learning Systems. Memristive devices have shown great promise to facilitate the acceleration and improve the power efficiency of Deep Learning (DL) systems. Crossbar architectures constructed using these Resistive Random-Access Memory (RRAM) devices can be used to efficiently implement various in-memory computing operations, such as Multiply Accumulate (MAC) and unrolled-convolutions, which are used extensively in Deep Neural Networks (DNNs) and Convolutional Neural Networks (CNNs). However, memristive devices face concerns of aging and non-idealities, which limit the accuracy, reliability, and robustness of Memristive Deep Learning Systems (MDLSs), that should be considered prior to circuit-level realization. This Original Software Publication (OSP) presents MemTorch, an open-source framework for customized large-scale memristive DL simulations, with a refined focus on the co-simulation of device non-idealities. MemTorch also facilitates co-modelling of key crossbar peripheral circuitry. MemTorch adopts a modernized soft-ware engineering methodology and integrates directly with the well-known PyTorch Machine Learning (ML) library"}, "answer": " Here is a 4-option multiple choice question on the MemTorch paper:\n\nQuestion: Which of the following statements is true about MemTorch according to the paper?\n\nA) MemTorch only focuses on simulating ideal memristive devices without non-idealities.\n\nB) MemTorch adopts outdated software engineering methodologies not compatible with PyTorch. \n\nC) MemTorch facilitates co-simulation of memristive device non-idealities and peripheral circuitry.\n\nD) MemTorch is a closed source framework that cannot be customized.\n\nCorrect Answer: C) MemTorch facilitates co-simulation of memristive device non-idealities and peripheral circuitry.\n\nExplanation: \nAccording to the paper, MemTorch enables customized large-scale memristive DL simulations with a focus on co-simulating device non-idealities. It also allows co-modelling of crossbar peripheral circuits. Additionally, it integrates with PyTorch and adopts modern software engineering practices, implying it is open-source and customizable."}, "9": {"documentation": {"title": "Model-based Convolutional De-Aliasing Network Learning for Parallel MR\n  Imaging", "source": "Yanxia Chen, Taohui Xiao, Cheng Li, Qiegen Liu and Shanshan Wang", "docs_id": "1908.02054", "section": ["eess.IV", "cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Model-based Convolutional De-Aliasing Network Learning for Parallel MR\n  Imaging. Parallel imaging has been an essential technique to accelerate MR imaging. Nevertheless, the acceleration rate is still limited due to the ill-condition and challenges associated with the undersampled reconstruction. In this paper, we propose a model-based convolutional de-aliasing network with adaptive parameter learning to achieve accurate reconstruction from multi-coil undersampled k-space data. Three main contributions have been made: a de-aliasing reconstruction model was proposed to accelerate parallel MR imaging with deep learning exploring both spatial redundancy and multi-coil correlations; a split Bregman iteration algorithm was developed to solve the model efficiently; and unlike most existing parallel imaging methods which rely on the accuracy of the estimated multi-coil sensitivity, the proposed method can perform parallel reconstruction from undersampled data without explicit sensitivity calculation. Evaluations were conducted on \\emph{in vivo} brain dataset with a variety of undersampling patterns and different acceleration factors. Our results demonstrated that this method could achieve superior performance in both quantitative and qualitative analysis, compared to three state-of-the-art methods."}, "answer": " Here is a 4-option multiple choice question on the paper:\n\nQuestion: Which of the following statements is true about the proposed model-based convolutional de-aliasing network method in the paper?\n\nA) It relies on accurate estimation of multi-coil sensitivities for parallel reconstruction from undersampled data.\n\nB) It uses a split Bregman algorithm to solve the proposed de-aliasing reconstruction model efficiently. \n\nC) It achieves inferior performance compared to state-of-the-art methods in quantitative and qualitative analysis.\n\nD) It explores spatial redundancy but not multi-coil correlations for accelerated parallel MR imaging.\n\nCorrect Answer: B\n\nExplanation: \nAccording to the paper, the proposed method does not rely on explicit sensitivity calculation and explores both spatial redundancy and multi-coil correlations. It uses a split Bregman algorithm to efficiently solve the proposed de-aliasing reconstruction model. It demonstrated superior performance over state-of-the-art methods in both quantitative and qualitative analysis."}, "10": {"documentation": {"title": "Truth as Utility: A Conceptual Synthesis", "source": "Enrique H. Ruspini", "docs_id": "1303.5744", "section": ["cs.AI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Truth as Utility: A Conceptual Synthesis. This paper introduces conceptual relations that synthesize utilitarian and logical concepts, extending the logics of preference of Rescher. We define first, in the context of a possible worlds model, constraint-dependent measures that quantify the relative quality of alternative solutions of reasoning problems or the relative desirability of various policies in control, decision, and planning problems. We show that these measures may be interpreted as truth values in a multi valued logic and propose mechanisms for the representation of complex constraints as combinations of simpler restrictions. These extended logical operations permit also the combination and aggregation of goal-specific quality measures into global measures of utility. We identify also relations that represent differential preferences between alternative solutions and relate them to the previously defined desirability measures. Extending conventional modal logic formulations, we introduce structures for the representation of ignorance about the utility of alternative solutions. Finally, we examine relations between these concepts and similarity based semantic models of fuzzy logic."}, "answer": " Here is a 4-option multiple choice question on the paper \"Truth as Utility: A Conceptual Synthesis\":\n\nQuestion: Which of the following statements accurately describes a key contribution of the paper?\n\nA) It proposes new semantic models for fuzzy logic based on similarity relations.\n\nB) It introduces constraint-dependent measures that can be interpreted as truth values in a multi-valued logic. \n\nC) It relates differential preference relations to conventional modal logic formulations.\n\nD) It aggregates global measures of utility using extended logical operations.\n\nCorrect Answer: B\n\nExplanation: The paper introduces constraint-dependent measures that quantify the relative quality or desirability of solutions/policies. These measures can be interpreted as truth values in a multi-valued logic, which is a key conceptual contribution of the work."}, "11": {"documentation": {"title": "Visual Reference Resolution using Attention Memory for Visual Dialog", "source": "Paul Hongsuck Seo, Andreas Lehrmann, Bohyung Han, Leonid Sigal", "docs_id": "1709.07992", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Visual Reference Resolution using Attention Memory for Visual Dialog. Visual dialog is a task of answering a series of inter-dependent questions given an input image, and often requires to resolve visual references among the questions. This problem is different from visual question answering (VQA), which relies on spatial attention (a.k.a. visual grounding) estimated from an image and question pair. We propose a novel attention mechanism that exploits visual attentions in the past to resolve the current reference in the visual dialog scenario. The proposed model is equipped with an associative attention memory storing a sequence of previous (attention, key) pairs. From this memory, the model retrieves the previous attention, taking into account recency, which is most relevant for the current question, in order to resolve potentially ambiguous references. The model then merges the retrieved attention with a tentative one to obtain the final attention for the current question; specifically, we use dynamic parameter prediction to combine the two attentions conditioned on the question. Through extensive experiments on a new synthetic visual dialog dataset, we show that our model significantly outperforms the state-of-the-art (by ~16 % points) in situations, where visual reference resolution plays an important role. Moreover, the proposed model achieves superior performance (~ 2 % points improvement) in the Visual Dialog dataset, despite having significantly fewer parameters than the baselines."}, "answer": " Here is a 4-option multiple choice question on the given visual dialog research:\n\nQuestion: Which of the following statements is true about the visual dialog task described in the research?\n\nA) It relies only on spatial attention like in VQA.\n\nB) It requires resolving references using the current image and question only. \n\nC) It exploits past visual attentions to resolve current ambiguous references.\n\nD) It uses a fixed parameter combination of tentative and retrieved attentions.\n\nCorrect Answer: C\n\nExplanation: The research proposes a novel attention mechanism for visual dialog that stores previous attention memories and retrieves the most relevant one to resolve potentially ambiguous references in the current question. It does not rely solely on spatial attention like VQA, nor resolve references using just the current input. It also uses dynamic parameter prediction to combine the retrieved and tentative attentions based on the question."}, "12": {"documentation": {"title": "METEOR: A Massive Dense & Heterogeneous Behavior Dataset for Autonomous\n  Driving", "source": "Rohan Chandra, Mridul Mahajan, Rahul Kala, Rishitha Palugulla,\n  Chandrababu Naidu, Alok Jain, and Dinesh Manocha", "docs_id": "2109.07648", "section": ["cs.CV", "cs.AI", "cs.RO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "METEOR: A Massive Dense & Heterogeneous Behavior Dataset for Autonomous\n  Driving. We present a new and complex traffic dataset, METEOR, which captures traffic patterns in unstructured scenarios in India. METEOR consists of more than 1000 one-minute video clips, over 2 million annotated frames with ego-vehicle trajectories, and more than 13 million bounding boxes for surrounding vehicles or traffic agents. METEOR is a unique dataset in terms of capturing the heterogeneity of microscopic and macroscopic traffic characteristics. Furthermore, we provide annotations for rare and interesting driving behaviors such as cut-ins, yielding, overtaking, overspeeding, zigzagging, sudden lane changing, running traffic signals, driving in the wrong lanes, taking wrong turns, lack of right-of-way rules at intersections, etc. We also present diverse traffic scenarios corresponding to rainy weather, nighttime driving, driving in rural areas with unmarked roads, and high-density traffic scenarios. We use our novel dataset to evaluate the performance of object detection and behavior prediction algorithms. We show that state-of-the-art object detectors fail in these challenging conditions and also propose a new benchmark test: action-behavior prediction with a baseline mAP score of 70.74."}, "answer": " Question: What is a key characteristic of the METEOR dataset that makes it unique compared to other autonomous driving datasets?\n\nA) It contains over 2 million annotated frames with ego-vehicle trajectories.\n\nB) It captures traffic patterns in structured scenarios in the United States. \n\nC) It provides annotations for common driving behaviors like lane changes and turning.\n\nD) It captures the heterogeneity of microscopic and macroscopic traffic characteristics.\n\nCorrect Answer: D) It captures the heterogeneity of microscopic and macroscopic traffic characteristics.\n\nThe passage states that \"METEOR is a unique dataset in terms of capturing the heterogeneity of microscopic and macroscopic traffic characteristics.\" This suggests that the heterogeneity of traffic patterns is a distinguishing factor of the METEOR dataset compared to others."}, "13": {"documentation": {"title": "Explaining dimensionality reduction results using Shapley values", "source": "Wilson Est\\'ecio Marc\\'ilio J\\'unior and Danilo Medeiros Eler", "docs_id": "2103.05678", "section": ["cs.LG", "cs.GR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Explaining dimensionality reduction results using Shapley values. Dimensionality reduction (DR) techniques have been consistently supporting high-dimensional data analysis in various applications. Besides the patterns uncovered by these techniques, the interpretation of DR results based on each feature's contribution to the low-dimensional representation supports new finds through exploratory analysis. Current literature approaches designed to interpret DR techniques do not explain the features' contributions well since they focus only on the low-dimensional representation or do not consider the relationship among features. This paper presents ClusterShapley to address these problems, using Shapley values to generate explanations of dimensionality reduction techniques and interpret these algorithms using a cluster-oriented analysis. ClusterShapley explains the formation of clusters and the meaning of their relationship, which is useful for exploratory data analysis in various domains. We propose novel visualization techniques to guide the interpretation of features' contributions on clustering formation and validate our methodology through case studies of publicly available datasets. The results demonstrate our approach's interpretability and analysis power to generate insights about pathologies and patients in different conditions using DR results."}, "answer": " Question: Which of the following statements is true about the ClusterShapley method described in the paper?\n\nA) It uses Shapley values to explain the clusters formed by dimensionality reduction techniques and interpret the relationship between features. \n\nB) It focuses only on the low-dimensional representation produced by dimensionality reduction techniques.\n\nC) It does not consider the relationship between features when explaining dimensionality reduction results.\n\nD) It is not useful for exploratory data analysis in various domains.\n\nCorrect Answer: A\n\nExplanation: The paper states that ClusterShapley uses Shapley values to generate explanations of dimensionality reduction techniques and interpret these algorithms using a cluster-oriented analysis. It explains the formation of clusters and the meaning of their relationship, which is useful for exploratory data analysis in various domains."}, "14": {"documentation": {"title": "MRI to FDG-PET: Cross-Modal Synthesis Using 3D U-Net For Multi-Modal\n  Alzheimer's Classification", "source": "Apoorva Sikka, Skand Vishwanath Peri, Deepti.R.Bathula", "docs_id": "1807.10111", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "MRI to FDG-PET: Cross-Modal Synthesis Using 3D U-Net For Multi-Modal\n  Alzheimer's Classification. Recent studies suggest that combined analysis of Magnetic resonance imaging~(MRI) that measures brain atrophy and positron emission tomography~(PET) that quantifies hypo-metabolism provides improved accuracy in diagnosing Alzheimer's disease. However, such techniques are limited by the availability of corresponding scans of each modality. Current work focuses on a cross-modal approach to estimate FDG-PET scans for the given MR scans using a 3D U-Net architecture. The use of the complete MR image instead of a local patch based approach helps in capturing non-local and non-linear correlations between MRI and PET modalities. The quality of the estimated PET scans is measured using quantitative metrics such as MAE, PSNR and SSIM. The efficacy of the proposed method is evaluated in the context of Alzheimer's disease classification. The accuracy using only MRI is 70.18% while joint classification using synthesized PET and MRI is 74.43% with a p-value of $0.06$. The significant improvement in diagnosis demonstrates the utility of the synthesized PET scans for multi-modal analysis."}, "answer": " Question: What architecture was used to estimate FDG-PET scans from MR scans for Alzheimer's disease classification in the paper?\n\nA) Convolutional neural network \n\nB) Recurrent neural network\n\nC) 3D U-Net\n\nD) Autoencoder\n\nCorrect Answer: C) 3D U-Net\n\nExplanation: The paper mentions that \"Current work focuses on a cross-modal approach to estimate FDG-PET scans for the given MR scans using a 3D U-Net architecture.\" Therefore, the correct answer is C) 3D U-Net."}, "15": {"documentation": {"title": "Multi-Agent Adversarial Inverse Reinforcement Learning", "source": "Lantao Yu, Jiaming Song, Stefano Ermon", "docs_id": "1907.13220", "section": ["cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Multi-Agent Adversarial Inverse Reinforcement Learning. Reinforcement learning agents are prone to undesired behaviors due to reward mis-specification. Finding a set of reward functions to properly guide agent behaviors is particularly challenging in multi-agent scenarios. Inverse reinforcement learning provides a framework to automatically acquire suitable reward functions from expert demonstrations. Its extension to multi-agent settings, however, is difficult due to the more complex notions of rational behaviors. In this paper, we propose MA-AIRL, a new framework for multi-agent inverse reinforcement learning, which is effective and scalable for Markov games with high-dimensional state-action space and unknown dynamics. We derive our algorithm based on a new solution concept and maximum pseudolikelihood estimation within an adversarial reward learning framework. In the experiments, we demonstrate that MA-AIRL can recover reward functions that are highly correlated with ground truth ones, and significantly outperforms prior methods in terms of policy imitation."}, "answer": " Question: What framework does the paper propose for multi-agent inverse reinforcement learning that is effective and scalable for Markov games with high-dimensional state-action space and unknown dynamics?\n\nA) A framework called MA-AIRL based on a new solution concept and maximum pseudolikelihood estimation within an adversarial reward learning framework.\n\nB) A model-based inverse reinforcement learning approach using Bayesian inference and known environment dynamics. \n\nC) An imitation learning approach based on behavior cloning from expert demonstrations.\n\nD) A multi-agent apprenticeship learning algorithm using linear reward functions.\n\nCorrect Answer: A\n\nExplanation: The paper proposes a new framework called MA-AIRL for multi-agent inverse reinforcement learning. It is based on a new solution concept and maximum pseudolikelihood estimation within an adversarial reward learning framework. This makes it effective and scalable for Markov games with high-dimensional state-action space and unknown dynamics."}, "16": {"documentation": {"title": "Recognising and evaluating the effectiveness of extortion in the\n  Iterated Prisoner's Dilemma", "source": "Vincent A. Knight and Marc Harper and Nikoleta E. Glynatsi and\n  Jonathan Gillard", "docs_id": "1904.00973", "section": ["cs.GT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Recognising and evaluating the effectiveness of extortion in the\n  Iterated Prisoner's Dilemma. Since the introduction of zero-determinant strategies, extortionate strategies have received considerable interest. While an interesting class of strategies, the definitions of extortionate strategies are algebraically rigid, apply only to memory-one strategies, and require complete knowledge of a strategy (memory-one cooperation probabilities). We describe a method to detect extortionate behaviour from the history of play of a strategy. When applied to a corpus of 204 strategies this method detects extortionate behaviour in well-known extortionate strategies as well others that do not fit the algebraic definition. The highest performing strategies in this corpus are able to exhibit selectively extortionate behavior, cooperating with strong strategies while exploiting weaker strategies, which no memory-one strategy can do. These strategies emerged from an evolutionary selection process and their existence contradicts widely-repeated folklore in the evolutionary game theory literature: complex strategies can be extraordinarily effective, zero-determinant strategies can be outperformed by non-zero determinant strategies, and longer memory strategies are able to outperform short memory strategies. Moreover, while resistance to extortion is critical for the evolution of cooperation, the extortion of weak opponents need not prevent cooperation between stronger opponents, and this adaptability may be crucial to maintaining cooperation in the long run."}, "answer": " Here is a 4-option multiple choice question on the key findings from the given research paper:\n\nQuestion: Which of the following statements accurately summarizes a key finding from the research on recognizing and evaluating extortion in iterated prisoner's dilemma strategies?\n\nA) Extortionate strategies are unable to outperform zero-determinant strategies in iterated prisoner's dilemma.\n\nB) Complex and longer memory strategies cannot be more effective than simple, short memory strategies. \n\nC) Selectively exhibiting extortionate behavior against weaker strategies while cooperating with stronger ones is an effective approach.\n\nD) Resistance to extortion is not important for the evolution of cooperation in iterated prisoner's dilemma.\n\nCorrect Answer: C\n\nExplanation: According to the given passage, the research found that the highest performing iterated prisoner's dilemma strategies were able to selectively exhibit extortionate behavior - cooperating with strong strategies while exploiting weaker ones. This contradicts the belief that complex, longer memory strategies cannot outperform simple, short memory ones."}, "17": {"documentation": {"title": "URSA: A System for Uniform Reduction to SAT", "source": "Predrag Janicic (University of Belgrade)", "docs_id": "1012.1255", "section": ["cs.AI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "URSA: A System for Uniform Reduction to SAT. There are a huge number of problems, from various areas, being solved by reducing them to SAT. However, for many applications, translation into SAT is performed by specialized, problem-specific tools. In this paper we describe a new system for uniform solving of a wide class of problems by reducing them to SAT. The system uses a new specification language URSA that combines imperative and declarative programming paradigms. The reduction to SAT is defined precisely by the semantics of the specification language. The domain of the approach is wide (e.g., many NP-complete problems can be simply specified and then solved by the system) and there are problems easily solvable by the proposed system, while they can be hardly solved by using other programming languages or constraint programming systems. So, the system can be seen not only as a tool for solving problems by reducing them to SAT, but also as a general-purpose constraint solving system (for finite domains). In this paper, we also describe an open-source implementation of the described approach. The performed experiments suggest that the system is competitive to state-of-the-art related modelling systems."}, "answer": " Here is a multi-form question on the given text:\n\nQuestion: Which of the following statements accurately summarizes a key contribution of the URSA system described in the text?\n\nA) URSA allows problems from many domains to be easily translated into SAT formulations using a specialized language.\n\nB) URSA can solve more problems than other constraint programming systems because it uses a declarative programming paradigm. \n\nC) URSA implements a new SAT solver that outperforms existing SAT solvers on certain problem classes.\n\nD) URSA provides the first general approach for reducing problems to SAT across many domains.\n\nCorrect Answer: A\n\nExplanation: The text states that URSA provides \"a new specification language\" that \"combines imperative and declarative programming paradigms\" to allow \"a wide class of problems\" to be reduced to SAT in a \"uniform\" way. So A accurately summarizes that URSA enables easy translation of diverse problems into SAT using its specialized language. B and C are incorrect interpretations - URSA uses SAT solvers, not a new one, and its advantage is the language, not performance. D is too broad, as the text notes there are other systems that reduce problems to SAT, but URSA aims for uniformity across more domains."}, "18": {"documentation": {"title": "Compatible Certificateless and Identity-Based Cryptosystems for\n  Heterogeneous IoT", "source": "Rouzbeh Behnia, Attila A. Yavuz, Muslum Ozgur Ozmen, Tsz Hon Yuen", "docs_id": "2103.09345", "section": ["cs.CR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Compatible Certificateless and Identity-Based Cryptosystems for\n  Heterogeneous IoT. Certificates ensure the authenticity of users' public keys, however their overhead (e.g., certificate chains) might be too costly for some IoT systems like aerial drones. Certificate-free cryptosystems, like identity-based and certificateless systems, lift the burden of certificates and could be a suitable alternative for such IoTs. However, despite their merits, there is a research gap in achieving compatible identity-based and certificateless systems to allow users from different domains (identity-based or certificateless) to communicate seamlessly. Moreover, more efficient constructions can enable their adoption in resource-limited IoTs. In this work, we propose new identity-based and certificateless cryptosystems that provide such compatibility and efficiency. This feature is beneficial for heterogeneous IoT settings (e.g., commercial aerial drones), where different levels of trust/control is assumed on the trusted third party. Our schemes are more communication efficient than their public key based counterparts, as they do not need certificate processing. Our experimental analysis on both commodity and embedded IoT devices show that, only with the cost of having a larger system public key, our cryptosystems are more computation and communication efficient than their certificate-free counterparts. We prove the security of our schemes (in the random oracle model) and open-source our cryptographic framework for public testing/adoption."}, "answer": " Here is a 4-option multiple choice question on the document:\n\nQuestion: Which of the following statements is true about the proposed identity-based and certificateless cryptosystems in the document?\n\nA) They provide compatibility between identity-based and certificateless systems to allow seamless communication between different domains. \n\nB) They are more computationally efficient than public key based systems as they do not require certificate processing.\n\nC) They have a smaller system public key compared to their certificate-free counterparts.\n\nD) They are proven secure without needing the random oracle model assumption.\n\nCorrect Answer: A\n\nExplanation: \nOption A is correct according to the document which states the cryptosystems \"provide compatibility between identity-based and certificateless systems to allow users from different domains (identity-based or certificateless) to communicate seamlessly.\"\n\nOption B is incorrect as the document states the cryptosystems are more communication efficient without certificate processing, not computationally efficient. \n\nOption C is incorrect as the document states the cryptosystems have a larger system public key compared to their certificate-free counterparts.\n\nOption D is incorrect as the document states the security of the schemes is proven in the random oracle model."}, "19": {"documentation": {"title": "Real-Time Decentralized knowledge Transfer at the Edge", "source": "Orpaz Goldstein, Mohammad Kachuee, Derek Shiell, Majid Sarrafzadeh", "docs_id": "2011.05961", "section": ["cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Real-Time Decentralized knowledge Transfer at the Edge. The proliferation of edge networks creates islands of learning agents working on local streams of data. Transferring knowledge between these agents in real-time without exposing private data allows for collaboration to decrease learning time and increase model confidence. Incorporating knowledge from data that a local model did not see creates an ability to debias a local model or add to classification abilities on data never before seen. Transferring knowledge in a selective decentralized approach enables models to retain their local insights, allowing for local flavors of a machine learning model. This approach suits the decentralized architecture of edge networks, as a local edge node will serve a community of learning agents that will likely encounter similar data. We propose a method based on knowledge distillation for pairwise knowledge transfer pipelines from models trained on non-i.i.d. data and compare it to other popular knowledge transfer methods. Additionally, we test different scenarios of knowledge transfer network construction and show the practicality of our approach. Our experiments show knowledge transfer using our model outperforms standard methods in a real-time transfer scenario."}, "answer": " Question: Which of the following statements best summarizes the key goal and approach of the real-time decentralized knowledge transfer method proposed in the paper?\n\nA) To enable collaboration between learning agents by exposing their private training data, in order to decrease learning time.\n\nB) To transfer knowledge between learning agents in real-time using knowledge distillation, without exposing private data. \n\nC) To create a centralized knowledge repository that all learning agents can access to increase their classification abilities.\n\nD) To have learning agents retain all their local insights without incorporating any external knowledge from other agents.\n\nCorrect Answer: B\n\nExplanation: The paper proposes a decentralized approach to enable real-time knowledge transfer between learning agents at the edge using knowledge distillation. The key goal is to allow collaboration between agents to improve their models, without exposing their private local data. Option B summarizes this approach correctly. Option A is incorrect as it mentions exposing private data. Option C proposes a centralized approach instead of a decentralized one. Option D contradicts the goal of transferring knowledge between agents."}, "20": {"documentation": {"title": "Using content features to enhance performance of user-based\n  collaborative filtering performance of user-based collaborative filtering", "source": "Niloofar Rastin and Mansoor Zolghadri Jahromi", "docs_id": "1402.2145", "section": ["cs.IR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Using content features to enhance performance of user-based\n  collaborative filtering performance of user-based collaborative filtering. Content-based and collaborative filtering methods are the most successful solutions in recommender systems. Content based method is based on items attributes. This method checks the features of users favourite items and then proposes the items which have the most similar characteristics with those items. Collaborative filtering method is based on the determination of similar items or similar users, which are called item-based and user-based collaborative filtering, respectively.In this paper we propose a hybrid method that integrates collaborative filtering and content-based methods. The proposed method can be viewed as user-based Collaborative filtering technique. However to find users with similar taste with active user, we used content features of the item under investigation to put more emphasis on users rating for similar items. In other words two users are similar if their ratings are similar on items that have similar context. This is achieved by assigning a weight to each rating when calculating the similarity of two users.We used movielens data set to access the performance of the proposed method in comparison with basic user-based collaborative filtering and other popular methods."}, "answer": " Question: What method is proposed in the paper that integrates collaborative filtering and content-based methods?\n\nA) A hybrid method that uses only content features to find similar users \n\nB) A hybrid method that uses only collaborative filtering to find similar users\n\nC) A hybrid method that uses content features to weight user similarities in collaborative filtering\n\nD) A hybrid method that uses collaborative filtering to weight content feature similarities\n\nCorrect Answer: C\n\nExplanations:\nThe paper proposes a hybrid method that integrates collaborative filtering and content-based methods. Specifically, it uses content features of items to weight user similarities in collaborative filtering. So the correct answer is C.\n\nA is incorrect because the method does not only use content features. \nB is incorrect because the method does not only use collaborative filtering.\nD is incorrect because collaborative filtering is not used to weight content feature similarities."}, "21": {"documentation": {"title": "Cortical oscillations implement a backbone for sampling-based\n  computation in spiking neural networks", "source": "Agnes Korcsak-Gorzo, Michael G. M\\\"uller, Andreas Baumbach, Luziwei\n  Leng, Oliver Julien Breitwieser, Sacha J. van Albada, Walter Senn, Karlheinz\n  Meier, Robert Legenstein, Mihai A. Petrovici", "docs_id": "2006.11099", "section": ["q-bio.NC", "cs.NE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Cortical oscillations implement a backbone for sampling-based\n  computation in spiking neural networks. Brains need to deal with an uncertain world. Often, this requires visiting multiple interpretations of the available information or multiple solutions to an encountered problem. This gives rise to the so-called mixing problem: since all of these \"valid\" states represent powerful attractors, but between themselves can be very dissimilar, switching between such states can be difficult. We propose that cortical oscillations can be effectively used to overcome this challenge. By acting as an effective temperature, background spiking activity modulates exploration. Rhythmic changes induced by cortical oscillations can then be interpreted as a form of simulated tempering. We provide a rigorous mathematical discussion of this link and study some of its phenomenological implications in computer simulations. This identifies a new computational role of cortical oscillations and connects them to various phenomena in the brain, such as sampling-based probabilistic inference, memory replay, multisensory cue combination and place cell flickering."}, "answer": " Here is a 4-option multiple choice question on the given abstract:\n\nQuestion: Which of the following statements best summarizes the main idea of the abstract?\n\nA) Cortical oscillations act as an effective temperature that allows the brain to switch between different interpretations or solutions to a problem by modulating background spiking activity. \n\nB) Cortical oscillations implement a sampling-based computation framework in spiking neural networks that connects to phenomena like memory replay.\n\nC) Cortical oscillations provide a mathematical link between simulated tempering and the mixing problem the brain faces in dealing with an uncertain world.\n\nD) Cortical oscillations induce rhythmic changes in the brain that allow it to deal with uncertainty by exploring multiple valid brain states.\n\nCorrect Answer: D\n\nExplanation: \nThe abstract states that cortical oscillations allow the brain to overcome the mixing problem of switching between dissimilar valid brain states that represent different interpretations/solutions. It proposes cortical oscillations act as an effective temperature that modulates exploration between these states through rhythmic changes. So D best summarizes the main idea."}, "22": {"documentation": {"title": "Practical Speech Recognition with HTK", "source": "Zulkarnaen Hatala", "docs_id": "1908.02119", "section": ["eess.AS", "cs.HC", "cs.LG", "cs.SD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Practical Speech Recognition with HTK. The practical aspects of developing an Automatic Speech Recognition System (ASR) with HTK are reviewed. Steps are explained concerning hardware, software, libraries, applications and computer programs used. The common procedure to rapidly apply speech recognition system is summarized. The procedure is illustrated, to implement a speech based electrical switch in home automation for the Indonesian language. The main key of the procedure is to match the environment for training and testing using the training data recorded from the testing program, HVite. Often the silence detector of HTK is wrongly triggered by noises because the microphone is too sensitive. This problem is mitigated by simply scaling down the volume. In this sub-word phone-based speech recognition, noise is included in the training database and labelled particularly. Illustration of the procedure is applied to a home automation application. Electrical switches are controlled by Indonesian speech recognizer. The results show 100% command completion rate."}, "answer": " Here is a 4-option multiple choice question with explanations for the given text:\n\nQuestion: What is a common issue that arises when implementing a speech recognition system according to the HTK practical guide, and how can it be mitigated?\n\nA) The training data does not match the testing environment, which can be fixed by recording testing data for training.\n\nB) The microphone is too insensitive, which can be fixed by increasing the volume input. \n\nC) The silence detector is triggered by noise, which can be reduced by scaling down the volume.\n\nD) The phone models are too small, which can be fixed by increasing the number of phones.\n\nCorrect Answer: C\n\nExplanations:\n\nA) Incorrect - the guide recommends matching training and testing data, not the other way around.\n\nB) Incorrect - the issue is the microphone is too sensitive, not insensitive. \n\nC) Correct - the guide states the silence detector can be wrongly triggered by noise from a sensitive microphone, and this can be mitigated by scaling down the volume.\n\nD) Incorrect - the guide uses sub-word phone models, but does not state issues with the size of the models."}, "23": {"documentation": {"title": "The FastMap Algorithm for Shortest Path Computations", "source": "Liron Cohen, Tansel Uras, Shiva Jahangiri, Aliyah Arunasalam, Sven\n  Koenig, T.K. Satish Kumar", "docs_id": "1706.02792", "section": ["cs.AI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The FastMap Algorithm for Shortest Path Computations. We present a new preprocessing algorithm for embedding the nodes of a given edge-weighted undirected graph into a Euclidean space. The Euclidean distance between any two nodes in this space approximates the length of the shortest path between them in the given graph. Later, at runtime, a shortest path between any two nodes can be computed with A* search using the Euclidean distances as heuristic. Our preprocessing algorithm, called FastMap, is inspired by the data mining algorithm of the same name and runs in near-linear time. Hence, FastMap is orders of magnitude faster than competing approaches that produce a Euclidean embedding using Semidefinite Programming. FastMap also produces admissible and consistent heuristics and therefore guarantees the generation of shortest paths. Moreover, FastMap applies to general undirected graphs for which many traditional heuristics, such as the Manhattan Distance heuristic, are not well defined. Empirically, we demonstrate that A* search using the FastMap heuristic is competitive with A* search using other state-of-the-art heuristics, such as the Differential heuristic."}, "answer": " Here is a 4-option multiple choice question on the FastMap algorithm documentation:\n\nQuestion: Which of the following statements is true about the FastMap algorithm presented in the documentation?\n\nA) FastMap embeds graph nodes into a high-dimensional Euclidean space to enable shortest path computations.\n\nB) FastMap runs in quadratic time in the number of nodes making it slower than competing approaches. \n\nC) FastMap produces admissible and consistent heuristics and guarantees shortest paths.\n\nD) FastMap applies only to grids and lattices where Manhattan distance heuristics are well-defined.\n\nCorrect Answer: C\n\nExplanation: The documentation states that FastMap embeds graph nodes into a Euclidean space, runs in near-linear time making it faster than competing approaches, produces admissible and consistent heuristics and guarantees shortest paths, and applies to general undirected graphs where Manhattan distance heuristics are not well-defined."}, "24": {"documentation": {"title": "Asymptotically Tight Steady-State Queue Length Bounds Implied By Drift\n  Conditions", "source": "Atilla Eryilmaz and R. Srikant", "docs_id": "1104.0327", "section": ["math.PR", "cs.NI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Asymptotically Tight Steady-State Queue Length Bounds Implied By Drift\n  Conditions. The Foster-Lyapunov theorem and its variants serve as the primary tools for studying the stability of queueing systems. In addition, it is well known that setting the drift of the Lyapunov function equal to zero in steady-state provides bounds on the expected queue lengths. However, such bounds are often very loose due to the fact that they fail to capture resource pooling effects. The main contribution of this paper is to show that the approach of \"setting the drift of a Lyapunov function equal to zero\" can be used to obtain bounds on the steady-state queue lengths which are tight in the heavy-traffic limit. The key is to establish an appropriate notion of state-space collapse in terms of steady-state moments of weighted queue length differences, and use this state-space collapse result when setting the Lyapunov drift equal to zero. As an application of the methodology, we prove the steady-state equivalent of the heavy-traffic optimality result of Stolyar for wireless networks operating under the MaxWeight scheduling policy."}, "answer": " Here is a multi-form question based on the abstract:\n\nQuestion: Which of the following statements is true about the main contribution of the paper?\n\nA) The paper shows that setting the drift of a Lyapunov function equal to zero provides tight bounds on expected queue lengths under all conditions. \n\nB) The paper develops a new Lyapunov function that captures resource pooling effects and provides tighter bounds.\n\nC) The paper shows that setting the drift equal to zero can provide asymptotically tight bounds by using state-space collapse results.\n\nD) The paper proves that the MaxWeight policy is heavy-traffic optimal by setting the Lyapunov drift to zero.\n\nCorrect Answer: C\n\nExplanation: The main contribution according to the abstract is using state-space collapse results when setting the Lyapunov drift equal to zero to obtain asymptotically tight steady-state queue length bounds. Options A and B are incorrect because the paper uses an existing Lyapunov function approach. Option D is incorrect because the optimality of MaxWeight is an application of the main result, not the main result itself."}, "25": {"documentation": {"title": "Solving a fractional parabolic-hyperbolic free boundary problem which\n  models the growth of tumor with drug application using finite\n  difference-spectral method", "source": "Sakine Esmaili, F. Nasresfahani, M.R. Eslahchi", "docs_id": "1908.07386", "section": ["math.NA", "cs.NA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Solving a fractional parabolic-hyperbolic free boundary problem which\n  models the growth of tumor with drug application using finite\n  difference-spectral method. In this paper, a free boundary problem modelling the growth of tumor is considered. The model includes two reaction-diffusion equations modelling the diffusion of nutrient and drug in the tumor and three hyperbolic equations describing the evolution of three types of cells (i.e. proliferative cells, quiescent cells and dead cells) considered in the tumor. Due to the fact that in the real situation, the subdiffusion of nutrient and drug in the tumor can be found, we have changed the reaction-diffusion equations to the fractional ones to consider other conditions and study a more general and reliable model of tumor growth. Since it is important to solve a problem to have a clear vision of the dynamic of tumor growth under the effect of the nutrient and drug, we have solved the fractional free boundary problem. We have solved the fractional parabolic equations employing a combination of spectral and finite difference methods and the hyperbolic equations are solved using characteristic equation and finite difference method. It is proved that the presented method is unconditionally convergent and stable to be sure that we have a correct vision of tumor growth dynamic. Finally, by presenting some numerical examples and showing the results, the theoretical statements are justified."}, "answer": " Here is a 4-option multiple choice question on the paper:\n\nQuestion: According to the paper, which of the following statements is true about the model presented for tumor growth?\n\nA) It uses integer-order partial differential equations to model the diffusion of nutrient and drug.\n\nB) It employs only spectral methods to solve the fractional parabolic equations.  \n\nC) The hyperbolic equations describing cell populations are solved using only finite difference methods.\n\nD) The model incorporates fractional-order equations to account for subdiffusion of nutrient and drug.\n\nCorrect Answer: D\n\nExplanation: The paper states that fractional partial differential equations are used to model the diffusion of nutrient and drug, in order to account for subdiffusion effects. The fractional parabolic equations are solved using a combination of spectral and finite difference methods. The hyperbolic equations use both characteristic equations and finite difference methods. Therefore, option D is correct."}, "26": {"documentation": {"title": "Fast-Convergent Dynamics for Distributed Allocation of Resources Over\n  Switching Sparse Networks with Quantized Communication Links", "source": "Mohammadreza Doostmohammadian, Alireza Aghasi, Mohammad Pirani, Ehsan\n  Nekouei, Usman A. Khan, Themistoklis Charalambous", "docs_id": "2012.08181", "section": ["eess.SY", "cs.LG", "cs.MA", "cs.SI", "cs.SY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Fast-Convergent Dynamics for Distributed Allocation of Resources Over\n  Switching Sparse Networks with Quantized Communication Links. This paper proposes networked dynamics to solve resource allocation problems over time-varying multi-agent networks. The state of each agent represents the amount of used resources (or produced utilities) while the total amount of resources is fixed. The idea is to optimally allocate the resources among the group of agents by minimizing the overall cost function subject to fixed sum of resources. Each agents' information is restricted to its own state and cost function and those of its immediate in-neighbors. This is motivated by distributed applications such as mobile edge-computing, economic dispatch over smart grids, and multi-agent coverage control. This work provides a fast convergent solution (in comparison with linear dynamics) while considering relaxed network connectivity with quantized communication links. The proposed dynamics reaches optimal solution over switching (possibly disconnected) undirected networks as far as their union over some bounded non-overlapping time-intervals has a spanning-tree. We prove feasibility of the solution, uniqueness of the optimal state, and convergence to the optimal value under the proposed dynamics, where the analysis is applicable to similar 1st-order allocation dynamics with strongly sign-preserving nonlinearities, such as actuator saturation."}, "answer": " Question: Which of the following statements accurately summarizes a key contribution of the paper \"Fast-Convergent Dynamics for Distributed Allocation of Resources Over Switching Sparse Networks with Quantized Communication Links\"?\n\nA) The paper proposes a distributed algorithm for resource allocation that converges slowly but works over sparse networks.\n\nB) The paper proves convergence of a class of distributed resource allocation algorithms over time-varying networks. \n\nC) The paper introduces a new class of nonlinear dynamics for distributed optimization that requires continuous communication.\n\nD) The paper provides a fast convergent solution for resource allocation over switching networks with quantized communication.\n\nCorrect Answer: D\n\nThe paper proposes fast converging dynamics for distributed resource allocation over switching/time-varying networks with quantized communication between agents. Option D summarizes this key contribution accurately. Options A, B, and C mention aspects covered in the paper but do not fully capture the key ideas."}, "27": {"documentation": {"title": "New Results for the Complexity of Resilience for Binary Conjunctive\n  Queries with Self-Joins", "source": "Cibele Freire, Wolfgang Gatterbauer, Neil Immerman, Alexandra Meliou", "docs_id": "1907.01129", "section": ["cs.DB", "cs.CC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "New Results for the Complexity of Resilience for Binary Conjunctive\n  Queries with Self-Joins. The resilience of a Boolean query is the minimum number of tuples that need to be deleted from the input tables in order to make the query false. A solution to this problem immediately translates into a solution for the more widely known problem of deletion propagation with source-side effects. In this paper, we give several novel results on the hardness of the resilience problem for $\\textit{binary conjunctive queries with self-joins}$ (i.e. conjunctive queries with relations of maximal arity 2) with one repeated relation. Unlike in the self-join free case, the concept of triad is not enough to fully characterize the complexity of resilience. We identify new structural properties, namely chains, confluences and permutations, which lead to various $NP$-hardness results. We also give novel involved reductions to network flow to show certain cases are in $P$. Overall, we give a dichotomy result for the restricted setting when one relation is repeated at most 2 times, and we cover many of the cases for 3. Although restricted, our results provide important insights into the problem of self-joins that we hope can help solve the general case of all conjunctive queries with self-joins in the future."}, "answer": " Here is a 4-option multiple choice question on the paper:\n\nQuestion: According to the paper, what is the resilience of a Boolean query defined as?\n\nA) The minimum number of tuples that need to be added to the input tables to make the query true.\n\nB) The minimum number of tuples that need to be deleted from the input tables to make the query false. \n\nC) The maximum number of tuples that can be deleted from the input tables while keeping the query true.\n\nD) The minimum number of tuples that need to be modified in the input tables to change the query result.\n\nCorrect Answer: B\n\nExplanation: The paper defines the resilience of a Boolean query as the minimum number of tuples that need to be deleted from the input tables in order to make the query false. Option B states this definition correctly."}, "28": {"documentation": {"title": "Learning to Unknot", "source": "Sergei Gukov, James Halverson, Fabian Ruehle, Piotr Su{\\l}kowski", "docs_id": "2010.16263", "section": ["math.GT", "cs.LG", "hep-th"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Learning to Unknot. We introduce natural language processing into the study of knot theory, as made natural by the braid word representation of knots. We study the UNKNOT problem of determining whether or not a given knot is the unknot. After describing an algorithm to randomly generate $N$-crossing braids and their knot closures and discussing the induced prior on the distribution of knots, we apply binary classification to the UNKNOT decision problem. We find that the Reformer and shared-QK Transformer network architectures outperform fully-connected networks, though all perform well. Perhaps surprisingly, we find that accuracy increases with the length of the braid word, and that the networks learn a direct correlation between the confidence of their predictions and the degree of the Jones polynomial. Finally, we utilize reinforcement learning (RL) to find sequences of Markov moves and braid relations that simplify knots and can identify unknots by explicitly giving the sequence of unknotting actions. Trust region policy optimization (TRPO) performs consistently well for a wide range of crossing numbers and thoroughly outperformed other RL algorithms and random walkers. Studying these actions, we find that braid relations are more useful in simplifying to the unknot than one of the Markov moves."}, "answer": " Question: Which of the following statements is true about the paper Learning to Unknot?\n\nA) The paper introduces knot theory into natural language processing by representing knots with braid words. Binary classification is applied to the UNKNOT decision problem and Reformer and Transformer networks outperform fully-connected networks. Reinforcement learning with TRPO consistently performs well at identifying unknotting actions. \n\nB) The paper studies the KNOT problem of determining if a knot is knotted. Fully-connected networks outperform Transformer architectures at classifying knots. Markov moves are more useful than braid relations for unknotting according to the reinforcement learning experiments.\n\nC) The paper introduces neuro-symbolic AI into knot theory by having language models read research papers. Fully-connected networks outperform Transformers, but all models struggle on long braid words. Random walkers outperform reinforcement learning algorithms at unknotting.\n\nD) The paper studies the LINK problem of finding links between knots. Reinforcement learning struggles to identify useful unknotting actions compared to random walkers. Braid relations are less useful than Markov moves according to the results.\n\nCorrect Answer: A\n\nExplanation: Option A accurately summarizes key points about the paper - using braid words, applying binary classification to UNKNOT problem, Transformer networks outperforming fully-connected networks, and TRPO reinforcement learning identifying useful unknotting actions. Options B, C, and D contain incorrect or fabricated details about the paper's content."}, "29": {"documentation": {"title": "Accurate Temporal Action Proposal Generation with Relation-Aware Pyramid\n  Network", "source": "Jialin Gao, Zhixiang Shi, Jiani Li, Guanshuo Wang, Yufeng Yuan,\n  Shiming Ge, and Xi Zhou", "docs_id": "2003.04145", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Accurate Temporal Action Proposal Generation with Relation-Aware Pyramid\n  Network. Accurate temporal action proposals play an important role in detecting actions from untrimmed videos. The existing approaches have difficulties in capturing global contextual information and simultaneously localizing actions with different durations. To this end, we propose a Relation-aware pyramid Network (RapNet) to generate highly accurate temporal action proposals. In RapNet, a novel relation-aware module is introduced to exploit bi-directional long-range relations between local features for context distilling. This embedded module enhances the RapNet in terms of its multi-granularity temporal proposal generation ability, given predefined anchor boxes. We further introduce a two-stage adjustment scheme to refine the proposal boundaries and measure their confidence in containing an action with snippet-level actionness. Extensive experiments on the challenging ActivityNet and THUMOS14 benchmarks demonstrate our RapNet generates superior accurate proposals over the existing state-of-the-art methods."}, "answer": " Here is a 4-option multiple choice question on the paper:\n\nQuestion: Which of the following statements is true about the Relation-aware Pyramid Network (RapNet) proposed in the paper?\n\nA) RapNet uses a novel relation-aware module to model long-range dependencies between local features for better context modeling. \n\nB) RapNet generates multi-scale proposals using predefined anchors without any refinement.\n\nC) RapNet directly predicts actionness scores for fixed-size snippets without boundary adjustment. \n\nD) RapNet relies only on local features within each proposal for actionness estimation.\n\nCorrect Answer: A\n\nExplanation: \nOption A is correct according to the paper - RapNet introduces a relation-aware module to exploit bi-directional long-range relations between local features for better context modeling.\nOption B is incorrect - RapNet further introduces a two-stage adjustment scheme to refine the proposal boundaries. \nOption C is incorrect - RapNet measures confidence of proposals containing an action using snippet-level actionness after boundary adjustment.\nOption D is incorrect - RapNet uses both local features and global context for actionness estimation."}, "30": {"documentation": {"title": "Pretraining boosts out-of-domain robustness for pose estimation", "source": "Alexander Mathis, Thomas Biasi, Steffen Schneider, Mert\n  Y\\\"uksekg\\\"on\\\"ul, Byron Rogers, Matthias Bethge, Mackenzie W. Mathis", "docs_id": "1909.11229", "section": ["cs.CV", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Pretraining boosts out-of-domain robustness for pose estimation. Neural networks are highly effective tools for pose estimation. However, as in other computer vision tasks, robustness to out-of-domain data remains a challenge, especially for small training sets that are common for real-world applications. Here, we probe the generalization ability with three architecture classes (MobileNetV2s, ResNets, and EfficientNets) for pose estimation. We developed a dataset of 30 horses that allowed for both \"within-domain\" and \"out-of-domain\" (unseen horse) benchmarking - this is a crucial test for robustness that current human pose estimation benchmarks do not directly address. We show that better ImageNet-performing architectures perform better on both within- and out-of-domain data if they are first pretrained on ImageNet. We additionally show that better ImageNet models generalize better across animal species. Furthermore, we introduce Horse-C, a new benchmark for common corruptions for pose estimation, and confirm that pretraining increases performance in this domain shift context as well. Overall, our results demonstrate that transfer learning is beneficial for out-of-domain robustness."}, "answer": " Question: Which of the following statements is supported by the research described in the given documentation?\n\nA) Pretraining on ImageNet improves performance on unseen horses, but not on seen horses from the same dataset.\n\nB) Better ImageNet models generalize worse across animal species for pose estimation. \n\nC) Pretraining boosts robustness to common corruptions introduced in the new Horse-C benchmark dataset.\n\nD) Small training sets are sufficient for good generalization to out-of-domain data in pose estimation tasks.\n\nCorrect Answer: C) Pretraining boosts robustness to common corruptions introduced in the new Horse-C benchmark dataset.\n\nExplanation: The documentation states that \"we additionally show that better ImageNet models generalize better across animal species. Furthermore, we introduce Horse-C, a new benchmark for common corruptions for pose estimation, and confirm that pretraining increases performance in this domain shift context as well.\" This supports answer choice C."}, "31": {"documentation": {"title": "Wireless-Powered Relays in Cooperative Communications: Time-Switching\n  Relaying Protocols and Throughput Analysis", "source": "Ali Arshad Nasir, Xiangyun Zhou, Salman Durrani, and Rodney A. Kennedy", "docs_id": "1310.7648", "section": ["cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Wireless-Powered Relays in Cooperative Communications: Time-Switching\n  Relaying Protocols and Throughput Analysis. We consider wireless-powered amplify-and-forward and decode-and-forward relaying in cooperative communications, where an energy constrained relay node first harvests energy through the received radio-frequency signal from the source and then uses the harvested energy to forward the source information to the destination node. We propose time-switching based energy harvesting (EH) and information transmission (IT) protocols with two modes of EH at the relay. For continuous time EH, the EH time can be any percentage of the total transmission block time. For discrete time EH, the whole transmission block is either used for EH or IT. The proposed protocols are attractive because they do not require channel state information at the transmitter side and enable relay transmission with preset fixed transmission power. We derive analytical expressions of the achievable throughput for the proposed protocols. The derived expressions are verified by comparison with simulations and allow the system performance to be determined as a function of the system parameters. Finally, we show that the proposed protocols outperform the existing fixed time duration EH protocols in the literature, since they intelligently track the level of the harvested energy to switch between EH and IT in an online fashion, allowing efficient use of resources."}, "answer": " Here is a 4-option multiple choice question on the paper:\n\nQuestion: Which of the following statements is true about the time-switching based energy harvesting protocols proposed in the paper?\n\nA) The protocols require channel state information at the transmitter side to optimize the time allocation between energy harvesting and information transmission.\n\nB) The protocols allow the relay to transmit with variable transmission power depending on the amount of harvested energy. \n\nC) The protocols outperform existing fixed time duration protocols by adapting the time split between energy harvesting and information transmission.\n\nD) The protocols use only discrete time energy harvesting where the whole block is used for either energy harvesting or information transmission.\n\nCorrect Answer: C\n\nExplanation: \nOption A is incorrect as the paper specifically mentions the protocols do not require CSI at the transmitter side. \nOption B is incorrect as the paper mentions the protocols enable transmission at fixed preset power.\nOption C is correct as the paper shows the adaptive time-switching protocols outperform fixed time protocols by tracking the harvested energy.\nOption D is incorrect as the paper proposes protocols with both continuous and discrete time energy harvesting."}, "32": {"documentation": {"title": "Topological optimization of hybrid quantum key distribution networks", "source": "Ya-Xing Wang, Qiong Li, Hao-Kun Mao, Qi Han, Fu-Rong Huang, Hong-Wei\n  Xu", "docs_id": "2003.14100", "section": ["quant-ph", "cs.NI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Topological optimization of hybrid quantum key distribution networks. With the growing complexity of quantum key distribution (QKD) network structures, aforehand topology design is of great significance to support a large-number of nodes over a large-spatial area. However, the exclusivity of quantum channels, the limitation of key generation capabilities, the variety of QKD protocols and the necessity of untrusted-relay selection, make the optimal topology design a very complicated task. In this research, a hybrid QKD network is studied for the first time from the perspective of topology, by analyzing the topological differences of various QKD protocols. In addition, to make full use of hybrid networking, an analytical model for optimal topology calculation is proposed, to reach the goal of best secure communication service by optimizing the deployment of various QKD devices and the selection of untrusted-relays under a given cost limit. Plentiful simulation results show that hybrid networking and untrusted-relay selection can bring great performance advantages, and then the universality and effectiveness of the proposed analytical model are verified."}, "answer": " Here is a 4-option multiple choice question on the paper with explanations:\n\nQuestion: Which of the following statements is true about the proposed analytical model for optimal topology calculation in the paper?\n\nA) It aims to optimize the deployment of various QKD devices under a given cost limit.\n\nB) It considers the selection of trusted relays as a key factor. \n\nC) It is limited to pure QKD networks and cannot handle hybrid networks.\n\nD) It focuses on minimizing the number of nodes in the network.\n\nCorrect Answer: A\n\nExplanations:\n\nA) Correct. According to the paper, the proposed analytical model aims to optimize the deployment of various QKD devices and the selection of untrusted relays under a given cost limit. \n\nB) Incorrect. The model considers the selection of untrusted relays, not trusted relays.\n\nC) Incorrect. The paper states that the model is proposed for hybrid QKD networks. \n\nD) Incorrect. The model aims to optimize topology to support a large number of nodes over a large area, not to minimize the number of nodes."}, "33": {"documentation": {"title": "Few-Example Object Detection with Model Communication", "source": "Xuanyi Dong, Liang Zheng, Fan Ma, Yi Yang, Deyu Meng", "docs_id": "1706.08249", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Few-Example Object Detection with Model Communication. In this paper, we study object detection using a large pool of unlabeled images and only a few labeled images per category, named \"few-example object detection\". The key challenge consists in generating trustworthy training samples as many as possible from the pool. Using few training examples as seeds, our method iterates between model training and high-confidence sample selection. In training, easy samples are generated first and, then the poorly initialized model undergoes improvement. As the model becomes more discriminative, challenging but reliable samples are selected. After that, another round of model improvement takes place. To further improve the precision and recall of the generated training samples, we embed multiple detection models in our framework, which has proven to outperform the single model baseline and the model ensemble method. Experiments on PASCAL VOC'07, MS COCO'14, and ILSVRC'13 indicate that by using as few as three or four samples selected for each category, our method produces very competitive results when compared to the state-of-the-art weakly-supervised approaches using a large number of image-level labels."}, "answer": " Question: What is the key challenge studied in the paper \"Few-Example Object Detection with Model Communication\"?\n\nA) Detecting objects using only image-level labels\nB) Detecting objects with a small labeled dataset per category \nC) Ensemble methods for object detection\nD) Unsupervised object detection\n\nCorrect Answer: B) Detecting objects with a small labeled dataset per category\n\nExplanation: The key challenge studied in this paper is object detection using only a few labeled images per category, which they refer to as \"few-example object detection\". The paper aims to generate reliable training samples from a pool of unlabeled images using just a small set of labeled examples per category."}, "34": {"documentation": {"title": "Data-Driven Extract Method Recommendations: A Study at ING", "source": "David van der Leij and Jasper Binda and Robbert van Dalen and Pieter\n  Vallen and Yaping Luo and Maur\\'icio Aniche", "docs_id": "2107.05396", "section": ["cs.SE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Data-Driven Extract Method Recommendations: A Study at ING. The sound identification of refactoring opportunities is still an open problem in software engineering. Recent studies have shown the effectiveness of machine learning models in recommending methods that should undergo different refactoring operations. In this work, we experiment with such approaches to identify methods that should undergo an Extract Method refactoring, in the context of ING, a large financial organization. More specifically, we (i) compare the code metrics distributions, which are used as features by the models, between open-source and ING systems, (ii) measure the accuracy of different machine learning models in recommending Extract Method refactorings, (iii) compare the recommendations given by the models with the opinions of ING experts. Our results show that the feature distributions of ING systems and open-source systems are somewhat different, that machine learning models can recommend Extract Method refactorings with high accuracy, and that experts tend to agree with most of the recommendations of the model."}, "answer": " Question: Which of the following statements is true about the study described in the given text?\n\nA) The feature distributions of open-source and ING systems were found to be very similar.\n\nB) Machine learning models were not very accurate in recommending Extract Method refactorings. \n\nC) Experts disagreed with most of the Extract Method recommendations made by the machine learning models.\n\nD) The study compared code metrics distributions between open-source and ING systems.\n\nCorrect Answer: D\n\nExplanation: \nThe given text states that the study \"(i) compare[d] the code metrics distributions, which are used as features by the models, between open-source and ING systems\". This matches option D. Options A, B and C are incorrect interpretations of the information provided in the text."}, "35": {"documentation": {"title": "Improve Learning from Crowds via Generative Augmentation", "source": "Zhendong Chu, Hongning Wang", "docs_id": "2107.10449", "section": ["cs.LG", "cs.CV", "cs.HC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Improve Learning from Crowds via Generative Augmentation. Crowdsourcing provides an efficient label collection schema for supervised machine learning. However, to control annotation cost, each instance in the crowdsourced data is typically annotated by a small number of annotators. This creates a sparsity issue and limits the quality of machine learning models trained on such data. In this paper, we study how to handle sparsity in crowdsourced data using data augmentation. Specifically, we propose to directly learn a classifier by augmenting the raw sparse annotations. We implement two principles of high-quality augmentation using Generative Adversarial Networks: 1) the generated annotations should follow the distribution of authentic ones, which is measured by a discriminator; 2) the generated annotations should have high mutual information with the ground-truth labels, which is measured by an auxiliary network. Extensive experiments and comparisons against an array of state-of-the-art learning from crowds methods on three real-world datasets proved the effectiveness of our data augmentation framework. It shows the potential of our algorithm for low-budget crowdsourcing in general."}, "answer": " Question: Which of the following is a principle of high-quality augmentation proposed in the paper for handling sparsity in crowdsourced data?\n\nA) The generated annotations should have low mutual information with the ground-truth labels\n\nB) The generated annotations should not follow the distribution of authentic annotations  \n\nC) The generated annotations should have high mutual information with the ground-truth labels\n\nD) The generated annotations should be very different from the authentic annotations\n\nCorrect Answer: C\n\nExplanation: According to the documentation, the proposed augmentation method uses two principles of high-quality augmentation: 1) the generated annotations should follow the distribution of authentic ones, and 2) the generated annotations should have high mutual information with the ground-truth labels. So C is the correct answer."}, "36": {"documentation": {"title": "Improving Universal Sound Separation Using Sound Classification", "source": "Efthymios Tzinis, Scott Wisdom, John R. Hershey, Aren Jansen, Daniel\n  P. W. Ellis", "docs_id": "1911.07951", "section": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Improving Universal Sound Separation Using Sound Classification. Deep learning approaches have recently achieved impressive performance on both audio source separation and sound classification. Most audio source separation approaches focus only on separating sources belonging to a restricted domain of source classes, such as speech and music. However, recent work has demonstrated the possibility of \"universal sound separation\", which aims to separate acoustic sources from an open domain, regardless of their class. In this paper, we utilize the semantic information learned by sound classifier networks trained on a vast amount of diverse sounds to improve universal sound separation. In particular, we show that semantic embeddings extracted from a sound classifier can be used to condition a separation network, providing it with useful additional information. This approach is especially useful in an iterative setup, where source estimates from an initial separation stage and their corresponding classifier-derived embeddings are fed to a second separation network. By performing a thorough hyperparameter search consisting of over a thousand experiments, we find that classifier embeddings from clean sources provide nearly one dB of SNR gain, and our best iterative models achieve a significant fraction of this oracle performance, establishing a new state-of-the-art for universal sound separation."}, "answer": " Here is a 4-option multiple choice question on the paper:\n\nQuestion: Which of the following statements is true about the approach presented in the paper for improving universal sound separation?\n\nA) It uses embeddings from a sound classifier trained only on speech data to condition the separation network.\n\nB) It shows that iterative separation provides no benefits over single-pass separation. \n\nC) It establishes a new state-of-the-art for music source separation specifically.\n\nD) It demonstrates that classifier embeddings from estimated sources can provide gains close to those from clean sources.\n\nCorrect Answer: D\n\nExplanation: The paper shows that using semantic embeddings from a classifier trained on diverse sounds can improve universal sound separation. In an iterative setup, embeddings from initial estimates help in the second pass, achieving gains close to those obtained using oracle embeddings from clean sources. This establishes a new overall state-of-the-art for universal separation."}, "37": {"documentation": {"title": "Towards Training Recurrent Neural Networks for Lifelong Learning", "source": "Shagun Sodhani, Sarath Chandar, Yoshua Bengio", "docs_id": "1811.07017", "section": ["cs.LG", "cs.AI", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Towards Training Recurrent Neural Networks for Lifelong Learning. Catastrophic forgetting and capacity saturation are the central challenges of any parametric lifelong learning system. In this work, we study these challenges in the context of sequential supervised learning with an emphasis on recurrent neural networks. To evaluate the models in the lifelong learning setting, we propose a curriculum-based, simple, and intuitive benchmark where the models are trained on tasks with increasing levels of difficulty. To measure the impact of catastrophic forgetting, the model is tested on all the previous tasks as it completes any task. As a step towards developing true lifelong learning systems, we unify Gradient Episodic Memory (a catastrophic forgetting alleviation approach) and Net2Net(a capacity expansion approach). Both these models are proposed in the context of feedforward networks and we evaluate the feasibility of using them for recurrent networks. Evaluation on the proposed benchmark shows that the unified model is more suitable than the constituent models for lifelong learning setting."}, "answer": " Here is a 4-option multiple choice question on the given text:\n\nQuestion: Which of the following statements best describes the main contribution of the paper?\n\nA) It proposes a new recurrent neural network architecture for lifelong learning that unifies Gradient Episodic Memory and Net2Net.\n\nB) It shows that catastrophic forgetting is more challenging for recurrent neural networks compared to feedforward networks. \n\nC) It introduces a new benchmark task for evaluating lifelong learning systems based on a curriculum of tasks with increasing difficulty.\n\nD) It demonstrates that Gradient Episodic Memory alone is sufficient for enabling lifelong learning in recurrent neural networks.\n\nCorrect Answer: C\n\nExplanation: The main contribution of the paper is proposing a new curriculum-based benchmark for evaluating lifelong learning systems. The benchmark involves training models on tasks of increasing difficulty and testing on all previous tasks to measure catastrophic forgetting. While the paper also evaluates a unified model combining Gradient Episodic Memory and Net2Net, the primary focus is on introducing the new benchmark."}, "38": {"documentation": {"title": "Modular Pipe Climber", "source": "Rama Vadapalli, Kartik Suryavanshi, Ruchita Vucha, Abhishek Sarkar, K\n  Madhava Krishna", "docs_id": "1909.10195", "section": ["cs.RO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Modular Pipe Climber. This paper discusses the design and implementation of the Modular Pipe Climber inside ASTM D1785 - 15e1 standard pipes [1]. The robot has three tracks which operate independently and are mounted on three modules which are oriented at 120{\\deg} to each other. The tracks provide for greater surface traction compared to wheels [2]. The tracks are pushed onto the inner wall of the pipe by passive springs which help in maintaining the contact with the pipe during vertical climb and while turning in bends. The modules have the provision to compress asymmetrically, which helps the robot to take turns in bends in all directions. The motor torque required by the robot and the desired spring stiffness are calculated at quasistatic and static equilibriums when the pipe climber is in a vertical climb. The springs were further simulated and analyzed in ADAMS MSC. The prototype built based on these obtained values was experimented on, in complex pipe networks. Differential speed is employed when turning in bends to improve the efficiency and reduce the stresses experienced by the robot."}, "answer": " Here is a 4-option multiple choice question on the Modular Pipe Climber paper:\n\nQuestion: Which of the following statements is true about the Modular Pipe Climber robot described in the paper?\n\nA) The robot uses wheels instead of tracks for better traction inside pipes.\n\nB) The modules are oriented at 90 degrees to each other to provide stability. \n\nC) Passive springs help maintain contact with the pipe wall when climbing and turning.\n\nD) Symmetric compression of modules helps the robot turn smoothly in all directions.\n\nCorrect Answer: C\n\nExplanation: According to the paper, the Modular Pipe Climber uses tracks instead of wheels for better traction. The three modules are oriented at 120 degrees to each other. Passive springs help maintain contact with the pipe wall during climbing and turning in bends. Asymmetric compression of the modules helps the robot turn in bends, not symmetric compression."}, "39": {"documentation": {"title": "Learning Efficient Structured Sparse Models", "source": "Alex Bronstein (Tel Aviv University), Pablo Sprechmann (University of\n  Minnesota), Guillermo Sapiro (University of Minnesota)", "docs_id": "1206.4649", "section": ["cs.LG", "cs.CV", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Learning Efficient Structured Sparse Models. We present a comprehensive framework for structured sparse coding and modeling extending the recent ideas of using learnable fast regressors to approximate exact sparse codes. For this purpose, we develop a novel block-coordinate proximal splitting method for the iterative solution of hierarchical sparse coding problems, and show an efficient feed forward architecture derived from its iteration. This architecture faithfully approximates the exact structured sparse codes with a fraction of the complexity of the standard optimization methods. We also show that by using different training objective functions, learnable sparse encoders are no longer restricted to be mere approximants of the exact sparse code for a pre-given dictionary, as in earlier formulations, but can be rather used as full-featured sparse encoders or even modelers. A simple implementation shows several orders of magnitude speedup compared to the state-of-the-art at minimal performance degradation, making the proposed framework suitable for real time and large-scale applications."}, "answer": " Here is a 4-option multiple choice question on the paper:\n\nQuestion: Which of the following statements is true about the structured sparse coding framework presented in the paper?\n\nA) It uses a novel block-coordinate proximal splitting method to approximate sparse codes more efficiently than standard optimization methods.\n\nB) It shows that learnable sparse encoders must be mere approximants of the exact sparse code for a fixed dictionary.\n\nC) It is restricted to hierarchical sparse coding problems and cannot handle more general structured sparsity.\n\nD) It achieves performance matching the state-of-the-art but with minimal speedup over existing methods.\n\nCorrect Answer: A\n\nExplanation: The paper presents a framework for structured sparse coding using a novel block-coordinate proximal splitting method to efficiently approximate sparse codes. It shows this method can achieve orders of magnitude speedup over existing methods with minimal performance loss. The framework is general for structured sparsity, not just hierarchical. And it shows learnable encoders can do more than just approximate fixed dictionaries."}, "40": {"documentation": {"title": "Generating Realistic Synthetic Population Datasets", "source": "Hao Wu, Yue Ning, Prithwish Chakraborty, Jilles Vreeken, Nikolaj Tatti\n  and Naren Ramakrishnan", "docs_id": "1602.06844", "section": ["cs.DB"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Generating Realistic Synthetic Population Datasets. Modern studies of societal phenomena rely on the availability of large datasets capturing attributes and activities of synthetic, city-level, populations. For instance, in epidemiology, synthetic population datasets are necessary to study disease propagation and intervention measures before implementation. In social science, synthetic population datasets are needed to understand how policy decisions might affect preferences and behaviors of individuals. In public health, synthetic population datasets are necessary to capture diagnostic and procedural characteristics of patient records without violating confidentialities of individuals. To generate such datasets over a large set of categorical variables, we propose the use of the maximum entropy principle to formalize a generative model such that in a statistically well-founded way we can optimally utilize given prior information about the data, and are unbiased otherwise. An efficient inference algorithm is designed to estimate the maximum entropy model, and we demonstrate how our approach is adept at estimating underlying data distributions. We evaluate this approach against both simulated data and on US census datasets, and demonstrate its feasibility using an epidemic simulation application."}, "answer": " Here is a 4-option multiple choice question with explanations for the given text:\n\nQuestion: Which of the following statements accurately describes a key contribution of the proposed maximum entropy approach for generating synthetic population datasets?\n\nA) It allows for efficient simulation of disease propagation without violating patient confidentiality.\n\nB) It provides an unbiased way to estimate underlying data distributions using available prior information. \n\nC) It can be used to understand how policy decisions affect individuals' preferences and behaviors.\n\nD) It is the first approach capable of generating city-level population data for societal studies.\n\nCorrect Answer: B\n\nExplanations:\n\nA) While the paper mentions applications in epidemiology and public health, violating patient confidentiality is not discussed as a key contribution of the maximum entropy approach itself.\n\nB) Correct. The paper states the maximum entropy principle allows optimally utilizing prior information while being unbiased otherwise to estimate underlying data distributions. \n\nC) While a potential application, understanding individual preferences/behaviors is not a direct contribution of the maximum entropy approach.\n\nD) The paper does not claim this is the first approach for generating population data, only that such data is necessary for modern societal studies."}, "41": {"documentation": {"title": "Connectivity-Aware Traffic Phase Scheduling for Heterogeneously\n  Connected Vehicles", "source": "Shanyu Zhou and Hulya Seferoglu", "docs_id": "1608.07352", "section": ["cs.SY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Connectivity-Aware Traffic Phase Scheduling for Heterogeneously\n  Connected Vehicles. We consider a transportation system of heterogeneously connected vehicles, where not all vehicles are able to communicate. Heterogeneous connectivity in transportation systems is coupled to practical constraints such that (i) not all vehicles may be equipped with devices having communication interfaces, (ii) some vehicles may not prefer to communicate due to privacy and security reasons, and (iii) communication links are not perfect and packet losses and delay occur in practice. In this context, it is crucial to develop control algorithms by taking into account the heterogeneity. In this paper, we particularly focus on making traffic phase scheduling decisions. We develop a connectivity-aware traffic phase scheduling algorithm for heterogeneously connected vehicles that increases the intersection efficiency (in terms of the average number of vehicles that are allowed to pass the intersection) by taking into account the heterogeneity. The simulation results show that our algorithm significantly improves the efficiency of intersections as compared to the baselines."}, "answer": " Question: What is a key consideration when developing traffic phase scheduling algorithms for intersections with heterogeneously connected vehicles?\n\nA) Ensuring that all vehicles are equipped with the same communication interfaces.\n\nB) Prioritizing vehicle privacy and security over efficient traffic flow. \n\nC) Assuming that all vehicles will have perfect communication links without any packet losses or delays.\n\nD) Taking into account the heterogeneity in vehicle connectivity when making scheduling decisions.\n\nCorrect Answer: D) Taking into account the heterogeneity in vehicle connectivity when making scheduling decisions.\n\nExplanations:\n\nA) The passage states that not all vehicles may be equipped with the same communication interfaces, so assuming homogeneous interfaces is incorrect.\n\nB) The passage mentions privacy and security as practical constraints, but the focus is on efficiently scheduling phases despite heterogeneous connectivity.\n\nC) The passage notes that communication links are not perfect in practice, with packet losses and delays occurring. \n\nD) Correct. The passage emphasizes the need to develop algorithms that account for heterogeneous vehicle connectivity when scheduling traffic phases."}, "42": {"documentation": {"title": "Efficient Distributed Medium Access", "source": "Devavrat Shah, Jinwoo Shin and Prasad Tetali", "docs_id": "1104.2380", "section": ["cs.NI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Efficient Distributed Medium Access. Consider a wireless network of n nodes represented by a graph G=(V, E) where an edge (i,j) models the fact that transmissions of i and j interfere with each other, i.e. simultaneous transmissions of i and j become unsuccessful. Hence it is required that at each time instance a set of non-interfering nodes (corresponding to an independent set in G) access the wireless medium. To utilize wireless resources efficiently, it is required to arbitrate the access of medium among interfering nodes properly. Moreover, to be of practical use, such a mechanism is required to be totally distributed as well as simple. As the main result of this paper, we provide such a medium access algorithm. It is randomized, totally distributed and simple: each node attempts to access medium at each time with probability that is a function of its local information. We establish efficiency of the algorithm by showing that the corresponding network Markov chain is positive recurrent as long as the demand imposed on the network can be supported by the wireless network (using any algorithm). In that sense, the proposed algorithm is optimal in terms of utilizing wireless resources. The algorithm is oblivious to the network graph structure, in contrast with the so-called `polynomial back-off' algorithm by Hastad-Leighton-Rogoff (STOC '87, SICOMP '96) that is established to be optimal for the complete graph and bipartite graphs (by Goldberg-MacKenzie (SODA '96, JCSS '99))."}, "answer": " Here is a multi-form question on the paper:\n\nQuestion: Which of the following statements about the proposed medium access algorithm in the paper is true?\n\nA) It requires global knowledge of the network graph structure.\n\nB) It is based on a polynomial back-off scheme like the algorithm by Hastad-Leighton-Rogoff. \n\nC) It is randomized, distributed, and simple with each node accessing the medium with a probability based on local information.\n\nD) It has been proven optimal for complete graphs and bipartite graphs.\n\nCorrect Answer: C\n\nExplanation: The paper proposes a randomized, distributed, and simple medium access algorithm where each node attempts to access the medium with a probability based on its local information. It does not require global knowledge of the network structure, unlike the polynomial back-off algorithm by Hastad-Leighton-Rogoff. The algorithm's optimality has not been established for specific graph structures like complete or bipartite graphs."}, "43": {"documentation": {"title": "Building an Aerial-Ground Robotics System for Precision Farming: An\n  Adaptable Solution", "source": "Alberto Pretto, St\\'ephanie Aravecchia, Wolfram Burgard, Nived\n  Chebrolu, Christian Dornhege, Tillmann Falck, Freya Fleckenstein, Alessandra\n  Fontenla, Marco Imperoli, Raghav Khanna, Frank Liebisch, Philipp Lottes,\n  Andres Milioto, Daniele Nardi, Sandro Nardi, Johannes Pfeifer, Marija\n  Popovi\\'c, Ciro Potena, C\\'edric Pradalier, Elisa Rothacker-Feder, Inkyu Sa,\n  Alexander Schaefer, Roland Siegwart, Cyrill Stachniss, Achim Walter, Wera\n  Winterhalter, Xiaolong Wu and Juan Nieto", "docs_id": "1911.03098", "section": ["cs.RO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Building an Aerial-Ground Robotics System for Precision Farming: An\n  Adaptable Solution. The application of autonomous robots in agriculture is gaining increasing popularity thanks to the high impact it may have on food security, sustainability, resource use efficiency, reduction of chemical treatments, and the optimization of human effort and yield. With this vision, the Flourish research project aimed to develop an adaptable robotic solution for precision farming that combines the aerial survey capabilities of small autonomous unmanned aerial vehicles (UAVs) with targeted intervention performed by multi-purpose unmanned ground vehicles (UGVs). This paper presents an overview of the scientific and technological advances and outcomes obtained in the project. We introduce multi-spectral perception algorithms and aerial and ground-based systems developed for monitoring crop density, weed pressure, crop nitrogen nutrition status, and to accurately classify and locate weeds. We then introduce the navigation and mapping systems tailored to our robots in the agricultural environment, as well as the modules for collaborative mapping. We finally present the ground intervention hardware, software solutions, and interfaces we implemented and tested in different field conditions and with different crops. We describe a real use case in which a UAV collaborates with a UGV to monitor the field and to perform selective spraying without human intervention."}, "answer": " Here is a 4-option multiple choice question on the aerial-ground robotics system for precision farming:\n\nQuestion: Which of the following capabilities was NOT mentioned as being developed for the aerial and ground-based systems in the Flourish project?\n\nA) Monitoring crop density using multi-spectral perception algorithms \n\nB) Classifying and locating weeds using aerial systems\n\nC) Assessing crop nitrogen status using ground robots\n\nD) Physically removing weeds using the ground robots\n\nCorrect Answer: D\n\nExplanation: \nThe passage mentions that the Flourish project developed multi-spectral perception algorithms and aerial systems to monitor crop density and weed pressure, as well as assess crop nitrogen status using ground robots (options A, B and C). However, it does not state that the ground robots were capable of physically removing weeds. The ground robots were used for selective spraying, but not physical weed removal."}, "44": {"documentation": {"title": "Absent Subsequences in Words", "source": "Maria Kosche, Tore Ko{\\ss}, Florin Manea, Stefan Siemer", "docs_id": "2108.13968", "section": ["cs.FL", "cs.DS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Absent Subsequences in Words. An absent factor of a string $w$ is a string $u$ which does not occur as a contiguous substring (a.k.a. factor) inside $w$. We extend this well-studied notion and define absent subsequences: a string $u$ is an absent subsequence of a string $w$ if $u$ does not occur as subsequence (a.k.a. scattered factor) inside $w$. Of particular interest to us are minimal absent subsequences, i.e., absent subsequences whose every subsequence is not absent, and shortest absent subsequences, i.e., absent subsequences of minimal length. We show a series of combinatorial and algorithmic results regarding these two notions. For instance: we give combinatorial characterisations of the sets of minimal and, respectively, shortest absent subsequences in a word, as well as compact representations of these sets; we show how we can test efficiently if a string is a shortest or minimal absent subsequence in a word, and we give efficient algorithms computing the lexicographically smallest absent subsequence of each kind; also, we show how a data structure for answering shortest absent subsequence-queries for the factors of a given string can be efficiently computed."}, "answer": " Here is a multi-form question on absent subsequences in words:\n\nQuestion: Which of the following strings is NOT necessarily an absent subsequence of the string \"banana\"?\n\nA) \"nnbaa\"\nB) \"nbna\" \nC) \"ban\"\nD) \"nbaa\"\n\nCorrect Answer: C\n\nExplanation: \nA) \"nnbaa\" is not a subsequence of \"banana\" as the characters do not occur in that order. So it is an absent subsequence.\nB) \"nbna\" is not a subsequence of \"banana\" as the characters do not occur in that order. So it is an absent subsequence.  \nC) \"ban\" is a subsequence of \"banana\" as the characters do occur in that order. So it is not necessarily an absent subsequence.\nD) \"nbaa\" is not a subsequence of \"banana\" as the characters do not occur in that order. So it is an absent subsequence.\n\nThe correct answer is C. \"ban\" is not necessarily an absent subsequence of \"banana\" since it does occur as a subsequence. The other options are all absent subsequences."}, "45": {"documentation": {"title": "An Integrated First-Order Theory of Points and Intervals over Linear\n  Orders (Part II)", "source": "Willem Conradie, Salih Durhan and Guido Sciavicco", "docs_id": "1809.04468", "section": ["cs.LO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "An Integrated First-Order Theory of Points and Intervals over Linear\n  Orders (Part II). There are two natural and well-studied approaches to temporal ontology and reasoning: point-based and interval-based. Usually, interval-based temporal reasoning deals with points as a particular case of duration-less intervals. A recent result by Balbiani, Goranko, and Sciavicco presented an explicit two-sorted point-interval temporal framework in which time instants (points) and time periods (intervals) are considered on a par, allowing the perspective to shift between these within the formal discourse. We consider here two-sorted first-order languages based on the same principle, and therefore including relations, as first studied by Reich, among others, between points, between intervals, and inter-sort. We give complete classifications of its sub-languages in terms of relative expressive power, thus determining how many, and which, are the intrinsically different extensions of two-sorted first-order logic with one or more such relations. This approach roots out the classical problem of whether or not points should be included in a interval-based semantics. In this Part II, we deal with the cases of all dense and the case of all unbounded linearly ordered sets."}, "answer": " Here is a 4-option multiple choice question on the given text:\n\nQuestion: Which of the following statements best summarizes the key ideas presented in the given text?\n\nA) Interval-based and point-based approaches to temporal reasoning have been well-studied, but a recent framework by Balbiani et al. allows perspectives to shift between points and intervals within the discourse. The text classifies sub-languages of this two-sorted first-order logic in terms of expressive power.\n\nB) Reich first studied relations between points, intervals, and across sorts. This text presents a complete classification of sub-languages of two-sorted first-order logic with such relations based on relative expressive power.\n\nC) Interval-based temporal reasoning has traditionally dealt with points as duration-less intervals. This text argues that points should not be included in interval-based semantics and roots out this classical problem.\n\nD) Balbiani et al. were the first to present a two-sorted point-interval temporal framework. This text criticizes their work for not allowing perspectives to shift between points and intervals.\n\nCorrect Answer: A\n\nExplanation: Option A best summarizes the key ideas that the given text presents a two-sorted first-order logic framework by Balbiani et al. that allows shifting perspectives between points and intervals, and classifies its sub-languages based on expressive power. Options B, C and D include inaccurate or incomplete information from the text."}, "46": {"documentation": {"title": "Cross-Domain Perceptual Reward Functions", "source": "Ashley D. Edwards, Srijan Sood, and Charles L. Isbell Jr", "docs_id": "1705.09045", "section": ["cs.AI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Cross-Domain Perceptual Reward Functions. In reinforcement learning, we often define goals by specifying rewards within desirable states. One problem with this approach is that we typically need to redefine the rewards each time the goal changes, which often requires some understanding of the solution in the agents environment. When humans are learning to complete tasks, we regularly utilize alternative sources that guide our understanding of the problem. Such task representations allow one to specify goals on their own terms, thus providing specifications that can be appropriately interpreted across various environments. This motivates our own work, in which we represent goals in environments that are different from the agents. We introduce Cross-Domain Perceptual Reward (CDPR) functions, learned rewards that represent the visual similarity between an agents state and a cross-domain goal image. We report results for learning the CDPRs with a deep neural network and using them to solve two tasks with deep reinforcement learning."}, "answer": " Here is a 4-option multiple choice question on the given Arxiv documentation:\n\nQuestion: Which of the following statements best describes a key limitation of specifying goals only through rewards in reinforcement learning that the paper aims to address?\n\nA) Rewards defined only within desirable states often need to be redefined from scratch each time the goal changes.\n\nB) Rewards specified only for desirable states do not provide enough guidance for the agent to learn efficiently. \n\nC) Goals specified only through rewards lack interpretability across different environments.\n\nD) Rewards alone fail to leverage alternative representations that can guide an agent's understanding.\n\nCorrect Answer: A\n\nExplanation: The documentation states that a key problem with defining goals only through rewards is that the rewards typically need to be redefined each time the goal changes. This requires some understanding of the solution in the agent's environment. Option A best captures this key limitation that the paper aims to address through cross-domain perceptual reward functions."}, "47": {"documentation": {"title": "An Optimal LiDAR Configuration Approach for Self-Driving Cars", "source": "Shenyu Mou, Yan Chang, Wenshuo Wang, and Ding Zhao", "docs_id": "1805.07843", "section": ["cs.RO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "An Optimal LiDAR Configuration Approach for Self-Driving Cars. LiDARs plays an important role in self-driving cars and its configuration such as the location placement for each LiDAR can influence object detection performance. This paper aims to investigate an optimal configuration that maximizes the utility of on-hand LiDARs. First, a perception model of LiDAR is built based on its physical attributes. Then a generalized optimization model is developed to find the optimal configuration, including the pitch angle, roll angle, and position of LiDARs. In order to fix the optimization issue with off-the-shelf solvers, we proposed a lattice-based approach by segmenting the LiDAR's range of interest into finite subspaces, thus turning the optimal configuration into a nonlinear optimization problem. A cylinder-based method is also proposed to approximate the objective function, thereby making the nonlinear optimization problem solvable. A series of simulations are conducted to validate our proposed method. This proposed approach to optimal LiDAR configuration can provide a guideline to researchers to maximize the utility of LiDARs."}, "answer": " Question: Which of the following statements accurately describes a key contribution of the paper \"An Optimal LiDAR Configuration Approach for Self-Driving Cars\"?\n\nA) The paper proposes an optimization model to find the optimal LiDAR configuration, including pitch angle, roll angle, and position, to maximize utility.\n\nB) The paper investigates different machine learning models for detecting objects using data from optimally configured LiDARs. \n\nC) The paper evaluates different types of LiDARs and recommends the best LiDAR hardware for self-driving cars.\n\nD) The paper develops a new LiDAR sensor design that is optimized for placement on self-driving cars.\n\nCorrect Answer: A\n\nExplanation: The paper focuses on developing an optimization model to find the optimal configuration of existing LiDARs to maximize utility, including determining pitch angle, roll angle, and position. It does not evaluate different LiDAR hardware or develop new LiDAR designs. While optimized LiDAR configuration could improve object detection, the paper does not investigate specific machine learning models."}, "48": {"documentation": {"title": "Enhancing Visual Fashion Recommendations with Users in the Loop", "source": "Anurag Bhardwaj, Vignesh Jagadeesh, Wei Di, Robinson Piramuthu,\n  Elizabeth Churchill", "docs_id": "1405.4013", "section": ["cs.HC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Enhancing Visual Fashion Recommendations with Users in the Loop. We describe a completely automated large scale visual recommendation system for fashion. Existing approaches have primarily relied on purely computational models to solving this problem that ignore the role of users in the system. In this paper, we propose to overcome this limitation by incorporating a user-centric design of visual fashion recommendations. Specifically, we propose a technique that augments 'user preferences' in models by exploiting elasticity in fashion choices. We further design a user study on these choices and gather results from the 'wisdom of crowd' for deeper analysis. Our key insights learnt through these results suggest that fashion preferences when constrained to a particular class, contain important behavioral signals that are often ignored in recommendation design. Further, presence of such classes also reflect strong correlations to visual perception which can be utilized to provide aesthetically pleasing user experiences. Finally, we illustrate that user approval of visual fashion recommendations can be substantially improved by carefully incorporating these user-centric feedback into the system framework."}, "answer": " Here is a 4-option multiple choice question on the paper:\n\nQuestion: Which of the following statements accurately describes a key insight from the paper about incorporating user preferences into fashion recommendation systems?\n\nA) Ignoring user preferences leads to poor performance, so user preferences should be incorporated even if they constrain the recommendation space.\n\nB) User preferences are unimportant for fashion recommendation systems which should rely solely on computational models.\n\nC) Fashion preferences are highly elastic, so constraints based on user preferences limit the effectiveness of recommendations.\n\nD) User preferences contain important signals about aesthetics and perception that can improve recommendation quality if utilized appropriately.\n\nCorrect Answer: D\n\nExplanation: The paper finds that fashion preferences, when constrained to a class, contain important behavioral signals often ignored in recommendations. The presence of classes also reflects correlations to visual perception that can improve aesthetic appeal and user satisfaction if incorporated properly into the system framework."}, "49": {"documentation": {"title": "VR Hackathon with Goethe Institute: Lessons Learned from Organizing a\n  Transdisciplinary VR Hackathon", "source": "Wies{\\l}aw Kope\\'c, Krzysztof Kalinowski, Monika Kornacka, Kinga\n  Skorupska, Julia Paluch, Anna Jaskulska, Grzegorz Pochwatko, Jakub Mo\\.zaryn,\n  Pawe{\\l} Kobyli\\'nski, Piotr Gago", "docs_id": "2104.02100", "section": ["cs.HC", "cs.CY", "cs.MM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "VR Hackathon with Goethe Institute: Lessons Learned from Organizing a\n  Transdisciplinary VR Hackathon. In this article we report a case study of a Language Learning Bauhaus VR hackathon with Goethe Institute. It was organized as an educational and research project to tap into the dynamics of transdisciplinary teams challenged with a specific requirement. In our case, it was to build a Bauhaus-themed German Language Learning VR App. We constructed this experiment to simulate how representatives of different disciplines may work together towards a very specific purpose under time pressure. So, each participating team consisted of members of various expert-fields: software development (Unity or Unreal), design, psychology and linguistics. The results of this study cast light on the recommended cycle of design thinking and customer-centered design in VR. Especially in interdisciplinary rapid prototyping conditions, where stakeholders initially do not share competences. They also showcase educational benefits of working in transdisciplinary environments. This study, combined with our previous work on human factors in rapid software development and co-design, including hackathon dynamics, allowed us to formulate recommendations for organizing content creation VR hackathons for specific purposes. We also provide guidelines on how to prepare the participants to work in rapid prototyping VR environments and benefit from such experiences in the long term."}, "answer": " Here is a multi-form question based on the given text:\n\nQuestion: What was the main purpose of the VR hackathon organized by the Goethe Institute as described in the text?\n\nA) To develop a general purpose VR app for language learning\nB) To study the dynamics of transdisciplinary teams working under time constraints \nC) To promote the use of VR for educational purposes\nD) To compare different VR development platforms like Unity and Unreal\n\nCorrect Answer: B\n\nExplanation: The text states that the VR hackathon was \"constructed this experiment to simulate how representatives of different disciplines may work together towards a very specific purpose under time pressure.\" So the main purpose was to study the team dynamics in this kind of transdisciplinary and time-constrained environment, not specifically to develop a VR app."}, "50": {"documentation": {"title": "Evolutionary Algorithm for Graph Coloring Problem", "source": "Robiul Islam and Arup Kumar Pramanik", "docs_id": "2111.09743", "section": ["cs.NE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Evolutionary Algorithm for Graph Coloring Problem. The graph coloring problem (GCP) is one of the most studied NP-HARD problems in computer science. Given a graph , the task is to assign a color to all vertices such that no vertices sharing an edge receive the same color and that the number of used colors, is minimal. Different heuristic, meta-heuristic, machine learning and hybrid solution methods have been applied to obtain the solution. To solve this problem we use mutation of evolutionary algorithm. For this purpose we introduce binary encoding for Graph Coloring Problem. This binary encoding help us for mutation, evaluate, immune system and merge color easily and also reduce coloring dynamically. In the traditional evolutionary algorithm (EA) for graph coloring, k-coloring approach is used and the EA is run repeatedly until the lowest possible is reached. In our paper, we start with the theoretical upper bound of chromatic number, that is, maximum out-degree + 1 and in the process of evolution some of the colors are made unused to dynamically reduce the number of color in every generation. We test few standard DIMACS benchmark and compare resent paper. Maximum results are same as expected chromatic color and few data sets are larger than expected chromatic number"}, "answer": " Here is a 4-option multiple choice question on the graph coloring problem paper:\n\nQuestion: Which of the following statements is true about the evolutionary algorithm proposed in the paper for solving the graph coloring problem?\n\nA) It uses a binary encoding to represent candidate colorings which allows easy mutation, evaluation, and merging of colors. \n\nB) It starts with an initial coloring that uses the maximum degree + 1 colors and evolves to reduce the number of colors dynamically.\n\nC) It runs the evolutionary algorithm repeatedly from scratch until reaching the chromatic number.\n\nD) It compares results against other evolutionary algorithms on DIMACS benchmark graphs.\n\nCorrect Answer: B\n\nExplanation: The paper proposes an evolutionary algorithm that starts with the upper bound on the chromatic number (maximum degree + 1) and evolves the population to dynamically reduce the number of colors used. It uses a binary encoding to help with evolutionary operators. It runs only once, not repeatedly from scratch. And it compares to other methods on DIMACS graphs, not other evolutionary algorithms."}, "51": {"documentation": {"title": "Logic as a distributive law", "source": "Mike Stay, Lucius Gregory Meredith", "docs_id": "1610.02247", "section": ["cs.LO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Logic as a distributive law. We present an algorithm for deriving a spatial-behavioral type system from a formal presentation of a computational calculus. Given a 2-monad Calc: Catv$\\to$ Cat for the free calculus on a category of terms and rewrites and a 2-monad BoolAlg for the free Boolean algebra on a category, we get a 2-monad Form = BoolAlg + Calc for the free category of formulae and proofs. We also get the 2-monad BoolAlg $\\circ$ Calc for subsets of terms. The interpretation of formulae is a natural transformation $\\interp{-}$: Form $\\Rightarrow$ BoolAlg $\\circ$ Calc defined by the units and multiplications of the monads and a distributive law transformation $\\delta$: Calc $\\circ$ BoolAlg $\\Rightarrow$ BoolAlg $\\circ$ Calc. This interpretation is consistent both with the Curry-Howard isomorphism and with realizability. We give an implementation of the \"possibly\" modal operator parametrized by a two-hole term context and show that, surprisingly, the arrow type constructor in the $\\lambda$-calculus is a specific case. We also exhibit nontrivial formulae encoding confinement and liveness properties for a reflective higher-order variant of the $\\pi$-calculus."}, "answer": " Here is a 4-option multiple choice question on the abstract:\n\nQuestion: Which of the following statements accurately summarizes a key contribution of the paper?\n\nA) The paper presents an algorithm for deriving a spatial-behavioral type system from a formal calculus presentation. The interpretation of formulae as a distributive law allows consistency with Curry-Howard and realizability.\n\nB) The paper shows how to encode liveness and confinement properties for a reflective higher-order \u03c0-calculus using non-trivial formulae. This demonstrates the expressiveness of the distributive law approach.\n\nC) The paper gives an implementation of the \"possibly\" modal operator parameterized by a two-hole term context. It shows the arrow type constructor in \u03bb-calculus is a specific case.\n\nD) The paper proves the Curry-Howard isomorphism and realizability are inconsistent. The distributive law interpretation resolves this inconsistency.\n\nCorrect Answer: A\n\nExplanation: Option A accurately summarizes the key contribution according to the abstract - deriving a spatial-behavioral type system from a calculus presentation using a distributive law interpretation that is consistent with Curry-Howard and realizability. Options B and C refer to specific examples given, not the core contribution. Option D incorrectly states the paper proves inconsistency and resolves it, contradicting the abstract."}, "52": {"documentation": {"title": "Kernel Two-Sample Tests for Manifold Data", "source": "Xiuyuan Cheng, Yao Xie", "docs_id": "2105.03425", "section": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Kernel Two-Sample Tests for Manifold Data. We present a study of kernel based two-sample test statistic, which is related to the Maximum Mean Discrepancy (MMD), in the manifold data setting, assuming that high-dimensional observations are close to a low-dimensional manifold. We characterize the test level and power in relation to the kernel bandwidth, the number of samples, and the intrinsic dimensionality of the manifold. Specifically, we show that when data densities are supported on a $d$-dimensional sub-manifold $\\mathcal{M}$ embedded in an $m$-dimensional space, the kernel two-sample test for data sampled from a pair of distributions $(p, q)$ that are H\\\"older with order $\\beta$ is consistent and powerful when the number of samples $n$ is greater than $\\delta_2(p,q)^{-2-d/\\beta}$ up to certain constant, where $\\delta_2$ is the squared $\\ell_2$-divergence between two distributions on manifold. Moreover, to achieve testing consistency under this scaling of $n$, our theory suggests that the kernel bandwidth $\\gamma$ scales with $n^{-1/(d+2\\beta)}$. These results indicate that the kernel two-sample test does not have a curse-of-dimensionality when the data lie on a low-dimensional manifold. We demonstrate the validity of our theory and the property of the kernel test for manifold data using several numerical experiments."}, "answer": " Here is a multi-form question based on the given text:\n\nQuestion: According to the text, what is the relationship between the kernel bandwidth \u03b3, the number of samples n, and the intrinsic dimensionality d of the manifold that allows the kernel two-sample test to achieve testing consistency?\n\nA) \u03b3 scales with n^{-1/(d+2\u03b2)}\nB) \u03b3 scales with n^{1/(d+2\u03b2)} \nC) \u03b3 scales with n^{d/2\u03b2}\nD) \u03b3 scales with n^{-(d+2\u03b2)}\n\nCorrect Answer: A\n\nExplanation: The text states that \"to achieve testing consistency under this scaling of n, our theory suggests that the kernel bandwidth \u03b3 scales with n^{-1/(d+2\u03b2)}.\" So the correct relationship is that \u03b3 scales with n^{-1/(d+2\u03b2)}, which is option A."}, "53": {"documentation": {"title": "Efficient LTE Access with Collision Resolution for Massive M2M\n  Communications", "source": "Germ\\'an Corrales Madue\\~no, \\v{C}edomir Stefanovi\\'c, Petar Popovski", "docs_id": "1410.6628", "section": ["cs.IT", "cs.NI", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Efficient LTE Access with Collision Resolution for Massive M2M\n  Communications. LTE random access procedure performs satisfactorily in case of asynchronous, uncorrelated traffic arrivals. However, when the arrivals are correlated and arrive synchronously, the performance of the random access channel (RACH) is drastically reduced, causing a large number of devices to experience outage. In this work we propose a LTE RACH scheme tailored for delay-sensitive M2M services with synchronous traffic arrivals. The key idea is, upon detection of a RACH overload, to apply a collision resolution algorithm based on splitting trees. The solution is implemented on top of the existing LTE RACH mechanism, requiring only minor modifications of the protocol operation and not incurring any changes to the physical layer. The results are very promising, outperforming the related solutions by a wide margin. As an illustration, the proposed scheme can resolve 30k devices with an average of 5 preamble transmissions and delay of 1.2 seconds, under a realistic probability of transmissions error both in the downlink and in the uplink."}, "answer": " Question: What is the key idea proposed in the paper to improve the performance of the LTE random access channel (RACH) in case of synchronous traffic arrivals from a large number of devices?\n\nA) Using a different physical layer technology instead of LTE \n\nB) Prioritizing delay-sensitive M2M traffic over other traffic\n\nC) Applying a collision resolution algorithm based on splitting trees upon detection of a RACH overload\n\nD) Increasing the number of random access preambles available \n\nCorrect Answer: C\n\nExplanation: The key idea proposed in the paper is to apply a collision resolution algorithm based on splitting trees upon detection of a RACH overload, in order to resolve collisions and improve performance for synchronous M2M traffic arrivals. Options A, B, and D do not match the key idea described in the summary."}, "54": {"documentation": {"title": "Computing Optimal Repairs for Functional Dependencies", "source": "Ester Livshits, Benny Kimelfeld, Sudeepa Roy", "docs_id": "1712.07705", "section": ["cs.DB"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Computing Optimal Repairs for Functional Dependencies. We investigate the complexity of computing an optimal repair of an inconsistent database, in the case where integrity constraints are Functional Dependencies (FDs). We focus on two types of repairs: an optimal subset repair (optimal S-repair) that is obtained by a minimum number of tuple deletions, and an optimal update repair (optimal U-repair) that is obtained by a minimum number of value (cell) updates. For computing an optimal S-repair, we present a polynomial-time algorithm that succeeds on certain sets of FDs and fails on others. We prove the following about the algorithm. When it succeeds, it can also incorporate weighted tuples and duplicate tuples. When it fails, the problem is NP-hard, and in fact, APX-complete (hence, cannot be approximated better than some constant). Thus, we establish a dichotomy in the complexity of computing an optimal S-repair. We present general analysis techniques for the complexity of computing an optimal U-repair, some based on the dichotomy for S-repairs. We also draw a connection to a past dichotomy in the complexity of finding a \"most probable database\" that satisfies a set of FDs with a single attribute on the left hand side; the case of general FDs was left open, and we show how our dichotomy provides the missing generalization and thereby settles the open problem."}, "answer": " Here is a 4-option multiple choice question on the paper:\n\nQuestion: According to the paper, what is the complexity of computing an optimal subset repair (optimal S-repair) for a set of functional dependencies (FDs) where the algorithm presented succeeds?\n\nA) It can be solved in polynomial time and can incorporate weighted tuples and duplicate tuples.\n\nB) It is NP-hard and APX-complete. \n\nC) It has unknown complexity.\n\nD) It is PSPACE-complete.\n\nCorrect Answer: A\n\nExplanation: The paper states that when the algorithm presented for computing an optimal S-repair succeeds, it runs in polynomial time and can handle weighted and duplicate tuples. Answer B refers to the complexity when the algorithm fails. Answers C and D are incorrect based on the results stated in the paper."}, "55": {"documentation": {"title": "Efficient and principled score estimation with Nystr\\\"om kernel\n  exponential families", "source": "Danica J. Sutherland, Heiko Strathmann, Michael Arbel, Arthur Gretton", "docs_id": "1705.08360", "section": ["stat.ML", "cs.LG", "stat.ME"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Efficient and principled score estimation with Nystr\\\"om kernel\n  exponential families. We propose a fast method with statistical guarantees for learning an exponential family density model where the natural parameter is in a reproducing kernel Hilbert space, and may be infinite-dimensional. The model is learned by fitting the derivative of the log density, the score, thus avoiding the need to compute a normalization constant. Our approach improves the computational efficiency of an earlier solution by using a low-rank, Nystr\\\"om-like solution. The new solution retains the consistency and convergence rates of the full-rank solution (exactly in Fisher distance, and nearly in other distances), with guarantees on the degree of cost and storage reduction. We evaluate the method in experiments on density estimation and in the construction of an adaptive Hamiltonian Monte Carlo sampler. Compared to an existing score learning approach using a denoising autoencoder, our estimator is empirically more data-efficient when estimating the score, runs faster, and has fewer parameters (which can be tuned in a principled and interpretable way), in addition to providing statistical guarantees."}, "answer": " Here is a 4-option multiple choice question with explanations for the given research summary:\n\nQuestion: Which of the following statements is true about the proposed method in the paper?\n\nA) The method uses a full-rank solution to fit the score, which improves computational efficiency over the low-rank Nystr\u00f6m solution.\n\nB) The method provides statistical guarantees on the consistency and convergence rates compared to the full-rank solution. \n\nC) The method uses a denoising autoencoder to estimate the score, which is more data-efficient than the proposed approach.\n\nD) The method requires tuning a large number of parameters in an unprincipled and uninterpretable way.\n\nCorrect Answer: B\n\nExplanations: \nA) Incorrect, the method uses a low-rank Nystr\u00f6m-like solution to improve computational efficiency over the full-rank solution.\nB) Correct. The method retains the consistency and convergence guarantees of the full-rank solution while reducing cost and storage. \nC) Incorrect, the proposed method is found to be more data-efficient than the denoising autoencoder approach empirically.\nD) Incorrect, the method provides a principled and interpretable way to tune parameters with statistical guarantees."}, "56": {"documentation": {"title": "Non-Differentiable Supervised Learning with Evolution Strategies and\n  Hybrid Methods", "source": "Karel Lenc, Erich Elsen, Tom Schaul, Karen Simonyan", "docs_id": "1906.03139", "section": ["cs.NE", "cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Non-Differentiable Supervised Learning with Evolution Strategies and\n  Hybrid Methods. In this work we show that Evolution Strategies (ES) are a viable method for learning non-differentiable parameters of large supervised models. ES are black-box optimization algorithms that estimate distributions of model parameters; however they have only been used for relatively small problems so far. We show that it is possible to scale ES to more complex tasks and models with millions of parameters. While using ES for differentiable parameters is computationally impractical (although possible), we show that a hybrid approach is practically feasible in the case where the model has both differentiable and non-differentiable parameters. In this approach we use standard gradient-based methods for learning differentiable weights, while using ES for learning non-differentiable parameters - in our case sparsity masks of the weights. This proposed method is surprisingly competitive, and when parallelized over multiple devices has only negligible training time overhead compared to training with gradient descent. Additionally, this method allows to train sparse models from the first training step, so they can be much larger than when using methods that require training dense models first. We present results and analysis of supervised feed-forward models (such as MNIST and CIFAR-10 classification), as well as recurrent models, such as SparseWaveRNN for text-to-speech."}, "answer": " Question: What type of optimization algorithm does the paper show can be scaled to complex tasks and models with millions of parameters, even though it has previously only been used for relatively small problems?\n\nA) Gradient descent \nB) Evolution strategies\nC) Sparse wave recurrent neural networks\nD) Hybrid methods\n\nCorrect Answer: B) Evolution strategies\n\nExplanation: The paper shows that Evolution Strategies (ES), which are black-box optimization algorithms, can be scaled to more complex tasks and models with millions of parameters, even though ES have previously only been used for relatively small problems."}, "57": {"documentation": {"title": "Advances in Scaling Community Discovery Methods for Large Signed Graph\n  Networks", "source": "Maria Tomasso and Lucas Rusnak and Jelena Te\\v{s}i\\'c", "docs_id": "2110.07514", "section": ["cs.SI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Advances in Scaling Community Discovery Methods for Large Signed Graph\n  Networks. Community detection is a common task in social network analysis (SNA) with applications in a variety of fields including medicine, criminology, and business. Despite the popularity of community detection, there is no clear consensus on the most effective methodology for signed networks. In this paper, we summarize the development of community detection in signed networks and evaluate current state-of-the-art techniques on several real-world data sets. First, we give a comprehensive background of community detection in signed graphs. Next, we compare various adaptations of the Laplacian matrix in recovering ground-truth community labels via spectral clustering in small signed graph data sets. Then, we evaluate the scalability of leading algorithms on small, large, dense, and sparse real-world signed graph networks. We conclude with a discussion of our novel findings and recommendations for extensions and improvements in state-of-the-art techniques for signed graph community discovery in large, sparse, real-world signed graphs."}, "answer": " Here is a 4-option multiple choice question with explanations for the given document:\n\nQuestion: Which of the following statements is true regarding community detection in signed networks according to the document?\n\nA) There is a clear consensus on the most effective methodology for signed networks.\n\nB) The Laplacian matrix adaptations were evaluated on large real-world signed graph networks. \n\nC) Leading algorithms were found to have good scalability on small, sparse, real-world signed graphs.\n\nD) There is no clear consensus on the most effective methodology for signed networks.\n\nCorrect Answer: D\n\nExplanations:\n\nA) Incorrect. The document states there is no clear consensus on the most effective methodology for signed networks.\n\nB) Incorrect. The Laplacian matrix adaptations were only evaluated on small signed graph data sets, not large ones. \n\nC) Incorrect. The document states the leading algorithms were evaluated for scalability on large and sparse real-world signed graphs, not that they had good scalability.\n\nD) Correct. The document clearly states there is no consensus on the most effective methodology for signed networks."}, "58": {"documentation": {"title": "Leveraging Trust and Distrust in Recommender Systems via Deep Learning", "source": "Dimitrios Rafailidis", "docs_id": "1905.13612", "section": ["cs.LG", "cs.IR", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Leveraging Trust and Distrust in Recommender Systems via Deep Learning. The data scarcity of user preferences and the cold-start problem often appear in real-world applications and limit the recommendation accuracy of collaborative filtering strategies. Leveraging the selections of social friends and foes can efficiently face both problems. In this study, we propose a strategy that performs social deep pairwise learning. Firstly, we design a ranking loss function incorporating multiple ranking criteria based on the choice in users, and the choice in their friends and foes to improve the accuracy in the top-k recommendation task. We capture the nonlinear correlations between user preferences and the social information of trust and distrust relationships via a deep learning strategy. In each backpropagation step, we follow a social negative sampling strategy to meet the multiple ranking criteria of our ranking loss function. We conduct comprehensive experiments on a benchmark dataset from Epinions, among the largest publicly available that has been reported in the relevant literature. The experimental results demonstrate that the proposed model beats other state-of-the art methods, attaining an 11.49% average improvement over the most competitive model. We show that our deep learning strategy plays an important role in capturing the nonlinear correlations between user preferences and the social information of trust and distrust relationships, and demonstrate the importance of our social negative sampling strategy on the proposed model."}, "answer": " Here is a 4-option multiple choice question on the given research paper:\n\nQuestion: Which of the following statements is true about the proposed model in the research paper?\n\nA) It uses a ranking loss function with multiple criteria based on user choices, friends' choices and foes' choices.\n\nB) It captures linear correlations between user preferences and social trust/distrust via deep learning. \n\nC) It follows a random negative sampling strategy for backpropagation.\n\nD) It achieves a 5.49% average improvement over the best baseline model.\n\nCorrect Answer: A\n\nExplanation: The paper proposes a model that uses a ranking loss function incorporating multiple ranking criteria based on user choices, friends' choices and foes' choices. It captures nonlinear, not linear, correlations between user preferences and social trust/distrust via deep learning. It follows a social negative sampling strategy, not a random one. And it achieves 11.49%, not 5.49%, average improvement over the best baseline."}, "59": {"documentation": {"title": "Counterfactual Mean Embeddings", "source": "Krikamol Muandet, Motonobu Kanagawa, Sorawit Saengkyongam, Sanparith\n  Marukatat", "docs_id": "1805.08845", "section": ["stat.ML", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Counterfactual Mean Embeddings. Counterfactual inference has become a ubiquitous tool in online advertisement, recommendation systems, medical diagnosis, and econometrics. Accurate modeling of outcome distributions associated with different interventions -- known as counterfactual distributions -- is crucial for the success of these applications. In this work, we propose to model counterfactual distributions using a novel Hilbert space representation called counterfactual mean embedding (CME). The CME embeds the associated counterfactual distribution into a reproducing kernel Hilbert space (RKHS) endowed with a positive definite kernel, which allows us to perform causal inference over the entire landscape of the counterfactual distribution. Based on this representation, we propose a distributional treatment effect (DTE) that can quantify the causal effect over entire outcome distributions. Our approach is nonparametric as the CME can be estimated under the unconfoundedness assumption from observational data without requiring any parametric assumption about the underlying distributions. We also establish a rate of convergence of the proposed estimator which depends on the smoothness of the conditional mean and the Radon-Nikodym derivative of the underlying marginal distributions. Furthermore, our framework allows for more complex outcomes such as images, sequences, and graphs. Our experimental results on synthetic data and off-policy evaluation tasks demonstrate the advantages of the proposed estimator."}, "answer": " Here is a 4-option multiple choice question on the Counterfactual Mean Embeddings paper:\n\nQuestion: Which of the following statements about Counterfactual Mean Embeddings is incorrect?\n\nA) The Counterfactual Mean Embedding embeds the counterfactual distribution into a reproducing kernel Hilbert space. \n\nB) The approach requires parametric assumptions about the underlying distributions.\n\nC) The method allows for modeling complex outcomes like images and graphs.  \n\nD) The convergence rate of the estimator depends on the smoothness and Radon-Nikodym derivative.\n\nCorrect Answer: B\n\nExplanation: The paper states that the Counterfactual Mean Embedding approach is nonparametric and does not require parametric assumptions about the underlying distributions. The other statements about embedding into an RKHS, allowing complex outcomes, and convergence rate are correct based on the paper."}}