{"0": {"documentation": {"title": "Bounds Preserving Temporal Integration Methods for Hyperbolic\n  Conservation Laws", "source": "Tarik Dzanic, Will Trojak, and Freddie D. Witherden", "docs_id": "2107.04899", "section": ["math.NA", "cs.NA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Bounds Preserving Temporal Integration Methods for Hyperbolic\n  Conservation Laws. In this work, we present a modification of explicit Runge-Kutta temporal integration schemes that guarantees the preservation of any locally-defined quasiconvex set of bounds for the solution. These schemes operate on the basis of a bijective mapping between an admissible set of solutions and the real domain to strictly enforce bounds. Within this framework, we show that it is possible to recover a wide range of methods independently of the spatial discretization, including positivity preserving, discrete maximum principle satisfying, entropy dissipative, and invariant domain preserving schemes. Furthermore, these schemes are proven to recover the order of accuracy of the underlying Runge-Kutta method upon which they are built. The additional computational cost is the evaluation of two nonlinear mappings which generally have closed-form solutions. We show the utility of this approach in numerical experiments using a pseudospectral spatial discretization without any explicit shock capturing schemes for nonlinear hyperbolic problems with discontinuities."}, "answer": " Here is a 4-option multiple choice question with explanations for the given documentation:\n\nQuestion: Which of the following statements is true about the bounds preserving temporal integration methods presented in the paper?\n\nA) The methods are based on a bijective mapping between an admissible set of solutions and the complex domain to enforce bounds.\n\nB) The methods can recover positivity preserving and entropy satisfying schemes, but not maximum principle satisfying schemes. \n\nC) The methods recover the order of accuracy of the underlying Runge-Kutta method at the cost of evaluating two nonlinear mappings.\n\nD) The methods require the use of explicit shock capturing schemes for nonlinear hyperbolic problems with discontinuities.\n\nCorrect Answer: C\n\nExplanations:\n\nA) Incorrect. The paper states the methods use a mapping between an admissible set of solutions and the real domain, not complex domain. \n\nB) Incorrect. The paper specifically states the methods can recover discrete maximum principle satisfying schemes.\n\nC) Correct. The paper mentions the methods recover the accuracy of the underlying RK method by evaluating two nonlinear mappings. \n\nD) Incorrect. The paper shows the utility of these methods without needing explicit shock capturing schemes."}, "1": {"documentation": {"title": "DTCA: Decision Tree-based Co-Attention Networks for Explainable Claim\n  Verification", "source": "Lianwei Wu, Yuan Rao, Yongqiang Zhao, Hao Liang, Ambreen Nazir", "docs_id": "2004.13455", "section": ["cs.CL", "cs.AI", "cs.CY", "cs.SI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "DTCA: Decision Tree-based Co-Attention Networks for Explainable Claim\n  Verification. Recently, many methods discover effective evidence from reliable sources by appropriate neural networks for explainable claim verification, which has been widely recognized. However, in these methods, the discovery process of evidence is nontransparent and unexplained. Simultaneously, the discovered evidence only roughly aims at the interpretability of the whole sequence of claims but insufficient to focus on the false parts of claims. In this paper, we propose a Decision Tree-based Co-Attention model (DTCA) to discover evidence for explainable claim verification. Specifically, we first construct Decision Tree-based Evidence model (DTE) to select comments with high credibility as evidence in a transparent and interpretable way. Then we design Co-attention Self-attention networks (CaSa) to make the selected evidence interact with claims, which is for 1) training DTE to determine the optimal decision thresholds and obtain more powerful evidence; and 2) utilizing the evidence to find the false parts in the claim. Experiments on two public datasets, RumourEval and PHEME, demonstrate that DTCA not only provides explanations for the results of claim verification but also achieves the state-of-the-art performance, boosting the F1-score by 3.11%, 2.41%, respectively."}, "answer": " Here is a 4-option multiple choice question on the given Arxiv documentation:\n\nQuestion: Which of the following statements is true about the Decision Tree-based Co-Attention (DTCA) model proposed in the paper?\n\nA) It uses a convolutional neural network to select credible comments as evidence.\n\nB) It trains the Decision Tree-based Evidence model using reinforcement learning. \n\nC) It aims to find false parts in claims by focusing the evidence on them.\n\nD) It achieves worse performance than previous methods on the RumourEval dataset.\n\nCorrect Answer: C\n\nExplanation: According to the documentation, the DTCA model uses a Decision Tree-based Evidence (DTE) model to select credible comments as evidence in an interpretable way. It then utilizes a Co-attention Self-attention network (CaSa) to focus this evidence on false parts in the claims. The model achieves state-of-the-art performance on RumourEval and PHEME datasets."}, "2": {"documentation": {"title": "Toxicity Detection can be Sensitive to the Conversational Context", "source": "Alexandros Xenos, John Pavlopoulos, Ion Androutsopoulos, Lucas Dixon,\n  Jeffrey Sorensen and Leo Laugier", "docs_id": "2111.10223", "section": ["cs.CL"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Toxicity Detection can be Sensitive to the Conversational Context. User posts whose perceived toxicity depends on the conversational context are rare in current toxicity detection datasets. Hence, toxicity detectors trained on existing datasets will also tend to disregard context, making the detection of context-sensitive toxicity harder when it does occur. We construct and publicly release a dataset of 10,000 posts with two kinds of toxicity labels: (i) annotators considered each post with the previous one as context; and (ii) annotators had no additional context. Based on this, we introduce a new task, context sensitivity estimation, which aims to identify posts whose perceived toxicity changes if the context (previous post) is also considered. We then evaluate machine learning systems on this task, showing that classifiers of practical quality can be developed, and we show that data augmentation with knowledge distillation can improve the performance further. Such systems could be used to enhance toxicity detection datasets with more context-dependent posts, or to suggest when moderators should consider the parent posts, which often may be unnecessary and may otherwise introduce significant additional cost."}, "answer": " Question: What is the main purpose of the new context sensitivity estimation task introduced in the paper?\n\nA) To identify posts whose toxicity does not depend on the previous post as context\n\nB) To construct a new dataset with posts labeled for toxicity both with and without context \n\nC) To identify posts whose perceived toxicity changes if the previous post is considered as context\n\nD) To show that toxicity detectors should disregard context when detecting toxic posts\n\nCorrect Answer: C) To identify posts whose perceived toxicity changes if the previous post is considered as context\n\nExplanation: The paper introduces the new context sensitivity estimation task to identify posts whose perceived toxicity changes depending on whether the previous post is considered as context or not. The other options are incorrect based on details provided in the summary."}, "3": {"documentation": {"title": "Deep Neural Networks with Koopman Operators for Modeling and Control of\n  Autonomous Vehicles", "source": "Yongqian Xiao, Xinglong Zhang, Xin Xu, Xueqing Liu, Jiahang Liu", "docs_id": "2007.02219", "section": ["eess.SY", "cs.SY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Deep Neural Networks with Koopman Operators for Modeling and Control of\n  Autonomous Vehicles. Autonomous driving technologies have received notable attention in the past decades. In autonomous driving systems, identifying a precise dynamical model for motion control is nontrivial due to the strong nonlinearity and uncertainty in vehicle dynamics. Recent efforts have resorted to machine learning techniques for building vehicle dynamical models, but the generalization ability and interpretability of existing methods still need to be improved. In this paper, we propose a data-driven vehicle modeling approach based on deep neural networks with an interpretable Koopman operator. The main advantage of using the Koopman operator is to represent the nonlinear dynamics in a linear lifted feature space. In the proposed approach, a deep learning-based extended dynamic mode decomposition algorithm is presented to learn a finite-dimensional approximation of the Koopman operator. Furthermore, a data-driven model predictive controller with the learned Koopman model is designed for path tracking control of autonomous vehicles. Simulation results in a high-fidelity CarSim environment show that our approach exhibit a high modeling precision at a wide operating range and outperforms previously developed methods in terms of modeling performance. Path tracking tests of the autonomous vehicle are also performed in the CarSim environment and the results show the effectiveness of the proposed approach."}, "answer": " Here is a 4-option multiple choice question on the document:\n\nQuestion: Which of the following statements is true about the proposed approach in the paper?\n\nA) It uses deep neural networks to learn a finite-dimensional approximation of the Koopman operator for modeling vehicle dynamics.\n\nB) It relies on classical system identification methods to identify the vehicle model. \n\nC) It uses model predictive control with a linear vehicle model learned through dynamic mode decomposition.\n\nD) It models the vehicle dynamics using only deep neural networks without the Koopman operator.\n\nCorrect Answer: A\n\nExplanation: The paper proposes using deep neural networks to learn a finite-dimensional approximation of the Koopman operator for modeling vehicle dynamics. Option A summarizes this key aspect of the proposed approach. Options B, C and D describe different modeling approaches not presented in the paper."}, "4": {"documentation": {"title": "Design Verifiably Correct Model Patterns to Facilitate Modeling Medical\n  Best Practice Guidelines with Statecharts (Technical Report)", "source": "Chunhui Guo, Zhicheng Fu, Zhenyu Zhang, Shangping Ren, Lui Sha", "docs_id": "1811.00694", "section": ["cs.SE", "cs.FL", "cs.SY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Design Verifiably Correct Model Patterns to Facilitate Modeling Medical\n  Best Practice Guidelines with Statecharts (Technical Report). Improving patient care safety is an ultimate objective for medical cyber-physical systems. A recent study shows that the patients' death rate can be significantly reduced by computerizing medical best practice guidelines. To facilitate the development of computerized medical best practice guidelines, statecharts are often used as a modeling tool because of their high resemblances to disease and treatment models and their capabilities to provide rapid prototyping and simulation for clinical validations. However, some implementations of statecharts, such as Yakindu statecharts, are priority-based and have synchronous execution semantics which makes it difficult to model certain functionalities that are essential in modeling medical guidelines, such as two-way communications and configurable execution orders. Rather than introducing new statechart elements or changing the statechart implementation's underline semantics, we use existing basic statechart elements to design model patterns for the commonly occurring issues. In particular, we show the design of model patterns for two-way communications and configurable execution orders and formally prove the correctness of these model patterns. We further use a simplified airway laser surgery scenario as a case study to demonstrate how the developed model patterns address the two-way communication and configurable execution order issues and their impact on validation and verification of medical safety properties."}, "answer": " Here is a multi-form question based on the given text:\n\nQuestion: Which of the following statements is true about the model patterns developed in the paper?\n\nA) The model patterns use new statechart elements to address issues with two-way communications and configurable execution orders.\n\nB) The model patterns change the underlying semantics of the statechart implementation to address issues. \n\nC) The model patterns use existing basic statechart elements to address issues with two-way communications and configurable execution orders.\n\nD) The model patterns make it more difficult to model certain essential functionalities in medical guidelines.\n\nCorrect Answer: C\n\nExplanation: According to the text, rather than introducing new statechart elements or changing the statechart implementation's underlying semantics, the authors use existing basic statechart elements to design model patterns that address issues with two-way communications and configurable execution orders."}, "5": {"documentation": {"title": "A General Rate Duality of the MIMO Multiple Access Channel and the MIMO\n  Broadcast Channel", "source": "Raphael Hunger, Michael Joham", "docs_id": "0803.2427", "section": ["cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A General Rate Duality of the MIMO Multiple Access Channel and the MIMO\n  Broadcast Channel. We present a general rate duality between the multiple access channel (MAC) and the broadcast channel (BC) which is applicable to systems with and without nonlinear interference cancellation. Different to the state-of-the-art rate duality with interference subtraction from Vishwanath et al., the proposed duality is filter-based instead of covariance-based and exploits the arising unitary degree of freedom to decorrelate every point-to-point link. Therefore, it allows for noncooperative stream-wise decoding which reduces complexity and latency. Moreover, the conversion from one domain to the other does not exhibit any dependencies during its computation making it accessible to a parallel implementation instead of a serial one. We additionally derive a rate duality for systems with multi-antenna terminals when linear filtering without interference (pre-)subtraction is applied and the different streams of a single user are not treated as self-interference. Both dualities are based on a framework already applied to a mean-square-error duality between the MAC and the BC. Thanks to this novel rate duality, any rate-based optimization with linear filtering in the BC can now be handled in the dual MAC where the arising expressions lead to more efficient algorithmic solutions than in the BC due to the alignment of the channel and precoder indices."}, "answer": " Here is a 4-option multiple choice question on the paper:\n\nQuestion: Which of the following statements is true about the proposed rate duality in the paper?\n\nA) It is covariance-based like the duality by Vishwanath et al.\n\nB) It allows for noncooperative stream-wise decoding, reducing complexity and latency. \n\nC) Converting between MAC and BC domains has dependencies, requiring serial implementation.\n\nD) It is only applicable to systems with nonlinear interference cancellation.\n\nCorrect Answer: B\n\nExplanation: The paper proposes a filter-based rather than covariance-based rate duality, which enables noncooperative decoding to reduce complexity and latency. It does not exhibit dependencies during conversion, allowing parallel implementation. It is applicable to systems with and without nonlinear interference cancellation."}, "6": {"documentation": {"title": "Learning transition times in event sequences: the Event-Based Hidden\n  Markov Model of disease progression", "source": "Peter A. Wijeratne and Daniel C. Alexander", "docs_id": "2011.01023", "section": ["cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Learning transition times in event sequences: the Event-Based Hidden\n  Markov Model of disease progression. Progressive diseases worsen over time and are characterised by monotonic change in features that track disease progression. Here we connect ideas from two formerly separate methodologies -- event-based and hidden Markov modelling -- to derive a new generative model of disease progression. Our model can uniquely infer the most likely group-level sequence and timing of events (natural history) from limited datasets. Moreover, it can infer and predict individual-level trajectories (prognosis) even when data are missing, giving it high clinical utility. Here we derive the model and provide an inference scheme based on the expectation maximisation algorithm. We use clinical, imaging and biofluid data from the Alzheimer's Disease Neuroimaging Initiative to demonstrate the validity and utility of our model. First, we train our model to uncover a new group-level sequence of feature changes in Alzheimer's disease over a period of ${\\sim}17.3$ years. Next, we demonstrate that our model provides improved utility over a continuous time hidden Markov model by area under the receiver operator characteristic curve ${\\sim}0.23$. Finally, we demonstrate that our model maintains predictive accuracy with up to $50\\%$ missing data. These results support the clinical validity of our model and its broader utility in resource-limited medical applications."}, "answer": " Here is a 4-option multiple choice question on the document:\n\nQuestion: Which of the following statements is true about the Event-Based Hidden Markov Model (EBHMM) of disease progression described in the document?\n\nA) The EBHMM uses only event-based modeling without any hidden Markov modeling.\n\nB) The EBHMM can infer group-level disease progression sequences but not individual-level trajectories. \n\nC) The EBHMM provides no improvement in utility over continuous time hidden Markov models.\n\nD) The EBHMM maintains predictive accuracy even with up to 50% missing data.\n\nCorrect Answer: D\n\nExplanation: The document states that the EBHMM combines ideas from event-based and hidden Markov modeling. It can infer both group-level sequences and individual trajectories. It also provides improved utility over continuous hidden Markov models. Finally, it maintains predictive accuracy with up to 50% missing data."}, "7": {"documentation": {"title": "Blockchain for Genomics: A Systematic Literature Review", "source": "Mohammed Alghazwi, Fatih Turkmen, Joeri van der Velde, Dimka\n  Karastoyanova", "docs_id": "2111.10153", "section": ["cs.CR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Blockchain for Genomics: A Systematic Literature Review. Human genomic data carry unique information about an individual and offer unprecedented opportunities for healthcare. The clinical interpretations derived from large genomic datasets can greatly improve healthcare and pave the way for personalized medicine. Sharing genomic datasets, however, pose major challenges, as genomic data is different from traditional medical data, indirectly revealing information about descendants and relatives of the data owner and carrying valid information even after the owner passes away. Therefore, stringent data ownership and control measures are required when dealing with genomic data. In order to provide secure and accountable infrastructure, blockchain technologies offer a promising alternative to traditional distributed systems. Indeed, the research on blockchain-based infrastructures tailored to genomics is on the rise. However, there is a lack of a comprehensive literature review that summarizes the current state-of-the-art methods in the applications of blockchain in genomics. In this paper, we systematically look at the existing work both commercial and academic, and discuss the major opportunities and challenges. Our study is driven by five research questions that we aim to answer in our review. We also present our projections of future research directions which we hope the researchers interested in the area can benefit from."}, "answer": " Here is a 4-option multiple choice question on the given text:\n\nQuestion: Which of the following is NOT mentioned in the text as a benefit of sharing genomic datasets?\n\nA) Improving healthcare through clinical interpretations derived from large genomic datasets \n\nB) Paving the way for personalized medicine\n\nC) Revealing information about descendants and relatives of the data owner\n\nD) Carrying valid information even after the owner passes away\n\nCorrect Answer: C\n\nExplanation: \nOption A is mentioned in the text as a benefit of sharing genomic datasets - \"The clinical interpretations derived from large genomic datasets can greatly improve healthcare and pave the way for personalized medicine.\"\n\nOption B is also mentioned - \"pave the way for personalized medicine.\"\n\nOption D is mentioned - \"carrying valid information even after the owner passes away.\"\n\nOption C is not mentioned as a benefit. The text states \"Sharing genomic datasets, however, pose major challenges, as genomic data is different from traditional medical data, indirectly revealing information about descendants and relatives of the data owner.\" So revealing information about relatives is mentioned as a challenge, not a benefit."}, "8": {"documentation": {"title": "Compressed Sensing: How sharp is the Restricted Isometry Property", "source": "Jeffrey D. Blanchard, Coralia Cartis, and Jared Tanner", "docs_id": "1004.5026", "section": ["cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Compressed Sensing: How sharp is the Restricted Isometry Property. Compressed Sensing (CS) seeks to recover an unknown vector with $N$ entries by making far fewer than $N$ measurements; it posits that the number of compressed sensing measurements should be comparable to the information content of the vector, not simply $N$. CS combines the important task of compression directly with the measurement task. Since its introduction in 2004 there have been hundreds of manuscripts on CS, a large fraction of which develop algorithms to recover a signal from its compressed measurements. Because of the paradoxical nature of CS -- exact reconstruction from seemingly undersampled measurements -- it is crucial for acceptance of an algorithm that rigorous analyses verify the degree of undersampling the algorithm permits. The Restricted Isometry Property (RIP) has become the dominant tool used for the analysis in such cases. We present here an asymmetric form of RIP which gives tighter bounds than the usual symmetric one. We give the best known bounds on the RIP constants for matrices from the Gaussian ensemble. Our derivations illustrate the way in which the combinatorial nature of CS is controlled. Our quantitative bounds on the RIP allow precise statements as to how aggressively a signal can be undersampled, the essential question for practitioners. We also document the extent to which RIP gives precise information about the true performance limits of CS, by comparing with approaches from high-dimensional geometry."}, "answer": " Here is a 4-option multiple choice question on the given passage:\n\nQuestion: Which of the following statements best summarizes a key contribution of the paper?\n\nA) The paper introduces a new asymmetric form of the Restricted Isometry Property that provides tighter bounds than the usual symmetric version. \n\nB) The paper shows that the Restricted Isometry Property constants allow precise statements about how aggressively a signal can be undersampled in Compressed Sensing.\n\nC) The paper proves that the Restricted Isometry Property is the only valid tool for analyzing Compressed Sensing recovery algorithms.\n\nD) The paper documents that the Restricted Isometry Property provides no useful information about the true limits of Compressed Sensing.\n\nCorrect Answer: B\n\nExplanation: The passage states that the paper presents an asymmetric form of the RIP that gives tighter bounds, but this is not its main contribution. It also does not claim the RIP is the only analysis tool nor that it provides no useful information. The key point is that the quantitative RIP bounds allow precise statements about undersampling limits in Compressed Sensing, which is summarized in option B."}, "9": {"documentation": {"title": "Data based reconstruction of complex multiplex networks", "source": "Chuang Ma, Han-Shuang Chen, Xiang Li, Ying-Cheng Lai, Hai-Feng Zhang", "docs_id": "1806.03405", "section": ["physics.soc-ph", "cs.SI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Data based reconstruction of complex multiplex networks. It has been recognized that many complex dynamical systems in the real world require a description in terms of multiplex networks, where a set of common, mutually connected nodes belong to distinct network layers and play a different role in each layer. In spite of recent progress towards data based inference of single-layer networks, to reconstruct complex systems with a multiplex structure remains largely open. We articulate a mean-field based maximum likelihood estimation framework to solve this outstanding and challenging problem. We demonstrate the power of the reconstruction framework and characterize its performance using binary time series from a class of prototypical duplex network systems that host two distinct types of spreading dynamics. In addition to validating the framework using synthetic and real-world multiplex networks, we carry out a detailed analysis to elucidate the impacts of structural and dynamical parameters as well as noise on the reconstruction accuracy and robustness."}, "answer": " Here is a 4-option multiple choice question on the given research paper summary:\n\nQuestion: Which of the following statements is true about the research described in the paper?\n\nA) The framework can reconstruct single-layer but not multiplex networks from time series data.\n\nB) The framework uses a maximum likelihood approach to reconstruct multiplex networks. \n\nC) The framework was tested on multiplex networks with more than two layers.\n\nD) The research focuses on validating the framework on empirical rather than synthetic networks.\n\nCorrect Answer: B\n\nExplanation: The paper describes a maximum likelihood estimation framework to reconstruct complex multiplex networks from time series data. It was tested on binary time series from duplex (two-layer) network systems. The framework aims to reconstruct multiplex rather than single-layer networks. While it was validated on both synthetic and real-world networks, a detailed analysis was done to study the impacts of various parameters and noise on reconstructing synthetic networks."}, "10": {"documentation": {"title": "Caffeinated FPGAs: FPGA Framework For Convolutional Neural Networks", "source": "Roberto DiCecco, Griffin Lacey, Jasmina Vasiljevic, Paul Chow, Graham\n  Taylor and Shawki Areibi", "docs_id": "1609.09671", "section": ["cs.CV", "cs.DC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Caffeinated FPGAs: FPGA Framework For Convolutional Neural Networks. Convolutional Neural Networks (CNNs) have gained significant traction in the field of machine learning, particularly due to their high accuracy in visual recognition. Recent works have pushed the performance of GPU implementations of CNNs to significantly improve their classification and training times. With these improvements, many frameworks have become available for implementing CNNs on both CPUs and GPUs, with no support for FPGA implementations. In this work we present a modified version of the popular CNN framework Caffe, with FPGA support. This allows for classification using CNN models and specialized FPGA implementations with the flexibility of reprogramming the device when necessary, seamless memory transactions between host and device, simple-to-use test benches, and the ability to create pipelined layer implementations. To validate the framework, we use the Xilinx SDAccel environment to implement an FPGA-based Winograd convolution engine and show that the FPGA layer can be used alongside other layers running on a host processor to run several popular CNNs (AlexNet, GoogleNet, VGG A, Overfeat). The results show that our framework achieves 50 GFLOPS across 3x3 convolutions in the benchmarks. This is achieved within a practical framework, which will aid in future development of FPGA-based CNNs."}, "answer": " Here is a multi-form question on the document:\n\nQuestion: Which of the following statements is true about the Caffeinated FPGAs framework presented in the document?\n\nA) It allows implementation of CNNs on FPGAs using the Caffe framework with no CPU or GPU support. \n\nB) It achieves poor performance compared to GPU implementations of CNNs.\n\nC) It implements a Winograd convolution engine on an FPGA to validate the framework.\n\nD) It only supports AlexNet and no other CNN models.\n\nCorrect Answer: C\n\nExplanation: \nThe document states that the Caffeinated FPGAs framework is a modified version of Caffe with FPGA support. It allows CNN implementation on FPGAs with host-device memory transactions. To validate the framework, a Winograd convolution engine is implemented on an FPGA which is used with other layers on the CPU to run CNNs like AlexNet and GoogleNet. So option C is correct."}, "11": {"documentation": {"title": "Kara: A System for Visualising and Visual Editing of Interpretations for\n  Answer-Set Programs", "source": "Christian Kloim\\\"ullner, Johannes Oetsch, J\\\"org P\\\"uhrer, and Hans\n  Tompits", "docs_id": "1109.4095", "section": ["cs.LO", "cs.AI", "cs.GR", "cs.PL"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Kara: A System for Visualising and Visual Editing of Interpretations for\n  Answer-Set Programs. In answer-set programming (ASP), the solutions of a problem are encoded in dedicated models, called answer sets, of a logical theory. These answer sets are computed from the program that represents the theory by means of an ASP solver and returned to the user as sets of ground first-order literals. As this type of representation is often cumbersome for the user to interpret, tools like ASPVIZ and IDPDraw were developed that allow for visualising answer sets. The tool Kara, introduced in this paper, follows these approaches, using ASP itself as a language for defining visualisations of interpretations. Unlike existing tools that position graphic primitives according to static coordinates only, Kara allows for more high-level specifications, supporting graph structures, grids, and relative positioning of graphical elements. Moreover, generalising the functionality of previous tools, Kara provides modifiable visualisations such that interpretations can be manipulated by graphically editing their visualisations. This is realised by resorting to abductive reasoning techniques. Kara is part of SeaLion, a forthcoming integrated development environment (IDE) for ASP."}, "answer": " Question: Which of the following statements about Kara is true?\n\nA) Kara is a system for visualising and editing answer sets that uses static coordinates to position graphic primitives.\n\nB) Kara allows for high-level specifications like graph structures and relative positioning when visualising answer sets. \n\nC) Kara provides static visualisations of answer sets that cannot be edited or manipulated.\n\nD) Kara is completely separate from SeaLion, the IDE for answer-set programming.\n\nCorrect Answer: B\n\nExplanation: The passage states that unlike existing tools, Kara allows for more high-level specifications like graph structures and relative positioning when visualising answer sets. It also mentions that Kara provides modifiable visualisations where interpretations can be edited by graphically manipulating their visualisations. Therefore, option B is correct. Options A, C, and D contradict information provided in the passage."}, "12": {"documentation": {"title": "Latent fingerprint minutia extraction using fully convolutional network", "source": "Yao Tang, Fei Gao, Jufu Feng", "docs_id": "1609.09850", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Latent fingerprint minutia extraction using fully convolutional network. Minutiae play a major role in fingerprint identification. Extracting reliable minutiae is difficult for latent fingerprints which are usually of poor quality. As the limitation of traditional handcrafted features, a fully convolutional network (FCN) is utilized to learn features directly from data to overcome complex background noises. Raw fingerprints are mapped to a correspondingly-sized minutia-score map with a fixed stride. And thus a large number of minutiae will be extracted through a given threshold. Then small regions centering at these minutia points are entered into a convolutional neural network (CNN) to reclassify these minutiae and calculate their orientations. The CNN shares convolutional layers with the fully convolutional network to speed up. 0.45 second is used on average to detect one fingerprint on a GPU. On the NIST SD27 database, we achieve 53\\% recall rate and 53\\% precise rate that outperform many other algorithms. Our trained model is also visualized to show that we have successfully extracted features preserving ridge information of a latent fingerprint."}, "answer": " Question: What method is used to extract minutiae from latent fingerprints in the paper?\n\nA) A convolutional neural network \nB) A fully connected neural network\nC) A fully convolutional network\nD) A recurrent neural network\n\nCorrect Answer: C\n\nExplanations:\nThe paper states that \"a fully convolutional network (FCN) is utilized to learn features directly from data to overcome complex background noises.\" This indicates that a fully convolutional network is used to extract minutiae from latent fingerprints."}, "13": {"documentation": {"title": "Short-term Load Forecasting Based on Hybrid Strategy Using Warm-start\n  Gradient Tree Boosting", "source": "Yuexin Zhang, Jiahong Wang", "docs_id": "2005.11478", "section": ["cs.LG", "eess.SP", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Short-term Load Forecasting Based on Hybrid Strategy Using Warm-start\n  Gradient Tree Boosting. A deep-learning-based hybrid strategy for short-term load forecasting is presented. The strategy proposes a novel tree-based ensemble method Warm-start Gradient Tree Boosting (WGTB). Current strategies either ensemble submodels of a single type, which fail to take advantage of the statistical strengths of different inference models. Or they simply sum the outputs from completely different inference models, which doesn't maximize the potential of ensemble. Inspired by the bias-variance trade-off, WGTB is proposed and tailored to the great disparity among different inference models on accuracy, volatility and linearity. The complete strategy integrates four different inference models of different capacities. WGTB then ensembles their outputs by a warm-start and a hybrid of bagging and boosting, which lowers bias and variance concurrently. It is validated on two real datasets from State Grid Corporation of China of hourly resolution. The result demonstrates the effectiveness of the proposed strategy that hybridizes the statistical strengths of both low-bias and low-variance inference models."}, "answer": " Question: Which of the following statements accurately describes a key contribution of the proposed Warm-start Gradient Tree Boosting (WGTB) method for short-term load forecasting?\n\nA) It ensembles multiple deep learning models to maximize predictive accuracy.\n\nB) It combines completely different types of inference models by simply summing their outputs.\n\nC) It ensembles submodels of a single type to take advantage of their statistical strengths. \n\nD) It ensembles different inference models to balance bias and variance based on their capacities.\n\nCorrect Answer: D\n\nExplanation: The passage states that WGTB ensembles four different inference models of varying capacities to concurrently lower bias and variance, balancing their statistical strengths. Options A, B, and C describe other ensemble strategies but not the key contribution of WGTB."}, "14": {"documentation": {"title": "A Vision-Guided Multi-Robot Cooperation Framework for\n  Learning-by-Demonstration and Task Reproduction", "source": "Bidan Huang, Menglong Ye, Su-Lin Lee, Guang-Zhong Yang", "docs_id": "1706.00508", "section": ["cs.RO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Vision-Guided Multi-Robot Cooperation Framework for\n  Learning-by-Demonstration and Task Reproduction. This paper presents a vision-based learning-by-demonstration approach to enable robots to learn and complete a manipulation task cooperatively. With this method, a vision system is involved in both the task demonstration and reproduction stages. An expert first demonstrates how to use tools to perform a task, while the tool motion is observed using a vision system. The demonstrations are then encoded using a statistical model to generate a reference motion trajectory. Equipped with the same tools and the learned model, the robot is guided by vision to reproduce the task. The task performance was evaluated in terms of both accuracy and speed. However, simply increasing the robot's speed could decrease the reproduction accuracy. To this end, a dual-rate Kalman filter is employed to compensate for latency between the robot and vision system. More importantly, the sampling rates of the reference trajectory and the robot speed are optimised adaptively according to the learned motion model. We demonstrate the effectiveness of our approach by performing two tasks: a trajectory reproduction task and a bimanual sewing task. We show that using our vision-based approach, the robots can conduct effective learning by demonstrations and perform accurate and fast task reproduction. The proposed approach is generalisable to other manipulation tasks, where bimanual or multi-robot cooperation is required."}, "answer": " Here is a 4-option multiple choice question on the paper:\n\nQuestion: Which of the following statements is true about the vision-guided multi-robot cooperation framework presented in the paper?\n\nA) The framework uses vision to demonstrate tasks to robots, but not during task reproduction.\n\nB) Increasing robot speed always improves task reproduction accuracy. \n\nC) A dual-rate Kalman filter is used to optimize the sampling rates of the reference trajectory and robot speed.\n\nD) The approach has only been tested on trajectory reproduction tasks.\n\nCorrect Answer: C\n\nExplanation: According to the paper, the framework uses vision during both task demonstration and reproduction. Simply increasing robot speed can decrease accuracy. A dual-rate Kalman filter is used to compensate for latency and adaptively optimize the sampling rates. The approach was evaluated on both a trajectory task and a bimanual sewing task."}, "15": {"documentation": {"title": "On the Power of Simple Reductions for the Maximum Independent Set\n  Problem", "source": "Darren Strash", "docs_id": "1608.00724", "section": ["cs.DS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On the Power of Simple Reductions for the Maximum Independent Set\n  Problem. Reductions---rules that reduce input size while maintaining the ability to compute an optimal solution---are critical for developing efficient maximum independent set algorithms in both theory and practice. While several simple reductions have previously been shown to make small domain-specific instances tractable in practice, it was only recently shown that advanced reductions (in a measure-and-conquer approach) can be used to solve real-world networks on millions of vertices [Akiba and Iwata, TCS 2016]. In this paper we compare these state-of-the-art reductions against a small suite of simple reductions, and come to two conclusions: just two simple reductions---vertex folding and isolated vertex removal---are sufficient for many real-world instances, and further, the power of the advanced rules comes largely from their initial application (i.e., kernelization), and not their repeated application during branch-and-bound. As a part of our comparison, we give the first experimental evaluation of a reduction based on maximum critical independent sets, and show it is highly effective in practice for medium-sized networks."}, "answer": " Here is a multi-form question based on the given text:\n\nQuestion: According to the paper, what are the two simple reductions that are sufficient for solving many real-world maximum independent set instances?\n\nA) Vertex contraction and edge contraction\nB) Vertex folding and isolated vertex removal \nC) Node merging and leaf removal\nD) Clustering and pruning\n\nCorrect Answer: B\n\nExplanation: The text states that \"just two simple reductions---vertex folding and isolated vertex removal---are sufficient for many real-world instances\". So vertex folding and isolated vertex removal are the two simple reductions that are sufficient for solving many real-world maximum independent set problems."}, "16": {"documentation": {"title": "Fast Topological Clustering with Wasserstein Distance", "source": "Tananun Songdechakraiwut, Bryan M. Krause, Matthew I. Banks, Kirill V.\n  Nourski and Barry D. Van Veen", "docs_id": "2112.00101", "section": ["cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Fast Topological Clustering with Wasserstein Distance. The topological patterns exhibited by many real-world networks motivate the development of topology-based methods for assessing the similarity of networks. However, extracting topological structure is difficult, especially for large and dense networks whose node degrees range over multiple orders of magnitude. In this paper, we propose a novel and computationally practical topological clustering method that clusters complex networks with intricate topology using principled theory from persistent homology and optimal transport. Such networks are aggregated into clusters through a centroid-based clustering strategy based on both their topological and geometric structure, preserving correspondence between nodes in different networks. The notions of topological proximity and centroid are characterized using a novel and efficient approach to computation of the Wasserstein distance and barycenter for persistence barcodes associated with connected components and cycles. The proposed method is demonstrated to be effective using both simulated networks and measured functional brain networks."}, "answer": " Question: Which of the following statements accurately describes a key contribution of the paper \"Fast Topological Clustering with Wasserstein Distance\"?\n\nA) The paper proposes a new method for topological data analysis that is faster than previous methods.\n\nB) The paper introduces a novel way to compute the Wasserstein distance and barycenter for persistence barcodes. \n\nC) The paper develops an approach for clustering complex networks based on their topological and geometric structure.\n\nD) The paper shows how to extract topological structure from large and dense networks.\n\nCorrect Answer: C\n\nExplanation: The paper develops a new topological clustering method that clusters networks based on their topological and geometric structure, as captured by persistent homology barcodes and Wasserstein distances/barycenters. While the paper does propose a novel way to compute Wasserstein distances/barycenters for persistence barcodes, and this enables the overall clustering approach, the key contribution is the clustering method itself. Choices A and D describe related but incorrect contributions."}, "17": {"documentation": {"title": "Ethical behavior in humans and machines -- Evaluating training data\n  quality for beneficial machine learning", "source": "Thilo Hagendorff", "docs_id": "2008.11463", "section": ["cs.CY", "cs.AI", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Ethical behavior in humans and machines -- Evaluating training data\n  quality for beneficial machine learning. Machine behavior that is based on learning algorithms can be significantly influenced by the exposure to data of different qualities. Up to now, those qualities are solely measured in technical terms, but not in ethical ones, despite the significant role of training and annotation data in supervised machine learning. This is the first study to fill this gap by describing new dimensions of data quality for supervised machine learning applications. Based on the rationale that different social and psychological backgrounds of individuals correlate in practice with different modes of human-computer-interaction, the paper describes from an ethical perspective how varying qualities of behavioral data that individuals leave behind while using digital technologies have socially relevant ramification for the development of machine learning applications. The specific objective of this study is to describe how training data can be selected according to ethical assessments of the behavior it originates from, establishing an innovative filter regime to transition from the big data rationale n = all to a more selective way of processing data for training sets in machine learning. The overarching aim of this research is to promote methods for achieving beneficial machine learning applications that could be widely useful for industry as well as academia."}, "answer": " Here is a 4-option multiple choice question on the document:\n\nQuestion: Which of the following is NOT mentioned in the document as a way to promote beneficial machine learning?\n\nA) Describing new dimensions of data quality for supervised machine learning applications based on ethical assessments. \n\nB) Establishing a filter regime to transition from using all available data to a more selective approach for training sets.\n\nC) Measuring data quality solely in technical terms without considering ethical dimensions.\n\nD) Selecting training data according to assessments of the behavior it originates from.\n\nCorrect Answer: C\n\nExplanation: The document discusses new ethical dimensions of data quality, selective data usage for training, and selecting data based on assessments of behavior it comes from. It does NOT mention only measuring data quality in technical terms without ethics."}, "18": {"documentation": {"title": "Iterative Model Predictive Control for Piecewise Systems", "source": "Ugo Rosolia and Aaron D. Ames", "docs_id": "2104.08267", "section": ["eess.SY", "cs.SY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Iterative Model Predictive Control for Piecewise Systems. In this paper, we present an iterative Model Predictive Control (MPC) design for piecewise nonlinear systems. We consider finite time control tasks where the goal of the controller is to steer the system from a starting configuration to a goal state while minimizing a cost function. First, we present an algorithm that leverages a feasible trajectory that completes the task to construct a control policy which guarantees that state and input constraints are recursively satisfied and that the closed-loop system reaches the goal state in finite time. Utilizing this construction, we present a policy iteration scheme that iteratively generates safe trajectories which have non-decreasing performance. Finally, we test the proposed strategy on a discretized Spring Loaded Inverted Pendulum (SLIP) model with massless legs. We show that our methodology is robust to changes in initial conditions and disturbances acting on the system. Furthermore, we demonstrate the effectiveness of our policy iteration algorithm in a minimum time control task."}, "answer": " Here is a 4-option multiple choice question on the paper:\n\nQuestion: Which of the following statements is true about the iterative Model Predictive Control (MPC) design presented in the paper for piecewise nonlinear systems?\n\nA) The controller aims to minimize a cost function while steering the system from an initial state to a goal state over a finite time horizon. \n\nB) The algorithm constructs a policy that guarantees state and input constraints are satisfied but does not necessarily reach the goal state.\n\nC) The policy iteration scheme generates trajectories with decreasing performance at each iteration.\n\nD) The method was tested on a Spring Loaded Inverted Pendulum model and shown to be robust to disturbances.\n\nCorrect Answer: D\n\nExplanation: \nThe paper presents an iterative MPC design that steers the system to a goal state while minimizing a cost function and satisfying constraints (A). The algorithm constructs a policy that reaches the goal state (not B). The policy iteration generates improving trajectories (not C). The method was demonstrated to be robust to disturbances when tested on a SLIP model (D)."}, "19": {"documentation": {"title": "Anchor: Locating Android Framework-specific Crashing Faults", "source": "Pingfan Kong, Li Li, Jun Gao, Timoth\\'ee Riom, Yanjie Zhao,\n  Tegawend\\'e F. Bissyand\\'e, Jacques Klein", "docs_id": "2008.01676", "section": ["cs.SE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Anchor: Locating Android Framework-specific Crashing Faults. Android framework-specific app crashes are hard to debug. Indeed, the callback-based event-driven mechanism of Android challenges crash localization techniques that are developed for traditional Java programs. The key challenge stems from the fact that the buggy code location may not even be listed within the stack trace. For example, our empirical study on 500 framework-specific crashes from an open benchmark has revealed that 37 percent of the crash types are related to bugs that are outside the stack traces. Moreover, Android programs are a mixture of code and extra-code artifacts such as the Manifest file. The fact that any artifact can lead to failures in the app execution creates the need to position the localization target beyond the code realm. In this paper, we propose Anchor, a two-phase suspicious bug location suggestion tool. Anchor specializes in finding crash-inducing bugs outside the stack trace. Anchor is lightweight and source code independent since it only requires the crash message and the apk file to locate the fault. Experimental results, collected via cross-validation and in-the-wild dataset evaluation, show that Anchor is effective in locating Android framework-specific crashing faults."}, "answer": " Question: What is the key challenge in locating Android framework-specific crashing faults according to the paper?\n\nA) The callback-based event-driven mechanism of Android\nB) The fact that any artifact can lead to failures in the app execution \nC) The buggy code location may not even be listed within the stack trace\nD) Experimental results show Anchor is effective in locating faults\n\nCorrect Answer: C) The buggy code location may not even be listed within the stack trace\n\nExplanations:\nThe paper states that \"the key challenge stems from the fact that the buggy code location may not even be listed within the stack trace. For example, our empirical study on 500 framework-specific crashes from an open benchmark has revealed that 37 percent of the crash types are related to bugs that are outside the stack traces.\" So the key challenge is that the buggy code may not be in the stack trace."}, "20": {"documentation": {"title": "Pixel-wise Orthogonal Decomposition for Color Illumination Invariant and\n  Shadow-free Image", "source": "Liangqiong Qu, Jiandong Tian, Zhi Han, and Yandong Tang", "docs_id": "1407.0010", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Pixel-wise Orthogonal Decomposition for Color Illumination Invariant and\n  Shadow-free Image. In this paper, we propose a novel, effective and fast method to obtain a color illumination invariant and shadow-free image from a single outdoor image. Different from state-of-the-art methods for shadow-free image that either need shadow detection or statistical learning, we set up a linear equation set for each pixel value vector based on physically-based shadow invariants, deduce a pixel-wise orthogonal decomposition for its solutions, and then get an illumination invariant vector for each pixel value vector on an image. The illumination invariant vector is the unique particular solution of the linear equation set, which is orthogonal to its free solutions. With this illumination invariant vector and Lab color space, we propose an algorithm to generate a shadow-free image which well preserves the texture and color information of the original image. A series of experiments on a diverse set of outdoor images and the comparisons with the state-of-the-art methods validate our method."}, "answer": " Question: What method does the paper propose to obtain a color illumination invariant and shadow-free image from a single outdoor image?\n\nA) Using shadow detection or statistical learning like other methods\nB) Setting up a linear equation set for each pixel value vector based on physically-based shadow invariants and deducing a pixel-wise orthogonal decomposition\nC) Using a convolutional neural network to detect and remove shadows\nD) Applying a global color constancy algorithm to estimate the scene illumination \n\nCorrect Answer: B\n\nExplanation: The paper proposes setting up a linear equation set for each pixel value vector based on physically-based shadow invariants, deducing a pixel-wise orthogonal decomposition for its solutions, and getting an illumination invariant vector for each pixel value vector. This differs from other methods that use shadow detection or statistical learning."}, "21": {"documentation": {"title": "On Learning Discrete Graphical Models Using Greedy Methods", "source": "Ali Jalali and Chris Johnson and Pradeep Ravikumar", "docs_id": "1107.3258", "section": ["cs.LG", "math.ST", "stat.ML", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On Learning Discrete Graphical Models Using Greedy Methods. In this paper, we address the problem of learning the structure of a pairwise graphical model from samples in a high-dimensional setting. Our first main result studies the sparsistency, or consistency in sparsity pattern recovery, properties of a forward-backward greedy algorithm as applied to general statistical models. As a special case, we then apply this algorithm to learn the structure of a discrete graphical model via neighborhood estimation. As a corollary of our general result, we derive sufficient conditions on the number of samples n, the maximum node-degree d and the problem size p, as well as other conditions on the model parameters, so that the algorithm recovers all the edges with high probability. Our result guarantees graph selection for samples scaling as n = Omega(d^2 log(p)), in contrast to existing convex-optimization based algorithms that require a sample complexity of \\Omega(d^3 log(p)). Further, the greedy algorithm only requires a restricted strong convexity condition which is typically milder than irrepresentability assumptions. We corroborate these results using numerical simulations at the end."}, "answer": " Here is a potential multi-form question on the paper:\n\nQuestion: Which of the following statements accurately summarizes a key contribution of the paper?\n\nA) The paper proposes a new convex optimization algorithm for learning discrete graphical models that requires milder assumptions than previous methods.\n\nB) The paper proves that the forward-backward greedy algorithm can recover the true graph structure with a sample complexity scaling as n = \u03a9(d^2 log p). \n\nC) The paper shows that greedy methods cannot recover the correct graph structure without strong assumptions on restricted strong convexity.\n\nD) The paper provides sufficient conditions on n, d, and p for consistency of the lasso method for graphical model selection.\n\nCorrect Answer: B\n\nThe paper analyzes the sample complexity of a greedy algorithm for structure learning in discrete graphical models. A key result is that the forward-backward greedy approach can recover the true graph with n = \u03a9(d^2 log p) samples under certain assumptions, improving on the \u03a9(d^3 log p) requirement of convex methods."}, "22": {"documentation": {"title": "Dual Graph-Laplacian PCA: A Closed-Form Solution for Bi-clustering to\n  Find \"Checkerboard\" Structures on Gene Expression Data", "source": "Jin-Xing Liu, Chun-Mei Feng, Xiang-Zhen Kong, Yong Xu", "docs_id": "1901.06794", "section": ["q-bio.GN", "cs.CE", "q-bio.QM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Dual Graph-Laplacian PCA: A Closed-Form Solution for Bi-clustering to\n  Find \"Checkerboard\" Structures on Gene Expression Data. In the context of cancer, internal \"checkerboard\" structures are normally found in the matrices of gene expression data, which correspond to genes that are significantly up- or down-regulated in patients with specific types of tumors. In this paper, we propose a novel method, called dual graph-regularization principal component analysis (DGPCA). The main innovation of this method is that it simultaneously considers the internal geometric structures of the condition manifold and the gene manifold. Specifically, we obtain principal components (PCs) to represent the data and approximate the cluster membership indicators through Laplacian embedding. This new method is endowed with internal geometric structures, such as the condition manifold and gene manifold, which are both suitable for bi-clustering. A closed-form solution is provided for DGPCA. We apply this new method to simultaneously cluster genes and conditions (e.g., different samples) with the aim of finding internal \"checkerboard\" structures on gene expression data, if they exist. Then, we use this new method to identify regulatory genes under the particular conditions and to compare the results with those of other state-of-the-art PCA-based methods. Promising results on gene expression data have been verified by extensive experiments"}, "answer": " Here is a multi-form question on the given documentation:\n\nQuestion: Which of the following statements is true about the dual graph-Laplacian PCA (DGPCA) method proposed in the paper?\n\nA) It uses graph regularization to capture the geometric structure of only the condition manifold, not the gene manifold.\n\nB) It provides an iterative algorithm to approximate the solution for bi-clustering genes and conditions. \n\nC) It aims to find checkerboard patterns in gene expression data that indicate co-regulated genes.\n\nD) It outperforms standard PCA methods but does not have a closed-form solution.\n\nCorrect Answer: C\n\nExplanation: According to the documentation, DGPCA uses graph regularization to capture the geometric structures of both the condition and gene manifolds. It provides a closed-form solution rather than an iterative algorithm. A key goal is to identify checkerboard patterns indicating co-regulated genes, which differs from standard PCA."}, "23": {"documentation": {"title": "Locality-Sensitive Hashing for f-Divergences: Mutual Information Loss\n  and Beyond", "source": "Lin Chen, Hossein Esfandiari, Thomas Fu, Vahab S. Mirrokni", "docs_id": "1910.12414", "section": ["cs.LG", "cs.DB", "cs.DS", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Locality-Sensitive Hashing for f-Divergences: Mutual Information Loss\n  and Beyond. Computing approximate nearest neighbors in high dimensional spaces is a central problem in large-scale data mining with a wide range of applications in machine learning and data science. A popular and effective technique in computing nearest neighbors approximately is the locality-sensitive hashing (LSH) scheme. In this paper, we aim to develop LSH schemes for distance functions that measure the distance between two probability distributions, particularly for f-divergences as well as a generalization to capture mutual information loss. First, we provide a general framework to design LHS schemes for f-divergence distance functions and develop LSH schemes for the generalized Jensen-Shannon divergence and triangular discrimination in this framework. We show a two-sided approximation result for approximation of the generalized Jensen-Shannon divergence by the Hellinger distance, which may be of independent interest. Next, we show a general method of reducing the problem of designing an LSH scheme for a Krein kernel (which can be expressed as the difference of two positive definite kernels) to the problem of maximum inner product search. We exemplify this method by applying it to the mutual information loss, due to its several important applications such as model compression."}, "answer": " Question: What is the main contribution of the paper \"Locality-Sensitive Hashing for f-Divergences: Mutual Information Loss and Beyond\"?\n\nA) Developing a general framework to design LSH schemes for f-divergence distance functions and applying it to specific divergences like generalized Jensen-Shannon divergence.\n\nB) Providing a two-sided approximation result for the generalized Jensen-Shannon divergence using the Hellinger distance. \n\nC) Showing a method to reduce designing an LSH scheme for a Krein kernel to maximum inner product search, and applying it to mutual information loss.\n\nD) Computing approximate nearest neighbors for probability distributions using locality-sensitive hashing.\n\nCorrect Answer: C\n\nExplanation: The paper's main contribution is showing a general method to reduce designing an LSH scheme for a Krein kernel (expressible as a difference of two positive definite kernels) to maximum inner product search, and applying this method specifically to mutual information loss. Options A and B describe other contributions but not the main one. Option D describes the overall goal but not the core contribution."}, "24": {"documentation": {"title": "Nonadaptive Mastermind Algorithms for String and Vector Databases, with\n  Case Studies", "source": "Arthur U. Asuncion and Michael T. Goodrich (Department of Computer\n  Science, University of California, Irvine)", "docs_id": "1012.2509", "section": ["cs.CR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Nonadaptive Mastermind Algorithms for String and Vector Databases, with\n  Case Studies. In this paper, we study sparsity-exploiting Mastermind algorithms for attacking the privacy of an entire database of character strings or vectors, such as DNA strings, movie ratings, or social network friendship data. Based on reductions to nonadaptive group testing, our methods are able to take advantage of minimal amounts of privacy leakage, such as contained in a single bit that indicates if two people in a medical database have any common genetic mutations, or if two people have any common friends in an online social network. We analyze our Mastermind attack algorithms using theoretical characterizations that provide sublinear bounds on the number of queries needed to clone the database, as well as experimental tests on genomic information, collaborative filtering data, and online social networks. By taking advantage of the generally sparse nature of these real-world databases and modulating a parameter that controls query sparsity, we demonstrate that relatively few nonadaptive queries are needed to recover a large majority of each database."}, "answer": " Question: What is the main focus of the paper \"Nonadaptive Mastermind Algorithms for String and Vector Databases, with Case Studies\"?\n\nA) Developing Mastermind algorithms that exploit sparsity to attack the privacy of databases of strings or vectors, like DNA strings or movie ratings, using a small number of nonadaptive queries.\n\nB) Analyzing the performance of different machine learning algorithms on genomic and collaborative filtering datasets. \n\nC) Proposing new methods for collaborative filtering and making recommendations based on sparse user data.\n\nD) Designing adaptive Mastermind algorithms that can clone social network databases using a minimal number of queries.\n\nCorrect Answer: A\n\nExplanation: The paper focuses on developing sparsity-exploiting Mastermind algorithms to attack the privacy of entire databases using a small number of nonadaptive queries. It analyzes the algorithms theoretically and experimentally on genomic, collaborative filtering, and social network data."}, "25": {"documentation": {"title": "Classification of breast cancer histology images using transfer learning", "source": "Sulaiman Vesal, Nishant Ravikumar, AmirAbbas Davari, Stephan Ellmann,\n  Andreas Maier", "docs_id": "1802.09424", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Classification of breast cancer histology images using transfer learning. Breast cancer is one of the leading causes of mortality in women. Early detection and treatment are imperative for improving survival rates, which have steadily increased in recent years as a result of more sophisticated computer-aided-diagnosis (CAD) systems. A critical component of breast cancer diagnosis relies on histopathology, a laborious and highly subjective process. Consequently, CAD systems are essential to reduce inter-rater variability and supplement the analyses conducted by specialists. In this paper, a transfer-learning based approach is proposed, for the task of breast histology image classification into four tissue sub-types, namely, normal, benign, \\textit{in situ} carcinoma and invasive carcinoma. The histology images, provided as part of the BACH 2018 grand challenge, were first normalized to correct for color variations resulting from inconsistencies during slide preparation. Subsequently, image patches were extracted and used to fine-tune Google`s Inception-V3 and ResNet50 convolutional neural networks (CNNs), both pre-trained on the ImageNet database, enabling them to learn domain-specific features, necessary to classify the histology images. The ResNet50 network (based on residual learning) achieved a test classification accuracy of 97.50% for four classes, outperforming the Inception-V3 network which achieved an accuracy of 91.25%."}, "answer": " Here is a 4-option multiple choice question on the document:\n\nQuestion: Which of the following statements is true regarding the transfer learning-based approach for breast histology image classification proposed in the paper?\n\nA) It involved fine-tuning the ResNet50 CNN pre-trained on ImageNet to enable it to learn domain-specific features needed for classifying the histology images into 4 subtypes.\n\nB) It used the Inception-V3 model pre-trained on ImageNet without any fine-tuning and achieved 97.5% test accuracy on classifying histology images into 4 subtypes. \n\nC) It classified the histology images into 3 subtypes - normal, benign and malignant.\n\nD) It did not involve any pre-trained CNNs and used a model trained from scratch on the BACH dataset.\n\nCorrect Answer: A\n\nExplanation: \nThe paper proposes a transfer learning-based approach where Google's Inception-V3 and ResNet50 CNNs, pre-trained on ImageNet, are fine-tuned on the BACH dataset to enable them to learn domain-specific features needed to classify histology images into 4 subtypes - normal, benign, in situ carcinoma and invasive carcinoma. The ResNet50 model achieved the best accuracy of 97.5% after fine-tuning, outperforming Inception-V3."}, "26": {"documentation": {"title": "Optimal Perimeter Guarding with Heterogeneous Robot Teams: Complexity\n  Analysis and Effective Algorithms", "source": "Si Wei Feng and Jingjin Yu", "docs_id": "1912.08591", "section": ["math.OC", "cs.RO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Optimal Perimeter Guarding with Heterogeneous Robot Teams: Complexity\n  Analysis and Effective Algorithms. We perform structural and algorithmic studies of significantly generalized versions of the optimal perimeter guarding (OPG) problem. As compared with the original OPG where robots are uniform, in this paper, many mobile robots with heterogeneous sensing capabilities are to be deployed to optimally guard a set of one-dimensional segments. Two complimentary formulations are investigated where one limits the number of available robots (OPG_LR) and the other seeks to minimize the total deployment cost (OPG_MC). In contrast to the original OPG which admits low-polynomial time solutions, both OPG_LR and OPG_MC are computationally intractable with OPG_LR being strongly NP-hard. Nevertheless, we develop fairly scalable pseudo-polynomial time algorithms for practical, fixed-parameter subcase of OPG_LR; we also develop pseudo-polynomial time algorithm for general OPG_MC and polynomial time algorithm for the fixed-parameter OPG_MC case. The applicability and effectiveness of selected algorithms are demonstrated through extensive numerical experiments."}, "answer": " Here is a multi-form question on the paper:\n\nQuestion: Which of the following statements is true about the Optimal Perimeter Guarding with Heterogeneous Robot Teams paper?\n\nA) The original Optimal Perimeter Guarding problem admits polynomial time solutions, but the generalized versions studied in this paper are NP-hard. \n\nB) The paper develops pseudo-polynomial time algorithms for the fixed-parameter cases of both OPG_LR and OPG_MC.\n\nC) The paper shows that OPG_MC is strongly NP-hard, but develops a polynomial time algorithm for OPG_LR.\n\nD) Both OPG_LR and OPG_MC can be solved in polynomial time for the general case without restrictions.\n\nCorrect Answer: B\n\nThe paper shows that both OPG_LR and OPG_MC are computationally intractable in the general case, with OPG_LR being strongly NP-hard. However, for the fixed-parameter versions of these problems, the paper develops pseudo-polynomial time algorithms. So B is the correct statement."}, "27": {"documentation": {"title": "Apache VXQuery: A Scalable XQuery Implementation", "source": "E. Preston Carman Jr. (1), Till Westmann (2), Vinayak R. Borkar (3),\n  Michael J. Carey (3) and Vassilis J. Tsotras (1) ((1) UC Riverside, (2)\n  Oracle Labs, (3) UC Irvine)", "docs_id": "1504.00331", "section": ["cs.DB"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Apache VXQuery: A Scalable XQuery Implementation. The wide use of XML for document management and data exchange has created the need to query large repositories of XML data. To efficiently query such large data collections and take advantage of parallelism, we have implemented Apache VXQuery, an open-source scalable XQuery processor. The system builds upon two other open-source frameworks -- Hyracks, a parallel execution engine, and Algebricks, a language agnostic compiler toolbox. Apache VXQuery extends these two frameworks and provides an implementation of the XQuery specifics (data model, data-model dependent functions and optimizations, and a parser). We describe the architecture of Apache VXQuery, its integration with Hyracks and Algebricks, and the XQuery optimization rules applied to the query plan to improve path expression efficiency and to enable query parallelism. An experimental evaluation using a real 500GB dataset with various selection, aggregation and join XML queries shows that Apache VXQuery performs well both in terms of scale-up and speed-up. Our experiments show that it is about 3x faster than Saxon (an open-source and commercial XQuery processor) on a 4-core, single node implementation, and around 2.5x faster than Apache MRQL (a MapReduce-based parallel query processor) on an eight (4-core) node cluster."}, "answer": " Question: Which of the following statements is true about Apache VXQuery based on the documentation?\n\nA) It is an open-source XQuery processor that builds on Hyracks and Algebricks to enable scalable and parallel XQuery processing.\n\nB) It is slower than Saxon, an open-source XQuery processor, in single node experiments. \n\nC) It is an order of magnitude faster than Apache MRQL, a MapReduce based system, in multi-node experiments.\n\nD) It only focuses on XQuery parsing and does not provide any query optimization.\n\nCorrect Answer: A\n\nExplanation: \nThe documentation states that Apache VXQuery is an open-source scalable XQuery processor that builds upon Hyracks and Algebricks frameworks to provide parallel execution and XQuery specific optimizations. It is about 3x faster than Saxon on a single 4-core node and 2.5x faster than Apache MRQL on an 8 node cluster. So A is the correct option."}, "28": {"documentation": {"title": "Living on the Edge: The Role of Proactive Caching in 5G Wireless\n  Networks", "source": "Ejder Ba\\c{s}tu\\u{g}, Mehdi Bennis, M\\'erouane Debbah", "docs_id": "1405.5974", "section": ["cs.NI", "cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Living on the Edge: The Role of Proactive Caching in 5G Wireless\n  Networks. This article explores one of the key enablers of beyond $4$G wireless networks leveraging small cell network deployments, namely proactive caching. Endowed with predictive capabilities and harnessing recent developments in storage, context-awareness and social networks, peak traffic demands can be substantially reduced by proactively serving predictable user demands, via caching at base stations and users' devices. In order to show the effectiveness of proactive caching, we examine two case studies which exploit the spatial and social structure of the network, where proactive caching plays a crucial role. Firstly, in order to alleviate backhaul congestion, we propose a mechanism whereby files are proactively cached during off-peak demands based on file popularity and correlations among users and files patterns. Secondly, leveraging social networks and device-to-device (D2D) communications, we propose a procedure that exploits the social structure of the network by predicting the set of influential users to (proactively) cache strategic contents and disseminate them to their social ties via D2D communications. Exploiting this proactive caching paradigm, numerical results show that important gains can be obtained for each case study, with backhaul savings and a higher ratio of satisfied users of up to $22\\%$ and $26\\%$, respectively. Higher gains can be further obtained by increasing the storage capability at the network edge."}, "answer": " Here is a multi-form question based on the given text:\n\nQuestion: Which of the following is NOT mentioned in the text as a key benefit provided by proactive caching in 5G wireless networks?\n\nA) Reducing peak traffic demands by serving predictable user requests from caches\nB) Alleviating backhaul congestion by caching popular files during off-peak times\nC) Exploiting social networks to identify influential users to cache and share content  \nD) Increasing battery life of user devices by proactively downloading content during idle times\n\nCorrect Answer: D\n\nExplanation: The text discusses using proactive caching to reduce peak demands by serving predictable requests from caches, alleviate backhaul congestion, and leverage social networks to identify influential users to cache and share content. However, it does not mention anything about increasing battery life of user devices."}, "29": {"documentation": {"title": "Secure Wireless Communications via Cooperation", "source": "Lun Dong, Zhu Han, Athina P. Petropulu, H. Vincent Poor", "docs_id": "0809.4807", "section": ["cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Secure Wireless Communications via Cooperation. The feasibility of physical-layer-based security approaches for wireless communications in the presence of one or more eavesdroppers is hampered by channel conditions. In this paper, cooperation is investigated as an approach to overcome this problem and improve the performance of secure communications. In particular, a decode-and-forward (DF) based cooperative protocol is considered, and the objective is to design the system for secrecy capacity maximization or transmit power minimization. System design for the DF-based cooperative protocol is first studied by assuming the availability of global channel state information (CSI). For the case of one eavesdropper, an iterative scheme is proposed to obtain the optimal solution for the problem of transmit power minimization. For the case of multiple eavesdroppers, the problem of secrecy capacity maximization or transmit power minimization is in general intractable. Suboptimal system design is proposed by adding an additional constraint, i.e., the complete nulling of signals at all eavesdroppers, which yields simple closed-form solutions for the aforementioned two problems. Then, the impact of imperfect CSI of eavesdroppers on system design is studied, in which the ergodic secrecy capacity is of interest."}, "answer": " Here is a 4-option multiple choice question on the document:\n\nQuestion: Which of the following statements is true about the decode-and-forward (DF) based cooperative protocol considered in the paper?\n\nA) It assumes availability of only local CSI for system design.\n\nB) For multiple eavesdroppers, it gives optimal solutions for secrecy capacity maximization. \n\nC) It completely nulls signals at all eavesdroppers to get suboptimal solutions.\n\nD) It maximizes secrecy capacity by minimizing transmit power with global CSI.\n\nCorrect Answer: C\n\nExplanation: The paper considers a DF-based cooperative protocol and studies system design. For multiple eavesdroppers, suboptimal solutions are obtained by adding a constraint to completely null signals at all eavesdroppers, as stated in option C. Options A, B and D are incorrect interpretations of the paper."}, "30": {"documentation": {"title": "Distance Metric Learning for Kernel Machines", "source": "Zhixiang Xu, Kilian Q. Weinberger, Olivier Chapelle", "docs_id": "1208.3422", "section": ["stat.ML", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Distance Metric Learning for Kernel Machines. Recent work in metric learning has significantly improved the state-of-the-art in k-nearest neighbor classification. Support vector machines (SVM), particularly with RBF kernels, are amongst the most popular classification algorithms that uses distance metrics to compare examples. This paper provides an empirical analysis of the efficacy of three of the most popular Mahalanobis metric learning algorithms as pre-processing for SVM training. We show that none of these algorithms generate metrics that lead to particularly satisfying improvements for SVM-RBF classification. As a remedy we introduce support vector metric learning (SVML), a novel algorithm that seamlessly combines the learning of a Mahalanobis metric with the training of the RBF-SVM parameters. We demonstrate the capabilities of SVML on nine benchmark data sets of varying sizes and difficulties. In our study, SVML outperforms all alternative state-of-the-art metric learning algorithms in terms of accuracy and establishes itself as a serious alternative to the standard Euclidean metric with model selection by cross validation."}, "answer": " Question: Which of the following statements accurately summarizes a key finding from the research described in the given text?\n\nA) Support vector machines with RBF kernels perform worse than k-nearest neighbor classification, even when using metrics learned by recent metric learning algorithms.\n\nB) The support vector metric learning (SVML) algorithm introduced in the paper significantly outperforms other metric learning algorithms when used for preprocessing data for SVM-RBF classification. \n\nC) Metric learning algorithms like SVML are not effective for improving SVM-RBF classification performance compared to just using the Euclidean metric.\n\nD) SVML provides only marginal improvements in accuracy compared to cross-validation for model selection with the Euclidean metric.\n\nCorrect Answer: B\n\nExplanation: The key finding is that SVML, the new algorithm introduced in the paper, outperforms other metric learning algorithms as well as the standard Euclidean metric at improving accuracy for SVM-RBF classification. The paper shows empirically that SVML generates better metrics for SVM-RBF compared to other popular metric learning algorithms."}, "31": {"documentation": {"title": "Structure-aware Person Image Generation with Pose Decomposition and\n  Semantic Correlation", "source": "Jilin Tang, Yi Yuan, Tianjia Shao, Yong Liu, Mengmeng Wang, Kun Zhou", "docs_id": "2102.02972", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Structure-aware Person Image Generation with Pose Decomposition and\n  Semantic Correlation. In this paper we tackle the problem of pose guided person image generation, which aims to transfer a person image from the source pose to a novel target pose while maintaining the source appearance. Given the inefficiency of standard CNNs in handling large spatial transformation, we propose a structure-aware flow based method for high-quality person image generation. Specifically, instead of learning the complex overall pose changes of human body, we decompose the human body into different semantic parts (e.g., head, torso, and legs) and apply different networks to predict the flow fields for these parts separately. Moreover, we carefully design the network modules to effectively capture the local and global semantic correlations of features within and among the human parts respectively. Extensive experimental results show that our method can generate high-quality results under large pose discrepancy and outperforms state-of-the-art methods in both qualitative and quantitative comparisons."}, "answer": " Question: What method does the paper propose for pose guided person image generation that can handle large spatial transformations efficiently?\n\nA) Using a standard CNN architecture to predict the overall pose change.\n\nB) Decomposing the human body into semantic parts like head, torso, legs and using separate networks to predict flow fields. \n\nC) Using a convolutional LSTM network to model the temporal pose changes.\n\nD) Employing a generative adversarial network with a pose discriminator.\n\nCorrect Answer: B) Decomposing the human body into semantic parts like head, torso, legs and using separate networks to predict flow fields.\n\nExplanation: The paper proposes decomposing the human body into semantic parts and using separate networks to predict flow fields for each part to efficiently handle large spatial transformations in pose guided person image generation. It avoids learning the complex overall pose changes with a single network."}, "32": {"documentation": {"title": "Siamese Labels Auxiliary Network(SiLaNet)", "source": "Wenrui Gan, Zhulin Liu, C. L. Philip Chen, Tong Zhang", "docs_id": "2103.00200", "section": ["cs.AI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Siamese Labels Auxiliary Network(SiLaNet). Auxiliary information attracts more and more attention in the area of machine learning. Attempts so far to include such auxiliary information in state-of-the-art learning process have often been based on simply appending these auxiliary features to the data level or feature level. In this paper, we intend to propose a novel training method with new options and architectures. Siamese labels, which were used in the training phase as auxiliary modules. While in the testing phase, the auxiliary module should be removed. Siamese label module makes it easier to train and improves the performance in testing process. In general, the main contributions can be summarized as, 1) Siamese Labels are firstly proposed as auxiliary information to improve the learning efficiency; 2) We establish a new architecture, Siamese Labels Auxiliary Network (SilaNet), which is to assist the training of the model; 3) Siamese Labels Auxiliary Network is applied to compress the model parameters by 50% and ensure the high accuracy at the same time. For the purpose of comparison, we tested the network on CIFAR-10 and CIFAR100 using some common models. The proposed SilaNet performs excellent efficiency both on the accuracy and robustness."}, "answer": " Here is a 4-option multiple choice question on the Siamese Labels Auxiliary Network paper:\n\nQuestion: Which of the following is NOT a main contribution of the Siamese Labels Auxiliary Network (SilaNet) paper?\n\nA) Proposing Siamese Labels as a new form of auxiliary information to improve learning efficiency. \n\nB) Establishing a new neural network architecture called SilaNet that utilizes Siamese Labels.\n\nC) Showing SilaNet can compress model parameters by 50% while maintaining accuracy.\n\nD) Demonstrating state-of-the-art performance on CIFAR-10 and CIFAR-100 datasets.\n\nCorrect Answer: D\n\nExplanation: The paper does not claim state-of-the-art performance on CIFAR-10 and CIFAR-100. While SilaNet shows improved efficiency and accuracy over baseline models, it does not surpass state-of-the-art results on these datasets. The main contributions are proposing Siamese Labels, the SilaNet architecture using them, and compressing parameters while maintaining accuracy."}, "33": {"documentation": {"title": "Theoretical links between universal and Bayesian compressed sensing\n  algorithms", "source": "Shirin Jalali", "docs_id": "1801.01069", "section": ["cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Theoretical links between universal and Bayesian compressed sensing\n  algorithms. Quantized maximum a posteriori (Q-MAP) is a recently-proposed Bayesian compressed sensing algorithm that, given the source distribution, recovers $X^n$ from its linear measurements $Y^m=AX^n$, where $A\\in R^{m\\times n}$ denotes the known measurement matrix. On the other hand, Lagrangian minimum entropy pursuit (L-MEP) is a universal compressed sensing algorithm that aims at recovering $X^n$ from its linear measurements $Y^m=AX^n$, without having access to the source distribution. Both Q-MAP and L-MEP provably achieve the minimum required sampling rates, in noiseless cases where such fundamental limits are known. L-MEP is based on minimizing a cost function that consists of a linear combination of the conditional empirical entropy of a potential reconstruction vector and its corresponding measurement error. In this paper, using a first-order linear approximation of the conditional empirical entropy function, L-MEP is connected with Q-MAP. The established connection between L-MEP and Q-MAP leads to variants of Q-MAP which have the same asymptotic performance as Q-MAP in terms of their required sampling rates. Moreover, these variants suggest that Q-MAP is robust to small error in estimating the source distribution. This robustness is theoretically proven and the effect of a non-vanishing estimation error on the required sampling rate is characterized."}, "answer": " Here is a multi-form question on the document:\n\nQuestion: Which of the following statements accurately describes a key finding from the research described in the document?\n\nA) L-MEP and Q-MAP achieve the same sampling rates in noiseless environments, but Q-MAP performs better in noisy environments. \n\nB) Approximating the conditional empirical entropy function shows that L-MEP and Q-MAP are equivalent algorithms.\n\nC) Variants of Q-MAP are robust to small errors in estimating the source distribution, as proven theoretically.\n\nD) Q-MAP requires knowledge of the source distribution, while L-MEP does not need this information.\n\nCorrect Answer: C\n\nExplanation: The document states that using a linear approximation of the conditional empirical entropy function connects L-MEP and Q-MAP. Furthermore, it mentions that this connection suggests variants of Q-MAP are robust to small errors in estimating the source distribution, which is also theoretically proven. Therefore, option C accurately summarizes a key finding from the research."}, "34": {"documentation": {"title": "Fundamentals of the Backoff Process in 802.11: Dichotomy of the\n  Aggregation", "source": "Jeong-woo Cho, Yuming Jiang", "docs_id": "0904.4155", "section": ["cs.NI", "cs.PF"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Fundamentals of the Backoff Process in 802.11: Dichotomy of the\n  Aggregation. This paper discovers fundamental principles of the backoff process that governs the performance of IEEE 802.11. A simplistic principle founded upon regular variation theory is that the backoff time has a truncated Pareto-type tail distribution with an exponent of $(\\log \\gamma)/\\log m$ ($m$ is the multiplicative factor and $\\gamma$ is the collision probability). This reveals that the per-node backoff process is heavy-tailed in the strict sense for $\\gamma>1/m^2$, and paves the way for the following unifying result. The state-of-the-art theory on the superposition of the heavy-tailed processes is applied to establish a dichotomy exhibited by the aggregate backoff process, putting emphasis on the importance of time-scale on which we view the backoff processes. While the aggregation on normal time-scales leads to a Poisson process, it is approximated by a new limiting process possessing long-range dependence (LRD) on coarse time-scales. This dichotomy turns out to be instrumental in formulating short-term fairness, extending existing formulas to arbitrary population, and to elucidate the absence of LRD in practical situations. A refined wavelet analysis is conducted to strengthen this argument."}, "answer": " Here is a multi-form question on the paper:\n\nQuestion: Which of the following statements is true about the backoff process in 802.11 networks according to the paper?\n\nA) The backoff time always follows a truncated Pareto distribution regardless of the collision probability.\n\nB) The per-node backoff process exhibits heavy-tailed behavior only when the collision probability is less than 1/m^2. \n\nC) The aggregate backoff process displays Poisson characteristics on normal time-scales but long-range dependence on coarse time-scales.\n\nD) There is strong evidence of long-range dependence in practical 802.11 networks.\n\nCorrect Answer: C\n\nExplanation: \nThe paper shows that the per-node backoff time follows a truncated Pareto distribution, with heavy-tailed behavior arising when the collision probability \u03b3 is greater than 1/m^2. The key finding is that aggregation of the backoff processes leads to a dichotomy - Poisson on normal timescales but long-range dependence on coarse scales. The paper argues that long-range dependence is not seen in practice."}, "35": {"documentation": {"title": "On the Tasks and Characteristics of Product Owners: A Case Study in the\n  Oil & Gas Industry", "source": "Carolin Unger-Windeler, Jil Kluender", "docs_id": "1809.00830", "section": ["cs.SE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On the Tasks and Characteristics of Product Owners: A Case Study in the\n  Oil & Gas Industry. Product owners in the Scrum framework - respectively the on-site customer when applying eXtreme Programming - have an important role in the development process. They are responsible for the requirements and backlog deciding about the next steps within the development process. However, many companies face the difficulty of defining the tasks and the responsibilities of a product owner on their way towards an agile work environment. While literature addresses the tailoring of the product owner's role in general, research does not particularly consider the specifics of this role in the context of a systems development as we find for example in the oil and gas industry. Consequently, the question arises whether there are any differences between these two areas. In order to answer this question, we investigated on the current state of characteristics and tasks of product owners at Baker Hughes, a GE company (BHGE). In this position paper, we present initial results based on an online survey with answers of ten active product owners within the technical software department of BHGE. The results indicate that current product owners at BHGE primarily act as a nexus between all ends. While technical tasks are performed scarcely, communication skills seem even more important for product owners in a system development organization. However, to obtain more reliable results additional research in this area is required."}, "answer": " Question: What role do product owners at BHGE primarily play according to the initial survey results presented in the paper?\n\nA) They focus mostly on technical tasks related to software development.\n\nB) They act as a main point of contact between different groups involved in the development process. \n\nC) They decide which requirements should be prioritized in the product backlog.\n\nD) They have strong communication skills but do not interact much with stakeholders.\n\nCorrect Answer: B\n\nExplanation: According to the passage, the initial survey results indicate that product owners at BHGE \"primarily act as a nexus between all ends\" rather than focusing on technical tasks. While communication skills are important, the main role is serving as the main point of contact between different groups."}, "36": {"documentation": {"title": "Formalization of malware through process calculi", "source": "Gregoire Jacob, Eric Filiol and Herve Debar", "docs_id": "0902.0469", "section": ["cs.CR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Formalization of malware through process calculi. Since the seminal work from F. Cohen in the eighties, abstract virology has seen the apparition of successive viral models, all based on Turing-equivalent formalisms. But considering recent malware such as rootkits or k-ary codes, these viral models only partially cover these evolved threats. The problem is that Turing-equivalent models do not support interactive computations. New models have thus appeared, offering support for these evolved malware, but loosing the unified approach in the way. This article provides a basis for a unified malware model founded on process algebras and in particular the Join-Calculus. In terms of expressiveness, the new model supports the fundamental definitions based on self-replication and adds support for interactions, concurrency and non-termination allows the definition of more complex behaviors. Evolved malware such as rootkits can now be thoroughly modeled. In terms of detection and prevention, the fundamental results of undecidability and isolation still hold. However the process-based model has permitted to establish new results: identification of fragments from the Join-Calculus where malware detection becomes decidable, formal definition of the non-infection property, approximate solutions to restrict malware propagation."}, "answer": " Here is a 4-option multiple choice question on the document:\n\nQuestion: According to the document, which of the following statements is true about the process calculi-based malware models compared to previous Turing-equivalent models?\n\nA) The process calculi models offer better support for interactive computations but lose the unified approach.\n\nB) The process calculi models offer worse support for interactive computations and lose the unified approach. \n\nC) The process calculi models offer better support for interactive computations and maintain the unified approach.\n\nD) The process calculi models offer worse support for interactive computations but maintain the unified approach.\n\nCorrect Answer: A\n\nExplanation: The document states that the process calculi models offer support for interactive computations, which previous Turing-equivalent models did not. However, it also mentions that the new models lose the unified approach present in previous models."}, "37": {"documentation": {"title": "A Probabilistic Approach to Knowledge Translation", "source": "Shangpu Jiang, Daniel Lowd, Dejing Dou", "docs_id": "1507.03181", "section": ["cs.AI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Probabilistic Approach to Knowledge Translation. In this paper, we focus on a novel knowledge reuse scenario where the knowledge in the source schema needs to be translated to a semantically heterogeneous target schema. We refer to this task as \"knowledge translation\" (KT). Unlike data translation and transfer learning, KT does not require any data from the source or target schema. We adopt a probabilistic approach to KT by representing the knowledge in the source schema, the mapping between the source and target schemas, and the resulting knowledge in the target schema all as probability distributions, specially using Markov random fields and Markov logic networks. Given the source knowledge and mappings, we use standard learning and inference algorithms for probabilistic graphical models to find an explicit probability distribution in the target schema that minimizes the Kullback-Leibler divergence from the implicit distribution. This gives us a compact probabilistic model that represents knowledge from the source schema as well as possible, respecting the uncertainty in both the source knowledge and the mapping. In experiments on both propositional and relational domains, we find that the knowledge obtained by KT is comparable to other approaches that require data, demonstrating that knowledge can be reused without data."}, "answer": " Here is a 4-option multiple choice question on the paper:\n\nQuestion: Which of the following statements is true about the knowledge translation (KT) approach presented in the paper?\n\nA) KT requires data from the source and target schemas. \n\nB) KT represents the source knowledge, mapping, and target knowledge all as probability distributions.\n\nC) KT uses neural networks for learning and inference.\n\nD) KT results in knowledge that is worse than approaches requiring data.\n\nCorrect Answer: B\n\nExplanation: The paper states that unlike data translation and transfer learning, KT does not require any data from the source or target schema. It adopts a probabilistic approach by representing the source knowledge, mapping, and target knowledge as probability distributions using graphical models. Experiments show KT gives comparable knowledge to approaches needing data."}, "38": {"documentation": {"title": "Learnable Fourier Features for Multi-Dimensional Spatial Positional\n  Encoding", "source": "Yang Li, Si Si, Gang Li, Cho-Jui Hsieh, Samy Bengio", "docs_id": "2106.02795", "section": ["cs.LG", "cs.AI", "cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Learnable Fourier Features for Multi-Dimensional Spatial Positional\n  Encoding. Attentional mechanisms are order-invariant. Positional encoding is a crucial component to allow attention-based deep model architectures such as Transformer to address sequences or images where the position of information matters. In this paper, we propose a novel positional encoding method based on learnable Fourier features. Instead of hard-coding each position as a token or a vector, we represent each position, which can be multi-dimensional, as a trainable encoding based on learnable Fourier feature mapping, modulated with a multi-layer perceptron. The representation is particularly advantageous for a spatial multi-dimensional position, e.g., pixel positions on an image, where $L_2$ distances or more complex positional relationships need to be captured. Our experiments based on several public benchmark tasks show that our learnable Fourier feature representation for multi-dimensional positional encoding outperforms existing methods by both improving the accuracy and allowing faster convergence."}, "answer": " Question: What method is proposed in the paper for representing positional encodings that can capture complex positional relationships?\n\nA) Using a fixed encoding vector for each position \n\nB) Assigning each position a learnable token ID\n\nC) Encoding positions as a function of their Cartesian coordinates  \n\nD) Representing positions as trainable encodings based on learnable Fourier feature mapping\n\nCorrect Answer: D) Representing positions as trainable encodings based on learnable Fourier feature mapping\n\nExplanation: The paper proposes a novel positional encoding method based on learnable Fourier features. Instead of using predefined encodings for each position, the positions are represented as trainable encodings based on a learnable Fourier feature mapping. This allows capturing complex positional relationships like L2 distances, making it advantageous for multi-dimensional spatial positions like pixels in an image."}, "39": {"documentation": {"title": "A Review on Explainability in Multimodal Deep Neural Nets", "source": "Gargi Joshi, Rahee Walambe, Ketan Kotecha", "docs_id": "2105.07878", "section": ["cs.AI", "cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Review on Explainability in Multimodal Deep Neural Nets. Artificial Intelligence techniques powered by deep neural nets have achieved much success in several application domains, most significantly and notably in the Computer Vision applications and Natural Language Processing tasks. Surpassing human-level performance propelled the research in the applications where different modalities amongst language, vision, sensory, text play an important role in accurate predictions and identification. Several multimodal fusion methods employing deep learning models are proposed in the literature. Despite their outstanding performance, the complex, opaque and black-box nature of the deep neural nets limits their social acceptance and usability. This has given rise to the quest for model interpretability and explainability, more so in the complex tasks involving multimodal AI methods. This paper extensively reviews the present literature to present a comprehensive survey and commentary on the explainability in multimodal deep neural nets, especially for the vision and language tasks. Several topics on multimodal AI and its applications for generic domains have been covered in this paper, including the significance, datasets, fundamental building blocks of the methods and techniques, challenges, applications, and future trends in this domain"}, "answer": " Here is a 4-option multiple choice question on the given text:\n\nQuestion: Which of the following statements is true regarding explainability in multimodal deep neural nets according to the given text?\n\nA) Multimodal deep neural nets have achieved human-level performance in computer vision and natural language processing tasks.\n\nB) The complex and opaque nature of deep neural nets limits their acceptance despite outstanding performance. \n\nC) Several topics like significance, datasets, techniques, challenges etc of multimodal AI are covered.\n\nD) Multimodal fusion methods do not employ deep learning models as per literature.\n\nCorrect Answer: C) Several topics like significance, datasets, techniques, challenges etc of multimodal AI are covered.\n\nExplanation: The text mentions that it covers several topics on multimodal AI including significance, datasets used, fundamental techniques, challenges, applications and future trends. Option C summarizes this correctly. Options A, B and D are facts mentioned in the text but do not summarize the overall coverage of topics. Hence, option C is the right answer."}, "40": {"documentation": {"title": "Multi-Fact Correction in Abstractive Text Summarization", "source": "Yue Dong, Shuohang Wang, Zhe Gan, Yu Cheng, Jackie Chi Kit Cheung and\n  Jingjing Liu", "docs_id": "2010.02443", "section": ["cs.CL"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Multi-Fact Correction in Abstractive Text Summarization. Pre-trained neural abstractive summarization systems have dominated extractive strategies on news summarization performance, at least in terms of ROUGE. However, system-generated abstractive summaries often face the pitfall of factual inconsistency: generating incorrect facts with respect to the source text. To address this challenge, we propose Span-Fact, a suite of two factual correction models that leverages knowledge learned from question answering models to make corrections in system-generated summaries via span selection. Our models employ single or multi-masking strategies to either iteratively or auto-regressively replace entities in order to ensure semantic consistency w.r.t. the source text, while retaining the syntactic structure of summaries generated by abstractive summarization models. Experiments show that our models significantly boost the factual consistency of system-generated summaries without sacrificing summary quality in terms of both automatic metrics and human evaluation."}, "answer": " Question: What method is proposed in the paper to address the challenge of factual inconsistency in abstractive text summarization?\n\nA) Using extractive strategies instead of abstractive summarization \n\nB) Employing question answering models to make corrections via span selection\n\nC) Iteratively replacing nouns in the summary to ensure semantic consistency \n\nD) Retaining the syntactic structure of the original summary\n\nCorrect Answer: B) Employing question answering models to make corrections via span selection\n\nExplanation: The paper proposes Span-Fact, which uses knowledge from question answering models to make factual corrections in abstractive summaries by replacing entities via span selection. This allows the model to retain the syntactic structure of the original summary while ensuring semantic consistency."}, "41": {"documentation": {"title": "Programmable 3D snapshot microscopy with Fourier convolutional networks", "source": "Diptodip Deb, Zhenfei Jiao, Alex B. Chen, Misha B. Ahrens, Kaspar\n  Podgorski, Srinivas C. Turaga", "docs_id": "2104.10611", "section": ["eess.IV", "cs.CV", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Programmable 3D snapshot microscopy with Fourier convolutional networks. 3D snapshot microscopy enables fast volumetric imaging by capturing a 3D volume in a single 2D camera image, and has found a variety of biological applications such as whole brain imaging of fast neural activity in larval zebrafish. The optimal microscope design for this optical 3D-to-2D encoding is both sample- and task-dependent, with no general solution known. Highly programmable optical elements create new possibilities for sample-specific computational optimization of microscope parameters, e.g. tuning the collection of light for a given sample structure. We perform such optimization with deep learning, using a differentiable wave-optics simulation of light propagation through a programmable microscope and a neural network to reconstruct volumes from the microscope image. We introduce a class of global kernel Fourier convolutional neural networks which can efficiently decode information from multiple depths in the volume, globally encoded across a 3D snapshot image. We show that our proposed networks succeed in large field of view volume reconstruction and microscope parameter optimization where traditional networks fail. We also show that our networks outperform the state-of-the-art learned reconstruction algorithms for lensless computational photography."}, "answer": " Here is a 4-option multiple choice question on the 3D snapshot microscopy paper:\n\nQuestion: Which of the following statements is true about the global kernel Fourier convolutional neural networks proposed in the paper?\n\nA) They use a differentiable wave-optics simulation to optimize microscope parameters like the point spread function.\n\nB) They can efficiently extract information from multiple depths in the volume encoded in a single 2D snapshot image. \n\nC) They achieve better performance than traditional convolutional networks by using global rather than local kernels.\n\nD) They introduce a new optical element design that enables highly programmable 3D-to-2D encoding.\n\nCorrect Answer: B\n\nExplanation: \nOption B is correct. According to the paper, the global kernel Fourier convolutional networks can efficiently decode information from multiple depths in the volume that are globally encoded across the 3D snapshot image. \n\nOption A describes the differentiable simulation used for optimizing microscope parameters, not the networks themselves. \n\nOption C compares the global kernel networks to traditional convolutional networks, but does not describe their key capability.\n\nOption D refers to the highly programmable optical elements that enable optimization, not something introduced by the networks."}, "42": {"documentation": {"title": "UV-Net: Learning from Boundary Representations", "source": "Pradeep Kumar Jayaraman, Aditya Sanghi, Joseph G. Lambourne, Karl D.D.\n  Willis, Thomas Davies, Hooman Shayani, Nigel Morris", "docs_id": "2006.10211", "section": ["cs.CV", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "UV-Net: Learning from Boundary Representations. We introduce UV-Net, a novel neural network architecture and representation designed to operate directly on Boundary representation (B-rep) data from 3D CAD models. The B-rep format is widely used in the design, simulation and manufacturing industries to enable sophisticated and precise CAD modeling operations. However, B-rep data presents some unique challenges when used with modern machine learning due to the complexity of the data structure and its support for both continuous non-Euclidean geometric entities and discrete topological entities. In this paper, we propose a unified representation for B-rep data that exploits the U and V parameter domain of curves and surfaces to model geometry, and an adjacency graph to explicitly model topology. This leads to a unique and efficient network architecture, UV-Net, that couples image and graph convolutional neural networks in a compute and memory-efficient manner. To aid in future research we present a synthetic labelled B-rep dataset, SolidLetters, derived from human designed fonts with variations in both geometry and topology. Finally we demonstrate that UV-Net can generalize to supervised and unsupervised tasks on five datasets, while outperforming alternate 3D shape representations such as point clouds, voxels, and meshes."}, "answer": " Question: Which of the following statements is true about UV-Net, the neural network architecture presented in the paper?\n\nA) It operates on voxel representations of 3D CAD models.\n\nB) It uses image and graph convolutional neural networks to process boundary representations. \n\nC) It was tested on the ModelNet dataset.\n\nD) It cannot handle models with varying topology.\n\nCorrect Answer: B\n\nExplanation: The paper introduces UV-Net, a neural network that operates on boundary representations (B-reps) of 3D CAD models. It uses a unified representation that models geometry with U and V parameter domains of curves/surfaces and topology with an adjacency graph. This allows coupling of image and graph CNNs. It was tested on a new synthetic dataset called SolidLetters, not ModelNet. The method is designed to handle varying topology."}, "43": {"documentation": {"title": "Deep Multi-Agent Reinforcement Learning for Cost Efficient Distributed\n  Load Frequency Control", "source": "Sergio Rozada, Dimitra Apostolopoulou, and Eduardo Alonso", "docs_id": "2010.06293", "section": ["eess.SY", "cs.SY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Deep Multi-Agent Reinforcement Learning for Cost Efficient Distributed\n  Load Frequency Control. The rise of microgrid-based architectures is heavily modifying the energy control landscape in distribution systems making distributed control mechanisms necessary to ensure reliable power system operations. In this paper, we propose the use of Reinforcement Learning techniques to implement load frequency control without requiring a central authority. To this end, we approximate the optimal solution of the primary, secondary, and tertiary control with the use of the Multi- Agent Deep Deterministic Policy Gradient (MADDPG) algorithm. Generation units are characterised as agents that learn how to maximise their long-term performance by acting and interacting with the environment to balance generation and load in a cost efficient way. Network effects are also modelled in our framework for the restoration of frequency to the nominal value. We validate our Reinforcement Learning methodology through numerical results and show that it can be used to implement the load frequency control in a distributed and cost efficient way."}, "answer": " Here is a 4-option multiple choice question on the paper:\n\nQuestion: Which of the following statements is true about the proposed approach in the paper?\n\nA) It uses model-free reinforcement learning to implement primary frequency control in a distributed way.\n\nB) It aims to minimize the cost of secondary frequency control through a central controller. \n\nC) It employs multi-agent deep Q-learning to restore frequency to its nominal value.\n\nD) It focuses on tertiary control and coordination between multiple microgrids.\n\nCorrect Answer: A\n\nExplanation: \nThe paper proposes using the multi-agent deep deterministic policy gradient (MADDPG) algorithm for distributed load frequency control without a central authority. It aims to approximate the optimal solution for primary, secondary and tertiary control in a decentralized way through reinforcement learning. Therefore, option A, which states the approach uses model-free RL for distributed primary control, is correct. Options B, C and D refer to different aspects not focused on in the paper."}, "44": {"documentation": {"title": "Combinatorial Optimization based Feature Selection Method: A study on\n  Network Intrusion Detection", "source": "Anjum Nazir, Rizwan Ahmed Khan", "docs_id": "1906.04494", "section": ["cs.CR", "cs.NI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Combinatorial Optimization based Feature Selection Method: A study on\n  Network Intrusion Detection. Advancements in computer networks and communication technologies like software defined networks (SDN), Internet of things (IoT), microservices architecture, cloud computing and network function virtualization (NFV) have opened new fronts and challenges for security experts to combat against modern cyberattacks. Relying on perimeter defense and signature-based network security solutions like Intrusion Detection and Prevention Systems (IDS/IPS) have failed to deliver adequate level of security against new attack vectors such as advance persistent threats, zero days, ransomware, botnets and other forms of targeted attacks. Recent developments in machine learning and cognitive computing have shown great potential to detect unknown and new intrusion events where legacy misuse and anomaly based intrusion detection systems usually fail. In this research study we applied state of the art machine learning algorithms on UNSW-NB15 dataset for potential applicability to detect new attacks. We also proposed a novel wrapper based feature selection technique TS-RF using metaheuristic Tabu Search (TS) algorithm and Random Forest (RF) ensemble classifier. Results obtained by applying proposed feature selection technique i.e. TS-RF on UNSW-NB15 dataset show improvement in overall intrusion detection accuracy while it reduces computation complexity as it removes more than 60% features."}, "answer": " Here is a 4-option multiple choice question on the document:\n\nQuestion: Which of the following statements is true about the proposed feature selection technique TS-RF in the document?\n\nA) It uses Tabu Search and Random Forest to reduce computation complexity by removing irrelevant features.\n\nB) It combines Tabu Search and Random Forest to improve intrusion detection accuracy on the UNSW-NB15 dataset. \n\nC) It removes more than 60% of features from the UNSW-NB15 dataset to reduce computation time.\n\nD) It uses only Random Forest for feature selection on the UNSW-NB15 dataset to improve accuracy.\n\nCorrect Answer: B\n\nExplanation: The document states that the proposed TS-RF technique combines Tabu Search and Random Forest for feature selection, and results show it improves overall intrusion detection accuracy on the UNSW-NB15 dataset while reducing computation complexity by removing over 60% of features."}, "45": {"documentation": {"title": "Learning Discriminative Shrinkage Deep Networks for Image Deconvolution", "source": "Pin-Hung Kuo, Jinshan Pan, Shao-Yi Chien and Ming-Hsuan Yang", "docs_id": "2111.13876", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Learning Discriminative Shrinkage Deep Networks for Image Deconvolution. Non-blind deconvolution is an ill-posed problem. Most existing methods usually formulate this problem into a maximum-a-posteriori framework and address it by designing kinds of regularization terms and data terms of the latent clear images. In this paper, we propose an effective non-blind deconvolution approach by learning discriminative shrinkage functions to implicitly model these terms. In contrast to most existing methods that use deep convolutional neural networks (CNNs) or radial basis functions to simply learn the regularization term, we formulate both the data term and regularization term and split the deconvolution model into data-related and regularization-related sub-problems according to the alternating direction method of multipliers. We explore the properties of the Maxout function and develop a deep CNN model with a Maxout layer to learn discriminative shrinkage functions to directly approximate the solutions of these two sub-problems. Moreover, given the fast Fourier transform based image restoration usually leads to ringing artifacts while conjugate gradient-based image restoration is time-consuming, we develop the conjugate gradient network to restore the latent clear images effectively and efficiently. Experimental results show that the proposed method performs favorably against the state-of-the-art ones in terms of efficiency and accuracy."}, "answer": " Question: Which of the following statements is true about the proposed approach for non-blind image deconvolution in the paper?\n\nA) It uses a deep CNN with radial basis functions to learn the regularization term.\n\nB) It splits the deconvolution model into data and regularization sub-problems solved with Maxout networks. \n\nC) It relies on FFT-based image restoration to avoid ringing artifacts.\n\nD) It directly models the MAP objective's data and regularization terms with a single network.\n\nCorrect Answer: B\n\nExplanation: The paper proposes learning discriminative shrinkage functions with a Maxout CNN to approximate solutions to the data and regularization sub-problems in an alternating direction method. It does not use radial basis functions or rely solely on FFT, and it splits the problem into two sub-problems rather than modeling both terms with one network."}, "46": {"documentation": {"title": "Near-field Perception for Low-Speed Vehicle Automation using\n  Surround-view Fisheye Cameras", "source": "Ciaran Eising, Jonathan Horgan and Senthil Yogamani", "docs_id": "2103.17001", "section": ["cs.CV", "cs.RO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Near-field Perception for Low-Speed Vehicle Automation using\n  Surround-view Fisheye Cameras. Cameras are the primary sensor in automated driving systems. They provide high information density and are optimal for detecting road infrastructure cues laid out for human vision. Surround-view camera systems typically comprise of four fisheye cameras with 190{\\deg}+ field of view covering the entire 360{\\deg} around the vehicle focused on near-field sensing. They are the principal sensors for low-speed, high accuracy, and close-range sensing applications, such as automated parking, traffic jam assistance, and low-speed emergency braking. In this work, we provide a detailed survey of such vision systems, setting up the survey in the context of an architecture that can be decomposed into four modular components namely Recognition, Reconstruction, Relocalization, and Reorganization. We jointly call this the 4R Architecture. We discuss how each component accomplishes a specific aspect and provide a positional argument that they can be synergized to form a complete perception system for low-speed automation. We support this argument by presenting results from previous works and by presenting architecture proposals for such a system. Qualitative results are presented in the video at https://youtu.be/ae8bCOF77uY."}, "answer": " Question: What are the four modular components of the 4R Architecture for near-field perception using surround-view fisheye cameras according to the given documentation?\n\nA) Recognition, Reconstruction, Relocalization, and Reorganization \n\nB) Recording, Rewinding, Relaying, and Replaying\n\nC) Redundancy, Resilience, Robustness, and Reliability  \n\nD) Resolution, Range, Rotation, and Rectification\n\nCorrect Answer: A) Recognition, Reconstruction, Relocalization, and Reorganization\n\nExplanation: According to the documentation, the 4R Architecture can be decomposed into four modular components namely Recognition, Reconstruction, Relocalization, and Reorganization. These four components each accomplish a specific aspect and can be synergized to form a complete perception system for low-speed vehicle automation using surround-view fisheye cameras."}, "47": {"documentation": {"title": "Stochastic Convolutional Sparse Coding", "source": "Jinhui Xiong, Peter Richt\\'arik, Wolfgang Heidrich", "docs_id": "1909.00145", "section": ["eess.IV", "cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Stochastic Convolutional Sparse Coding. State-of-the-art methods for Convolutional Sparse Coding usually employ Fourier-domain solvers in order to speed up the convolution operators. However, this approach is not without shortcomings. For example, Fourier-domain representations implicitly assume circular boundary conditions and make it hard to fully exploit the sparsity of the problem as well as the small spatial support of the filters. In this work, we propose a novel stochastic spatial-domain solver, in which a randomized subsampling strategy is introduced during the learning sparse codes. Afterwards, we extend the proposed strategy in conjunction with online learning, scaling the CSC model up to very large sample sizes. In both cases, we show experimentally that the proposed subsampling strategy, with a reasonable selection of the subsampling rate, outperforms the state-of-the-art frequency-domain solvers in terms of execution time without losing the learning quality. Finally, we evaluate the effectiveness of the over-complete dictionary learned from large-scale datasets, which demonstrates an improved sparse representation of the natural images on account of more abundant learned image features."}, "answer": " Question: Which of the following statements accurately describes a limitation of current state-of-the-art Convolutional Sparse Coding methods that employ Fourier-domain solvers?\n\nA) They are unable to fully exploit sparsity and small spatial filter support.\n\nB) They have slow execution times compared to spatial-domain methods. \n\nC) They require circular boundary conditions which are unrealistic.\n\nD) They cannot be applied to large-scale datasets.\n\nCorrect Answer: A\n\nExplanation: The passage states that Fourier-domain representations \"make it hard to fully exploit the sparsity of the problem as well as the small spatial support of the filters.\" This matches option A. While B and C are also true, they are not mentioned as limitations of current state-of-the-art methods. D is incorrect as the passage states the proposed method scales up to large datasets, not that current methods cannot."}, "48": {"documentation": {"title": "Bounds on Asymptotic Rate of Capacitive Crosstalk Avoidance Codes for\n  On-chip Buses", "source": "Tadashi Wadayama and Taisuke Izumi", "docs_id": "1601.06880", "section": ["cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Bounds on Asymptotic Rate of Capacitive Crosstalk Avoidance Codes for\n  On-chip Buses. In order to prevent the capacitive crosstalk in on-chip buses, several types of capacitive crosstalk avoidance codes have been devised. These codes are designed to prohibit transition patterns prone to the capacity crosstalk from any consecutive two words transmitted to on-chip buses. This paper provides a rigorous analysis on the asymptotic rate of (p,q)-transition free word sequences under the assumption that coding is based on a pair of a stateful encoder and a stateless decoder. The symbols p and q represent k-bit transition patterns that should not be appeared in any consecutive two words at the same adjacent k-bit positions. It is proved that the maximum rate of the sequences equals to the subgraph domatic number of (p,q)-transition free graph. Based on the theoretical results on the subgraph domatic partition problem, a pair of lower and upper bounds on the asymptotic rate is derived. We also present that the asymptotic rate 0.8325 is achievable for the (10,01)-transition free word sequences."}, "answer": " Here is a 4-option multiple choice question on the document:\n\nQuestion: Which of the following statements is true about the asymptotic rate of (10,01)-transition free word sequences, according to the document?\n\nA) The asymptotic rate is lower bounded by 0.8325.\n\nB) The asymptotic rate equals the subgraph domatic number of (10,01)-transition free graph. \n\nC) The asymptotic rate is 0.8325 based on theoretical results on subgraph domatic partition problem.\n\nD) The asymptotic rate cannot exceed 0.8325 based on bounds derived from subgraph domatic number.\n\nCorrect Answer: C"}, "49": {"documentation": {"title": "Parameter Sensitivity Analysis of the SparTen High Performance Sparse\n  Tensor Decomposition Software: Extended Analysis", "source": "Jeremy M. Myers, Daniel M. Dunlavy, Keita Teranishi, D. S. Hollman", "docs_id": "2012.01520", "section": ["math.NA", "cs.MS", "cs.NA", "cs.PF", "stat.CO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Parameter Sensitivity Analysis of the SparTen High Performance Sparse\n  Tensor Decomposition Software: Extended Analysis. Tensor decomposition models play an increasingly important role in modern data science applications. One problem of particular interest is fitting a low-rank Canonical Polyadic (CP) tensor decomposition model when the tensor has sparse structure and the tensor elements are nonnegative count data. SparTen is a high-performance C++ library which computes a low-rank decomposition using different solvers: a first-order quasi-Newton or a second-order damped Newton method, along with the appropriate choice of runtime parameters. Since default parameters in SparTen are tuned to experimental results in prior published work on a single real-world dataset conducted using MATLAB implementations of these methods, it remains unclear if the parameter defaults in SparTen are appropriate for general tensor data. Furthermore, it is unknown how sensitive algorithm convergence is to changes in the input parameter values. This report addresses these unresolved issues with large-scale experimentation on three benchmark tensor data sets. Experiments were conducted on several different CPU architectures and replicated with many initial states to establish generalized profiles of algorithm convergence behavior."}, "answer": " Question: What was the main purpose of the parameter sensitivity analysis conducted on SparTen in the given documentation?\n\nA) To determine if the default parameters in SparTen are suitable for general tensor data and how sensitive convergence is to changes in parameter values.\n\nB) To compare the performance of SparTen's solvers across different CPU architectures. \n\nC) To establish new default parameter values in SparTen based on large-scale experimentation.\n\nD) To validate that SparTen computes accurate tensor decompositions compared to MATLAB implementations.\n\nCorrect Answer: A\n\nExplanation: The given documentation states that \"it remains unclear if the parameter defaults in SparTen are appropriate for general tensor data\" and it was \"unknown how sensitive algorithm convergence is to changes in the input parameter values.\" The analysis aimed to address these issues by testing SparTen on multiple datasets and architectures with different parameters."}, "50": {"documentation": {"title": "OCTAVA: an open-source toolbox for quantitative analysis of optical\n  coherence tomography angiography images", "source": "Gavrielle R. Untracht, Rolando Matos, Nikolaos Dikaios, Mariam Bapir,\n  Abdullah K. Durrani, Teemapron Butsabong, Paola Campagnolo, David D. Sampson,\n  Christian Heiss and Danuta M. Sampson", "docs_id": "2109.01835", "section": ["eess.IV", "cs.CV", "q-bio.TO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "OCTAVA: an open-source toolbox for quantitative analysis of optical\n  coherence tomography angiography images. Optical coherence tomography angiography (OCTA) performs non-invasive visualization and characterization of microvasculature in research and clinical applications mainly in ophthalmology and dermatology. A wide variety of instruments, imaging protocols, processing methods and metrics have been used to describe the microvasculature, such that comparing different study outcomes is currently not feasible. With the goal of contributing to standardization of OCTA data analysis, we report a user-friendly, open-source toolbox, OCTAVA (OCTA Vascular Analyzer), to automate the pre-processing, segmentation, and quantitative analysis of en face OCTA maximum intensity projection images in a standardized workflow. We present each analysis step, including optimization of filtering and choice of segmentation algorithm, and definition of metrics. We perform quantitative analysis of OCTA images from different commercial and non-commercial instruments and samples and show OCTAVA can accurately and reproducibly determine metrics for characterization of microvasculature. Wide adoption could enable studies and aggregation of data on a scale sufficient to develop reliable microvascular biomarkers for early detection, and to guide treatment, of microvascular disease."}, "answer": " Here is a 4-option multiple choice question on the OCTAVA toolbox for OCTA image analysis:\n\nQuestion: Which of the following statements is true about the OCTAVA toolbox for analyzing OCT angiography images?\n\nA) It is a closed-source commercial software developed by a medical device company.\n\nB) It enables standardized quantitative analysis of OCTA images from different instruments. \n\nC) It can segment and analyze OCT structural images but not OCTA images.\n\nD) It focuses on qualitative analysis and visualization but not quantification.\n\nCorrect Answer: B) It enables standardized quantitative analysis of OCTA images from different instruments.\n\nExplanation: According to the documentation, OCTAVA is an open-source toolbox that aims to standardize the quantitative analysis of OCTA images across different instruments and protocols. It automates pre-processing, segmentation and quantification of en face OCTA images in a standardized workflow. So option B summarizing this key functionality of the toolbox as enabling standardized quantitative analysis of OCTA images from different instruments is the correct answer."}, "51": {"documentation": {"title": "Complexity of Equilibrium in Diffusion Games on Social Networks", "source": "Seyed Rasoul Etesami, Tamer Basar", "docs_id": "1403.3881", "section": ["cs.GT", "cs.CC", "cs.DC", "cs.DM", "cs.MA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Complexity of Equilibrium in Diffusion Games on Social Networks. In this paper, we consider the competitive diffusion game, and study the existence of its pure-strategy Nash equilibrium when defined over general undirected networks. We first determine the set of pure-strategy Nash equilibria for two special but well-known classes of networks, namely the lattice and the hypercube. Characterizing the utility of the players in terms of graphical distances of their initial seed placements to other nodes in the network, we show that in general networks the decision process on the existence of pure-strategy Nash equilibrium is an NP-hard problem. Following this, we provide some necessary conditions for a given profile to be a Nash equilibrium. Furthermore, we study players' utilities in the competitive diffusion game over Erdos-Renyi random graphs and show that as the size of the network grows, the utilities of the players are highly concentrated around their expectation, and are bounded below by some threshold based on the parameters of the network. Finally, we obtain a lower bound for the maximum social welfare of the game with two players, and study sub-modularity of the players' utilities."}, "answer": " Here is a 4-option multiple choice question on the paper:\n\nQuestion: According to the paper, which of the following statements is true about the competitive diffusion game on general undirected networks?\n\nA) The decision process on the existence of pure-strategy Nash equilibrium is always polynomial time. \n\nB) The players' utilities are uniformly distributed over Erdos-Renyi random graphs.\n\nC) A lower bound was obtained for the maximum social welfare with any number of players.\n\nD) Determining if a pure-strategy Nash equilibrium exists is NP-hard.\n\nCorrect Answer: D) Determining if a pure-strategy Nash equilibrium exists is NP-hard.\n\nExplanation: The paper shows that determining if a pure-strategy Nash equilibrium exists in the competitive diffusion game on a general undirected network is NP-hard. The other statements are not supported by the results in the paper. Specifically, the decision process was shown to be NP-hard, not necessarily polynomial time; the player utilities were shown to be concentrated around their expectation, not uniformly distributed; the lower bound on social welfare was only for the 2 player case; and the NP-hardness result was for existence of equilibrium in general networks."}, "52": {"documentation": {"title": "Simulation Studies on Deep Reinforcement Learning for Building Control\n  with Human Interaction", "source": "Donghwan Lee, Niao He, Seungjae Lee, Panagiota Karava, Jianghai Hu", "docs_id": "2103.07919", "section": ["cs.AI", "cs.SY", "eess.SY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Simulation Studies on Deep Reinforcement Learning for Building Control\n  with Human Interaction. The building sector consumes the largest energy in the world, and there have been considerable research interests in energy consumption and comfort management of buildings. Inspired by recent advances in reinforcement learning (RL), this paper aims at assessing the potential of RL in building climate control problems with occupant interaction. We apply a recent RL approach, called DDPG (deep deterministic policy gradient), for the continuous building control tasks and assess its performance with simulation studies in terms of its ability to handle (a) the partial state observability due to sensor limitations; (b) complex stochastic system with high-dimensional state-spaces, which are jointly continuous and discrete; (c) uncertainties due to ambient weather conditions, occupant's behavior, and comfort feelings. Especially, the partial observability and uncertainty due to the occupant interaction significantly complicate the control problem. Through simulation studies, the policy learned by DDPG demonstrates reasonable performance and computational tractability."}, "answer": " Here is a multi-form question on the document:\n\nQuestion: Which of the following statements accurately summarizes a key finding from the simulation studies on using deep reinforcement learning for building climate control with human interaction?\n\nA) The policy learned by DDPG struggled to handle the high-dimensional state spaces and complex dynamics of the building climate control problem.\n\nB) DDPG was unable to learn an effective policy due to the partial observability caused by limited sensors in the building. \n\nC) The DDPG algorithm demonstrated reasonable performance in handling challenges like partial observability, high-dimensional spaces, and uncertainty.\n\nD) Occupant comfort preferences were simple to incorporate into the DDPG algorithm for building climate control.\n\nCorrect Answer: C\n\nExplanation: According to the document, the simulation studies showed that the policy learned by the DDPG algorithm exhibited reasonable performance on the building climate control problem despite challenges like partial observability from sensor limitations, high-dimensional state spaces, and uncertainties from factors like occupant behavior. So option C summarizes a key finding from the studies."}, "53": {"documentation": {"title": "Towards a more flexible Language of Thought: Bayesian grammar updates\n  after each concept exposure", "source": "Pablo Tano, Sergio Romano, Mariano Sigman, Alejo Salles and Santiago\n  Figueira", "docs_id": "1805.06924", "section": ["cs.AI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Towards a more flexible Language of Thought: Bayesian grammar updates\n  after each concept exposure. Recent approaches to human concept learning have successfully combined the power of symbolic, infinitely productive rule systems and statistical learning to explain our ability to learn new concepts from just a few examples. The aim of most of these studies is to reveal the underlying language structuring these representations and providing a general substrate for thought. However, describing a model of thought that is fixed once trained is against the extensive literature that shows how experience shapes concept learning. Here, we ask about the plasticity of these symbolic descriptive languages. We perform a concept learning experiment that demonstrates that humans can change very rapidly the repertoire of symbols they use to identify concepts, by compiling expressions which are frequently used into new symbols of the language. The pattern of concept learning times is accurately described by a Bayesian agent that rationally updates the probability of compiling a new expression according to how useful it has been to compress concepts so far. By portraying the Language of Thought as a flexible system of rules, we also highlight the difficulties to pin it down empirically."}, "answer": " Here is a 4-option multiple choice question on the document:\n\nQuestion: Which of the following statements accurately describes a key finding from the concept learning experiment discussed in the document?\n\nA) Humans can rapidly change the symbols they use to identify concepts by compiling frequently used expressions into new symbols. \n\nB) Humans identify new concepts using a fixed set of underlying symbolic rules that do not change with experience.\n\nC) Statistical learning alone, without any symbolic rule system, explains humans' ability to learn new concepts.\n\nD) The experiment shows that the Language of Thought is easy to characterize empirically.\n\nCorrect Answer: A\n\nExplanation: The document states that the concept learning experiment demonstrates that humans can quickly change the repertoire of symbols they use to identify concepts, by compiling commonly used expressions into new symbols. This matches option A. Option B contradicts the document's claim that describing thought as a fixed system goes against literature showing experience shapes learning. Option C conflicts with the document's combination of statistical and symbolic approaches. Option D is opposite to the document portraying the Language of Thought as a flexible, difficult to pin down system."}, "54": {"documentation": {"title": "An Algorithm for Road Coloring", "source": "A.N. Trahtman", "docs_id": "0801.2838", "section": ["cs.DM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "An Algorithm for Road Coloring. A coloring of edges of a finite directed graph turns the graph into finite-state automaton. The synchronizing word of a deterministic automaton is a word in the alphabet of colors (considered as letters) of its edges that maps the automaton to a single state. A coloring of edges of a directed graph of uniform outdegree (constant outdegree of any vertex) is synchronizing if the coloring turns the graph into a deterministic finite automaton possessing a synchronizing word. The road coloring problem is the problem of synchronizing coloring of a directed finite strongly connected graph of uniform outdegree if the greatest common divisor of the lengths of all its cycles is one. The problem posed in 1970 had evoked a noticeable interest among the specialists in the theory of graphs, automata, codes, symbolic dynamics as well as among the wide mathematical community. A polynomial time algorithm of $O(n^3)$ complexity in the most worst case and quadratic in majority of studied cases for the road coloring of the considered graph is presented below. The work is based on recent positive solution of the road coloring problem. The algorithm was implemented in the package TESTAS"}, "answer": " Here is a 4-option multiple choice question on the road coloring algorithm documentation:\n\nQuestion: Which of the following statements is true about the road coloring algorithm described in the documentation?\n\nA) It has quadratic time complexity in the worst case.\n\nB) It colors the edges of a graph to turn it into a finite automaton with a synchronizing word. \n\nC) It solves the road coloring problem in linear time.\n\nD) It requires the graph to have constant indegree at all vertices.\n\nCorrect Answer: B\n\nExplanation: According to the documentation, the road coloring algorithm colors the edges of a directed graph to turn it into a finite automaton that possesses a synchronizing word. It has a polynomial time complexity of O(n^3) in the worst case and quadratic in most cases studied. It does not require constant indegree or achieve linear time complexity."}, "55": {"documentation": {"title": "Using learning to control artificial avatars in human motor coordination\n  tasks", "source": "Maria Lombardi, Davide Liuzza, Mario di Bernardo", "docs_id": "1810.04191", "section": ["cs.SY", "q-bio.NC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Using learning to control artificial avatars in human motor coordination\n  tasks. Designing artificial cyber-agents able to interact with human safely, smartly and in a natural way is a current open problem in control. Solving such an issue will allow the design of cyber-agents capable of co-operatively interacting with people in order to fulfil common joint tasks in a multitude of different applications. This is particularly relevant in the context of healthcare applications. Indeed, the use has been proposed of artificial agents interacting and coordinating their movements with those of a patient suffering from social or motor disorders. Specifically, it has been shown that an artificial agent exhibiting certain kinematic properties could provide innovative and efficient rehabilitation strategies for these patients. Moreover, it has also been shown that the level of motor coordination is enhanced if these kinematic properties are similar to those of the individual it is interacting with. In this paper we discuss, first, a new method based on Markov Chains to confer \"human motor characteristics\" on a virtual agent, so as that it can coordinate its motion with that of a target individual while exhibiting specific kinematic properties. Then, we embed such synthetic model in a control architecture based on reinforcement learning to synthesize a cyber-agent able to mimic the behaviour of a specific human performing a joint motor task with one or more individuals."}, "answer": " Here is a 4-option multiple choice question on the document:\n\nQuestion: Which of the following statements is true about the proposed method to design an artificial agent that can coordinate motion with a human individual?\n\nA) It uses reinforcement learning to mimic general human motor coordination behavior without focusing on an individual.\n\nB) It relies on Markov chains to model the specific motor coordination patterns of a target individual. \n\nC) It aims to build agents that can interact with patients suffering from social disorders but not motor disorders.\n\nD) It focuses on designing agents that coordinate with multiple humans but not a specific individual.\n\nCorrect Answer: B\n\nExplanation: The document mentions using Markov chains to model the kinematic properties of a specific target individual so that the artificial agent can coordinate its motion with that individual in particular."}, "56": {"documentation": {"title": "Approximate Frank-Wolfe Algorithms over Graph-structured Support Sets", "source": "Baojian Zhou, Yifan Sun", "docs_id": "2107.00472", "section": ["math.OC", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Approximate Frank-Wolfe Algorithms over Graph-structured Support Sets. In this paper, we propose approximate Frank-Wolfe (FW) algorithms to solve convex optimization problems over graph-structured support sets where the \\textit{linear minimization oracle} (LMO) cannot be efficiently obtained in general. We first demonstrate that two popular approximation assumptions (\\textit{additive} and \\textit{multiplicative gap errors)}, are not valid for our problem, in that no cheap gap-approximate LMO oracle exists in general. Instead, a new \\textit{approximate dual maximization oracle} (DMO) is proposed, which approximates the inner product rather than the gap. When the objective is $L$-smooth, we prove that the standard FW method using a $\\delta$-approximate DMO converges as $\\mathcal{O}(L / \\delta t + (1-\\delta)(\\delta^{-1} + \\delta^{-2}))$ in general, and as $\\mathcal{O}(L/(\\delta^2(t+2)))$ over a $\\delta$-relaxation of the constraint set. Additionally, when the objective is $\\mu$-strongly convex and the solution is unique, a variant of FW converges to $\\mathcal{O}(L^2\\log(t)/(\\mu \\delta^6 t^2))$ with the same per-iteration complexity. Our empirical results suggest that even these improved bounds are pessimistic, with significant improvement in recovering real-world images with graph-structured sparsity."}, "answer": " Here is a multi-form question on the paper:\n\nQuestion: Which of the following statements is true about the linear minimization oracle (LMO) for the graph-structured support set optimization problem studied in the paper?\n\nA) The LMO can be efficiently obtained in general for this problem.\n\nB) The additive and multiplicative gap error assumptions are valid approximations for the LMO. \n\nC) The paper proposes using an approximate dual maximization oracle instead of approximating the LMO.\n\nD) The LMO provides an exact solution to the linear subproblem at each iteration.\n\nCorrect Answer: C\n\nExplanation: The paper states that the LMO cannot be efficiently obtained in general for this graph-structured optimization problem. It also mentions that the additive and multiplicative gap error assumptions used in other FW methods are not valid here. Instead, the paper proposes using an approximate dual maximization oracle which approximates the inner product rather than the gap."}, "57": {"documentation": {"title": "AdvFoolGen: Creating Persistent Troubles for Deep Classifiers", "source": "Yuzhen Ding, Nupur Thakur, Baoxin Li", "docs_id": "2007.10485", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "AdvFoolGen: Creating Persistent Troubles for Deep Classifiers. Researches have shown that deep neural networks are vulnerable to malicious attacks, where adversarial images are created to trick a network into misclassification even if the images may give rise to totally different labels by human eyes. To make deep networks more robust to such attacks, many defense mechanisms have been proposed in the literature, some of which are quite effective for guarding against typical attacks. In this paper, we present a new black-box attack termed AdvFoolGen, which can generate attacking images from the same feature space as that of the natural images, so as to keep baffling the network even though state-of-the-art defense mechanisms have been applied. We systematically evaluate our model by comparing with well-established attack algorithms. Through experiments, we demonstrate the effectiveness and robustness of our attack in the face of state-of-the-art defense techniques and unveil the potential reasons for its effectiveness through principled analysis. As such, AdvFoolGen contributes to understanding the vulnerability of deep networks from a new perspective and may, in turn, help in developing and evaluating new defense mechanisms."}, "answer": " Here is a 4-option multiple choice question with explanations for the given research summary:\n\nQuestion: Which of the following statements is true about the AdvFoolGen attack described in the research?\n\nA) It generates adversarial images that are visually very different from natural images.\n\nB) It is ineffective against state-of-the-art defense techniques used by deep neural networks. \n\nC) It creates adversarial images from the same feature space as natural images to evade defenses.\n\nD) It is a white-box attack that requires access to the model's parameters and architecture.\n\nCorrect Answer: C\n\nExplanations: \nA) Incorrect. The research states that AdvFoolGen generates attacking images from the same feature space as natural images.\n\nB) Incorrect. The research shows that AdvFoolGen is effective against state-of-the-art defense techniques. \n\nC) Correct. The research states that AdvFoolGen creates adversarial images from the same feature space as natural images to evade defenses.\n\nD) Incorrect. The research describes AdvFoolGen as a black-box attack that does not require internal model information."}, "58": {"documentation": {"title": "Lower bounds on the Probability of Error for Classical and\n  Classical-Quantum Channels", "source": "Marco Dalai", "docs_id": "1201.5411", "section": ["cs.IT", "math.IT", "quant-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Lower bounds on the Probability of Error for Classical and\n  Classical-Quantum Channels. In this paper, lower bounds on error probability in coding for discrete classical and classical-quantum channels are studied. The contribution of the paper goes in two main directions: i) extending classical bounds of Shannon, Gallager and Berlekamp to classical-quantum channels, and ii) proposing a new framework for lower bounding the probability of error of channels with a zero-error capacity in the low rate region. The relation between these two problems is revealed by showing that Lov\\'asz' bound on zero-error capacity emerges as a natural consequence of the sphere packing bound once we move to the more general context of classical-quantum channels. A variation of Lov\\'asz' bound is then derived to lower bound the probability of error in the low rate region by means of auxiliary channels. As a result of this study, connections between the Lov\\'asz theta function, the expurgated bound of Gallager, the cutoff rate of a classical channel and the sphere packing bound for classical-quantum channels are established."}, "answer": " Here is a 4-option multiple choice question on the paper:\n\nQuestion: Which of the following statements accurately summarizes a key contribution of the paper?\n\nA) The paper proposes a new framework for lower bounding the probability of error of channels with a zero-error capacity only in the high rate region.\n\nB) The paper reveals connections between the Lov\u00e1sz theta function, the expurgated bound of Gallager, and the sphere packing bound for classical-quantum channels. \n\nC) The paper shows that Lov\u00e1sz' bound on zero-error capacity emerges from the sphere packing bound when applied to classical channels, but not to classical-quantum channels.\n\nD) The paper derives a variation of the sphere packing bound to lower bound the probability of error in the low rate region by means of auxiliary quantum channels.\n\nCorrect Answer: B\n\nExplanation: The paper establishes connections between the Lov\u00e1sz theta function, the expurgated bound of Gallager, the cutoff rate of a classical channel and the sphere packing bound for classical-quantum channels. It does not focus specifically on only high or low rate regions."}, "59": {"documentation": {"title": "A Domain-Independent Algorithm for Plan Adaptation", "source": "S. Hanks, D. S. Weld", "docs_id": "cs/9501102", "section": ["cs.AI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Domain-Independent Algorithm for Plan Adaptation. The paradigms of transformational planning, case-based planning, and plan debugging all involve a process known as plan adaptation - modifying or repairing an old plan so it solves a new problem. In this paper we provide a domain-independent algorithm for plan adaptation, demonstrate that it is sound, complete, and systematic, and compare it to other adaptation algorithms in the literature. Our approach is based on a view of planning as searching a graph of partial plans. Generative planning starts at the graph's root and moves from node to node using plan-refinement operators. In planning by adaptation, a library plan - an arbitrary node in the plan graph - is the starting point for the search, and the plan-adaptation algorithm can apply both the same refinement operators available to a generative planner and can also retract constraints and steps from the plan. Our algorithm's completeness ensures that the adaptation algorithm will eventually search the entire graph and its systematicity ensures that it will do so without redundantly searching any parts of the graph."}, "answer": " Question: What property of the domain-independent plan adaptation algorithm presented in the paper ensures that it will search the entire plan graph without redundantly searching any parts?\n\nA) Soundness\nB) Completeness \nC) Systematicity\nD) Transformability\n\nCorrect Answer: C) Systematicity\n\nExplanations:\nThe paper states that the algorithm's completeness ensures it will search the entire graph, and its systematicity ensures it will do so without redundantly searching parts of the graph. Therefore, systematicity is the property that prevents redundant searching."}}