{"0": {"documentation": {"title": "Some variations on Lyndon words", "source": "Francesco Dolce, Antonio Restivo, Christophe Reutenauer", "docs_id": "1904.00954", "section": ["cs.DM", "math.CO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Some variations on Lyndon words. In this paper we compare two finite words $u$ and $v$ by the lexicographical order of the infinite words $u^\\omega$ and $v^\\omega$. Informally, we say that we compare $u$ and $v$ by the infinite order. We show several properties of Lyndon words expressed using this infinite order. The innovative aspect of this approach is that it allows to take into account also non trivial conditions on the prefixes of a word, instead that only on the suffixes. In particular, we derive a result of Ufnarovskij [V. Ufnarovskij, \"Combinatorial and asymptotic methods in algebra\", 1995] that characterizes a Lyndon word as a word which is greater, with respect to the infinite order, than all its prefixes. Motivated by this result, we introduce the prefix standard permutation of a Lyndon word and the corresponding (left) Cartesian tree. We prove that the left Cartesian tree is equal to the left Lyndon tree, defined by the left standard factorization of Viennot [G. Viennot, \"Alg\\`ebres de Lie libres et mono\\\"ides libres\", 1978]. This result is dual with respect to a theorem of Hohlweg and Reutenauer [C. Hohlweg and C. Reutenauer, \"Lyndon words, permutations and trees\", 2003]."}, "answer": " Here is a multi-form question based on the given text:\n\nQuestion: In the paper \"Some variations on Lyndon words\", how are two finite words $u$ and $v$ compared?\n\nA) By comparing the lengths of $u$ and $v$. \n\nB) By comparing $u$ and $v$ lexicographically.\n\nC) By comparing the infinite words $u^\\omega$ and $v^\\omega$ lexicographically. \n\nD) By comparing the number of prefixes of $u$ and $v$.\n\nCorrect Answer: C\n\nExplanation: According to the text, two finite words $u$ and $v$ are compared \"by the lexicographical order of the infinite words $u^\\omega$ and $v^\\omega$.\" This is known as comparing $u$ and $v$ by the infinite order."}, "1": {"documentation": {"title": "SAF- BAGE: Salient Approach for Facial Soft-Biometric Classification -\n  Age, Gender, and Facial Expression", "source": "Ayesha Gurnani, Kenil Shah, Vandit Gajjar, Viraj Mavani, Yash\n  Khandhediya", "docs_id": "1803.05719", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "SAF- BAGE: Salient Approach for Facial Soft-Biometric Classification -\n  Age, Gender, and Facial Expression. How can we improve the facial soft-biometric classification with help of the human visual system? This paper explores the use of saliency which is equivalent to the human visual system to classify Age, Gender and Facial Expression soft-biometric for facial images. Using the Deep Multi-level Network (ML-Net) [1] and off-the-shelf face detector [2], we propose our approach - SAF-BAGE, which first detects the face in the test image, increases the Bounding Box (B-Box) margin by 30%, finds the saliency map using ML-Net, with 30% reweighted ratio of saliency map, it multiplies with the input cropped face and extracts the Convolutional Neural Networks (CNN) predictions on the multiplied reweighted salient face. Our CNN uses the model AlexNet [3], which is pre-trained on ImageNet. The proposed approach surpasses the performance of other approaches, increasing the state-of-the-art by approximately 0.8% on the widely-used Adience [28] dataset for Age and Gender classification and by nearly 3% on the recent AffectNet [36] dataset for Facial Expression classification. We hope our simple, reproducible and effective approach will help ease future research in facial soft-biometric classification using saliency."}, "answer": " Here is a multi-form question on the paper:\n\nQuestion: Which of the following best describes how the proposed SAF-BAGE approach improves facial soft-biometric classification compared to previous methods?\n\nA) By using a larger bounding box to detect more contextual information around the face.\n\nB) By multiplying the input image with a saliency map to focus on visually salient facial regions. \n\nC) By fine-tuning the CNN model AlexNet with additional training data.\n\nD) By combining predictions from multiple pre-trained CNN models into an ensemble.\n\nCorrect Answer: B\n\nExplanation: The key contribution of SAF-BAGE is using a saliency map, inspired by human visual attention, to reweight the input image. This focuses the CNN on visually salient facial regions to improve soft-biometric classification. Options A, C, and D describe other techniques, but not the core idea proposed in this paper."}, "2": {"documentation": {"title": "Which Digraphs with Ring Structure are Essentially Cyclic?", "source": "Rafig Agaev and Pavel Chebotarev", "docs_id": "0910.3113", "section": ["math.CO", "cs.DM", "cs.MA", "math.SP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Which Digraphs with Ring Structure are Essentially Cyclic?. We say that a digraph is essentially cyclic if its Laplacian spectrum is not completely real. The essential cyclicity implies the presence of directed cycles, but not vice versa. The problem of characterizing essential cyclicity in terms of graph topology is difficult and yet unsolved. Its solution is important for some applications of graph theory, including that in decentralized control. In the present paper, this problem is solved with respect to the class of digraphs with ring structure, which models some typical communication networks. It is shown that the digraphs in this class are essentially cyclic, except for certain specified digraphs. The main technical tool we employ is the Chebyshev polynomials of the second kind. A by-product of this study is a theorem on the zeros of polynomials that differ by one from the products of Chebyshev polynomials of the second kind. We also consider the problem of essential cyclicity for weighted digraphs and enumerate the spanning trees in some digraphs with ring structure."}, "answer": " Here is a 4-option multiple choice question with explanations for the given abstract:\n\nQuestion: Which of the following statements about essential cyclicity of digraphs with ring structure is true according to the abstract?\n\nA) All digraphs with ring structure are essentially cyclic except for some specified ones. \n\nB) Digraphs with ring structure are essentially cyclic if and only if they contain directed cycles.\n\nC) The problem of characterizing essential cyclicity for general digraphs has been completely solved.\n\nD) Essential cyclicity can be determined solely from the Laplacian spectrum of a digraph.\n\nCorrect Answer: A\n\nExplanations: \nThe abstract states that digraphs with ring structure are essentially cyclic, except for certain specified digraphs. So option A is correct. \n\nOption B is incorrect because the abstract says essential cyclicity implies directed cycles but not vice versa. \n\nOption C is incorrect because the abstract says the problem of characterizing essential cyclicity for general digraphs is still unsolved. \n\nOption D is incorrect because the abstract says essential cyclicity implies certain properties of the Laplacian spectrum, but not that it can be completely determined from it."}, "3": {"documentation": {"title": "Are pre-trained text representations useful for multilingual and\n  multi-dimensional language proficiency modeling?", "source": "Taraka Rama and Sowmya Vajjala", "docs_id": "2102.12971", "section": ["cs.CL"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Are pre-trained text representations useful for multilingual and\n  multi-dimensional language proficiency modeling?. Development of language proficiency models for non-native learners has been an active area of interest in NLP research for the past few years. Although language proficiency is multidimensional in nature, existing research typically considers a single \"overall proficiency\" while building models. Further, existing approaches also considers only one language at a time. This paper describes our experiments and observations about the role of pre-trained and fine-tuned multilingual embeddings in performing multi-dimensional, multilingual language proficiency classification. We report experiments with three languages -- German, Italian, and Czech -- and model seven dimensions of proficiency ranging from vocabulary control to sociolinguistic appropriateness. Our results indicate that while fine-tuned embeddings are useful for multilingual proficiency modeling, none of the features achieve consistently best performance for all dimensions of language proficiency. All code, data and related supplementary material can be found at: https://github.com/nishkalavallabhi/MultidimCEFRScoring."}, "answer": " Question: Which of the following statements is true about the research described in the given text?\n\nA) The research focused on modeling overall language proficiency for non-native learners using pre-trained embeddings. \n\nB) The research modeled multiple dimensions of language proficiency for three languages using fine-tuned embeddings.\n\nC) The research found that fine-tuned embeddings consistently achieved the best performance across all dimensions of proficiency. \n\nD) The research only considered sociolinguistic appropriateness as a dimension of language proficiency.\n\nCorrect Answer: B\n\nExplanation: The text states that the research modeled multiple dimensions of language proficiency, including vocabulary control and sociolinguistic appropriateness, for three languages - German, Italian and Czech - using fine-tuned multilingual embeddings. It does not state that the research focused only on overall proficiency or sociolinguistic appropriateness, or that fine-tuned embeddings consistently performed best."}, "4": {"documentation": {"title": "A coarse-to-fine framework for unsupervised multi-contrast MR image\n  deformable registration with dual consistency constraint", "source": "Weijian Huang, Hao Yang, Xinfeng Liu, Cheng Li, Ian Zhang, Rongpin\n  Wang, Hairong Zheng, Shanshan Wang", "docs_id": "2008.01896", "section": ["eess.IV", "cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A coarse-to-fine framework for unsupervised multi-contrast MR image\n  deformable registration with dual consistency constraint. Multi-contrast magnetic resonance (MR) image registration is useful in the clinic to achieve fast and accurate imaging-based disease diagnosis and treatment planning. Nevertheless, the efficiency and performance of the existing registration algorithms can still be improved. In this paper, we propose a novel unsupervised learning-based framework to achieve accurate and efficient multi-contrast MR image registrations. Specifically, an end-to-end coarse-to-fine network architecture consisting of affine and deformable transformations is designed to improve the robustness and achieve end-to-end registration. Furthermore, a dual consistency constraint and a new prior knowledge-based loss function are developed to enhance the registration performances. The proposed method has been evaluated on a clinical dataset containing 555 cases, and encouraging performances have been achieved. Compared to the commonly utilized registration methods, including VoxelMorph, SyN, and LT-Net, the proposed method achieves better registration performance with a Dice score of 0.8397 in identifying stroke lesions. With regards to the registration speed, our method is about 10 times faster than the most competitive method of SyN (Affine) when testing on a CPU. Moreover, we prove that our method can still perform well on more challenging tasks with lacking scanning information data, showing high robustness for the clinical application."}, "answer": " Here is a 4-option multiple choice question on the document:\n\nQuestion: Which of the following statements is true about the proposed registration framework described in the document?\n\nA) It uses a coarse-to-fine network with affine and deformable transformations to improve robustness. \n\nB) It employs a dual consistency constraint and a new loss function to enhance registration performance.\n\nC) Compared to methods like VoxelMorph and SyN, it achieves worse registration performance in terms of Dice score.\n\nD) It is slower than SyN (Affine) when tested on a CPU.\n\nCorrect Answer: B\n\nExplanation: \nOption A describes the network architecture but not the enhancements to the framework. Option C is incorrect as the document states the proposed method achieves better Dice score than other methods. Option D is incorrect as the document mentions the proposed method is 10 times faster than SyN (Affine) on CPU. Option B correctly states that the dual consistency constraint and new loss function help enhance registration performance."}, "5": {"documentation": {"title": "Leader-Contention-Based User Matching for 802.11 Multiuser MIMO Networks", "source": "Tung-Wei Kuo, Kuang-Che Lee, Kate Ching-Ju Lin and Ming-Jer Tsai", "docs_id": "1404.6041", "section": ["cs.NI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Leader-Contention-Based User Matching for 802.11 Multiuser MIMO Networks. In multiuser MIMO (MU-MIMO) LANs, the achievable throughput of a client depends on who are transmitting concurrently with it. Existing MU-MIMO MAC protocols however enable clients to use the traditional 802.11 contention to contend for concurrent transmission opportunities on the uplink. Such a contention-based protocol not only wastes lots of channel time on multiple rounds of contention, but also fails to maximally deliver the gain of MU-MIMO because users randomly join concurrent transmissions without considering their channel characteristics. To address such inefficiency, this paper introduces MIMOMate, a leader-contention-based MU-MIMO MAC protocol that matches clients as concurrent transmitters according to their channel characteristics to maximally deliver the MU-MIMO gain, while ensuring all users to fairly share concurrent transmission opportunities. Furthermore, MIMOMate elects the leader of the matched users to contend for transmission opportunities using traditional 802.11 CSMA/CA. It hence requires only a single contention overhead for concurrent streams, and can be compatible with legacy 802.11 devices. A prototype implementation in USRP-N200 shows that MIMOMate achieves an average throughput gain of 1.42x and 1.52x over the traditional contention-based protocol for 2-antenna and 3-antenna AP scenarios, respectively, and also provides fairness for clients."}, "answer": " Here is a multi-form question based on the given text:\n\nQuestion: Which of the following statements accurately describes a key limitation of existing MU-MIMO MAC protocols that MIMOMate aims to address?\n\nA) Existing protocols do not allow the AP to optimize client grouping to maximize MU-MIMO gain. \n\nB) Existing protocols have clients contend simultaneously, wasting channel time.\n\nC) Existing protocols do not ensure fair sharing of transmission opportunities.\n\nD) Existing protocols are not compatible with legacy 802.11 devices.\n\nCorrect Answer: B\n\nExplanation: The text states that existing MU-MIMO MAC protocols \"enable clients to use the traditional 802.11 contention to contend for concurrent transmission opportunities on the uplink. Such a contention-based protocol not only wastes lots of channel time on multiple rounds of contention, but also fails to maximally deliver the gain of MU-MIMO because users randomly join concurrent transmissions without considering their channel characteristics.\" This matches option B."}, "6": {"documentation": {"title": "Reliable Over-the-Air Computation by Amplify-and-Forward Based Relay", "source": "Suhua Tang, Huarui Yin, Chao Zhang, Sadao Obana", "docs_id": "2010.12146", "section": ["cs.NI", "eess.SP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Reliable Over-the-Air Computation by Amplify-and-Forward Based Relay. In typical sensor networks, data collection and processing are separated. A sink collects data from all nodes sequentially, which is very time consuming. Over-the-air computation, as a new diagram of sensor networks, integrates data collection and processing in one slot: all nodes transmit their signals simultaneously in the analog wave and the processing is done in the air. This method, although efficient, requires that signals from all nodes arrive at the sink, aligned in signal magnitude so as to enable an unbiased estimation. For nodes far away from the sink with a low channel gain, misalignment in signal magnitude is unavoidable. To solve this problem, in this paper, we investigate the amplify-and-forward based relay, in which a relay node amplifies signals from many nodes at the same time. We first discuss the general relay model and a simple relay policy. Then, a coherent relay policy is proposed to reduce relay transmission power. Directly minimizing the computation error tends to over-increase node transmission power. Therefore, the two relay policies are further refined with a new metric, and the transmission power is reduced while the computation error is kept low. In addition, the coherent relay policy helps to reduce the relay transmission power by half, to below the limit, which makes it one step ahead towards practical applications."}, "answer": " Here is a 4-option multiple choice question on the document:\n\nQuestion: Which of the following statements is true about the amplify-and-forward based relay approach discussed in the document?\n\nA) It reduces the computation error by minimizing the transmission power of the nodes.\n\nB) It helps align the signal magnitudes from different nodes by amplifying the signals at the relay. \n\nC) It reduces the relay transmission power below the limit by using coherent relay policy.\n\nD) It collects and processes data from nodes in multiple slots instead of one slot.\n\nCorrect Answer: C\n\nExplanation: \nOption A is incorrect as the document states that directly minimizing computation error tends to over-increase node transmission power. \n\nOption B is incorrect as the document does not state that the relay helps align signal magnitudes. It only amplifies signals.\n\nOption D is incorrect as the document states that over-the-air computation integrates data collection and processing in one slot, while typical networks do it in multiple slots.\n\nOption C is the correct answer. The document states that the coherent relay policy helps reduce the relay transmission power by half, below the limit."}, "7": {"documentation": {"title": "Adaptive Energy-aware Scheduling of Dynamic Event Analytics across Edge\n  and Cloud Resources", "source": "Rajrup Ghosh, Siva Prakash Reddy Komma and Yogesh Simmhan", "docs_id": "1801.01087", "section": ["cs.DC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Adaptive Energy-aware Scheduling of Dynamic Event Analytics across Edge\n  and Cloud Resources. The growing deployment of sensors as part of Internet of Things (IoT) is generating thousands of event streams. Complex Event Processing (CEP) queries offer a useful paradigm for rapid decision-making over such data sources. While often centralized in the Cloud, the deployment of capable edge devices on the field motivates the need for cooperative event analytics that span Edge and Cloud computing. Here, we identify a novel problem of query placement on edge and Cloud resources for dynamically arriving and departing analytic dataflows. We define this as an optimization problem to minimize the total makespan for all event analytics, while meeting energy and compute constraints of the resources. We propose 4 adaptive heuristics and 3 rebalancing strategies for such dynamic dataflows, and validate them using detailed simulations for 100 - 1000 edge devices and VMs. The results show that our heuristics offer O(seconds) planning time, give a valid and high quality solution in all cases, and reduce the number of query migrations. Furthermore, rebalance strategies when applied in these heuristics have significantly reduced the makespan by around 20 - 25%."}, "answer": " Here is a multi-form question on the given text:\n\nQuestion: Which of the following is NOT mentioned in the text as a motivation for the work on adaptive energy-aware scheduling of dynamic event analytics?\n\nA) The growing deployment of sensors as part of the Internet of Things is generating large volumes of event data that needs to be analyzed. \n\nB) Complex event processing queries provide a useful paradigm for rapid decision making over event data sources.\n\nC) Event analytics tasks are often centralized in the cloud, but capable edge devices allow distributed processing.\n\nD) Existing systems provide optimal scheduling of dynamic event analytics across edge and cloud resources.\n\nCorrect Answer: D\n\nExplanation: The text discusses the need for adaptive scheduling of event analytics across edge and cloud resources, implying that existing systems do not provide optimal scheduling. Options A, B, and C are all mentioned as motivations in the text."}, "8": {"documentation": {"title": "Improving Scalability of Contrast Pattern Mining for Network Traffic\n  Using Closed Patterns", "source": "Elaheh AlipourChavary, Sarah M. Erfani, Christopher Leckie", "docs_id": "2011.14830", "section": ["cs.NI", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Improving Scalability of Contrast Pattern Mining for Network Traffic\n  Using Closed Patterns. Contrast pattern mining (CPM) aims to discover patterns whose support increases significantly from a background dataset compared to a target dataset. CPM is particularly useful for characterising changes in evolving systems, e.g., in network traffic analysis to detect unusual activity. While most existing techniques focus on extracting either the whole set of contrast patterns (CPs) or minimal sets, the problem of efficiently finding a relevant subset of CPs, especially in high dimensional datasets, is an open challenge. In this paper, we focus on extracting the most specific set of CPs to discover significant changes between two datasets. Our approach to this problem uses closed patterns to substantially reduce redundant patterns. Our experimental results on several real and emulated network traffic datasets demonstrate that our proposed unsupervised algorithm is up to 100 times faster than an existing approach for CPM on network traffic data [2]. In addition, as an application of CPs, we demonstrate that CPM is a highly effective method for detection of meaningful changes in network traffic."}, "answer": " Question: Which of the following statements is true about the contrast pattern mining approach presented in the paper?\n\nA) It uses closed patterns to reduce redundant patterns, making it up to 100 times faster than existing methods on network traffic data.\n\nB) It focuses on extracting the complete set of contrast patterns rather than a relevant subset, which improves scalability. \n\nC) It is an unsupervised algorithm that is less effective for detecting changes in network traffic compared to existing methods.\n\nD) It aims to find patterns whose support decreases significantly from a background dataset compared to a target dataset.\n\nCorrect Answer: A\n\nExplanation: The paper presents an approach to contrast pattern mining that uses closed patterns to substantially reduce redundant patterns. Experiments show it is up to 100 times faster than existing methods on network traffic data while still being highly effective for detecting changes."}, "9": {"documentation": {"title": "A Crossing Lemma for Families of Jordan Curves with a Bounded\n  Intersection Number", "source": "Maya Bechler-Speicher", "docs_id": "1911.07287", "section": ["cs.CG", "cs.DM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Crossing Lemma for Families of Jordan Curves with a Bounded\n  Intersection Number. A family of closed simple (i.e., Jordan) curves is $m$-intersecting if any pair of its curves have at most $m$ points of common intersection. We say that a pair of such curves touch if they intersect at a single point of common tangency. In this work we show that any $m$-intersecting family of $n$ Jordan curves in general position in the plane contains $O\\left(n^{2-\\frac{1}{3m+15}}\\right)$ touching pairs Furthermore, we use the string separator theorem of Fox and Pach in order to establish the following Crossing Lemma for contact graphs of Jordan curves: Let $\\Gamma$ be an $m$-intersecting family of closed Jordan curves in general position in the plane with exactly $T=\\Omega(n)$ touching pairs of curves, then the curves of $\\Gamma$ determine $\\Omega\\left(T\\cdot\\left(\\frac{T}{n}\\right)^{\\frac{1}{9m+45}}\\right)$ intersection points. This extends the similar bounds that were previously established by Salazar for the special case of pairwise intersecting (and $m$-intersecting) curves. Specializing to the case at hand, this substantially improves the bounds that were recently derived by Pach, Rubin and Tardos for arbitrary families of Jordan curves."}, "answer": " Here is a multi-form question based on the given text:\n\nQuestion: According to the text, what is the upper bound established for the number of touching pairs in an m-intersecting family of n Jordan curves in general position in the plane?\n\nA) O(n^(2-1/(3m+15))) \n\nB) O(T(T/n)^(1/(9m+45)))\n\nC) O(n^2)\n\nD) O(nm)\n\nCorrect Answer: A\n\nExplanation: The text states that \"any m-intersecting family of n Jordan curves in general position in the plane contains O(n^(2-1/(3m+15))) touching pairs.\" So A is the correct upper bound given in the text.\n\nB refers to the bound given later for intersection points based on the number of touching pairs T. \n\nC and D do not match the bound given in the text."}, "10": {"documentation": {"title": "The Internet AS-Level Topology: Three Data Sources and One Definitive\n  Metric", "source": "Priya Mahadevan, Dmitri Krioukov, Marina Fomenkov, Bradley Huffaker,\n  Xenofontas Dimitropoulos, kc claffy, Amin Vahdat", "docs_id": "cs/0512095", "section": ["cs.NI", "physics.soc-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Internet AS-Level Topology: Three Data Sources and One Definitive\n  Metric. We calculate an extensive set of characteristics for Internet AS topologies extracted from the three data sources most frequently used by the research community: traceroutes, BGP, and WHOIS. We discover that traceroute and BGP topologies are similar to one another but differ substantially from the WHOIS topology. Among the widely considered metrics, we find that the joint degree distribution appears to fundamentally characterize Internet AS topologies as well as narrowly define values for other important metrics. We discuss the interplay between the specifics of the three data collection mechanisms and the resulting topology views. In particular, we show how the data collection peculiarities explain differences in the resulting joint degree distributions of the respective topologies. Finally, we release to the community the input topology datasets, along with the scripts and output of our calculations. This supplement should enable researchers to validate their models against real data and to make more informed selection of topology data sources for their specific needs."}, "answer": " Here is a 4-option multiple choice question on the paper:\n\nQuestion: Which of the following statements is true about the findings in the paper regarding Internet AS topologies extracted from different data sources?\n\nA) The WHOIS topology was found to be very similar to the traceroute and BGP topologies.\n\nB) The traceroute and BGP topologies were found to be similar to each other but differed substantially from the WHOIS topology. \n\nC) The joint degree distribution was found not to characterize the different AS topologies well.\n\nD) The specifics of data collection did not explain the differences in joint degree distributions.\n\nCorrect Answer: B\n\nExplanation: The paper found that the traceroute and BGP topologies were similar to each other but differed substantially from the WHOIS topology. The joint degree distribution was found to characterize the AS topologies well. The data collection specifics explained the differences in joint degree distributions between topologies."}, "11": {"documentation": {"title": "Syntactically Look-Ahead Attention Network for Sentence Compression", "source": "Hidetaka Kamigaito, Manabu Okumura", "docs_id": "2002.01145", "section": ["cs.CL"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Syntactically Look-Ahead Attention Network for Sentence Compression. Sentence compression is the task of compressing a long sentence into a short one by deleting redundant words. In sequence-to-sequence (Seq2Seq) based models, the decoder unidirectionally decides to retain or delete words. Thus, it cannot usually explicitly capture the relationships between decoded words and unseen words that will be decoded in the future time steps. Therefore, to avoid generating ungrammatical sentences, the decoder sometimes drops important words in compressing sentences. To solve this problem, we propose a novel Seq2Seq model, syntactically look-ahead attention network (SLAHAN), that can generate informative summaries by explicitly tracking both dependency parent and child words during decoding and capturing important words that will be decoded in the future. The results of the automatic evaluation on the Google sentence compression dataset showed that SLAHAN achieved the best kept-token-based-F1, ROUGE-1, ROUGE-2 and ROUGE-L scores of 85.5, 79.3, 71.3 and 79.1, respectively. SLAHAN also improved the summarization performance on longer sentences. Furthermore, in the human evaluation, SLAHAN improved informativeness without losing readability."}, "answer": " Here is a 4-option multiple choice question on the given text:\n\nQuestion: What is the main contribution of the syntactically look-ahead attention network (SLAHAN) model for sentence compression?\n\nA) It uses a sequence-to-sequence model with an attention mechanism.\n\nB) It explicitly tracks dependency relationships between words during decoding. \n\nC) It achieves state-of-the-art results on the Google sentence compression dataset.\n\nD) It can generate informative summaries while maintaining readability.\n\nCorrect Answer: D\n\nExplanation: The main contribution of SLAHAN highlighted in the given text is that it can generate informative summaries by capturing important words to be decoded in the future, without losing readability. While the other options describe aspects of SLAHAN, the key advantage emphasized is improving informativeness without sacrificing readability."}, "12": {"documentation": {"title": "Adversarial Imitation via Variational Inverse Reinforcement Learning", "source": "Ahmed H. Qureshi, Byron Boots and Michael C. Yip", "docs_id": "1809.06404", "section": ["cs.LG", "cs.AI", "cs.RO", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Adversarial Imitation via Variational Inverse Reinforcement Learning. We consider a problem of learning the reward and policy from expert examples under unknown dynamics. Our proposed method builds on the framework of generative adversarial networks and introduces the empowerment-regularized maximum-entropy inverse reinforcement learning to learn near-optimal rewards and policies. Empowerment-based regularization prevents the policy from overfitting to expert demonstrations, which advantageously leads to more generalized behaviors that result in learning near-optimal rewards. Our method simultaneously learns empowerment through variational information maximization along with the reward and policy under the adversarial learning formulation. We evaluate our approach on various high-dimensional complex control tasks. We also test our learned rewards in challenging transfer learning problems where training and testing environments are made to be different from each other in terms of dynamics or structure. The results show that our proposed method not only learns near-optimal rewards and policies that are matching expert behavior but also performs significantly better than state-of-the-art inverse reinforcement learning algorithms."}, "answer": " Question: What method does the paper propose for learning rewards and policies from expert examples under unknown dynamics?\n\nA) A method that uses generative adversarial networks and introduces empowerment-regularized maximum-entropy inverse reinforcement learning to learn near-optimal rewards and policies.\n\nB) A model-based reinforcement learning method that learns a dynamics model first and then uses it to learn optimal policies. \n\nC) A behavioral cloning approach that simply mimics expert demonstrations without learning any rewards.\n\nD) A multi-task learning approach that simultaneously learns dynamics, rewards and policies.\n\nCorrect Answer: A\n\nExplanation: The paper proposes a method that builds on generative adversarial networks and uses empowerment-regularized maximum-entropy inverse reinforcement learning to learn near-optimal rewards and policies from expert demonstrations under unknown dynamics."}, "13": {"documentation": {"title": "Sinkhorn Natural Gradient for Generative Models", "source": "Zebang Shen and Zhenfu Wang and Alejandro Ribeiro and Hamed Hassani", "docs_id": "2011.04162", "section": ["stat.ML", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Sinkhorn Natural Gradient for Generative Models. We consider the problem of minimizing a functional over a parametric family of probability measures, where the parameterization is characterized via a push-forward structure. An important application of this problem is in training generative adversarial networks. In this regard, we propose a novel Sinkhorn Natural Gradient (SiNG) algorithm which acts as a steepest descent method on the probability space endowed with the Sinkhorn divergence. We show that the Sinkhorn information matrix (SIM), a key component of SiNG, has an explicit expression and can be evaluated accurately in complexity that scales logarithmically with respect to the desired accuracy. This is in sharp contrast to existing natural gradient methods that can only be carried out approximately. Moreover, in practical applications when only Monte-Carlo type integration is available, we design an empirical estimator for SIM and provide the stability analysis. In our experiments, we quantitatively compare SiNG with state-of-the-art SGD-type solvers on generative tasks to demonstrate its efficiency and efficacy of our method."}, "answer": " Question: Which of the following statements accurately summarizes a key contribution of the Sinkhorn Natural Gradient (SiNG) algorithm proposed in the paper?\n\nA) SiNG provides an efficient way to evaluate the natural gradient exactly, in contrast to previous approximate natural gradient methods. \n\nB) SiNG results in faster convergence compared to SGD-based methods when training generative models.\n\nC) SiNG enables training generative adversarial networks with better sample quality than previous methods.\n\nD) SiNG gives a novel estimator for the Sinkhorn information matrix that has better stability than previous estimators.\n\nCorrect Answer: A\n\nExplanation: The key contribution highlighted in the summary is that SiNG can calculate the natural gradient exactly in a complexity that scales logarithmically, unlike previous natural gradient methods that were approximate. While SiNG is applied to training generative models, the core novelty is in the efficient exact computation of the natural gradient."}, "14": {"documentation": {"title": "Transcoded Video Restoration by Temporal Spatial Auxiliary Network", "source": "Li Xu, Gang He, Jinjia Zhou, Jie Lei, Weiying Xie, Yunsong Li, Yu-Wing\n  Tai", "docs_id": "2112.07948", "section": ["cs.CV", "eess.IV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Transcoded Video Restoration by Temporal Spatial Auxiliary Network. In most video platforms, such as Youtube, and TikTok, the played videos usually have undergone multiple video encodings such as hardware encoding by recording devices, software encoding by video editing apps, and single/multiple video transcoding by video application servers. Previous works in compressed video restoration typically assume the compression artifacts are caused by one-time encoding. Thus, the derived solution usually does not work very well in practice. In this paper, we propose a new method, temporal spatial auxiliary network (TSAN), for transcoded video restoration. Our method considers the unique traits between video encoding and transcoding, and we consider the initial shallow encoded videos as the intermediate labels to assist the network to conduct self-supervised attention training. In addition, we employ adjacent multi-frame information and propose the temporal deformable alignment and pyramidal spatial fusion for transcoded video restoration. The experimental results demonstrate that the performance of the proposed method is superior to that of the previous techniques. The code is available at https://github.com/icecherylXuli/TSAN."}, "answer": " Question: Which of the following statements is true about the proposed Temporal Spatial Auxiliary Network (TSAN) for transcoded video restoration?\n\nA) It assumes compression artifacts are caused by multiple video encodings like hardware encoding, software encoding, and video transcoding.\n\nB) It uses single-frame information only and does not utilize adjacent multi-frame information. \n\nC) It employs temporal deformable alignment and pyramidal spatial fusion for transcoded video restoration.\n\nD) It is inferior in performance compared to previous techniques for compressed video restoration.\n\nCorrect Answer: C\n\nExplanation: The passage states that TSAN considers unique traits between video encoding and transcoding, uses adjacent multi-frame information, and employs temporal deformable alignment and pyramidal spatial fusion for transcoded video restoration. It also states that TSAN demonstrates superior performance to previous techniques. Therefore, option C is correct."}, "15": {"documentation": {"title": "A Survey on Contrastive Self-supervised Learning", "source": "Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya\n  Banerjee, Fillia Makedon", "docs_id": "2011.00362", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Survey on Contrastive Self-supervised Learning. Self-supervised learning has gained popularity because of its ability to avoid the cost of annotating large-scale datasets. It is capable of adopting self-defined pseudo labels as supervision and use the learned representations for several downstream tasks. Specifically, contrastive learning has recently become a dominant component in self-supervised learning methods for computer vision, natural language processing (NLP), and other domains. It aims at embedding augmented versions of the same sample close to each other while trying to push away embeddings from different samples. This paper provides an extensive review of self-supervised methods that follow the contrastive approach. The work explains commonly used pretext tasks in a contrastive learning setup, followed by different architectures that have been proposed so far. Next, we have a performance comparison of different methods for multiple downstream tasks such as image classification, object detection, and action recognition. Finally, we conclude with the limitations of the current methods and the need for further techniques and future directions to make substantial progress."}, "answer": " Question: What is the main goal of contrastive self-supervised learning methods according to the survey?\n\nA) To avoid the cost of annotating large-scale datasets by using self-defined pseudo labels as supervision {correct answer}\n\nB) To embed augmented versions of different samples close to each other\n\nC) To push away embeddings from the same sample \n\nD) To use the learned representations only for image classification\n\nCorrect Answer: A\n\nExplanation: The survey states that self-supervised learning, specifically contrastive learning, has gained popularity because of its ability to avoid the cost of annotating large-scale datasets by adopting self-defined pseudo labels as supervision. Contrastive learning aims to embed augmented versions of the same sample close to each other while pushing away embeddings from different samples."}, "16": {"documentation": {"title": "Consensus Based Sampling", "source": "J. A. Carrillo and F. Hoffmann and A. M. Stuart and U. Vaes", "docs_id": "2106.02519", "section": ["math.DS", "cs.NA", "math.NA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Consensus Based Sampling. We propose a novel method for sampling and optimization tasks based on a stochastic interacting particle system. We explain how this method can be used for the following two goals: (i) generating approximate samples from a given target distribution; (ii) optimizing a given objective function. The approach is derivative-free and affine invariant, and is therefore well-suited for solving inverse problems defined by complex forward models: (i) allows generation of samples from the Bayesian posterior and (ii) allows determination of the maximum a posteriori estimator. We investigate the properties of the proposed family of methods in terms of various parameter choices, both analytically and by means of numerical simulations. The analysis and numerical simulation establish that the method has potential for general purpose optimization tasks over Euclidean space; contraction properties of the algorithm are established under suitable conditions, and computational experiments demonstrate wide basins of attraction for various specific problems. The analysis and experiments also demonstrate the potential for the sampling methodology in regimes in which the target distribution is unimodal and close to Gaussian; indeed we prove that the method recovers a Laplace approximation to the measure in certain parametric regimes and provide numerical evidence that this Laplace approximation attracts a large set of initial conditions in a number of examples."}, "answer": " Here is a 4-option multiple choice question on the paper:\n\nQuestion: Which of the following statements is true about the consensus based sampling method proposed in the paper?\n\nA) It can only be used for optimization tasks and not for sampling tasks.\n\nB) It requires calculation of derivatives of the objective function. \n\nC) It is well-suited for solving inverse problems defined by complex forward models as it allows Bayesian posterior sampling and maximum a posteriori estimation.\n\nD) It cannot be used for optimization over Euclidean spaces.\n\nCorrect Answer: C\n\nExplanation: According to the paper, the consensus based sampling method is a derivative-free and affine invariant method that can be used for both sampling from a target distribution as well as optimizing an objective function. Specifically, it allows generation of samples from the Bayesian posterior and determination of the maximum a posteriori estimator, making it well-suited for solving inverse problems defined by complex forward models. The method does not require calculation of derivatives. The analysis and experiments in the paper also demonstrate the potential of the method for optimization tasks over Euclidean spaces."}, "17": {"documentation": {"title": "Need-based Communication for Smart Grid: When to Inquire Power Price?", "source": "Husheng Li and Robert C. Qiu", "docs_id": "1003.2138", "section": ["cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Need-based Communication for Smart Grid: When to Inquire Power Price?. In smart grid, a home appliance can adjust its power consumption level according to the realtime power price obtained from communication channels. Most studies on smart grid do not consider the cost of communications which cannot be ignored in many situations. Therefore, the total cost in smart grid should be jointly optimized with the communication cost. In this paper, a probabilistic mechanism of locational margin price (LMP) is applied and a model for the stochastic evolution of the underlying load which determines the power price is proposed. Based on this framework of power price, the problem of determining when to inquire the power price is formulated as a Markov decision process and the corresponding elements, namely the action space, system state and reward function, are defined. Dynamic programming is then applied to obtain the optimal strategy. A simpler myopic approach is proposed by comparing the cost of communications and the penalty incurred by using the old value of power price. Numerical results show the significant performance gain of the optimal strategy of price inquiry, as well as the near-optimality of the myopic approach."}, "answer": " Question: In the paper \"Need-based Communication for Smart Grid: When to Inquire Power Price?\", what is the framework used to formulate the problem of determining when to inquire the power price?\n\nA) A probabilistic mechanism of locational marginal cost (LMC) and a model for the evolution of underlying load.\n\nB) A deterministic mechanism of locational marginal price (LMP) and a model for the stochastic evolution of underlying load. \n\nC) A probabilistic mechanism of locational marginal price (LMP) and a model for the stochastic evolution of underlying load.\n\nD) A deterministic mechanism of locational marginal cost (LMC) and a model for the evolution of underlying load.\n\nCorrect Answer: C\n\nExplanation: The paper uses a probabilistic mechanism of locational marginal price (LMP) and proposes a model for the stochastic evolution of the underlying load which determines the power price. This framework is then used to formulate the problem of determining when to inquire the power price as a Markov decision process."}, "18": {"documentation": {"title": "View-Invariant Probabilistic Embedding for Human Pose", "source": "Jennifer J. Sun, Jiaping Zhao, Liang-Chieh Chen, Florian Schroff,\n  Hartwig Adam, Ting Liu", "docs_id": "1912.01001", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "View-Invariant Probabilistic Embedding for Human Pose. Depictions of similar human body configurations can vary with changing viewpoints. Using only 2D information, we would like to enable vision algorithms to recognize similarity in human body poses across multiple views. This ability is useful for analyzing body movements and human behaviors in images and videos. In this paper, we propose an approach for learning a compact view-invariant embedding space from 2D joint keypoints alone, without explicitly predicting 3D poses. Since 2D poses are projected from 3D space, they have an inherent ambiguity, which is difficult to represent through a deterministic mapping. Hence, we use probabilistic embeddings to model this input uncertainty. Experimental results show that our embedding model achieves higher accuracy when retrieving similar poses across different camera views, in comparison with 2D-to-3D pose lifting models. We also demonstrate the effectiveness of applying our embeddings to view-invariant action recognition and video alignment. Our code is available at https://github.com/google-research/google-research/tree/master/poem."}, "answer": " Question: Which of the following statements is true about the proposed approach in the paper?\n\nA) It uses 3D pose information to learn the embedding space.\n\nB) It aims to explicitly predict 3D poses from 2D keypoints. \n\nC) It uses probabilistic embeddings to model the ambiguity in projecting 3D poses to 2D.\n\nD) It achieves lower accuracy in retrieving similar poses across views compared to lifting models.\n\nCorrect Answer: C) It uses probabilistic embeddings to model the ambiguity in projecting 3D poses to 2D.\n\nExplanation: The paper proposes learning a view-invariant embedding space from 2D keypoints alone, without predicting 3D poses. It uses probabilistic embeddings to handle the inherent ambiguity in projecting 3D poses to 2D keypoints. Experimental results show it achieves higher accuracy in retrieving similar poses across views compared to lifting models."}, "19": {"documentation": {"title": "Estimating the confidence of speech spoofing countermeasure", "source": "Xin Wang, Junichi Yamagishi", "docs_id": "2110.04775", "section": ["eess.AS", "cs.CR", "cs.SD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Estimating the confidence of speech spoofing countermeasure. Conventional speech spoofing countermeasures (CMs) are designed to make a binary decision on an input trial. However, a CM trained on a closed-set database is theoretically not guaranteed to perform well on unknown spoofing attacks. In some scenarios, an alternative strategy is to let the CM defer a decision when it is not confident. The question is then how to estimate a CM's confidence regarding an input trial. We investigated a few confidence estimators that can be easily plugged into a CM. On the ASVspoof2019 logical access database, the results demonstrate that an energy-based estimator and a neural-network-based one achieved acceptable performance in identifying unknown attacks in the test set. On a test set with additional unknown attacks and bona fide trials from other databases, the confidence estimators performed moderately well, and the CMs better discriminated bona fide and spoofed trials that had a high confidence score. Additional results also revealed the difficulty in enhancing a confidence estimator by adding unknown attacks to the training set."}, "answer": " Here is a 4-option multiple choice question with explanations for the given text:\n\nQuestion: Which of the following statements is true about the confidence estimators investigated in the study?\n\nA) The energy-based estimator performed worse than the neural network-based one in identifying unknown attacks.\n\nB) The confidence estimators had poor performance in discriminating bona fide and spoofed trials with high confidence scores. \n\nC) On the test set with additional unknown attacks, the confidence estimators showed moderate performance.\n\nD) Adding unknown attacks to the training set did not help in enhancing the confidence estimators.\n\nCorrect Answer: C\n\nExplanations:\n\nA) Incorrect. The text states that the energy-based estimator and the neural network-based estimator both achieved acceptable performance in identifying unknown attacks. \n\nB) Incorrect. The text states that the confidence estimators better discriminated between bona fide and spoofed trials that had a high confidence score.\n\nC) Correct. The text mentions that on the test set with additional unknown attacks, the confidence estimators performed moderately well. \n\nD) Incorrect. The text states that additional results revealed the difficulty, not impossibility, of enhancing the confidence estimator by adding unknown attacks to the training set."}, "20": {"documentation": {"title": "Entity-Switched Datasets: An Approach to Auditing the In-Domain\n  Robustness of Named Entity Recognition Models", "source": "Oshin Agarwal, Yinfei Yang, Byron C. Wallace, Ani Nenkova", "docs_id": "2004.04123", "section": ["cs.CL"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Entity-Switched Datasets: An Approach to Auditing the In-Domain\n  Robustness of Named Entity Recognition Models. Named entity recognition systems perform well on standard datasets comprising English news. But given the paucity of data, it is difficult to draw conclusions about the robustness of systems with respect to recognizing a diverse set of entities. We propose a method for auditing the in-domain robustness of systems, focusing specifically on differences in performance due to the national origin of entities. We create entity-switched datasets, in which named entities in the original texts are replaced by plausible named entities of the same type but of different national origin. We find that state-of-the-art systems' performance vary widely even in-domain: In the same context, entities from certain origins are more reliably recognized than entities from elsewhere. Systems perform best on American and Indian entities, and worst on Vietnamese and Indonesian entities. This auditing approach can facilitate the development of more robust named entity recognition systems, and will allow research in this area to consider fairness criteria that have received heightened attention in other predictive technology work."}, "answer": " Question: What method is proposed in the paper to audit the in-domain robustness of named entity recognition systems, specifically with respect to recognizing entities of different national origins?\n\nA) Training separate models for each national origin of entities\nB) Creating entity-switched datasets by replacing entities in the text with entities of the same type but different national origin\nC) Adding more training data with a diverse set of entities \nD) Evaluating performance on datasets comprising news from different countries\n\nCorrect Answer: B) Creating entity-switched datasets by replacing entities in the text with entities of the same type but different national origin\n\nExplanations:\nThe paper proposes creating entity-switched datasets, where named entities in the original texts are replaced with plausible entities of the same type (e.g. person, location) but different national origin. This allows them to evaluate how well systems recognize entities from certain national origins versus others, even within the same textual context."}, "21": {"documentation": {"title": "Deterministic Pilot Design and Channel Estimation for Downlink Massive\n  MIMO-OTFS Systems in Presence of the Fractional Doppler", "source": "Ding Shi, Wenjin Wang, Li You, Xiaohang Song, Yi Hong, Xiqi Gao,\n  Gerhard Fettweis", "docs_id": "2105.09628", "section": ["cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Deterministic Pilot Design and Channel Estimation for Downlink Massive\n  MIMO-OTFS Systems in Presence of the Fractional Doppler. Although the combination of the orthogonal time frequency space (OTFS) modulation and the massive multiple-input multiple-output (MIMO) technology can make communication systems perform better in high-mobility scenarios, there are still many challenges in downlink channel estimation owing to inaccurate modeling and high pilot overhead in practical systems. In this paper, we propose a channel state information (CSI) acquisition scheme for downlink massive MIMO-OTFS in presence of the fractional Doppler, including deterministic pilot design and channel estimation algorithm. First, we analyze the input-output relationship of the single-input single-output (SISO) OTFS based on the orthogonal frequency division multiplexing (OFDM) modem and extend it to massive MIMO-OTFS. Moreover, we formulate an accurate model for the practical system in which the fractional Doppler is considered and the influence of subpaths is revealed. A deterministic pilot design is then proposed based on the model and the structure of the pilot matrix to reduce pilot overhead and save memory consumption. Since channel geometry changes very slowly relative to the communication timescale, we put forward a modified sensing matrix based channel estimation (MSMCE) algorithm to acquire the downlink CSI. Simulation results demonstrate that the proposed downlink CSI acquisition scheme has significant advantages over traditional algorithms."}, "answer": " Here is a 4-option multiple choice question on the document:\n\nQuestion: Which of the following statements is true about the proposed channel state information (CSI) acquisition scheme for downlink massive MIMO-OTFS systems in the paper?\n\nA) It uses a deterministic pilot design based on the fractional Doppler model to reduce pilot overhead. \n\nB) It puts forward a channel estimation algorithm called modified sensing matrix based channel estimation (MSMCE).\n\nC) It analyzes the input-output relationship of MIMO-OTFS based on code division multiple access (CDMA).\n\nD) It considers the influence of frequency selectivity but ignores the impact of subpaths.\n\nCorrect Answer: B\n\nExplanation: \nOption A is incorrect, as the paper proposes a deterministic pilot design based on the fractional Doppler model to save memory consumption, not to reduce pilot overhead. \nOption C is incorrect, as the paper analyzes the input-output relationship of MIMO-OTFS based on OFDM modem, not CDMA.\nOption D is incorrect, as the paper reveals the influence of subpaths in the fractional Doppler model.\nOption B is the correct answer, as the paper puts forward a modified sensing matrix based channel estimation (MSMCE) algorithm to acquire the downlink CSI."}, "22": {"documentation": {"title": "Hedging predictions in machine learning", "source": "Alexander Gammerman and Vladimir Vovk", "docs_id": "cs/0611011", "section": ["cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Hedging predictions in machine learning. Recent advances in machine learning make it possible to design efficient prediction algorithms for data sets with huge numbers of parameters. This paper describes a new technique for \"hedging\" the predictions output by many such algorithms, including support vector machines, kernel ridge regression, kernel nearest neighbours, and by many other state-of-the-art methods. The hedged predictions for the labels of new objects include quantitative measures of their own accuracy and reliability. These measures are provably valid under the assumption of randomness, traditional in machine learning: the objects and their labels are assumed to be generated independently from the same probability distribution. In particular, it becomes possible to control (up to statistical fluctuations) the number of erroneous predictions by selecting a suitable confidence level. Validity being achieved automatically, the remaining goal of hedged prediction is efficiency: taking full account of the new objects' features and other available information to produce as accurate predictions as possible. This can be done successfully using the powerful machinery of modern machine learning."}, "answer": " Question: Which of the following statements is true about the hedging technique described in the paper?\n\nA) It makes predictions using powerful modern machine learning methods like SVMs and kernel ridge regression more reliable by quantifying their accuracy. \n\nB) It assumes that the objects and labels are generated independently from different probability distributions.\n\nC) Its goal is to maximize the number of erroneous predictions by selecting a high confidence level.\n\nD) It cannot control the number of incorrect predictions within statistical fluctuations.\n\nCorrect Answer: A\n\nExplanation: The passage states that the hedging technique quantifies the accuracy and reliability of predictions from machine learning models like SVMs and kernel ridge regression. It does not assume objects and labels are from different distributions, aim to maximize errors, or fail to control errors statistically."}, "23": {"documentation": {"title": "Channel-coded Collision Resolution by Exploiting Symbol Misalignment", "source": "Lu Lu, Soung Chang Liew and Shengli Zhang", "docs_id": "1009.4046", "section": ["cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Channel-coded Collision Resolution by Exploiting Symbol Misalignment. In random-access networks, such as the IEEE 802.11 network, different users may transmit their packets simultaneously, resulting in packet collisions. Traditionally, the collided packets are simply discarded. To improve performance, advanced signal processing techniques can be applied to extract the individual packets from the collided signals. Prior work of ours has shown that the symbol misalignment among the collided packets can be exploited to improve the likelihood of successfully extracting the individual packets. However, the failure rate is still unacceptably high. This paper investigates how channel coding can be used to reduce the failure rate. We propose and investigate a decoding scheme that incorporates the exploitation of the aforementioned symbol misalignment into the channel decoding process. This is a fine-grained integration at the symbol level. In particular, collision resolution and channel decoding are applied in an integrated manner. Simulation results indicate that our method outperforms other schemes, including the straightforward method in which collision resolution and channel coding are applied separately."}, "answer": " Question: What technique does the paper propose to reduce the failure rate when extracting individual packets from collided signals?\n\nA) Using higher transmit power to overpower interference from other signals\n\nB) Applying advanced modulation techniques to encode more information per symbol \n\nC) Integrating collision resolution and channel decoding in a fine-grained manner at the symbol level\n\nD) Increasing the number of parity bits in each packet to enable better error correction\n\nCorrect Answer: C\n\nExplanations:\n\nA) Increasing transmit power is not discussed in the paper. The focus is on signal processing techniques.\n\nB) Advanced modulation techniques are not proposed. The paper focuses on decoding collided packets.\n\nC) Correct. The paper proposes integrating collision resolution and channel decoding at the symbol level for better performance. \n\nD) Increasing parity bits is not discussed. The focus is on decoding techniques."}, "24": {"documentation": {"title": "Nearly-tight VC-dimension and pseudodimension bounds for piecewise\n  linear neural networks", "source": "Peter L. Bartlett and Nick Harvey and Chris Liaw and Abbas Mehrabian", "docs_id": "1703.02930", "section": ["cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Nearly-tight VC-dimension and pseudodimension bounds for piecewise\n  linear neural networks. We prove new upper and lower bounds on the VC-dimension of deep neural networks with the ReLU activation function. These bounds are tight for almost the entire range of parameters. Letting $W$ be the number of weights and $L$ be the number of layers, we prove that the VC-dimension is $O(W L \\log(W))$, and provide examples with VC-dimension $\\Omega( W L \\log(W/L) )$. This improves both the previously known upper bounds and lower bounds. In terms of the number $U$ of non-linear units, we prove a tight bound $\\Theta(W U)$ on the VC-dimension. All of these bounds generalize to arbitrary piecewise linear activation functions, and also hold for the pseudodimensions of these function classes. Combined with previous results, this gives an intriguing range of dependencies of the VC-dimension on depth for networks with different non-linearities: there is no dependence for piecewise-constant, linear dependence for piecewise-linear, and no more than quadratic dependence for general piecewise-polynomial."}, "answer": " Here is a 4-option multiple choice question on the VC-dimension bounds for deep neural networks:\n\nQuestion: According to the given documentation, which of the following statements is true about the VC-dimension of a deep neural network with ReLU activation, with W weights and L layers?\n\nA) The VC-dimension is \u0398(WL) \n\nB) The VC-dimension is \u03a9(WL log(W/L)) and O(WL log(W))\n\nC) The VC-dimension is \u0398(WL log(W))\n\nD) The VC-dimension is \u03a9(WL) and O(WL log(W))\n\nCorrect Answer: B\n\nExplanation: \nThe documentation states that the VC-dimension is O(WL log(W)) and there exist examples with VC-dimension \u03a9(WL log(W/L)). So the tight bound is \u03a9(WL log(W/L)) and O(WL log(W)), which is option B. Options A, C and D are either looser bounds or incorrect based on the given information."}, "25": {"documentation": {"title": "Asymptotic behaviour of learning rates in Armijo's condition", "source": "Tuyen Trung Truong, Tuan Hang Nguyen", "docs_id": "2007.03618", "section": ["math.OC", "cs.LG", "cs.NA", "math.DS", "math.NA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Asymptotic behaviour of learning rates in Armijo's condition. Fix a constant $0<\\alpha <1$. For a $C^1$ function $f:\\mathbb{R}^k\\rightarrow \\mathbb{R}$, a point $x$ and a positive number $\\delta >0$, we say that Armijo's condition is satisfied if $f(x-\\delta \\nabla f(x))-f(x)\\leq -\\alpha \\delta ||\\nabla f(x)||^2$. It is a basis for the well known Backtracking Gradient Descent (Backtracking GD) algorithm. Consider a sequence $\\{x_n\\}$ defined by $x_{n+1}=x_n-\\delta _n\\nabla f(x_n)$, for positive numbers $\\delta _n$ for which Armijo's condition is satisfied. We show that if $\\{x_n\\}$ converges to a non-degenerate critical point, then $\\{\\delta _n\\}$ must be bounded. Moreover this boundedness can be quantified in terms of the norms of the Hessian $\\nabla ^2f$ and its inverse at the limit point. This complements the first author's results on Unbounded Backtracking GD, and shows that in case of convergence to a non-degenerate critical point the behaviour of Unbounded Backtracking GD is not too different from that of usual Backtracking GD. On the other hand, in case of convergence to a degenerate critical point the behaviours can be very much different. We run some experiments to illustrate that both scenrios can really happen. In another part of the paper, we argue that Backtracking GD has the correct unit (according to a definition by Zeiler in his Adadelta's paper). The main point is that since learning rate in Backtracking GD is bound by Armijo's condition, it is not unitless."}, "answer": " Here is a multi-form question based on the given text:\n\nQuestion: According to the text, what can be said about the sequence $\\{\\delta_n\\}$ if the sequence $\\{x_n\\}$ generated by Backtracking Gradient Descent converges to a non-degenerate critical point?\n\nA) The sequence $\\{\\delta_n\\}$ must converge to 0.\n\nB) The sequence $\\{\\delta_n\\}$ is guaranteed to be bounded, and this boundedness can be quantified in terms of the norms of the Hessian and its inverse at the limit point. \n\nC) The sequence $\\{\\delta_n\\}$ is unbounded.\n\nD) Nothing can be said about the sequence $\\{\\delta_n\\}$ in this case.\n\nCorrect Answer: B\n\nExplanation: The text states that if $\\{x_n\\}$ converges to a non-degenerate critical point, then $\\{\\delta_n\\}$ must be bounded, and this boundedness can be quantified in terms of the norms of the Hessian and its inverse at the limit point. So B is the correct option."}, "26": {"documentation": {"title": "Interpretable Run-Time Prediction and Planning in Co-Robotic\n  Environments", "source": "Rahul Peddi and Nicola Bezzo", "docs_id": "2109.03893", "section": ["cs.RO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Interpretable Run-Time Prediction and Planning in Co-Robotic\n  Environments. Mobile robots are traditionally developed to be reactive and avoid collisions with surrounding humans, often moving in unnatural ways without following social protocols, forcing people to behave very differently from human-human interaction rules. Humans, on the other hand, are seamlessly able to understand why they may interfere with surrounding humans and change their behavior based on their reasoning, resulting in smooth, intuitive avoiding behaviors. In this paper, we propose an approach for a mobile robot to avoid interfering with the desired paths of surrounding humans. We leverage a library of previously observed trajectories to design a decision-tree based interpretable monitor that: i) predicts whether the robot is interfering with surrounding humans, ii) explains what behaviors are causing either prediction, and iii) plans corrective behaviors if interference is predicted. We also propose a validation scheme to improve the predictive model at run-time. The proposed approach is validated with simulations and experiments involving an unmanned ground vehicle (UGV) performing go-to-goal operations in the presence of humans, demonstrating non-interfering behaviors and run-time learning."}, "answer": " Question: Which of the following statements is true about the proposed approach in the paper for enabling a mobile robot to avoid interfering with surrounding humans?\n\nA) It uses a decision-tree based monitor to only predict whether the robot will interfere with humans, but does not plan any corrective behaviors.\n\nB) It relies on pre-recorded human trajectories to train a neural network model that predicts and explains if the robot will interfere. \n\nC) It leverages a library of observed trajectories to design an interpretable monitor that predicts interference, explains the cause, and plans corrective actions if needed.\n\nD) It focuses on making the robot motions smooth and natural like human-human interactions, without any collision avoidance.\n\nCorrect Answer: C) It leverages a library of observed trajectories to design an interpretable monitor that predicts interference, explains the cause, and plans corrective actions if needed."}, "27": {"documentation": {"title": "Actor-Critic Method for High Dimensional Static\n  Hamilton--Jacobi--Bellman Partial Differential Equations based on Neural\n  Networks", "source": "Mo Zhou, Jiequn Han and Jianfeng Lu", "docs_id": "2102.11379", "section": ["math.OC", "cs.NA", "math.NA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Actor-Critic Method for High Dimensional Static\n  Hamilton--Jacobi--Bellman Partial Differential Equations based on Neural\n  Networks. We propose a novel numerical method for high dimensional Hamilton--Jacobi--Bellman (HJB) type elliptic partial differential equations (PDEs). The HJB PDEs, reformulated as optimal control problems, are tackled by the actor-critic framework inspired by reinforcement learning, based on neural network parametrization of the value and control functions. Within the actor-critic framework, we employ a policy gradient approach to improve the control, while for the value function, we derive a variance reduced least-squares temporal difference method using stochastic calculus. To numerically discretize the stochastic control problem, we employ an adaptive step size scheme to improve the accuracy near the domain boundary. Numerical examples up to $20$ spatial dimensions including the linear quadratic regulators, the stochastic Van der Pol oscillators, the diffusive Eikonal equations, and fully nonlinear elliptic PDEs derived from a regulator problem are presented to validate the effectiveness of our proposed method."}, "answer": " Here is a 4-option multiple choice question on the paper:\n\nQuestion: Which of the following statements is true about the proposed method in the paper?\n\nA) It employs policy iteration to improve the control function approximation.\n\nB) It uses a least-squares temporal difference method to learn the value function. \n\nC) It uses a fixed step size scheme for numerical discretization of the stochastic control problem.\n\nD) It is validated on linear quadratic regulator problems with up to 10 dimensions.\n\nCorrect Answer: B\n\nExplanation: The paper proposes using a policy gradient approach to improve the control function approximation in the actor-critic framework. It derives a variance reduced least-squares temporal difference method for learning the value function. It employs an adaptive step size scheme for discretization, not a fixed one. The method is validated on problems with up to 20 dimensions, not 10."}, "28": {"documentation": {"title": "DiNTS: Differentiable Neural Network Topology Search for 3D Medical\n  Image Segmentation", "source": "Yufan He, Dong Yang, Holger Roth, Can Zhao, Daguang Xu", "docs_id": "2103.15954", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "DiNTS: Differentiable Neural Network Topology Search for 3D Medical\n  Image Segmentation. Recently, neural architecture search (NAS) has been applied to automatically search high-performance networks for medical image segmentation. The NAS search space usually contains a network topology level (controlling connections among cells with different spatial scales) and a cell level (operations within each cell). Existing methods either require long searching time for large-scale 3D image datasets, or are limited to pre-defined topologies (such as U-shaped or single-path). In this work, we focus on three important aspects of NAS in 3D medical image segmentation: flexible multi-path network topology, high search efficiency, and budgeted GPU memory usage. A novel differentiable search framework is proposed to support fast gradient-based search within a highly flexible network topology search space. The discretization of the searched optimal continuous model in differentiable scheme may produce a sub-optimal final discrete model (discretization gap). Therefore, we propose a topology loss to alleviate this problem. In addition, the GPU memory usage for the searched 3D model is limited with budget constraints during search. Our Differentiable Network Topology Search scheme (DiNTS) is evaluated on the Medical Segmentation Decathlon (MSD) challenge, which contains ten challenging segmentation tasks. Our method achieves the state-of-the-art performance and the top ranking on the MSD challenge leaderboard."}, "answer": " Here is a 4-option multiple choice question on the document:\n\nQuestion: Which of the following statements is true about the proposed DiNTS framework for neural architecture search in 3D medical image segmentation?\n\nA) It searches over network cell architectures only, using a fixed U-shaped topology.\n\nB) It allows flexible multi-path topologies but has high GPU memory usage during search. \n\nC) It focuses on network topology search with high efficiency and constrained GPU memory.\n\nD) It directly outputs a discrete architecture without needing further training or refinement.\n\nCorrect Answer: C\n\nExplanation: The document states that the DiNTS framework focuses on flexible network topology search, high search efficiency, and budgeted GPU memory usage. It allows searching over multi-path topologies unlike fixed shapes like U-net. And it uses a differentiable search space unlike directly outputting a discrete model."}, "29": {"documentation": {"title": "A new hybrid approach for crude oil price forecasting: Evidence from\n  multi-scale data", "source": "Yang Yifan, Guo Ju'e, Sun Shaolong, and Li Yixin", "docs_id": "2002.09656", "section": ["q-fin.ST", "cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A new hybrid approach for crude oil price forecasting: Evidence from\n  multi-scale data. Faced with the growing research towards crude oil price fluctuations influential factors following the accelerated development of Internet technology, accessible data such as Google search volume index are increasingly quantified and incorporated into forecasting approaches. In this paper, we apply multi-scale data that including both GSVI data and traditional economic data related to crude oil price as independent variables and propose a new hybrid approach for monthly crude oil price forecasting. This hybrid approach, based on divide and conquer strategy, consists of K-means method, kernel principal component analysis and kernel extreme learning machine , where K-means method is adopted to divide input data into certain clusters, KPCA is applied to reduce dimension, and KELM is employed for final crude oil price forecasting. The empirical result can be analyzed from data and method levels. At the data level, GSVI data perform better than economic data in level forecasting accuracy but with opposite performance in directional forecasting accuracy because of Herd Behavior, while hybrid data combined their advantages and obtain best forecasting performance in both level and directional accuracy. At the method level, the approaches with K-means perform better than those without K-means, which demonstrates that divide and conquer strategy can effectively improve the forecasting performance."}, "answer": " Here is a multi-form question on the document:\n\nQuestion: Which of the following statements is true about the new hybrid approach for crude oil price forecasting proposed in the paper?\n\nA) It uses only Google search volume index (GSVI) data as the independent variable. \n\nB) It applies kernel principal component analysis (KPCA) before using K-means clustering.\n\nC) The approach with K-means clustering performed worse than without it.\n\nD) The GSVI data performed better in directional forecasting accuracy compared to economic data.\n\nCorrect Answer: B\n\nExplanations:\n\nA) Incorrect. The approach uses both GSVI data and traditional economic data as independent variables.\n\nB) Correct. The paper states KPCA is applied before K-means clustering to reduce dimension. \n\nC) Incorrect. The paper concludes approaches with K-means performed better than without, showing the clustering helps.\n\nD) Incorrect. The paper states economic data performed better in directional accuracy while GSVI data was better for level accuracy."}, "30": {"documentation": {"title": "VoIPLoc: Passive VoIP call provenance via acoustic side-channels", "source": "Shishir Nagaraja, Ryan Shah", "docs_id": "1909.01904", "section": ["cs.CR", "cs.SD", "eess.AS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "VoIPLoc: Passive VoIP call provenance via acoustic side-channels. We propose VoIPLoc, a novel location fingerprinting technique and apply it to the VoIP call provenance problem. It exploits echo-location information embedded within VoIP audio to support fine-grained location inference. We found consistent statistical features induced by the echo-reflection characteristics of the location into recorded speech. These features are discernible within traces received at the VoIP destination, enabling location inference. We evaluated VoIPLoc by developing a dataset of audio traces received through VoIP channels over the Tor network. We show that recording locations can be fingerprinted and detected remotely with a low false-positive rate, even when a majority of the audio samples are unlabelled. Finally, we note that the technique is fully passive and thus undetectable, unlike prior art. VoIPLoc is robust to the impact of environmental noise and background sounds, as well as the impact of compressive codecs and network jitter. The technique is also highly scalable and offers several degrees of freedom terms of the fingerprintable space."}, "answer": " Question: Which of the following statements accurately describes a key contribution of the VoIPLoc system proposed in the paper?\n\nA) VoIPLoc uses active acoustic probing to fingerprint recording locations.\n\nB) VoIPLoc fingerprints locations by analyzing background noise in VoIP calls. \n\nC) VoIPLoc exploits acoustic side-channels induced by echo reflections to fingerprint locations.\n\nD) VoIPLoc detects locations by analyzing jitter and compression artifacts in VoIP calls.\n\nCorrect Answer: C\n\nExplanation: The paper proposes VoIPLoc, which exploits echo-location information embedded within VoIP audio to support fine-grained location inference. It does this by analyzing consistent statistical features induced by the echo-reflection characteristics of the location into recorded speech."}, "31": {"documentation": {"title": "Shared-Control Teleoperation Paradigms on a Soft Growing Robot\n  Manipulator", "source": "Fabio Stroppa and Mario Selvaggio and Nathaniel Agharese and MingLuo\n  and Laura H. Blumenschein and Elliot W. Hawkes and Allison M. Okamura", "docs_id": "2108.00677", "section": ["cs.RO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Shared-Control Teleoperation Paradigms on a Soft Growing Robot\n  Manipulator. Semi-autonomous telerobotic systems allow both humans and robots to exploit their strengths, while enabling personalized execution of a task. However, for new soft robots with degrees of freedom dissimilar to those of human operators, it is unknown how the control of a task should be divided between the human and robot. This work presents a set of interaction paradigms between a human and a soft growing robot manipulator, and demonstrates them in both real and simulated scenarios. The robot can grow and retract by eversion and inversion of its tubular body, a property we exploit to implement interaction paradigms. We implemented and tested six different paradigms of human-robot interaction, beginning with full teleoperation and gradually adding automation to various aspects of the task execution. All paradigms were demonstrated by two expert and two naive operators. Results show that humans and the soft robot manipulator can split control along degrees of freedom while acting simultaneously. In the simple pick-and-place task studied in this work, performance improves as the control is gradually given to the robot, because the robot can correct certain human errors. However, human engagement and enjoyment may be maximized when the task is at least partially shared. Finally, when the human operator is assisted by haptic feedback based on soft robot position errors, we observed that the improvement in performance is highly dependent on the expertise of the human operator."}, "answer": " Here is a multi-form question based on the given text:\n\nQuestion: Which of the following statements is true about the interaction paradigms tested between the human operators and the soft growing robot manipulator?\n\nA) The robot corrected all human errors in the pick-and-place task when given full control.\n\nB) Performance improved but human engagement decreased as more control was given to the robot. \n\nC) Haptic feedback based on position errors always improved performance regardless of operator expertise.\n\nD) Operators could simultaneously split control with the robot along different degrees of freedom.\n\nCorrect Answer: D\n\nExplanation: According to the text, the interaction paradigms allowed the human operators and the soft growing robot manipulator to split control along degrees of freedom while acting simultaneously. Option D is correct. Options A, B, and C misrepresent or overstate the findings described in the text."}, "32": {"documentation": {"title": "A Relational Gradient Descent Algorithm For Support Vector Machine\n  Training", "source": "Mahmoud Abo-Khamis, Sungjin Im, Benjamin Moseley, Kirk Pruhs, Alireza\n  Samadian", "docs_id": "2005.05325", "section": ["cs.DS", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Relational Gradient Descent Algorithm For Support Vector Machine\n  Training. We consider gradient descent like algorithms for Support Vector Machine (SVM) training when the data is in relational form. The gradient of the SVM objective can not be efficiently computed by known techniques as it suffers from the ``subtraction problem''. We first show that the subtraction problem can not be surmounted by showing that computing any constant approximation of the gradient of the SVM objective function is $\\#P$-hard, even for acyclic joins. We, however, circumvent the subtraction problem by restricting our attention to stable instances, which intuitively are instances where a nearly optimal solution remains nearly optimal if the points are perturbed slightly. We give an efficient algorithm that computes a ``pseudo-gradient'' that guarantees convergence for stable instances at a rate comparable to that achieved by using the actual gradient. We believe that our results suggest that this sort of stability the analysis would likely yield useful insight in the context of designing algorithms on relational data for other learning problems in which the subtraction problem arises."}, "answer": " Here is a 4-option multiple choice question on the paper:\n\nQuestion: Which of the following statements is true about the gradient descent algorithm proposed in the paper for training SVMs on relational data?\n\nA) It computes the exact gradient of the SVM objective function, which allows it to achieve the optimal convergence rate. \n\nB) It uses a novel technique to efficiently compute an approximation of the gradient that avoids the subtraction problem.\n\nC) It shows that computing any constant approximation of the SVM objective gradient is NP-hard on relational data.\n\nD) It proves that gradient descent cannot be used for SVM training on relational data due to the subtraction problem.\n\nCorrect Answer: B\n\nExplanation: The paper proposes an algorithm that efficiently computes a \"pseudo-gradient\" that avoids the subtraction problem and allows convergence at a rate comparable to using the actual gradient. It does not compute the exact gradient, prove the hardness of approximating the gradient, or show gradient descent cannot be used."}, "33": {"documentation": {"title": "On the Complexity of Best Arm Identification in Multi-Armed Bandit\n  Models", "source": "Emilie Kaufmann (SEQUEL, LTCI), Olivier Capp\\'e (LTCI), Aur\\'elien\n  Garivier (IMT)", "docs_id": "1407.4443", "section": ["stat.ML", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On the Complexity of Best Arm Identification in Multi-Armed Bandit\n  Models. The stochastic multi-armed bandit model is a simple abstraction that has proven useful in many different contexts in statistics and machine learning. Whereas the achievable limit in terms of regret minimization is now well known, our aim is to contribute to a better understanding of the performance in terms of identifying the m best arms. We introduce generic notions of complexity for the two dominant frameworks considered in the literature: fixed-budget and fixed-confidence settings. In the fixed-confidence setting, we provide the first known distribution-dependent lower bound on the complexity that involves information-theoretic quantities and holds when m is larger than 1 under general assumptions. In the specific case of two armed-bandits, we derive refined lower bounds in both the fixed-confidence and fixed-budget settings, along with matching algorithms for Gaussian and Bernoulli bandit models. These results show in particular that the complexity of the fixed-budget setting may be smaller than the complexity of the fixed-confidence setting, contradicting the familiar behavior observed when testing fully specified alternatives. In addition, we also provide improved sequential stopping rules that have guaranteed error probabilities and shorter average running times. The proofs rely on two technical results that are of independent interest : a deviation lemma for self-normalized sums (Lemma 19) and a novel change of measure inequality for bandit models (Lemma 1)."}, "answer": " Here is a 4-option multiple choice question on the paper:\n\nQuestion: Which of the following statements is true about the complexity results presented in the paper?\n\nA) The paper shows that the fixed-budget setting always has lower complexity than the fixed-confidence setting. \n\nB) The paper provides the first distribution-dependent lower bound on the complexity for identifying the best arm in the fixed-confidence setting.\n\nC) The paper proves that the complexity of best arm identification is the same in the fixed-budget and fixed-confidence settings.\n\nD) The paper only considers the fixed-budget setting and does not analyze the fixed-confidence setting.\n\nCorrect Answer: B\n\nExplanation: The paper derives a new distribution-dependent lower bound on the complexity for identifying the best arm in the fixed-confidence multi-armed bandit setting when m>1 arms are to be identified. It also shows through examples that the fixed-budget setting can sometimes have lower complexity than the fixed-confidence setting, contradicting typical behavior. But it does not prove equality of the two settings."}, "34": {"documentation": {"title": "Query-Adaptive Hash Code Ranking for Large-Scale Multi-View Visual\n  Search", "source": "Xianglong Liu, Lei Huang, Cheng Deng, Bo Lang, Dacheng Tao", "docs_id": "1904.08623", "section": ["cs.IR", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Query-Adaptive Hash Code Ranking for Large-Scale Multi-View Visual\n  Search. Hash based nearest neighbor search has become attractive in many applications. However, the quantization in hashing usually degenerates the discriminative power when using Hamming distance ranking. Besides, for large-scale visual search, existing hashing methods cannot directly support the efficient search over the data with multiple sources, and while the literature has shown that adaptively incorporating complementary information from diverse sources or views can significantly boost the search performance. To address the problems, this paper proposes a novel and generic approach to building multiple hash tables with multiple views and generating fine-grained ranking results at bitwise and tablewise levels. For each hash table, a query-adaptive bitwise weighting is introduced to alleviate the quantization loss by simultaneously exploiting the quality of hash functions and their complement for nearest neighbor search. From the tablewise aspect, multiple hash tables are built for different data views as a joint index, over which a query-specific rank fusion is proposed to rerank all results from the bitwise ranking by diffusing in a graph. Comprehensive experiments on image search over three well-known benchmarks show that the proposed method achieves up to 17.11% and 20.28% performance gains on single and multiple table search over state-of-the-art methods."}, "answer": " Here is a 4-option multiple choice question on the document:\n\nQuestion: Which of the following statements is true about the proposed method in the paper?\n\nA) It uses a single hash table and reranks results using a graph-based approach.\n\nB) It builds multiple hash tables for different views but does not use query-adaptive weighting. \n\nC) It introduces query-adaptive bitwise weighting to reduce quantization loss and uses rank fusion across tables.\n\nD) It focuses only on Hamming distance ranking and does not consider complementary information.\n\nCorrect Answer: C\n\nExplanation: The paper proposes using multiple hash tables for different views of the data, with query-adaptive bitwise weighting to reduce quantization loss. It also uses query-specific rank fusion across tables to rerank results. So option C, using query-adaptive weighting and rank fusion across multiple tables, is correct."}, "35": {"documentation": {"title": "Documenting Large Webtext Corpora: A Case Study on the Colossal Clean\n  Crawled Corpus", "source": "Jesse Dodge, Maarten Sap, Ana Marasovi\\'c, William Agnew, Gabriel\n  Ilharco, Dirk Groeneveld, Margaret Mitchell, Matt Gardner", "docs_id": "2104.08758", "section": ["cs.CL", "cs.AI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Documenting Large Webtext Corpora: A Case Study on the Colossal Clean\n  Crawled Corpus. Large language models have led to remarkable progress on many NLP tasks, and researchers are turning to ever-larger text corpora to train them. Some of the largest corpora available are made by scraping significant portions of the internet, and are frequently introduced with only minimal documentation. In this work we provide some of the first documentation for the Colossal Clean Crawled Corpus (C4; Raffel et al., 2020), a dataset created by applying a set of filters to a single snapshot of Common Crawl. We begin by investigating where the data came from, and find a significant amount of text from unexpected sources like patents and US military websites. Then we explore the content of the text itself, and find machine-generated text (e.g., from machine translation systems) and evaluation examples from other benchmark NLP datasets. To understand the impact of the filters applied to create this dataset, we evaluate the text that was removed, and show that blocklist filtering disproportionately removes text from and about minority individuals. Finally, we conclude with some recommendations for how to created and document web-scale datasets from a scrape of the internet."}, "answer": " Question: What dataset was created by applying filters to a single snapshot of Common Crawl, as documented in the Arxiv paper?\n\nA) The Colossal Clean Crawled Corpus (C4)\nB) The Common Crawl Corpus  \nC) The OpenWebText Corpus\nD) The Pile Corpus\n\nCorrect Answer: A) The Colossal Clean Crawled Corpus (C4)\n\nExplanation: The passage states that the Colossal Clean Crawled Corpus (C4) was created by applying filters to a snapshot of Common Crawl, so A) is the correct answer. B) refers to the original Common Crawl dataset before filters were applied. C) and D) refer to other large web-scraped corpora, but not the one discussed in this passage."}, "36": {"documentation": {"title": "No-Reference Color Image Quality Assessment: From Entropy to Perceptual\n  Quality", "source": "Xiaoqiao Chen, Qingyi Zhang, Manhui Lin, Guangyi Yang, Chu He", "docs_id": "1812.10695", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "No-Reference Color Image Quality Assessment: From Entropy to Perceptual\n  Quality. This paper presents a high-performance general-purpose no-reference (NR) image quality assessment (IQA) method based on image entropy. The image features are extracted from two domains. In the spatial domain, the mutual information between the color channels and the two-dimensional entropy are calculated. In the frequency domain, the two-dimensional entropy and the mutual information of the filtered sub-band images are computed as the feature set of the input color image. Then, with all the extracted features, the support vector classifier (SVC) for distortion classification and support vector regression (SVR) are utilized for the quality prediction, to obtain the final quality assessment score. The proposed method, which we call entropy-based no-reference image quality assessment (ENIQA), can assess the quality of different categories of distorted images, and has a low complexity. The proposed ENIQA method was assessed on the LIVE and TID2013 databases and showed a superior performance. The experimental results confirmed that the proposed ENIQA method has a high consistency of objective and subjective assessment on color images, which indicates the good overall performance and generalization ability of ENIQA. The source code is available on github https://github.com/jacob6/ENIQA."}, "answer": " Here is a 4-option multiple choice question on the paper:\n\nQuestion: Which of the following is NOT an image feature extracted by the proposed ENIQA method for no-reference image quality assessment?\n\nA) The mutual information between the color channels in the spatial domain\n\nB) The two-dimensional entropy in the frequency domain \n\nC) The structural similarity index between the original and distorted images\n\nD) The mutual information of the filtered sub-band images in the frequency domain\n\nCorrect Answer: C\n\nExplanation: \nOption A is incorrect as the paper states that the mutual information between the color channels is calculated in the spatial domain as an image feature. \n\nOption B is incorrect as the paper mentions that the two-dimensional entropy in the frequency domain is computed as a feature. \n\nOption C is the correct answer as the structural similarity index requires the original reference image which is not available in no-reference image quality assessment. The paper focuses on no-reference methods only.\n\nOption D is incorrect as the paper states that the mutual information of the filtered sub-band images in the frequency domain is extracted as a feature."}, "37": {"documentation": {"title": "Normal numbers with digit dependencies", "source": "Christoph Aistleitner and Veronica Becher and Olivier Carton", "docs_id": "1804.02844", "section": ["math.NT", "cs.FL", "math.CO", "math.PR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Normal numbers with digit dependencies. We give metric theorems for the property of Borel normality for real numbers under the assumption of digit dependencies in their expansion in a given integer base. We quantify precisely how much digit dependence can be allowed such that, still, almost all real numbers are normal. Our theorem states that almost all real numbers are normal when at least slightly more than $\\log \\log n$ consecutive digits with indices starting at position $n$ are independent. As the main application, we consider the Toeplitz set $T_P$, which is the set of all sequences $a_1a_2 \\ldots $ of symbols from $\\{0, \\ldots, b-1\\}$ such that $a_n$ is equal to $a_{pn}$, for every $p$ in $P$ and $n=1,2,\\ldots$. Here $b$ is an integer base and $P$ is a finite set of prime numbers. We show that almost every real number whose base $b$ expansion is in $T_P$ is normal to base $b$. In the case when $P$ is the singleton set $\\{2\\}$ we prove that more is true: almost every real number whose base $b$ expansion is in $T_P$ is normal to all integer bases. We also consider the Toeplitz transform which maps the set of all sequences to the set $T_P$ and we characterize the normal sequences whose Toeplitz transform is normal as well."}, "answer": " Here is a multi-form question on the given documentation:\n\nQuestion: According to the documentation, which of the following statements is true about the normality of real numbers with digit dependencies in their expansion in a given integer base?\n\nA) Real numbers are normal even with very high levels of digit dependence, as long as at least 2 consecutive digits are independent.\n\nB) Real numbers are normal only if all digits are completely independent. Any amount of dependence destroys normality. \n\nC) Real numbers are normal if at least slightly more than log log n consecutive digits starting at position n are independent.\n\nD) It is impossible for real numbers with digit dependencies to be normal.\n\nCorrect Answer: C\n\nExplanation: The documentation states that real numbers are normal \"when at least slightly more than log log n consecutive digits with indices starting at position n are independent.\" So C is the correct option. A is incorrect because only slightly more than log log n digits need to be independent, not just 2. B is incorrect because some dependence is allowed. D is incorrect because the documentation shows it is possible for numbers with dependencies to be normal."}, "38": {"documentation": {"title": "Adversarial Encoder-Multi-Task-Decoder for Multi-Stage Processes", "source": "Andre Mendes, Julian Togelius, Leandro dos Santos Coelho", "docs_id": "2003.06899", "section": ["cs.LG", "cs.SI", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Adversarial Encoder-Multi-Task-Decoder for Multi-Stage Processes. In multi-stage processes, decisions occur in an ordered sequence of stages. Early stages usually have more observations with general information (easier/cheaper to collect), while later stages have fewer observations but more specific data. This situation can be represented by a dual funnel structure, in which the sample size decreases from one stage to the other while the information increases. Training classifiers in this scenario is challenging since information in the early stages may not contain distinct patterns to learn (underfitting). In contrast, the small sample size in later stages can cause overfitting. We address both cases by introducing a framework that combines adversarial autoencoders (AAE), multi-task learning (MTL), and multi-label semi-supervised learning (MLSSL). We improve the decoder of the AAE with an MTL component so it can jointly reconstruct the original input and use feature nets to predict the features for the next stages. We also introduce a sequence constraint in the output of an MLSSL classifier to guarantee the sequential pattern in the predictions. Using real-world data from different domains (selection process, medical diagnosis), we show that our approach outperforms other state-of-the-art methods."}, "answer": " Question: Which of the following best describes the key idea behind the proposed adversarial encoder-multi-task-decoder framework for multi-stage processes?\n\nA) Using an adversarial autoencoder with a multi-task decoder to jointly reconstruct the input and predict features for later stages.\n\nB) Employing semi-supervised learning on the small datasets in later stages to avoid overfitting. \n\nC) Introducing a sequence constraint in the classifier output to enforce the sequential pattern.\n\nD) Training separate classifiers for each stage to handle the differing data distributions.\n\nCorrect Answer: A\n\nExplanation: The key idea is to use an adversarial autoencoder with a multi-task decoder that can both reconstruct the original input and predict features for subsequent stages, addressing both underfitting early on and overfitting later. Options B and C relate to additional components of the full framework. Option D refers to a more traditional multi-stage approach without sharing information."}, "39": {"documentation": {"title": "Perception-Based Temporal Logic Planning in Uncertain Semantic Maps", "source": "Yiannis Kantaros, Samarth Kalluraya, Qi Jin, and George J. Pappas", "docs_id": "2012.10490", "section": ["cs.RO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Perception-Based Temporal Logic Planning in Uncertain Semantic Maps. This paper addresses a multi-robot planning problem in partially unknown semantic environments. The environment is assumed to have known geometric structure (e.g., walls) and to be occupied by static labeled landmarks with uncertain positions and classes. This modeling approach gives rise to an uncertain semantic map generated by semantic SLAM algorithms. Our goal is to design control policies for robots equipped with noisy perception systems so that they can accomplish collaborative tasks captured by global temporal logic specifications. To account for environmental and perceptual uncertainty, we extend a fragment of Linear Temporal Logic (LTL), called co-safe LTL, by including perception-based atomic predicates allowing us to incorporate uncertainty-wise and probabilistic satisfaction requirements directly into the task specification. The perception-based LTL planning problem gives rise to an optimal control problem, solved by a novel sampling-based algorithm, that generates open-loop control policies that are updated online to adapt to a continuously learned semantic map. We provide extensive experiments to demonstrate the efficiency of the proposed planning architecture."}, "answer": " Here is a 4-option multiple choice question on the paper:\n\nQuestion: Which of the following statements is true about the semantic map used in the paper?\n\nA) The semantic map has known geometric structure but uncertain landmark positions and classes.\n\nB) The semantic map has precisely known landmark positions and classes. \n\nC) The semantic map only has known geometric structure with no landmarks.\n\nD) The semantic map has completely uncertain geometry, landmark positions and classes.\n\nCorrect Answer: A\n\nExplanation: According to the paper, the environment is assumed to have known geometric structure like walls, but uncertain landmark positions and classes. So option A is correct. Options B, C and D are incorrect based on the description in the paper. The semantic map has known geometry but uncertain landmark information."}, "40": {"documentation": {"title": "ColloQL: Robust Cross-Domain Text-to-SQL Over Search Queries", "source": "Karthik Radhakrishnan, Arvind Srikantan, Xi Victoria Lin", "docs_id": "2010.09927", "section": ["cs.CL", "cs.AI", "cs.DB", "cs.IR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "ColloQL: Robust Cross-Domain Text-to-SQL Over Search Queries. Translating natural language utterances to executable queries is a helpful technique in making the vast amount of data stored in relational databases accessible to a wider range of non-tech-savvy end users. Prior work in this area has largely focused on textual input that is linguistically correct and semantically unambiguous. However, real-world user queries are often succinct, colloquial, and noisy, resembling the input of a search engine. In this work, we introduce data augmentation techniques and a sampling-based content-aware BERT model (ColloQL) to achieve robust text-to-SQL modeling over natural language search (NLS) questions. Due to the lack of evaluation data, we curate a new dataset of NLS questions and demonstrate the efficacy of our approach. ColloQL's superior performance extends to well-formed text, achieving 84.9% (logical) and 90.7% (execution) accuracy on the WikiSQL dataset, making it, to the best of our knowledge, the highest performing model that does not use execution guided decoding."}, "answer": " Question: Which of the following best summarizes the key contributions of the ColloQL model described in the paper?\n\nA) It uses data augmentation and a BERT model to achieve state-of-the-art text-to-SQL performance on well-formed natural language questions.\n\nB) It is the first model to translate search queries into SQL queries.\n\nC) It achieves higher accuracy on the WikiSQL dataset than any model without execution guided decoding. \n\nD) It introduces a new dataset for evaluating text-to-SQL models on search queries.\n\nCorrect Answer: C\n\nThe paper states that ColloQL achieves 84.9% logical and 90.7% execution accuracy on the WikiSQL dataset, which is the highest performance without using execution guided decoding. While the other statements relate to the paper, answer C best summarizes the key contribution highlighted."}, "41": {"documentation": {"title": "The Gaussian Many-to-1 Interference Channel with Confidential Messages", "source": "Xiang He and Aylin Yener", "docs_id": "1005.0624", "section": ["cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "The Gaussian Many-to-1 Interference Channel with Confidential Messages. The many-to-one interference channel has received interest by virtue of embodying the essence of an interference network while being more tractable than the general K-user interference channel. In this paper, we introduce information theoretic secrecy to this model and consider the many-to-one interference channel with confidential messages, in which each receiver, in particular, the one subject to interference, is also one from which the interfering users' messages need to be kept secret from. We derive the achievable secrecy sum rate for this channel using nested lattice codes, as well as an upper bound on the secrecy sum rate for all possible channel gain configurations. We identify several nontrivial cases where the gap between the upper bound and the achieved secrecy sum rate is only a function of the number of the users K, and is uniform over all possible channel gain configurations in each case. In addition, we identify the secure degree of freedom for this channel and show it to be equivalent to its degree of freedom, i.e., the secrecy in high SNR comes for free."}, "answer": " Here is a 4-option multiple choice question on the given passage:\n\nQuestion: Which of the following statements is true about the many-to-one interference channel with confidential messages studied in the paper?\n\nA) It has a lower secure degree of freedom compared to the degree of freedom at high SNR.\n\nB) The gap between the achievable secrecy sum rate and the upper bound depends only on the number of users. \n\nC) The achievable secrecy sum rate was derived using nested lattice codes.\n\nD) The general K-user interference channel is more tractable than the many-to-one interference channel.\n\nCorrect Answer: C\n\nExplanation: \nOption C is correct according to the passage, which states that the achievable secrecy sum rate for the many-to-one interference channel with confidential messages was derived using nested lattice codes. \n\nOption A is incorrect as the passage mentions that the secure degree of freedom is equivalent to the degree of freedom at high SNR. \n\nOption B is incorrect as the passage states the gap depends on the number of users only for certain specific cases, not in general. \n\nOption D is contradicted by the passage which states the many-to-one interference channel is more tractable than the general K-user interference channel."}, "42": {"documentation": {"title": "Coloring Drawings of Graphs", "source": "Christoph Hertrich, Felix Schr\\\"oder and Raphael Steiner", "docs_id": "2008.09692", "section": ["math.CO", "cs.DM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Coloring Drawings of Graphs. We consider face-colorings of drawings of graphs in the plane. Given a multi-graph $G$ together with a drawing $\\Gamma(G)$ in the plane with only finitely many crossings, we define a face-$k$-coloring of $\\Gamma(G)$ to be a coloring of the maximal connected regions of the drawing, the faces, with $k$ colors such that adjacent faces have different colors. By the $4$-color theorem, every drawing of a bridgeless graph has a face-$4$-coloring. A drawing of a graph is facially $2$-colorable if and only if the underlying graph is Eulerian. We show that every graph without degree 1 vertices admits a $3$-colorable drawing. This leads to the natural question which graphs $G$ have the property that each of its drawings has a $3$-coloring. We say that such a graph $G$ is facially $3$-colorable. We derive several sufficient and necessary conditions for this property: we show that every $4$-edge-connected graph and every graph admitting a nowhere-zero $3$-flow is facially $3$-colorable. We also discuss circumstances under which facial $3$-colorability guarantees the existence of a nowhere-zero $3$-flow. On the negative side, we present an infinite family of facially $3$-colorable graphs without a nowhere-zero $3$-flow. On the positive side, we formulate a conjecture which has a surprising relation to a famous open problem by Tutte known as the $3$-flow-conjecture. We prove our conjecture for subcubic and for $K_{3,3}$-minor-free graphs."}, "answer": " Here is a possible multi-form question based on the given text:\n\nQuestion: Which of the following statements about facial colorings of graph drawings is true?\n\nA) Every drawing of a bridgeless graph has a face-3-coloring.\n\nB) A drawing of a graph is facially 2-colorable if and only if the underlying graph has no degree 1 vertices. \n\nC) There exist infinite families of facially 3-colorable graphs without nowhere-zero 3-flows.\n\nD) Every 4-edge-connected graph is facially 3-colorable.\n\nCorrect Answer: C\n\nExplanation: \nA) is false, the text states that by the 4-color theorem, every drawing of a bridgeless graph has a face-4-coloring. \n\nB) is false, the text states that a drawing is facially 2-colorable if and only if the underlying graph is Eulerian.\n\nC) is true, the text presents an infinite family of facially 3-colorable graphs without nowhere-zero 3-flows.\n\nD) is false, the text states that every 4-edge-connected graph is facially 3-colorable."}, "43": {"documentation": {"title": "Very Sparse Stable Random Projections, Estimators and Tail Bounds for\n  Stable Random Projections", "source": "Ping Li", "docs_id": "cs/0611114", "section": ["cs.DS", "cs.IT", "cs.LG", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Very Sparse Stable Random Projections, Estimators and Tail Bounds for\n  Stable Random Projections. This paper will focus on three different aspects in improving the current practice of stable random projections. Firstly, we propose {\\em very sparse stable random projections} to significantly reduce the processing and storage cost, by replacing the $\\alpha$-stable distribution with a mixture of a symmetric $\\alpha$-Pareto distribution (with probability $\\beta$, $0<\\beta\\leq1$) and a point mass at the origin (with a probability $1-\\beta$). This leads to a significant $\\frac{1}{\\beta}$-fold speedup for small $\\beta$. Secondly, we provide an improved estimator for recovering the original $l_\\alpha$ norms from the projected data. The standard estimator is based on the (absolute) sample median, while we suggest using the geometric mean. The geometric mean estimator we propose is strictly unbiased and is easier to study. Moreover, the geometric mean estimator is more accurate, especially non-asymptotically. Thirdly, we provide an adequate answer to the basic question of how many projections (samples) are needed for achieving some pre-specified level of accuracy. \\cite{Proc:Indyk_FOCS00,Article:Indyk_TKDE03} did not provide a criterion that can be used in practice. The geometric mean estimator we propose allows us to derive sharp tail bounds which can be expressed in exponential forms with constants explicitly given."}, "answer": " Here is a multi-form question based on the given text:\n\nQuestion: Which of the following is NOT one of the three main aspects this paper focuses on in improving the current practice of stable random projections?\n\nA) Proposing very sparse stable random projections to reduce processing and storage costs. \n\nB) Providing an improved estimator for recovering original l_alpha norms from projected data.\n\nC) Giving tail bounds that can be expressed in exponential forms with explicit constants.\n\nD) Deriving closed-form solutions for the sample complexity of stable random projections.\n\nCorrect Answer: D\n\nExplanation: The text states the paper focuses on three aspects - very sparse stable random projections, an improved estimator using the geometric mean, and tail bounds expressed in exponential forms. Deriving closed-form sample complexity solutions is not mentioned as one of the main focuses."}, "44": {"documentation": {"title": "A Dynamical Model of Twitter Activity Profiles", "source": "Hoai Nguyen Huynh, Erika Fille Legara, Christopher Monterola", "docs_id": "1508.07097", "section": ["cs.SI", "cs.CY", "cs.HC", "physics.soc-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Dynamical Model of Twitter Activity Profiles. The advent of the era of Big Data has allowed many researchers to dig into various socio-technical systems, including social media platforms. In particular, these systems have provided them with certain verifiable means to look into certain aspects of human behavior. In this work, we are specifically interested in the behavior of individuals on social media platforms---how they handle the information they get, and how they share it. We look into Twitter to understand the dynamics behind the users' posting activities---tweets and retweets---zooming in on topics that peaked in popularity. Three mechanisms are considered: endogenous stimuli, exogenous stimuli, and a mechanism that dictates the decay of interest of the population in a topic. We propose a model involving two parameters $\\eta^\\star$ and $\\lambda$ describing the tweeting behaviour of users, which allow us to reconstruct the findings of Lehmann et al. (2012) on the temporal profiles of popular Twitter hashtags. With this model, we are able to accurately reproduce the temporal profile of user engagements on Twitter. Furthermore, we introduce an alternative in classifying the collective activities on the socio-technical system based on the model."}, "answer": " Here is a multi-form question based on the given text:\n\nQuestion: Which of the following statements accurately describes a key contribution of the paper?\n\nA) The paper proposes a model with parameters \u03b7^\u22c6 and \u03bb that can reproduce temporal profiles of Twitter hashtag popularity. \n\nB) The paper classifies Twitter users into different types based on their tweeting and retweeting behavior.\n\nC) The paper finds that endogenous stimuli are the main driver of Twitter activity around popular hashtags.\n\nD) The paper shows that interest in topics on Twitter decays exponentially over time according to a fixed rate.\n\nCorrect Answer: A\n\nThe paper introduces a model involving parameters \u03b7^\u22c6 and \u03bb that can reproduce temporal profiles of Twitter hashtag engagement, as mentioned in the given summary. Options B, C, and D are not accurate descriptions of key contributions made in the paper."}, "45": {"documentation": {"title": "Mixed-Resolution Image Representation and Compression with Convolutional\n  Neural Networks", "source": "Lijun Zhao, Huihui Bai, Feng Li, Anhong Wang and Yao Zhao", "docs_id": "1802.01447", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Mixed-Resolution Image Representation and Compression with Convolutional\n  Neural Networks. In this paper, we propose an end-to-end mixed-resolution image compression framework with convolutional neural networks. Firstly, given one input image, feature description neural network (FDNN) is used to generate a new representation of this image, so that this image representation can be more efficiently compressed by standard codec, as compared to the input image. Furthermore, we use post-processing neural network (PPNN) to remove the coding artifacts caused by quantization of codec. Secondly, low-resolution image representation is adopted for high efficiency compression in terms of most of bit spent by image's structures under low bit-rate. However, more bits should be assigned to image details in the high-resolution, when most of structures have been kept after compression at the high bit-rate. This comes from a fact that the low-resolution image representation can't burden more information than high-resolution representation beyond a certain bit-rate. Finally, to resolve the problem of error back-propagation from the PPNN network to the FDNN network, we introduce to learn a virtual codec neural network to imitate two continuous procedures of standard compression and post-processing. The objective experimental results have demonstrated the proposed method has a large margin improvement, when comparing with several state-of-the-art approaches."}, "answer": " Here is a 4-option multiple choice question on the paper:\n\nQuestion: Which of the following statements is true about the proposed mixed-resolution image compression framework in the paper?\n\nA) It uses a feature description neural network (FDNN) to compress images more efficiently than standard codecs. \n\nB) It introduces a virtual codec neural network to avoid error backpropagation from the post-processing network to the FDNN.\n\nC) Low-resolution image representation is better than high-resolution at low bitrates, and vice versa at high bitrates.\n\nD) The framework focuses only on post-processing to remove coding artifacts from standard codecs.\n\nCorrect Answer: B\n\nExplanation: \nOption B is correct. The paper proposes using a virtual codec network to imitate compression and post-processing, avoiding error backpropagation between the networks.\n\nOption A is incorrect. FDNN generates an efficient image representation, but standard codecs are still used for compression. \n\nOption C is incorrect. The framework uses low-resolution representations at low bitrates and high-resolution ones at high bitrates.\n\nOption D is incorrect. The framework also focuses on generating efficient image representations with FDNN before compression."}, "46": {"documentation": {"title": "Beamforming Design for Intelligent Reflecting Surface-Enhanced Symbiotic\n  Radio Systems", "source": "Shaokang Hu, Chang Liu, Zhiqiang Wei, Yuanxin Cai, Derrick Wing Kwan\n  Ng, and Jinhong Yuan", "docs_id": "2110.10316", "section": ["cs.IT", "eess.SP", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Beamforming Design for Intelligent Reflecting Surface-Enhanced Symbiotic\n  Radio Systems. This paper investigates multiuser multi-input single-output downlink symbiotic radio communication systems assisted by an intelligent reflecting surface (IRS). Different from existing methods ideally assuming the secondary user (SU) can jointly decode information symbols from both the access point (AP) and the IRS via multiuser detection, we consider a more practical SU that only non-coherent detection is available. To characterize the non-coherent decoding performance, a practical upper bound of the average symbol error rate (SER) is derived. Subsequently, we jointly optimize the beamformer at the AP and the phase shifts at the IRS to maximize the average sum-rate of the primary system taking into account the maximum tolerable SER constraint for the SU. To circumvent the couplings of variables, we exploit the Schur complement that facilitates the design of a suboptimal beamforming algorithm based on successive convex approximation. Our simulation results show that compared with various benchmark algorithms, the proposed scheme significantly improves the average sum-rate of the primary system, while guaranteeing the decoding performance of the secondary system."}, "answer": " Here is a 4-option multiple choice question on the paper:\n\nQuestion: Which of the following statements is true about the beamforming design proposed in the paper?\n\nA) The beamformer at the AP and phase shifts at the IRS are jointly optimized to maximize the average sum-rate of the primary system under the maximum tolerable SER constraint for the secondary user.\n\nB) The beamformer at the AP is optimized to maximize the average sum-rate of the primary system, while the phase shifts at the IRS are optimized to meet the maximum tolerable SER constraint for the secondary user. \n\nC) The beamformer at the AP is optimized to meet the maximum tolerable SER constraint for the secondary user, while the phase shifts at the IRS are optimized to maximize the average sum-rate of the primary system.\n\nD) The beamformer at the AP and phase shifts at the IRS are separately optimized to maximize the average sum-rate of the primary system and meet the maximum tolerable SER constraint for the secondary user, respectively.\n\nCorrect Answer: A\n\nExplanation: \nAccording to the paper, the beamformer at the AP and phase shifts at the IRS are jointly optimized to maximize the average sum-rate of the primary system under the maximum tolerable SER constraint for the secondary user. The couplings between the variables are handled using Schur complement and successive convex approximation."}, "47": {"documentation": {"title": "Exploring Temporal Information for Improved Video Understanding", "source": "Yi Zhu", "docs_id": "1905.10654", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Exploring Temporal Information for Improved Video Understanding. In this dissertation, I present my work towards exploring temporal information for better video understanding. Specifically, I have worked on two problems: action recognition and semantic segmentation. For action recognition, I have proposed a framework, termed hidden two-stream networks, to learn an optimal motion representation that does not require the computation of optical flow. My framework alleviates several challenges faced in video classification, such as learning motion representations, real-time inference, multi-framerate handling, generalizability to unseen actions, etc. For semantic segmentation, I have introduced a general framework that uses video prediction models to synthesize new training samples. By scaling up the training dataset, my trained models are more accurate and robust than previous models even without modifications to the network architectures or objective functions. I believe videos have much more potential to be mined, and temporal information is one of the most important cues for machines to perceive the visual world better."}, "answer": " Here is a 4-option multiple choice question with explanations:\n\nQuestion: Which of the following statements accurately summarizes a key contribution of the dissertation?\n\nA) The dissertation proposed a framework called hidden two-stream networks that computes optical flow for improved video classification.\n\nB) The dissertation introduced a general framework that uses video prediction models to synthesize new training samples for better semantic segmentation. \n\nC) The dissertation explored using temporal information for improved video understanding, proposing methods for both action recognition and semantic segmentation.\n\nD) The dissertation focused solely on semantic segmentation, using optical flow and video prediction to improve results.\n\nCorrect Answer: C\n\nExplanations:\n\nA) Incorrect. The dissertation proposed hidden two-stream networks for action recognition which does not require computing optical flow. \n\nB) Incorrect. The video prediction framework was used for improved semantic segmentation, not as the sole focus.\n\nC) Correct. The dissertation explored using temporal information for both action recognition (via hidden two-stream networks) and semantic segmentation (via video prediction and data augmentation).\n\nD) Incorrect. The dissertation covered both action recognition and semantic segmentation, not just segmentation. Optical flow was not used."}, "48": {"documentation": {"title": "In the Age of Web: Typed Functional-First Programming Revisited", "source": "Tomas Petricek (University of Cambridge), Don Syme (Microsoft\n  Research), Zach Bray (Type Inferred Ltd)", "docs_id": "1512.01896", "section": ["cs.PL", "cs.SE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "In the Age of Web: Typed Functional-First Programming Revisited. Most programming languages were designed before the age of web. This matters because the web changes many assumptions that typed functional language designers take for granted. For example, programs do not run in a closed world, but must instead interact with (changing and likely unreliable) services and data sources, communication is often asynchronous or event-driven, and programs need to interoperate with untyped environments. In this paper, we present how the F# language and libraries face the challenges posed by the web. Technically, this comprises using type providers for integration with external information sources and for integration with untyped programming environments, using lightweight meta-programming for targeting JavaScript and computation expressions for writing asynchronous code. In this inquiry, the holistic perspective is more important than each of the features in isolation. We use a practical case study as a starting point and look at how F# language and libraries approach the challenges posed by the web. The specific lessons learned are perhaps less interesting than our attempt to uncover hidden assumptions that no longer hold in the age of web."}, "answer": " Question: Which of the following is NOT mentioned in the document as an assumption that typed functional language designers take for granted, but is challenged in the age of web?\n\nA) Programs run in a closed world.\n\nB) Communication is often asynchronous or event-driven. \n\nC) Programs need to interoperate with strongly typed environments.\n\nD) Programs interact with changing and unreliable data sources.\n\nCorrect Answer: C\n\nExplanation: The document states that programs need to interoperate with untyped environments, not strongly typed environments. The other three answer choices are mentioned in the document as assumptions that no longer hold in the age of web."}, "49": {"documentation": {"title": "Design and Analysis of Distributed State Estimation Algorithms Based on\n  Belief Propagation and Applications in Smart Grids", "source": "Mirsad Cosovic", "docs_id": "1811.08355", "section": ["cs.IT", "cs.DC", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Design and Analysis of Distributed State Estimation Algorithms Based on\n  Belief Propagation and Applications in Smart Grids. We present a detailed study on application of factor graphs and the belief propagation (BP) algorithm to the power system state estimation (SE) problem. We start from the BP solution for the linear DC model, for which we provide a detailed convergence analysis. Using BP-based DC model we propose a fast real-time state estimator for the power system SE. The proposed estimator is easy to distribute and parallelize, thus alleviating computational limitations and allowing for processing measurements in real time. The presented algorithm may run as a continuous process. Using insights from the DC model, we use two different approaches to derive the BP algorithm for the non-linear model. The first method directly applies BP methodology, however, providing only approximate BP solution for the non-linear model. In the second approach, we make a key further step by providing the solution in which the BP is applied sequentially over the non-linear model, akin to what is done by the Gauss-Newton method. The resulting iterative Gauss-Newton belief propagation (GN-BP) algorithm can be interpreted as a distributed Gauss-Newton method with the same accuracy as the centralized SE."}, "answer": " Question: Which of the following statements is true about the belief propagation (BP) algorithm presented in the paper for power system state estimation (SE)?\n\nA) The BP solution for the linear DC model provides approximate convergence analysis. \n\nB) The proposed BP-based DC model estimator cannot be distributed and parallelized.\n\nC) The iterative Gauss-Newton belief propagation (GN-BP) algorithm has the same accuracy as centralized SE.\n\nD) The BP methodology is directly applied to the non-linear model to provide the exact BP solution.\n\nCorrect Answer: C\n\nThe paper presents the detailed convergence analysis for the BP solution of the linear DC model. The proposed BP-based DC model estimator is easy to distribute and parallelize. The GN-BP algorithm applies BP sequentially over the non-linear model like the Gauss-Newton method, and has the same accuracy as centralized SE. The BP methodology only provides an approximate solution for the non-linear model."}, "50": {"documentation": {"title": "Investigations of Process Damping Forces in Metal Cutting", "source": "Emily Stone, Suhail Ahmed, Abe Askari and Hong Tat", "docs_id": "cs/0508102", "section": ["cs.CE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Investigations of Process Damping Forces in Metal Cutting. Using finite element software developed for metal cutting by Third Wave Systems we investigate the forces involved in chatter, a self-sustained oscillation of the cutting tool. The phenomena is decomposed into a vibrating tool cutting a flat surface work piece, and motionless tool cutting a work piece with a wavy surface. While cutting the wavy surface, the shearplane was seen to oscillate in advance of the oscillation of the depth of cut, as were the cutting, thrust, and shear plane forces. The vibrating tool was used to investigate process damping through the interaction of the relief face of the tool and the workpiece. Crushing forces are isolated and compared to the contact length between the tool and workpiece. We found that the wavelength dependence of the forces depended on the relative size of the wavelength to the length of the relief face of the tool. The results indicate that the damping force from crushing will be proportional to the cutting speed for short tools, and inversely proportional for long tools."}, "answer": " Here is a 4-option multiple choice question on the paper's investigation of process damping forces in metal cutting:\n\nQuestion: According to the paper, what was observed when cutting the workpiece with a wavy surface using a motionless tool?\n\nA) The shearplane oscillated after the depth of cut oscillated.\n\nB) The cutting, thrust and shear plane forces remained constant. \n\nC) The shearplane oscillated in advance of the oscillation of the depth of cut, as did the cutting, thrust, and shear plane forces.\n\nD) The wavelength dependence of the forces was proportional to the cutting speed.\n\nCorrect Answer: C\n\nExplanation: The paper states that when cutting the wavy surface with a motionless tool, \"the shearplane was seen to oscillate in advance of the oscillation of the depth of cut, as were the cutting, thrust, and shear plane forces.\" Therefore option C is correct."}, "51": {"documentation": {"title": "Parametrized Invariance for Infinite State Processes", "source": "Alejandro S\\'anchez, C\\'esar S\\'anchez", "docs_id": "1312.4043", "section": ["cs.LO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Parametrized Invariance for Infinite State Processes. We study the uniform verification problem for infinite state processes, which consists of proving that the parallel composition of an arbitrary number of processes satisfies a temporal property. Our practical motivation is to build a general framework for the temporal verification of concurrent datatypes. The contribution of this paper is a general method for the verification of safety properties of parametrized programs that manipulate complex local and global data, including mutable state in the heap. This method is based on the separation between two concerns: (1) the interaction between executing threads---handled by novel parametrized invariance rules---,and the data being manipulated---handled by specialized decision procedures. The proof rules discharge automatically a finite collection of verification conditions (VCs), the number depending only on the size of the program description and the specification, but not on the number of processes in any given instance or on the kind of data manipulated. Moreover, all VCs are quantifier free, which eases the development of decision procedures for complex data-types on top of off-the-shelf SMT solvers. We discuss the practical verification (of shape and also functional correctness properties) of a concurrent list implementation based on the method presented in this paper. Our tool also all VCs using a decision procedure for a theory of list layouts in the heap built on top of state-of-the-art SMT solvers."}, "answer": " Here is a 4-option multiple choice question on the paper:\n\nQuestion: Which of the following is NOT a key contribution claimed in the paper?\n\nA) A general method for verifying safety properties of parametrized programs that manipulate complex data\n\nB) Novel parametrized invariance rules for handling thread interactions \n\nC) Specialized decision procedures for handling data manipulation\n\nD) Techniques to verify liveness properties of concurrent programs\n\nCorrect Answer: D\n\nExplanation: \nThe paper focuses on verifying safety properties of parametrized concurrent programs. It contributes a general verification method based on parametrized invariance rules for thread interactions and specialized decision procedures for data manipulation. Verifying liveness properties is not discussed or claimed as a contribution in the paper."}, "52": {"documentation": {"title": "An expressive dissimilarity measure for relational clustering using\n  neighbourhood trees", "source": "Sebastijan Dumancic and Hendrik Blockeel", "docs_id": "1604.08934", "section": ["stat.ML", "cs.AI", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "An expressive dissimilarity measure for relational clustering using\n  neighbourhood trees. Clustering is an underspecified task: there are no universal criteria for what makes a good clustering. This is especially true for relational data, where similarity can be based on the features of individuals, the relationships between them, or a mix of both. Existing methods for relational clustering have strong and often implicit biases in this respect. In this paper, we introduce a novel similarity measure for relational data. It is the first measure to incorporate a wide variety of types of similarity, including similarity of attributes, similarity of relational context, and proximity in a hypergraph. We experimentally evaluate how using this similarity affects the quality of clustering on very different types of datasets. The experiments demonstrate that (a) using this similarity in standard clustering methods consistently gives good results, whereas other measures work well only on datasets that match their bias; and (b) on most datasets, the novel similarity outperforms even the best among the existing ones."}, "answer": " Question: What is the main contribution of the paper \"An expressive dissimilarity measure for relational clustering using neighbourhood trees\"?\n\nA) Introducing a new clustering algorithm that works well on relational data\n\nB) Showing that existing relational clustering algorithms have strong implicit biases\n\nC) Proposing a novel similarity measure that incorporates various types of similarities in relational data\n\nD) Demonstrating that standard clustering methods work well when using the new similarity measure\n\nCorrect Answer: C\n\nExplanation: The main contribution of the paper is introducing a new similarity measure that can capture different types of similarities in relational data, including attribute similarity, relational context similarity and hypergraph proximity. The experiments then demonstrate the effectiveness of using this measure compared to existing biased measures."}, "53": {"documentation": {"title": "An Interference-Free Filter-Bank Multicarrier System Applicable for MIMO\n  Channels", "source": "Mohammad Towliat", "docs_id": "2006.03758", "section": ["eess.SP", "cs.SY", "eess.SY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "An Interference-Free Filter-Bank Multicarrier System Applicable for MIMO\n  Channels. In filter-bank multicarrier (FBMC) systems the intrinsic interference is the major challenge to achieve a full gain of diversity over multi-input multi-output (MIMO) channels. In this paper, we develop a novel multicarrier system called FBMC offset upper-lower polyphase network (FBMC/OULP) in which, to eliminate the intrinsic interference, the complex-valued symbols are alternatively transmitted via upper and lower half of polyphase network branches with an offset time. The symbol density of the FBMC/OULP system is equal to one complex-valued symbol in time-frequency lattice. Also, for transmission over frequency selective channels, a minimum mean square error (MMSE) estimator is employed at the receiver of the FBMC/OULP system to eliminate the interference caused by the frequency selectivity of the channel. The proposed scheme mitigates the produced interference between symbols in the upper and lower polyphase branches, based on the circular convolutional property. As a result of using complex-valued symbols and diminishing the interference, the full diversity gain of the orthogonal space-time block codes (OSTBC) can be achieved in MIMO channels by a low complex maximum likelihood (ML) detector. In comparison with the orthogonal frequency division multiplexing (OFDM) system, simulation results indicate that the proposed system achieves a superior performance in fast multi-path fading channels and a competitive performance in slow multi-path fading channels."}, "answer": " Question: In the FBMC/OULP system described in the paper, what technique is used to eliminate the intrinsic interference caused by the frequency selectivity of the channel?\n\nA) Using complex-valued symbols transmitted through the upper and lower polyphase network branches\n\nB) Employing a minimum mean square error (MMSE) estimator at the receiver \n\nC) Mitigating interference between symbols in the upper and lower polyphase branches through circular convolution\n\nD) Using orthogonal frequency division multiplexing (OFDM)\n\nCorrect Answer: B) Employing a minimum mean square error (MMSE) estimator at the receiver\n\nExplanation: According to the paper, to eliminate the intrinsic interference caused by the frequency selectivity of the channel in the FBMC/OULP system, a minimum mean square error (MMSE) estimator is employed at the receiver. The other options describe techniques used in the system to mitigate other sources of interference, but not the intrinsic interference caused specifically by frequency selective channels."}, "54": {"documentation": {"title": "Hybrid MPI-OpenMP Paradigm on SMP Clusters: MPEG-2 Encoder and N-Body\n  Simulation", "source": "Truong Vinh Truong Duy, Katsuhiro Yamazaki, Kosai Ikegami, and Shigeru\n  Oyanagi", "docs_id": "1211.2292", "section": ["cs.DC", "cs.CE", "cs.PF"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Hybrid MPI-OpenMP Paradigm on SMP Clusters: MPEG-2 Encoder and N-Body\n  Simulation. Clusters of SMP nodes provide support for a wide diversity of parallel programming paradigms. Combining both shared memory and message passing parallelizations within the same application, the hybrid MPI-OpenMP paradigm is an emerging trend for parallel programming to fully exploit distributed shared-memory architecture. In this paper, we improve the performance of MPEG-2 encoder and n-body simulation by employing the hybrid MPI-OpenMP programming paradigm on SMP clusters. The hierarchical image data structure of the MPEG bit-stream is eminently suitable for the hybrid model to achieve multiple levels of parallelism: MPI for parallelism at the group of pictures level across SMP nodes and OpenMP for parallelism within pictures at the slice level within each SMP node. Similarly, the work load of the force calculation which accounts for upwards of 90% of the cycles in typical computations in the n-body simulation is shared among OpenMP threads after ORB domain decomposition among MPI processes. Besides, loop scheduling of OpenMP threads is adopted with appropriate chunk size to provide better load balance of work, leading to enhanced performance. With the n-body simulation, experimental results demonstrate that the hybrid MPI-OpenMP program outperforms the corresponding pure MPI program by average factors of 1.52 on a 4-way cluster and 1.21 on a 2-way cluster. Likewise, the hybrid model offers a performance improvement of 18% compared to the MPI model for the MPEG-2 encoder."}, "answer": " Here is a multi-form question on the given text:\n\nQuestion: Which of the following statements is true about the hybrid MPI-OpenMP programming paradigm discussed in the text?\n\nA) It uses MPI for parallelism across nodes and OpenMP for parallelism within nodes to exploit distributed shared-memory architectures. \n\nB) It only uses MPI for inter-node parallelism and does not leverage shared-memory parallelism within nodes.\n\nC) It only uses OpenMP for shared-memory parallelism within nodes and does not use MPI across nodes. \n\nD) It is less efficient than using only MPI across all nodes.\n\nCorrect Answer: A\n\nExplanation: The text states that the hybrid MPI-OpenMP paradigm uses MPI for parallelism across nodes and OpenMP for parallelism within nodes to fully exploit distributed shared-memory architectures with SMP clusters. So option A is correct. Options B and C are incorrect as the hybrid model uses both MPI and OpenMP. Option D is incorrect as the results show the hybrid model improves performance over pure MPI."}, "55": {"documentation": {"title": "Submodular Order Functions and Assortment Optimization", "source": "Rajan Udwani", "docs_id": "2107.02743", "section": ["math.OC", "cs.DS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Submodular Order Functions and Assortment Optimization. We define a new class of set functions that in addition to being monotone and subadditive, also admit a very limited form of submodularity defined over a permutation of the ground set. We refer to this permutation as a submodular order. This class of functions includes monotone submodular functions as a sub-family. To understand the importance of this structure in optimization problems we consider the problem of maximizing function value under various types of constraints. To demonstrate the modeling power of submodular order functions we show applications in two different settings. First, we apply our results to the extensively studied problem of assortment optimization. While the objectives in assortment optimization are known to be non-submodular (and non-monotone) even for simple choice models, we show that they are compatible with the notion of submodular order. Consequently, we obtain new and in some cases the first constant factor guarantee for constrained assortment optimization in fundamental choice models. As a second application of submodular order functions, we show an intriguing connection to the maximization of monotone submodular functions in the streaming model. We recover some best known guarantees for this problem as a corollary of our results."}, "answer": " Here is a multi-form question on the document:\n\nQuestion: Which of the following statements is true about submodular order functions according to the document?\n\nA) Submodular order functions are a subclass of monotone submodular functions that admit a limited form of submodularity over a permutation of the ground set.\n\nB) Submodular order functions are more general than monotone submodular functions and always satisfy submodularity over any permutation of the ground set. \n\nC) Submodular order functions satisfy full submodularity, unlike monotone submodular functions which only satisfy limited submodularity.\n\nD) Submodular order functions do not include monotone submodular functions as a sub-family.\n\nCorrect Answer: A\n\nExplanation: According to the document, submodular order functions are monotone and subadditive functions that admit a limited form of submodularity defined over a permutation of the ground set, referred to as a submodular order. The document states that this class includes monotone submodular functions as a sub-family."}, "56": {"documentation": {"title": "dalex: Responsible Machine Learning with Interactive Explainability and\n  Fairness in Python", "source": "Hubert Baniecki, Wojciech Kretowicz, Piotr Piatyszek, Jakub\n  Wisniewski, Przemyslaw Biecek", "docs_id": "2012.14406", "section": ["cs.LG", "cs.HC", "cs.SE", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "dalex: Responsible Machine Learning with Interactive Explainability and\n  Fairness in Python. The increasing amount of available data, computing power, and the constant pursuit for higher performance results in the growing complexity of predictive models. Their black-box nature leads to opaqueness debt phenomenon inflicting increased risks of discrimination, lack of reproducibility, and deflated performance due to data drift. To manage these risks, good MLOps practices ask for better validation of model performance and fairness, higher explainability, and continuous monitoring. The necessity of deeper model transparency appears not only from scientific and social domains, but also emerging laws and regulations on artificial intelligence. To facilitate the development of responsible machine learning models, we showcase dalex, a Python package which implements the model-agnostic interface for interactive model exploration. It adopts the design crafted through the development of various tools for responsible machine learning; thus, it aims at the unification of the existing solutions. This library's source code and documentation are available under open license at https://python.drwhy.ai/."}, "answer": " Question: Which Python package provides a model-agnostic interface for interactive model exploration to facilitate responsible machine learning?\n\nA) scikit-learn\nB) TensorFlow \nC) dalex\nD) PyTorch\n\nCorrect Answer: C) dalex\n\nExplanations:\n- The paper discusses dalex, a Python package that implements a model-agnostic interface for interactive model exploration to enable responsible machine learning practices like explainability and fairness.\n- It mentions that dalex adopts design principles from various tools for responsible ML and aims to unify existing solutions. \n- The abstract states that dalex is showcased as a Python package for interactive model exploration to facilitate development of responsible ML models.\n- So the correct answer is C) dalex. The other options are general ML packages that do not specifically focus on responsible ML practices."}, "57": {"documentation": {"title": "Variational Information Bottleneck for Effective Low-Resource\n  Fine-Tuning", "source": "Rabeeh Karimi Mahabadi, Yonatan Belinkov, James Henderson", "docs_id": "2106.05469", "section": ["cs.CL"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Variational Information Bottleneck for Effective Low-Resource\n  Fine-Tuning. While large-scale pretrained language models have obtained impressive results when fine-tuned on a wide variety of tasks, they still often suffer from overfitting in low-resource scenarios. Since such models are general-purpose feature extractors, many of these features are inevitably irrelevant for a given target task. We propose to use Variational Information Bottleneck (VIB) to suppress irrelevant features when fine-tuning on low-resource target tasks, and show that our method successfully reduces overfitting. Moreover, we show that our VIB model finds sentence representations that are more robust to biases in natural language inference datasets, and thereby obtains better generalization to out-of-domain datasets. Evaluation on seven low-resource datasets in different tasks shows that our method significantly improves transfer learning in low-resource scenarios, surpassing prior work. Moreover, it improves generalization on 13 out of 15 out-of-domain natural language inference benchmarks. Our code is publicly available in https://github.com/rabeehk/vibert."}, "answer": " Question: Which of the following statements is true about the Variational Information Bottleneck (VIB) method proposed in the paper?\n\nA) It uses a bottleneck to reduce the number of parameters in the pretrained language model during fine-tuning.\n\nB) It suppresses irrelevant features in the pretrained language model when fine-tuning on low-resource target tasks. \n\nC) It improves generalization by making the representations invariant to different domains.\n\nD) It modifies the architecture of the pretrained language model by adding more layers.\n\nCorrect Answer: B\n\nExplanation: According to the documentation, the VIB method suppresses irrelevant features in the pretrained language model when fine-tuning on low-resource target tasks in order to reduce overfitting. It does not change the model architecture or number of parameters. While it improves generalization to out-of-domain datasets, the main benefit is reducing overfitting by suppressing irrelevant features."}, "58": {"documentation": {"title": "Perceptual Losses for Real-Time Style Transfer and Super-Resolution", "source": "Justin Johnson, Alexandre Alahi, Li Fei-Fei", "docs_id": "1603.08155", "section": ["cs.CV", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Perceptual Losses for Real-Time Style Transfer and Super-Resolution. We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks using a \\emph{per-pixel} loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by defining and optimizing \\emph{perceptual} loss functions based on high-level features extracted from pretrained networks. We combine the benefits of both approaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results."}, "answer": " Question: Which of the following statements accurately summarizes a key contribution of the paper \"Perceptual Losses for Real-Time Style Transfer and Super-Resolution\"?\n\nA) The paper proposes using per-pixel loss functions for training feed-forward networks for image transformation tasks.\n\nB) The paper shows that optimization-based methods for style transfer give better qualitative results than feed-forward networks. \n\nC) The paper demonstrates that using perceptual losses allows training of fast feed-forward networks that give results comparable to slower optimization-based methods.\n\nD) The paper focuses on using perceptual losses for single-image super-resolution and shows this gives superior quantitative performance compared to per-pixel losses.\n\nCorrect Answer: C\n\nExplanation: The key contribution is using perceptual losses to train feed-forward networks for style transfer and super-resolution. This allows real-time performance comparable to slower optimization-based methods that use perceptual losses. The paper does not focus on quantitatively evaluating super-resolution or claim perceptual losses give superior quantitative performance."}, "59": {"documentation": {"title": "A Map of Science in Wikipedia", "source": "Puyu Yang and Giovanni Colavizza", "docs_id": "2110.13790", "section": ["cs.DL", "cs.CY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Map of Science in Wikipedia. In recent decades, the rapid growth of Internet adoption is offering opportunities for convenient and inexpensive access to scientific information. Wikipedia, one of the largest encyclopedias worldwide, has become a reference in this respect, and has attracted widespread attention from scholars. However, a clear understanding of the scientific sources underpinning Wikipedia's contents remains elusive. In this work, we explore Wikipedia's role in the public understanding of science from the perspective of its scientific sources. We rely on an open dataset of citations from Wikipedia, and use network analysis to map the relationship between Wikipedia articles and scientific journal articles. We find that most journal articles cited from Wikipedia belong to STEM fields, in particular biology and medicine ($47.6$\\% of citations; $46.1$\\% of cited articles). Furthermore, Wikipedia's biographies play an important role in connecting STEM fields with the humanities, in particular history. Our results provide valuable insights into the reliance of Wikipedia on scientific sources, and its role in interconnecting knowledge across different topics."}, "answer": " Here is a 4-option multiple choice question on the given text:\n\nQuestion: According to the text, what percentage of journal articles cited from Wikipedia belong to biology and medicine fields?\n\nA) 47.6% of citations; 46.1% of cited articles\nB) 46.1% of citations; 47.6% of cited articles  \nC) 50% of citations; 50% of cited articles\nD) 46.1% of citations and articles\n\nCorrect Answer: A) 47.6% of citations; 46.1% of cited articles\n\nExplanation: The text states \"We find that most journal articles cited from Wikipedia belong to STEM fields, in particular biology and medicine (47.6% of citations; 46.1% of cited articles).\" So the correct percentage is 47.6% of citations and 46.1% of cited articles belong to biology and medicine."}}