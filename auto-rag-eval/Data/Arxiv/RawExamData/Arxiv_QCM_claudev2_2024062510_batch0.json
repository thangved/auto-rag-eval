{"0": {"documentation": {"title": "Leveraging blur information for plenoptic camera calibration", "source": "Mathieu Labussi\\`ere, C\\'eline Teuli\\`ere, Fr\\'ed\\'eric Bernardin,\n  Omar Ait-Aider", "docs_id": "2111.05226", "section": ["eess.IV", "cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Leveraging blur information for plenoptic camera calibration. This paper presents a novel calibration algorithm for plenoptic cameras, especially the multi-focus configuration, where several types of micro-lenses are used, using raw images only. Current calibration methods rely on simplified projection models, use features from reconstructed images, or require separated calibrations for each type of micro-lens. In the multi-focus configuration, the same part of a scene will demonstrate different amounts of blur according to the micro-lens focal length. Usually, only micro-images with the smallest amount of blur are used. In order to exploit all available data, we propose to explicitly model the defocus blur in a new camera model with the help of our newly introduced Blur Aware Plenoptic (BAP) feature. First, it is used in a pre-calibration step that retrieves initial camera parameters, and second, to express a new cost function to be minimized in our single optimization process. Third, it is exploited to calibrate the relative blur between micro-images. It links the geometric blur, i.e., the blur circle, to the physical blur, i.e., the point spread function. Finally, we use the resulting blur profile to characterize the camera's depth of field. Quantitative evaluations in controlled environment on real-world data demonstrate the effectiveness of our calibrations."}, "answer": " Here is a 4-option multiple choice question on the paper:\n\nQuestion: Which of the following statements is true about the novel calibration algorithm for plenoptic cameras presented in the paper?\n\nA) It relies on features extracted from reconstructed images.\n\nB) It models the geometric blur but not the physical blur. \n\nC) It performs separate calibrations for each type of micro-lens.\n\nD) It exploits the defocus blur information through a new Blur Aware Plenoptic feature.\n\nCorrect Answer: D) It exploits the defocus blur information through a new Blur Aware Plenoptic feature.\n\nExplanation: The paper presents a calibration algorithm that explicitly models defocus blur using a new Blur Aware Plenoptic feature. This is in contrast to prior methods that use simplified models or reconstructed image features. The algorithm does not do separate calibrations and models both geometric and physical blur."}, "1": {"documentation": {"title": "A probabilistic deep learning approach to automate the interpretation of\n  multi-phase diffraction spectra", "source": "Nathan J. Szymanski, Christopher J. Bartel, Yan Zeng, Qingsong Tu,\n  Gerbrand Ceder", "docs_id": "2103.16664", "section": ["cond-mat.mtrl-sci", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A probabilistic deep learning approach to automate the interpretation of\n  multi-phase diffraction spectra. Autonomous synthesis and characterization of inorganic materials requires the automatic and accurate analysis of X-ray diffraction spectra. For this task, we designed a probabilistic deep learning algorithm to identify complex multi-phase mixtures. At the core of this algorithm lies an ensemble convolutional neural network trained on simulated diffraction spectra, which are systematically augmented with physics-informed perturbations to account for artifacts that can arise during experimental sample preparation and synthesis. Larger perturbations associated with off-stoichiometry are also captured by supplementing the training set with hypothetical solid solutions. Spectra containing mixtures of materials are analyzed with a newly developed branching algorithm that utilizes the probabilistic nature of the neural network to explore suspected mixtures and identify the set of phases that maximize confidence in the prediction. Our model is benchmarked on simulated and experimentally measured diffraction spectra, showing exceptional performance with accuracies exceeding those given by previously reported methods based on profile matching and deep learning. We envision that the algorithm presented here may be integrated in experimental workflows to facilitate the high-throughput and autonomous discovery of inorganic materials."}, "answer": " Here is a 4-option multiple choice question with explanations for the candidates:\n\nQuestion: Which of the following best describes the main contribution of the probabilistic deep learning approach presented in the paper?\n\nA) It uses physics-based simulations to generate training data for identifying multi-phase mixtures in X-ray diffraction spectra.\n\nB) It employs a convolutional neural network ensemble trained on augmented diffraction spectra data to automate interpretation. \n\nC) It utilizes a branching algorithm and confidence scores to explore and identify phase mixtures that maximize prediction accuracy.\n\nD) It benchmarks the model on simulated and experimental diffraction spectra and shows improved performance.\n\nCorrect Answer: C\n\nExplanations:\nA) Using simulations to generate training data is described in the paper, but is not the main contribution. \n\nB) The convolutional neural network ensemble is part of the approach, but not the key novelty.\n\nC) The branching algorithm utilizing the probabilistic predictions to explore and identify phase mixtures is the main new technique presented. \n\nD) Benchmarking on simulated and experimental data shows strong performance, but is not the core contribution."}, "2": {"documentation": {"title": "A Fast General Methodology for Information-Theoretically Optimal\n  Encodings of Graphs", "source": "Xin He, Ming-Yang Kao, Hsueh-I Lu", "docs_id": "cs/0101021", "section": ["cs.DS", "cs.GR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Fast General Methodology for Information-Theoretically Optimal\n  Encodings of Graphs. We propose a fast methodology for encoding graphs with information-theoretically minimum numbers of bits. Specifically, a graph with property pi is called a pi-graph. If pi satisfies certain properties, then an n-node m-edge pi-graph G can be encoded by a binary string X such that (1) G and X can be obtained from each other in O(n log n) time, and (2) X has at most beta(n)+o(beta(n)) bits for any continuous super-additive function beta(n) so that there are at most 2^{beta(n)+o(beta(n))} distinct n-node pi-graphs. The methodology is applicable to general classes of graphs; this paper focuses on planar graphs. Examples of such pi include all conjunctions over the following groups of properties: (1) G is a planar graph or a plane graph; (2) G is directed or undirected; (3) G is triangulated, triconnected, biconnected, merely connected, or not required to be connected; (4) the nodes of G are labeled with labels from {1, ..., ell_1} for ell_1 <= n; (5) the edges of G are labeled with labels from {1, ..., ell_2} for ell_2 <= m; and (6) each node (respectively, edge) of G has at most ell_3 = O(1) self-loops (respectively, ell_4 = O(1) multiple edges). Moreover, ell_3 and ell_4 are not required to be O(1) for the cases of pi being a plane triangulation. These examples are novel applications of small cycle separators of planar graphs and are the only nontrivial classes of graphs, other than rooted trees, with known polynomial-time information-theoretically optimal coding schemes."}, "answer": " Here is a multi-form question based on the given documentation:\n\nQuestion: According to the documentation, which of the following statements is true about the proposed fast methodology for encoding graphs?\n\nA) It can encode any graph with the minimum number of bits in O(n log n) time.\n\nB) It can encode certain classes of graphs with close to the minimum number of bits in O(n log n) time. \n\nC) It can only encode trees optimally in polynomial time.\n\nD) It cannot encode graphs with node or edge labels.\n\nCorrect Answer: B\n\nExplanation: The documentation states that the proposed methodology can encode certain classes of graphs, such as planar graphs, with close to the minimum number of bits in O(n log n) time. It does not claim to encode all graphs optimally, nor is it limited only to trees. The methodology can handle node and edge labels. Therefore, option B is correct."}, "3": {"documentation": {"title": "Characterizing and Demystifying the Implicit Convolution Algorithm on\n  Commercial Matrix-Multiplication Accelerators", "source": "Yangjie Zhou, Mengtian Yang, Cong Guo, Jingwen Leng, Yun Liang, Quan\n  Chen, Minyi Guo, Yuhao Zhu", "docs_id": "2110.03901", "section": ["cs.DC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Characterizing and Demystifying the Implicit Convolution Algorithm on\n  Commercial Matrix-Multiplication Accelerators. Many of today's deep neural network accelerators, e.g., Google's TPU and NVIDIA's tensor core, are built around accelerating the general matrix multiplication (i.e., GEMM). However, supporting convolution on GEMM-based accelerators is not trivial. The naive method explicitly lowers the convolution to GEMM, commonly known as im2col, which introduces significant performance and memory overhead. Existing implicit im2col algorithms require unscalable hardware and are inefficient in supporting important convolution variants such as strided convolution. In this paper, we propose a memory-efficient and hardware-friendly implicit im2col algorithm used by Google's TPU, which dynamically converts a convolution into a GEMM with practically zero performance and memory overhead, fully unleashing the power of GEMM engines. Through comprehensive experimental results, we quantitatively argue that this algorithm has been adopted in commercial closed-source platforms, and we are the first to describe its high-level idea and implementation details. Finally, we show that our algorithm can also be generally applied to Nvidia's Tensor Cores (TC), matching and out-performing the measured performance on TCs."}, "answer": " Question: Which of the following statements is true about the implicit convolution algorithm described in the paper?\n\nA) It requires unscalable hardware and is inefficient at supporting strided convolution.\n\nB) It explicitly converts convolution to GEMM using the im2col method, which has high overhead. \n\nC) It dynamically converts convolution to GEMM with low overhead, fully utilizing GEMM engines like TPUs and Tensor Cores.\n\nD) It only works on Google's TPU and cannot be applied to other platforms like Nvidia Tensor Cores.\n\nCorrect Answer: C\n\nExplanation: The paper states that the proposed implicit convolution algorithm efficiently converts convolution to GEMM with little overhead, fully utilizing GEMM accelerators like TPUs and can also be applied to Tensor Cores. It does not require unscalable hardware or have high overhead like im2col."}, "4": {"documentation": {"title": "Beyond the rich-club: Properties of networks related to the better\n  connected nodes", "source": "Raul J Mondragon", "docs_id": "1810.12328", "section": ["physics.soc-ph", "cs.SI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Beyond the rich-club: Properties of networks related to the better\n  connected nodes. Many of the structural characteristics of a network depend on the connectivity with and within the hubs. These dependencies can be related to the degree of a node and the number of links that a node shares with nodes of higher degree. In here we revise and present new results showing how to construct network ensembles which give a good approximation to the degree-degree correlations, and hence to the projections of this correlation like the assortativity coefficient or the average neighbours degree. We present a new bound for the structural cut--off degree based on the connectivity within the hubs. Also we show that the connections with and within the hubs can be used to define different networks cores. Two of these cores are related to the spectral properties and walks of length one and two which contain at least on hub node, and they are related to the eigenvector centrality. We introduce a new centrality measured based on the connectivity with the hubs. In addition, as the ensembles and cores are related by the connectivity of the hubs, we show several examples how changes in the hubs linkage effects the degree--degree correlations and core properties."}, "answer": " Here is a multi-form question on the document:\n\nQuestion: Which of the following statements about the connectivity of hubs in networks is supported by the document?\n\nA) The connectivity within hubs can be used to define network cores related to spectral properties.\n\nB) The average neighbor degree is unrelated to the degree-degree correlations in the network. \n\nC) The rich-club phenomenon is the only factor affecting network structure.\n\nD) Degree-degree correlations have no effect on network ensembles.\n\nCorrect Answer: A\n\nExplanation: The document states that \"the connections with and within the hubs can be used to define different networks cores. Two of these cores are related to the spectral properties and walks of length one and two which contain at least on hub node, and they are related to the eigenvector centrality.\" This supports option A. Options B, C, and D are incorrect interpretations of the document."}, "5": {"documentation": {"title": "Generalized Fast Decoding of Polar Codes", "source": "Carlo Condo and Valerio Bioglio and Ingmar Land", "docs_id": "1804.09508", "section": ["cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Generalized Fast Decoding of Polar Codes. Research on polar codes has been constantly gaining attention over the last decade, by academia and industry alike, thanks to their capacity-achieving error-correction performance and low-complexity decoding algorithms. Recently, they have been selected as one of the coding schemes in the $5^{th}$ generation wireless standard (5G). Over the years various polar code decoding algorithms, like SC-list (SCL), have been proposed to improve the mediocre performance of the successive cancellation (SC) decoding algorithm for finite code lengths; however, like SC, they suffer from long decoding latency. Fast decoding of polar codes tries to overcome this problem by identifying particular subcodes in the polar code and decoding them with efficient decoders. In this work, we introduce a generalized approach to fast decoding of polar codes to further reduce SC-based decoding latency. We propose three multi-node polar code subcodes whose identification patterns include most of the existing subcodes, extending them to SCL decoding, and allow to apply fast decoding to larger subsets of bits. Without any error-correction performance degradation, the proposed technique shows up to $23.6\\%$ and $29.2\\%$ decoding latency gain with respect to fast SC and SCL decoding algorithms, respectively, and up to $63.6\\%$ and $49.8\\%$ if a performance loss is accepted, whose amount depends on code and decoding algorithm parameters, along with the desired speedup."}, "answer": " Here is a multi-form question on the document:\n\nQuestion: Which of the following statements is true about the generalized fast decoding approach for polar codes proposed in the paper?\n\nA) It identifies new subcode patterns that include most existing subcode patterns, allowing faster decoding of larger subsets of bits.\n\nB) It focuses only on improving the error-correction performance compared to SC and SCL decoding algorithms. \n\nC) It reduces latency by 23.6% compared to SC decoding, but increases latency compared to SCL.\n\nD) It degrades error-correction performance in order to achieve faster decoding.\n\nCorrect Answer: A\n\nExplanation: The paper proposes a generalized fast decoding approach that identifies new multi-node subcode patterns that include most existing subcodes. This allows applying fast decoding to larger subsets of bits without degrading error-correction performance. While it can trade off some performance for higher speedup, the approach itself does not inherently degrade performance."}, "6": {"documentation": {"title": "Optimal Filtering of Malicious IP Sources", "source": "Fabio Soldo, Athina Markopoulou, Katerina Argyraki", "docs_id": "0811.3828", "section": ["cs.NI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Optimal Filtering of Malicious IP Sources. How can we protect the network infrastructure from malicious traffic, such as scanning, malicious code propagation, and distributed denial-of-service (DDoS) attacks? One mechanism for blocking malicious traffic is filtering: access control lists (ACLs) can selectively block traffic based on fields of the IP header. Filters (ACLs) are already available in the routers today but are a scarce resource because they are stored in the expensive ternary content addressable memory (TCAM). In this paper, we develop, for the first time, a framework for studying filter selection as a resource allocation problem. Within this framework, we study five practical cases of source address/prefix filtering, which correspond to different attack scenarios and operator's policies. We show that filter selection optimization leads to novel variations of the multidimensional knapsack problem and we design optimal, yet computationally efficient, algorithms to solve them. We also evaluate our approach using data from Dshield.org and demonstrate that it brings significant benefits in practice. Our set of algorithms is a building block that can be immediately used by operators and manufacturers to block malicious traffic in a cost-efficient way."}, "answer": " Here is a 4-option multiple choice question on the paper:\n\nQuestion: Which of the following is NOT mentioned in the paper as a mechanism for blocking malicious traffic?\n\nA) Access control lists that selectively filter traffic based on IP header fields\nB) Anomaly detection systems that identify deviations from normal traffic patterns  \nC) Rate limiting traffic from suspicious IP addresses\nD) Encrypting network traffic to prevent inspection\n\nCorrect Answer: D) Encrypting network traffic to prevent inspection\n\nExplanations:\nA) The paper mentions access control lists (ACLs) that can selectively block traffic based on IP header fields as a filtering mechanism.\nB) The paper focuses on optimal filter selection and does not discuss anomaly detection systems.\nC) Rate limiting suspicious IP addresses is a viable approach, but not directly mentioned in the summary.  \nD) Encrypting network traffic is not discussed in the paper summary as a mechanism for blocking malicious traffic. The paper focuses on filter selection for ACLs."}, "7": {"documentation": {"title": "Sampling and Estimation for (Sparse) Exchangeable Graphs", "source": "Victor Veitch and Daniel M. Roy", "docs_id": "1611.00843", "section": ["math.ST", "cs.SI", "math.CO", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Sampling and Estimation for (Sparse) Exchangeable Graphs. Sparse exchangeable graphs on $\\mathbb{R}_+$, and the associated graphex framework for sparse graphs, generalize exchangeable graphs on $\\mathbb{N}$, and the associated graphon framework for dense graphs. We develop the graphex framework as a tool for statistical network analysis by identifying the sampling scheme that is naturally associated with the models of the framework, and by introducing a general consistent estimator for the parameter (the graphex) underlying these models. The sampling scheme is a modification of independent vertex sampling that throws away vertices that are isolated in the sampled subgraph. The estimator is a dilation of the empirical graphon estimator, which is known to be a consistent estimator for dense exchangeable graphs; both can be understood as graph analogues to the empirical distribution in the i.i.d. sequence setting. Our results may be viewed as a generalization of consistent estimation via the empirical graphon from the dense graph regime to also include sparse graphs."}, "answer": " Question: What is the framework that generalizes exchangeable graphs on N and allows for statistical analysis of sparse graphs?\n\nA) The graphex framework, which allows for a sampling scheme and estimator adapted to sparse exchangeable graphs on R+.\n\nB) The graphon framework, which allows for consistent estimation via the empirical graphon. \n\nC) The empirical distribution framework, which is the graph analogue of i.i.d. sequences.\n\nD) The dilation framework, which generalizes the empirical graphon estimator.\n\nCorrect Answer: A\n\nThe graphex framework generalizes exchangeable graphs on N to sparse exchangeable graphs on R+, allowing for statistical analysis through a modified sampling scheme and estimator adapted to sparse graphs. The graphon framework is for dense graphs, the empirical distribution analogy is for i.i.d. sequences, and the dilation framework modifies the empirical graphon estimator."}, "8": {"documentation": {"title": "A Learning and Masking Approach to Secure Learning", "source": "Linh Nguyen, Sky Wang, Arunesh Sinha", "docs_id": "1709.04447", "section": ["cs.CR", "cs.CV", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Learning and Masking Approach to Secure Learning. Deep Neural Networks (DNNs) have been shown to be vulnerable against adversarial examples, which are data points cleverly constructed to fool the classifier. Such attacks can be devastating in practice, especially as DNNs are being applied to ever increasing critical tasks like image recognition in autonomous driving. In this paper, we introduce a new perspective on the problem. We do so by first defining robustness of a classifier to adversarial exploitation. Next, we show that the problem of adversarial example generation can be posed as learning problem. We also categorize attacks in literature into high and low perturbation attacks; well-known attacks like fast-gradient sign method (FGSM) and our attack produce higher perturbation adversarial examples while the more potent but computationally inefficient Carlini-Wagner (CW) attack is low perturbation. Next, we show that the dual approach of the attack learning problem can be used as a defensive technique that is effective against high perturbation attacks. Finally, we show that a classifier masking method achieved by adding noise to the a neural network's logit output protects against low distortion attacks such as the CW attack. We also show that both our learning and masking defense can work simultaneously to protect against multiple attacks. We demonstrate the efficacy of our techniques by experimenting with the MNIST and CIFAR-10 datasets."}, "answer": " Here is a 4-option multiple choice question on the given Arxiv documentation:\n\nQuestion: Which of the following statements is true about adversarial examples and defenses according to the paper?\n\nA) Well-known attacks like FGSM produce low perturbation adversarial examples while CW attack produces high perturbation examples. A classifier masking method protects against FGSM.\n\nB) DNNs are vulnerable to adversarial examples which are data points crafted to fool the classifier. FGSM and the proposed attack generate high perturbation examples while CW attack generates low perturbation. \n\nC) The problem of generating adversarial examples can be posed as a learning problem. A masking defense protects against low perturbation CW attack while a learning defense protects against high perturbation attacks.\n\nD) CW attack generates high perturbation adversarial examples. A learning defense is effective against CW attack while a masking defense protects against FGSM.\n\nCorrect Answer: C\n\nExplanation: According to the paper, the problem of generating adversarial examples can be posed as a learning problem. FGSM and the proposed attack generate high perturbation examples while CW attack generates low perturbation ones. A masking defense added noise to protect against low perturbation CW attack while a learning defense is effective against high perturbation attacks like FGSM."}, "9": {"documentation": {"title": "Face Recognition Algorithms based on Transformed Shape Features", "source": "Sambhunath Biswas and Amrita Biswas", "docs_id": "1207.2537", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Face Recognition Algorithms based on Transformed Shape Features. Human face recognition is, indeed, a challenging task, especially under the illumination and pose variations. We examine in the present paper effectiveness of two simple algorithms using coiflet packet and Radon transforms to recognize human faces from some databases of still gray level images, under the environment of illumination and pose variations. Both the algorithms convert 2-D gray level training face images into their respective depth maps or physical shape which are subsequently transformed by Coiflet packet and Radon transforms to compute energy for feature extraction. Experiments show that such transformed shape features are robust to illumination and pose variations. With the features extracted, training classes are optimally separated through linear discriminant analysis (LDA), while classification for test face images is made through a k-NN classifier, based on L1 norm and Mahalanobis distance measures. Proposed algorithms are then tested on face images that differ in illumination,expression or pose separately, obtained from three databases,namely, ORL, Yale and Essex-Grimace databases. Results, so obtained, are compared with two different existing algorithms.Performance using Daubechies wavelets is also examined. It is seen that the proposed Coiflet packet and Radon transform based algorithms have significant performance, especially under different illumination conditions and pose variation. Comparison shows the proposed algorithms are superior."}, "answer": " Here is a 4-option multiple choice question on the paper:\n\nQuestion: Which of the following statements is true about the face recognition algorithms examined in the paper?\n\nA) The algorithms use Gaussian filters and Fourier transforms for feature extraction.\n\nB) The algorithms convert 2D images to depth maps and use wavelet and Radon transforms. \n\nC) The algorithms use neural networks for classification and achieve high accuracy.\n\nD) The algorithms work well under variations in illumination but not in pose.\n\nCorrect Answer: B\n\nExplanation: The paper examines two algorithms that convert 2D face images to depth maps and then apply Coiflet packet and Radon transforms to extract features. These features are robust to illumination and pose variations. The algorithms use LDA for separation and k-NN classifier for testing. Hence option B is correct."}, "10": {"documentation": {"title": "Modified second-order generalized integrators with modified frequency\n  locked loop for fast harmonics estimation of distorted single-phase signals\n  (LONG VERSION)", "source": "Christoph M. Hackl and Markus Landerer", "docs_id": "1902.04653", "section": ["cs.SY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Modified second-order generalized integrators with modified frequency\n  locked loop for fast harmonics estimation of distorted single-phase signals\n  (LONG VERSION). This paper proposes modified Second-Order Generalized Integrators (mSOGIs) for a fast estimation of all harmonic components of arbitrarily distorted single-phase signals such as voltages or currents in power systems. The estimation is based on the internal model principle leading to an overall observer system consisting of parallelized mSOGIs. The observer is tuned by pole placement. For a constant fundamental frequency, the observer is capable of estimating all harmonic components with prescribed settling time by choosing the observer poles appropriately. For time-varying fundamental frequencies, the harmonic estimation is combined with a modified Frequency Locked Loop (mFLL) with gain normalization, sign-correct anti-windup and rate limitation. The estimation performances of the proposed parallelized mSOGIs with and without mFLL are illustrated and validated by measurement results. The results are compared to standard approaches such as parallelized standard SOGIs (sSOGIs) and adaptive notch filters (ANFs)."}, "answer": " Question: What is the main contribution of the paper \"Modified second-order generalized integrators with modified frequency locked loop for fast harmonics estimation of distorted single-phase signals\"?\n\nA) Proposing a method for estimating the fundamental frequency of distorted single-phase signals using a modified frequency locked loop (mFLL)\n\nB) Developing modified second-order generalized integrators (mSOGIs) for estimating harmonic components of distorted single-phase signals \n\nC) Comparing the performance of parallelized mSOGIs with standard SOGIs and adaptive notch filters (ANFs)\n\nD) Using the internal model principle to design an observer system with parallelized mSOGIs and mFLL for fast harmonic estimation\n\nCorrect Answer: D\n\nThe main contribution of the paper is proposing an observer system consisting of parallelized mSOGIs combined with a mFLL for fast estimation of all harmonic components of distorted single-phase signals, based on the internal model principle."}, "11": {"documentation": {"title": "Differentiable Signal Processing With Black-Box Audio Effects", "source": "Marco A. Mart\\'inez Ram\\'irez, Oliver Wang, Paris Smaragdis, Nicholas\n  J. Bryan", "docs_id": "2105.04752", "section": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Differentiable Signal Processing With Black-Box Audio Effects. We present a data-driven approach to automate audio signal processing by incorporating stateful third-party, audio effects as layers within a deep neural network. We then train a deep encoder to analyze input audio and control effect parameters to perform the desired signal manipulation, requiring only input-target paired audio data as supervision. To train our network with non-differentiable black-box effects layers, we use a fast, parallel stochastic gradient approximation scheme within a standard auto differentiation graph, yielding efficient end-to-end backpropagation. We demonstrate the power of our approach with three separate automatic audio production applications: tube amplifier emulation, automatic removal of breaths and pops from voice recordings, and automatic music mastering. We validate our results with a subjective listening test, showing our approach not only can enable new automatic audio effects tasks, but can yield results comparable to a specialized, state-of-the-art commercial solution for music mastering."}, "answer": " Question: What approach does the paper present for automating audio signal processing by incorporating third-party audio effects as layers within a deep neural network?\n\nA) Using a convolutional neural network to directly emulate the audio effects.\n\nB) Training an encoder network to control the parameters of black-box audio effect layers given input-target audio pairs.\n\nC) Replacing hand-crafted audio effects with differentiable neural network layers. \n\nD) Using generative adversarial networks to learn the mappings between input and output audio.\n\nCorrect Answer: B) Training an encoder network to control the parameters of black-box audio effect layers given input-target audio pairs.\n\nExplanation: The paper presents a data-driven approach that trains a deep encoder to analyze input audio and control the parameters of third-party, non-differentiable audio effects layers within the network. It uses input-target audio pairs as supervision rather than directly modeling the effects."}, "12": {"documentation": {"title": "An Integrated Dynamic Method for Allocating Roles and Planning Tasks for\n  Mixed Human-Robot Teams", "source": "Fabio Fusaro (1 and 2), Edoardo Lamon (1), Elena De Momi (2), Arash\n  Ajoudani (1) ((1) Human-Robot Interfaces and physical Interaction, Istituto\n  Italiano di Tecnologia, Genoa, Italy, (2) Department of Electronics,\n  Information and Bioengineering, Politecnico di Milano Politecnico di Milano,\n  Milan, Italy)", "docs_id": "2105.12031", "section": ["cs.RO", "cs.AI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "An Integrated Dynamic Method for Allocating Roles and Planning Tasks for\n  Mixed Human-Robot Teams. This paper proposes a novel integrated dynamic method based on Behavior Trees for planning and allocating tasks in mixed human robot teams, suitable for manufacturing environments. The Behavior Tree formulation allows encoding a single job as a compound of different tasks with temporal and logic constraints. In this way, instead of the well-studied offline centralized optimization problem, the role allocation problem is solved with multiple simplified online optimization sub-problem, without complex and cross-schedule task dependencies. These sub-problems are defined as Mixed-Integer Linear Programs, that, according to the worker-actions related costs and the workers' availability, allocate the yet-to-execute tasks among the available workers. To characterize the behavior of the developed method, we opted to perform different simulation experiments in which the results of the action-worker allocation and computational complexity are evaluated. The obtained results, due to the nature of the algorithm and to the possibility of simulating the agents' behavior, should describe well also how the algorithm performs in real experiments."}, "answer": " Question: Which of the following statements best describes the key contribution of the paper \"An Integrated Dynamic Method for Allocating Roles and Planning Tasks for Mixed Human-Robot Teams\"?\n\nA) It proposes a new reinforcement learning algorithm for optimizing task allocation in human-robot teams.\n\nB) It develops a novel neural network architecture for modeling interactions between humans and robots. \n\nC) It presents an integrated dynamic method based on Behavior Trees for planning and allocating tasks in human-robot teams.\n\nD) It designs a decentralized multi-agent system for task coordination between humans and robots.\n\nCorrect Answer: C\n\nExplanation: The paper proposes a novel integrated dynamic method based on Behavior Trees to plan and allocate tasks between humans and robots in manufacturing environments. It formulates jobs as compounds of tasks with temporal/logic constraints and solves the allocation problem through simplified online optimization rather than complex offline optimization."}, "13": {"documentation": {"title": "Solving Target Set Selection with Bounded Thresholds Faster than $2^n$", "source": "Ivan Bliznets and Danil Sagunov", "docs_id": "1807.10789", "section": ["cs.DS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Solving Target Set Selection with Bounded Thresholds Faster than $2^n$. In this paper we consider the Target Set Selection problem. The problem naturally arises in many fields like economy, sociology, medicine. In the Target Set Selection problem one is given a graph $G$ with a function $\\operatorname{thr}: V(G) \\to \\mathbb{N} \\cup \\{0\\}$ and integers $k, \\ell$. The goal of the problem is to activate at most $k$ vertices initially so that at the end of the activation process there is at least $\\ell$ activated vertices. The activation process occurs in the following way: (i) once activated, a vertex stays activated forever; (ii) vertex $v$ becomes activated if at least $\\operatorname{thr}(v)$ of its neighbours are activated. The problem and its different special cases were extensively studied from approximation and parameterized points of view. For example, parameterizations by the following parameters were studied: treewidth, feedback vertex set, diameter, size of target set, vertex cover, cluster editing number and others. Despite the extensive study of the problem it is still unknown whether the problem can be solved in $\\mathcal{O}^*((2-\\epsilon)^n)$ time for some $\\epsilon >0$. We partially answer this question by presenting several faster-than-trivial algorithms that work in cases of constant thresholds, constant dual thresholds or when the threshold value of each vertex is bounded by one-third of its degree. Also, we show that the problem parameterized by $\\ell$ is W[1]-hard even when all thresholds are constant."}, "answer": " Here is a multi-form question on the Target Set Selection problem:\n\nQuestion: In the Target Set Selection problem with bounded thresholds, which of the following statements is true?\n\nA) The problem is solvable in O*(2^n) time even when thresholds are unbounded. \n\nB) The problem admits an O*(c^n) time algorithm for some c < 2 when thresholds are bounded by a constant.\n\nC) The problem is W[1]-hard parameterized by the target size l even when thresholds are bounded. \n\nD) The problem has a polynomial time approximation scheme when thresholds are bounded by vertex degrees.\n\nCorrect Answer: B\n\nExplanation: The paper shows that the Target Set Selection problem can be solved faster than O*(2^n) time when thresholds are bounded by constants. Specifically, it presents an O*(c^n) time algorithm for some c < 2 in this case. Option B is correct. Option A is incorrect since the O*(2^n) barrier applies to the general case. Option C is incorrect since W[1]-hardness holds even for constant thresholds. Option D is unrelated to the results in the paper."}, "14": {"documentation": {"title": "User Review-Based Change File Localization for Mobile Applications", "source": "Yu Zhou, Yanqi Su, Taolue Chen, Zhiqiu Huang, Harald Gall, Sebastiano\n  Panichella", "docs_id": "1903.00894", "section": ["cs.SE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "User Review-Based Change File Localization for Mobile Applications. In the current mobile app development, novel and emerging DevOps practices (e.g., Continuous Delivery, Integration, and user feedback analysis) and tools are becoming more widespread. For instance, the integration of user feedback (provided in the form of user reviews) in the software release cycle represents a valuable asset for the maintenance and evolution of mobile apps. To fully make use of these assets, it is highly desirable for developers to establish semantic links between the user reviews and the software artefacts to be changed (e.g., source code and documentation), and thus to localize the potential files to change for addressing the user feedback. In this paper, we propose RISING (Review Integration via claSsification, clusterIng, and linkiNG), an automated approach to support the continuous integration of user feedback via classification, clustering, and linking of user reviews. RISING leverages domain-specific constraint information and semi-supervised learning to group user reviews into multiple fine-grained clusters concerning similar users' requests. Then, by combining the textual information from both commit messages and source code, it automatically localizes potential change files to accommodate the users' requests. Our empirical studies demonstrate that the proposed approach outperforms the state-of-the-art baseline work in terms of clustering and localization accuracy, and thus produces more reliable results."}, "answer": " Here is a 4-option multiple choice question on the document:\n\nQuestion: Which of the following statements is true about the RISING approach proposed in the paper?\n\nA) It uses unsupervised learning to group user reviews into clusters.\n\nB) It relies solely on commit messages to localize files to change. \n\nC) It has lower clustering accuracy compared to baseline approaches.\n\nD) It leverages user-provided links between reviews and code.\n\nCorrect Answer: A\n\nExplanation: According to the document, RISING uses domain-specific constraint information and semi-supervised learning to group user reviews into fine-grained clusters. It does not rely only on commit messages for localization. The document also states that RISING outperforms baseline approaches in clustering accuracy. And there is no mention of RISING using user-provided links between reviews and code."}, "15": {"documentation": {"title": "TextRank Based Search Term Identification for Software Change Tasks", "source": "Mohammad Masudur Rahman and Chanchal K. Roy", "docs_id": "1807.02263", "section": ["cs.SE", "cs.IR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "TextRank Based Search Term Identification for Software Change Tasks. During maintenance, software developers deal with a number of software change requests. Each of those requests is generally written using natural language texts, and it involves one or more domain related concepts. A developer needs to map those concepts to exact source code locations within the project in order to implement the requested change. This mapping generally starts with a search within the project that requires one or more suitable search terms. Studies suggest that the developers often perform poorly in coming up with good search terms for a change task. In this paper, we propose and evaluate a novel TextRank-based technique that automatically identifies and suggests search terms for a software change task by analyzing its task description. Experiments with 349 change tasks from two subject systems and comparison with one of the latest and closely related state-of-the-art approaches show that our technique is highly promising in terms of suggestion accuracy, mean average precision and recall."}, "answer": " Question: During software maintenance, developers often struggle to identify good search terms when implementing requested changes. What technique does the paper propose and evaluate to help suggest search terms by analyzing the task description?\n\nA) A technique based on Latent Semantic Analysis that identifies key terms in the task description.\n\nB) A technique based on Word2Vec embeddings that finds similar terms to words in the task description. \n\nC) A technique based on TextRank that analyzes the task description and suggests relevant search terms.\n\nD) A technique based on supervised learning that trains a model to suggest search terms from task descriptions.\n\nCorrect Answer: C\n\nExplanation: The paper proposes and evaluates a TextRank-based technique that analyzes the task description to identify and suggest search terms for a software change task. TextRank is an unsupervised algorithm that builds a graph representation of the text and identifies important terms."}, "16": {"documentation": {"title": "DRST: Deep Residual Shearlet Transform for Densely Sampled Light Field\n  Reconstruction", "source": "Yuan Gao, Robert Bregovic, Reinhard Koch and Atanas Gotchev", "docs_id": "2003.08865", "section": ["cs.MM", "cs.CV", "eess.IV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "DRST: Deep Residual Shearlet Transform for Densely Sampled Light Field\n  Reconstruction. The Image-Based Rendering (IBR) approach using Shearlet Transform (ST) is one of the most effective methods for Densely-Sampled Light Field (DSLF) reconstruction. The ST-based DSLF reconstruction typically relies on an iterative thresholding algorithm for Epipolar-Plane Image (EPI) sparse regularization in shearlet domain, involving dozens of transformations between image domain and shearlet domain, which are in general time-consuming. To overcome this limitation, a novel learning-based ST approach, referred to as Deep Residual Shearlet Transform (DRST), is proposed in this paper. Specifically, for an input sparsely-sampled EPI, DRST employs a deep fully Convolutional Neural Network (CNN) to predict the residuals of the shearlet coefficients in shearlet domain in order to reconstruct a densely-sampled EPI in image domain. The DRST network is trained on synthetic Sparsely-Sampled Light Field (SSLF) data only by leveraging elaborately-designed masks. Experimental results on three challenging real-world light field evaluation datasets with varying moderate disparity ranges (8 - 16 pixels) demonstrate the superiority of the proposed learning-based DRST approach over the non-learning-based ST method for DSLF reconstruction. Moreover, DRST provides a 2.4x speedup over ST, at least."}, "answer": " Here is a multi-form question on the DRST paper:\n\nQuestion: Which of the following statements is true about the Deep Residual Shearlet Transform (DRST) approach for densely-sampled light field reconstruction?\n\nA) DRST uses a convolutional neural network to directly predict the dense shearlet coefficients from the sparsely-sampled input.\n\nB) DRST relies on dozens of iterative transformations between image and shearlet domains like traditional shearlet methods. \n\nC) DRST employs a CNN to predict the residuals of the shearlet coefficients from the input to reconstruct the image.\n\nD) DRST achieves a 10x speedup over traditional shearlet transform methods.\n\nCorrect Answer: C\n\nExplanation: According to the passage, DRST uses a CNN to predict the residuals of the shearlet coefficients from the sparsely-sampled input EPI in order to reconstruct the densely-sampled output EPI. It does not directly predict the dense shearlet coefficients (A is incorrect) and does not rely on iterative transformations like traditional shearlet methods (B is incorrect). The passage states it provides at least a 2.4x speedup over traditional shearlet methods, not 10x (D is incorrect)."}, "17": {"documentation": {"title": "Seizure Detection using Least EEG Channels by Deep Convolutional Neural\n  Network", "source": "Mustafa Talha Avcu, Zhuo Zhang, Derrick Wei Shih Chan", "docs_id": "1901.05305", "section": ["eess.SP", "cs.CV", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Seizure Detection using Least EEG Channels by Deep Convolutional Neural\n  Network. This work aims to develop an end-to-end solution for seizure onset detection. We design the SeizNet, a Convolutional Neural Network for seizure detection. To compare SeizNet with traditional machine learning approach, a baseline classifier is implemented using spectrum band power features with Support Vector Machines (BPsvm). We explore the possibility to use the least number of channels for accurate seizure detection by evaluating SeizNet and BPsvm approaches using all channels and two channels settings respectively. EEG Data is acquired from 29 pediatric patients admitted to KK Woman's and Children's Hospital who were diagnosed as typical absence seizures. We conduct leave-one-out cross validation for all subjects. Using full channel data, BPsvm yields a sensitivity of 86.6\\% and 0.84 false alarm (per hour) while SeizNet yields overall sensitivity of 95.8 \\% with 0.17 false alarm. More interestingly, two channels seizNet outperforms full channel BPsvm with a sensitivity of 93.3\\% and 0.58 false alarm. We further investigate interpretability of SeizNet by decoding the filters learned along convolutional layers. Seizure-like characteristics can be clearly observed in the filters from third and forth convolutional layers."}, "answer": " Question: What machine learning approach did the researchers implement as a baseline classifier using spectrum band power features for seizure detection?\n\nA) Convolutional Neural Network \nB) Support Vector Machines\nC) Random Forest\nD) k-Nearest Neighbors\n\nCorrect Answer: B) Support Vector Machines\n\nExplanation: The researchers implemented a baseline classifier using spectrum band power features with Support Vector Machines (BPsvm) to compare with their proposed SeizNet Convolutional Neural Network approach."}, "18": {"documentation": {"title": "Saliency Guided Self-attention Network for Weakly and Semi-supervised\n  Semantic Segmentation", "source": "Qi Yao, Xiaojin Gong", "docs_id": "1910.05475", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Saliency Guided Self-attention Network for Weakly and Semi-supervised\n  Semantic Segmentation. Weakly supervised semantic segmentation (WSSS) using only image-level labels can greatly reduce the annotation cost and therefore has attracted considerable research interest. However, its performance is still inferior to the fully supervised counterparts. To mitigate the performance gap, we propose a saliency guided self-attention network (SGAN) to address the WSSS problem. The introduced self-attention mechanism is able to capture rich and extensive contextual information but may mis-spread attentions to unexpected regions. In order to enable this mechanism to work effectively under weak supervision, we integrate class-agnostic saliency priors into the self-attention mechanism and utilize class-specific attention cues as an additional supervision for SGAN. Our SGAN is able to produce dense and accurate localization cues so that the segmentation performance is boosted. Moreover, by simply replacing the additional supervisions with partially labeled ground-truth, SGAN works effectively for semi-supervised semantic segmentation as well. Experiments on the PASCAL VOC 2012 and COCO datasets show that our approach outperforms all other state-of-the-art methods in both weakly and semi-supervised settings."}, "answer": " Question: What method is proposed in the paper to address the problem of weakly supervised semantic segmentation?\n\nA) A recurrent neural network with attention\nB) A convolutional neural network with conditional random fields \nC) A saliency guided self-attention network\nD) A fully convolutional network with multi-scale inputs\n\nCorrect Answer: C) A saliency guided self-attention network\n\nExplanation: The paper proposes a saliency guided self-attention network (SGAN) to address the problem of weakly supervised semantic segmentation. SGAN incorporates saliency priors into the self-attention mechanism to guide the model to focus on salient regions and utilize attention cues as additional supervision."}, "19": {"documentation": {"title": "It's Moving! A Probabilistic Model for Causal Motion Segmentation in\n  Moving Camera Videos", "source": "Pia Bideau, Erik Learned-Miller", "docs_id": "1604.00136", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "It's Moving! A Probabilistic Model for Causal Motion Segmentation in\n  Moving Camera Videos. The human ability to detect and segment moving objects works in the presence of multiple objects, complex background geometry, motion of the observer, and even camouflage. In addition to all of this, the ability to detect motion is nearly instantaneous. While there has been much recent progress in motion segmentation, it still appears we are far from human capabilities. In this work, we derive from first principles a new likelihood function for assessing the probability of an optical flow vector given the 3D motion direction of an object. This likelihood uses a novel combination of the angle and magnitude of the optical flow to maximize the information about the true motions of objects. Using this new likelihood and several innovations in initialization, we develop a motion segmentation algorithm that beats current state-of-the-art methods by a large margin. We compare to five state-of-the-art methods on two established benchmarks, and a third new data set of camouflaged animals, which we introduce to push motion segmentation to the next level."}, "answer": " Question: What does the new likelihood function developed in the paper assess?\n\nA) The probability of an optical flow vector given the 3D motion direction of an object.\n\nB) The probability of detecting camouflaged animals. \n\nC) The probability of segmenting background geometry.\n\nD) The probability of instantaneous human motion detection.\n\nCorrect Answer: A\n\nThe paper states \"In this work, we derive from first principles a new likelihood function for assessing the probability of an optical flow vector given the 3D motion direction of an object.\" The new likelihood function assesses the probability of an optical flow vector given the 3D motion direction of an object."}, "20": {"documentation": {"title": "Asymmetric Action Abstractions for Multi-Unit Control in Adversarial\n  Real-Time Games", "source": "Rubens O. Moraes and Levi H. S. Lelis", "docs_id": "1711.08101", "section": ["cs.AI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Asymmetric Action Abstractions for Multi-Unit Control in Adversarial\n  Real-Time Games. Action abstractions restrict the number of legal actions available during search in multi-unit real-time adversarial games, thus allowing algorithms to focus their search on a set of promising actions. Optimal strategies derived from un-abstracted spaces are guaranteed to be no worse than optimal strategies derived from action-abstracted spaces. In practice, however, due to real-time constraints and the state space size, one is only able to derive good strategies in un-abstracted spaces in small-scale games. In this paper we introduce search algorithms that use an action abstraction scheme we call asymmetric abstraction. Asymmetric abstractions retain the un-abstracted spaces' theoretical advantage over regularly abstracted spaces while still allowing the search algorithms to derive effective strategies, even in large-scale games. Empirical results on combat scenarios that arise in a real-time strategy game show that our search algorithms are able to substantially outperform state-of-the-art approaches."}, "answer": " Question: What is the main contribution of the paper \"Asymmetric Action Abstractions for Multi-Unit Control in Adversarial Real-Time Games\"?\n\nA) Introducing a new abstraction scheme called asymmetric abstraction that retains the theoretical benefits of unabstracted action spaces while allowing effective strategies to be derived in large games.\n\nB) Showing that optimal strategies from abstracted action spaces are guaranteed to be no worse than strategies from unabstracted action spaces. \n\nC) Empirically demonstrating that asymmetric abstraction outperforms state-of-the-art approaches on combat scenarios in real-time strategy games.\n\nD) Restricting the number of legal actions during search in multi-unit real-time adversarial games.\n\nCorrect Answer: C\n\nExplanation: The main contribution of the paper is empirically demonstrating that the proposed asymmetric abstraction scheme outperforms state-of-the-art approaches on combat scenarios in real-time strategy games. While the paper does introduce asymmetric abstraction and discuss its theoretical benefits compared to regular abstraction, the focus is on the empirical results showing improved performance over existing methods."}, "21": {"documentation": {"title": "A New Method of Construction of Permutation Trinomials with Coefficients\n  1", "source": "Hua Guo, Shuo Wang, Hutao Song, Xiyong Zhang, Jianwei Liu", "docs_id": "2112.14547", "section": ["math.NT", "cs.IT", "math.CO", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A New Method of Construction of Permutation Trinomials with Coefficients\n  1. Permutation polynomials over finite fields are an interesting and constantly active research subject of study for many years. They have important applications in areas of mathematics and engineering. In recent years, permutation binomials and permutation trinomials attract people's interests due to their simple algebraic forms. By reversely using Tu's method for the characterization of permutation polynomials with exponents of Niho type, we construct a class of permutation trinomials with coefficients 1 in this paper. As applications, two conjectures of [19] and a conjecture of [13] are all special cases of our result. To our knowledge, the construction method of permutation polynomials by polar decomposition in this paper is new. Moreover, we prove that in new class of permutation trinomials, there exists a permutation polynomial which is EA-inequivalent to known permutation polynomials for all m greater than or equal to 2. Also we give the explicit compositional inverses of the new permutation trinomials for a special case."}, "answer": " Here is a 4-option multiple choice question on the given research summary:\n\nQuestion: Which of the following statements is true about the new class of permutation trinomials constructed in the paper?\n\nA) They are obtained by using Tu's method for characterizing permutation polynomials with exponents of Niho type.\n\nB) For all m \u2265 2, they contain a permutation polynomial that is EA-inequivalent to known permutation polynomials. \n\nC) They have applications in mathematics but not in engineering.\n\nD) Their explicit compositional inverses are given for all cases.\n\nCorrect Answer: B\n\nExplanation: According to the given summary, the new class of permutation trinomials was constructed using a new polar decomposition method, not Tu's method. Also, it is stated that for m \u2265 2, the new class contains a permutation polynomial EA-inequivalent to known ones. The applications in both mathematics and engineering are mentioned. The explicit inverses are given only for a special case, not all cases. Hence, option B is the correct answer."}, "22": {"documentation": {"title": "Twitter for Sparking a Movement, Reddit for Sharing the Moment: #metoo\n  through the Lens of Social Media", "source": "Lydia Manikonda, Ghazaleh Beigi, Huan Liu, and Subbarao Kambhampati", "docs_id": "1803.08022", "section": ["cs.SI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Twitter for Sparking a Movement, Reddit for Sharing the Moment: #metoo\n  through the Lens of Social Media. Social media platforms are revolutionizing the way users communicate by increasing the exposure to highly stigmatized issues in the society. Sexual abuse is one such issue that recently took over social media via attaching the hashtag #metoo to the shared posts. Individuals with different backgrounds and ethnicities began sharing their unfortunate personal experiences of being assaulted. Through comparative analysis of the tweets via #meToo on Twitter versus the posts shared on the #meToo subreddit, this paper makes an initial attempt to assess public reactions and emotions. Though nearly equal ratios of negative and positive posts are shared on both platforms, Reddit posts are focused on the sexual assaults within families and workplaces while Twitter posts are on showing empathy and encouraging others to continue the #metoo movement. The data collected in this research and preliminary analysis demonstrate that users use various ways to share their experience, exchange ideas and encourage each other, and social media is suitable for groundswells such as #metoo movement."}, "answer": " Question: Which social media platform had more posts focused on sexual assaults within families and workplaces for the #metoo movement?\n\nA) Twitter\nB) Reddit \nC) Facebook\nD) Instagram\n\nCorrect Answer: B) Reddit\n\nExplanation: The document states that \"Reddit posts are focused on the sexual assaults within families and workplaces while Twitter posts are on showing empathy and encouraging others to continue the #metoo movement.\" This indicates that Reddit had more posts focused on sexual assaults within families and workplaces compared to Twitter for the #metoo movement."}, "23": {"documentation": {"title": "Application of Machine Learning in Rock Facies Classification with\n  Physics-Motivated Feature Augmentation", "source": "Jie Chen, Yu Zeng (Corresponding author)", "docs_id": "1808.09856", "section": ["stat.ML", "cs.LG", "physics.geo-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Application of Machine Learning in Rock Facies Classification with\n  Physics-Motivated Feature Augmentation. With recent progress in algorithms and the availability of massive amounts of computation power, application of machine learning techniques is becoming a hot topic in the oil and gas industry. One of the most promising aspects to apply machine learning to the upstream field is the rock facies classification in reservoir characterization, which is crucial in determining the net pay thickness of reservoirs, thus a definitive factor in drilling decision making process. For complex machine learning tasks like facies classification, feature engineering is often critical. This paper shows the inclusion of physics-motivated feature interaction in feature augmentation can further improve the capability of machine learning in rock facies classification. We demonstrate this approach with the SEG 2016 machine learning contest dataset and the top winning algorithms. The improvement is roboust and can be $\\sim5\\%$ better than current existing best F-1 score, where F-1 is an evaluation metric used to quantify average prediction accuracy."}, "answer": " Here is a 4-option multiple choice question on the document:\n\nQuestion: What metric was used to quantify the average prediction accuracy when evaluating the improvement from including physics-motivated feature interaction in feature augmentation for rock facies classification?\n\nA) Precision \nB) Recall\nC) F-1 score\nD) Accuracy\n\nCorrect Answer: C) F-1 score\n\nExplanation: The document states that \"The improvement is roboust and can be \u223c5% better than current existing best F-1 score, where F-1 is an evaluation metric used to quantify average prediction accuracy.\" So F-1 score was the metric used to evaluate the improvement in average prediction accuracy from including physics-motivated feature interaction."}, "24": {"documentation": {"title": "Safe Testing", "source": "Peter Gr\\\"unwald, Rianne de Heide, and Wouter Koolen", "docs_id": "1906.07801", "section": ["math.ST", "cs.IT", "cs.LG", "math.IT", "stat.ME", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Safe Testing. We develop the theory of hypothesis testing based on the E-value, a notion of evidence that, unlike the p-value, allows for effortlessly combining results from several studies in the common scenario where the decision to perform a new study may depend on previous outcomes. Tests based on E-values are safe, i.e. they preserve Type-I error guarantees, under such optional continuation. We define growth-rate optimality (GRO) as an analogue of power in an optional continuation context, and we show how to construct GRO E-variables for general testing problems with composite null and alternative, emphasizing models with nuisance parameters. GRO E-values take the form of Bayes factors with special priors. We illustrate the theory using several classic examples including a one-sample safe t-test (in which the right Haar prior turns out to be GRO) and the 2x2 contingency table (in which the GRO prior is different from standard priors). Sharing Fisherian, Neymanian and Jeffreys-Bayesian interpretations, E-values and the corresponding tests may provide a methodology acceptable to adherents of all three schools."}, "answer": " Here is a multi-form question on the given documentation:\n\nQuestion: Which of the following statements about E-values is true according to the documentation?\n\nA) E-values, unlike p-values, do not allow for combining results from multiple studies where the decision to do a new study depends on previous outcomes.\n\nB) Tests based on E-values preserve Type-I error guarantees under optional continuation. \n\nC) Growth-rate optimal E-variables are analogous to power in a frequentist context.\n\nD) E-values take the form of Bayes factors with arbitrary priors.\n\nCorrect Answer: B\n\nExplanation: The documentation states that \"Tests based on E-values are safe, i.e. they preserve Type-I error guarantees, under such optional continuation.\" Therefore, option B is correct. Options A, C, and D are incorrect interpretations of the documentation."}, "25": {"documentation": {"title": "Discovering Picturesque Highlights from Egocentric Vacation Videos", "source": "Vinay Bettadapura, Daniel Castro, Irfan Essa", "docs_id": "1601.04406", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Discovering Picturesque Highlights from Egocentric Vacation Videos. We present an approach for identifying picturesque highlights from large amounts of egocentric video data. Given a set of egocentric videos captured over the course of a vacation, our method analyzes the videos and looks for images that have good picturesque and artistic properties. We introduce novel techniques to automatically determine aesthetic features such as composition, symmetry and color vibrancy in egocentric videos and rank the video frames based on their photographic qualities to generate highlights. Our approach also uses contextual information such as GPS, when available, to assess the relative importance of each geographic location where the vacation videos were shot. Furthermore, we specifically leverage the properties of egocentric videos to improve our highlight detection. We demonstrate results on a new egocentric vacation dataset which includes 26.5 hours of videos taken over a 14 day vacation that spans many famous tourist destinations and also provide results from a user-study to access our results."}, "answer": " Here is a 4-option multiple choice question on the paper:\n\nQuestion: Which of the following is NOT mentioned in the paper as a technique used to automatically determine aesthetic features in egocentric videos?\n\nA) Assessing composition \n\nB) Evaluating color vibrancy\n\nC) Analyzing object recognition\n\nD) Measuring symmetry\n\nCorrect Answer: C\n\nExplanation: \nThe paper mentions using novel techniques to automatically determine aesthetic features such as composition, symmetry and color vibrancy in egocentric videos. However, it does not mention using object recognition as a technique for this purpose."}, "26": {"documentation": {"title": "Quantized VCG Mechanisms for Polymatroid Environments", "source": "Hao Ge, Randall Berry", "docs_id": "1904.11663", "section": ["cs.GT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Quantized VCG Mechanisms for Polymatroid Environments. Many network resource allocation problems can be viewed as allocating a divisible resource, where the allocations are constrained to lie in a polymatroid. We consider market-based mechanisms for such problems. Though the Vickrey-Clarke-Groves (VCG) mechanism can provide the efficient allocation with strong incentive properties (namely dominant strategy incentive compatibility), its well-known high communication requirements can prevent it from being used. There have been a number of approaches for reducing the communication costs of VCG by weakening its incentive properties. Here, instead we take a different approach of reducing communication costs via quantization while maintaining VCG's dominant strategy incentive properties. The cost for this approach is a loss in efficiency which we characterize. We first consider quantizing the resource allocations so that agents need only submit a finite number of bids instead of full utility function. We subsequently consider quantizing the agent's bids."}, "answer": " Here is a 4-option multiple choice question based on the given text:\n\nQuestion: Which of the following statements accurately describes a key contribution of the paper?\n\nA) The paper proposes a new mechanism that reduces communication costs while maintaining efficiency.\n\nB) The paper analyzes the loss in efficiency from quantizing allocations in the VCG mechanism. \n\nC) The paper shows how to implement the VCG mechanism with lower communication costs but weakened incentives.\n\nD) The paper introduces a novel bidding language that allows agents to submit compressed bids.\n\nCorrect Answer: B\n\nExplanation: The paper analyzes the loss in efficiency resulting from quantizing resource allocations in the VCG mechanism in order to reduce communication costs. While maintaining the dominant strategy incentives of VCG, efficiency is sacrificed. The other options do not accurately reflect the main contribution of the paper."}, "27": {"documentation": {"title": "Selfless Sequential Learning", "source": "Rahaf Aljundi, Marcus Rohrbach and Tinne Tuytelaars", "docs_id": "1806.05421", "section": ["stat.ML", "cs.AI", "cs.CV", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Selfless Sequential Learning. Sequential learning, also called lifelong learning, studies the problem of learning tasks in a sequence with access restricted to only the data of the current task. In this paper we look at a scenario with fixed model capacity, and postulate that the learning process should not be selfish, i.e. it should account for future tasks to be added and thus leave enough capacity for them. To achieve Selfless Sequential Learning we study different regularization strategies and activation functions. We find that imposing sparsity at the level of the representation (i.e.~neuron activations) is more beneficial for sequential learning than encouraging parameter sparsity. In particular, we propose a novel regularizer, that encourages representation sparsity by means of neural inhibition. It results in few active neurons which in turn leaves more free neurons to be utilized by upcoming tasks. As neural inhibition over an entire layer can be too drastic, especially for complex tasks requiring strong representations, our regularizer only inhibits other neurons in a local neighbourhood, inspired by lateral inhibition processes in the brain. We combine our novel regularizer, with state-of-the-art lifelong learning methods that penalize changes to important previously learned parts of the network. We show that our new regularizer leads to increased sparsity which translates in consistent performance improvement %over alternative regularizers we studied on diverse datasets."}, "answer": " Here is a 4-option multiple choice question on the given text:\n\nQuestion: Which of the following statements best describes the key idea behind selfless sequential learning as proposed in the paper?\n\nA) It uses a fixed model capacity and regularizes the network to leave capacity for future tasks by encouraging parameter sparsity.\n\nB) It accounts for future tasks by using a dynamically expanding model capacity as more tasks arrive. \n\nC) It leaves capacity for future tasks by encouraging sparsity in the learned representations via neural inhibition.\n\nD) It relies on penalizing changes to previously learned parts of the network to account for future tasks.\n\nCorrect Answer: C\n\nExplanation: The key idea behind selfless sequential learning is to account for future tasks in a fixed capacity model by encouraging sparsity in the learned representations, specifically via a proposed neural inhibition regularizer that locally inhibits neurons to leave more free neurons for future tasks."}, "28": {"documentation": {"title": "Two betweenness centrality measures based on Randomized Shortest Paths", "source": "Ilkka Kivim\\\"aki, Bertrand Lebichot, Jari Saram\\\"aki, Marco Saerens", "docs_id": "1509.03147", "section": ["cs.SI", "cs.DS", "physics.soc-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Two betweenness centrality measures based on Randomized Shortest Paths. This paper introduces two new closely related betweenness centrality measures based on the Randomized Shortest Paths (RSP) framework, which fill a gap between traditional network centrality measures based on shortest paths and more recent methods considering random walks or current flows. The framework defines Boltzmann probability distributions over paths of the network which focus on the shortest paths, but also take into account longer paths depending on an inverse temperature parameter. RSP's have previously proven to be useful in defining distance measures on networks. In this work we study their utility in quantifying the importance of the nodes of a network. The proposed RSP betweenness centralities combine, in an optimal way, the ideas of using the shortest and purely random paths for analysing the roles of network nodes, avoiding issues involving these two paradigms. We present the derivations of these measures and how they can be computed in an efficient way. In addition, we show with real world examples the potential of the RSP betweenness centralities in identifying interesting nodes of a network that more traditional methods might fail to notice."}, "answer": " Here is a 4-option multiple choice question on the key ideas from the given text:\n\nQuestion: Which of the following best describes how the new betweenness centrality measures introduced in the paper combine the ideas of using shortest paths and random walks to analyze node importance in a network?\n\nA) The measures use both shortest paths and random walks equally to quantify node betweenness.\n\nB) The measures only consider shortest paths and do not incorporate random walks. \n\nC) The measures are based on randomized shortest paths that balance between shortest paths and random walks through a temperature parameter.\n\nD) The measures first compute shortest path betweenness then add a random walk component to improve the centrality values.\n\nCorrect Answer: C\n\nExplanation: According to the text, the new betweenness centrality measures are based on the Randomized Shortest Paths framework, which defines Boltzmann probability distributions over paths that focus on shortest paths but also incorporate longer random walk-like paths based on a temperature parameter. So these measures optimally combine shortest paths and random walks."}, "29": {"documentation": {"title": "DeepProposal: Hunting Objects by Cascading Deep Convolutional Layers", "source": "Amir Ghodrati, Ali Diba, Marco Pedersoli, Tinne Tuytelaars, Luc Van\n  Gool", "docs_id": "1510.04445", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "DeepProposal: Hunting Objects by Cascading Deep Convolutional Layers. In this paper we evaluate the quality of the activation layers of a convolutional neural network (CNN) for the gen- eration of object proposals. We generate hypotheses in a sliding-window fashion over different activation layers and show that the final convolutional layers can find the object of interest with high recall but poor localization due to the coarseness of the feature maps. Instead, the first layers of the network can better localize the object of interest but with a reduced recall. Based on this observation we design a method for proposing object locations that is based on CNN features and that combines the best of both worlds. We build an inverse cascade that, going from the final to the initial convolutional layers of the CNN, selects the most promising object locations and refines their boxes in a coarse-to-fine manner. The method is efficient, because i) it uses the same features extracted for detection, ii) it aggregates features using integral images, and iii) it avoids a dense evaluation of the proposals due to the inverse coarse-to-fine cascade. The method is also accurate; it outperforms most of the previously proposed object proposals approaches and when plugged into a CNN-based detector produces state-of-the- art detection performance."}, "answer": " Question: Which of the following statements best summarizes the key findings and contributions of the DeepProposal paper?\n\nA) The paper shows that early convolutional layers in a CNN have high recall for finding objects of interest, but poor localization. Later layers have good localization but lower recall. The paper proposes an inverse cascade method that combines early and late layers to get both high recall and good localization.\n\nB) The paper introduces a new CNN architecture called DeepProposal that is optimized for generating object proposals. It outperforms previous proposal methods and improves detection when used in a detector.\n\nC) The paper evaluates different CNN activation layers for object proposal generation. It finds that proposals from early layers have high recall but poor localization, while late layers are opposite. This leads to a coarse-to-fine cascade method.\n\nD) The paper shows convolutional layers closer to the input image have better localization ability but lower recall than deeper layers. A new coarse-to-fine cascade method is proposed to leverage both early and late layers.\n\nCorrect Answer: D\n\nExplanation:\nD) summarizes the key findings that early convolutional layers have better localization but lower recall compared to later layers, which leads to proposing a coarse-to-fine cascade method to leverage both. A) and C) are close, but miss details about early vs late layer properties. B) describes contributions but not the key findings leading to them."}, "30": {"documentation": {"title": "Adaptive guaranteed-performance consensus design for high-order\n  multiagent systems", "source": "Jianxiang Xi, Jie Yang, Hao Liu, Tang Zheng", "docs_id": "1806.09757", "section": ["cs.MA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Adaptive guaranteed-performance consensus design for high-order\n  multiagent systems. The current paper addresses the distributed guaranteed-performance consensus design problems for general high-order linear multiagent systems with leaderless and leader-follower structures, respectively. The information about the Laplacian matrix of the interaction topology or its minimum nonzero eigenvalue is usually required in existing works on the guaranteed-performance consensus, which means that their conclusions are not completely distributed. A new translation-adaptive strategy is proposed to realize the completely distributed guaranteed-performance consensus control by using the structure feature of a complete graph in the current paper. For the leaderless case, an adaptive guaranteed-performance consensualization criterion is given in terms of Riccati inequalities and a regulation approach of the consensus control gain is presented by linear matrix inequalities. Extensions to the leader-follower cases are further investigated. Especially, the guaranteed-performance costs for leaderless and leader-follower cases are determined, respectively, which are associated with the intrinsic structure characteristic of the interaction topologies. Finally, two numerical examples are provided to demonstrate theoretical results."}, "answer": " Question: Which of the following statements best summarizes a key contribution of the paper \"Adaptive guaranteed-performance consensus design for high-order multiagent systems\"?\n\nA) The paper proposes a new translation-adaptive strategy to achieve distributed guaranteed-performance consensus control without needing information about the Laplacian matrix or its minimum eigenvalue.\n\nB) The paper gives an adaptive guaranteed-performance consensualization criterion in terms of Riccati inequalities and presents a way to regulate the consensus control gain using linear matrix inequalities. \n\nC) The paper investigates extensions to leader-follower multi-agent systems and determines the guaranteed-performance costs for leaderless and leader-follower cases.\n\nD) The paper addresses distributed guaranteed-performance consensus design problems for general high-order linear multi-agent systems with leaderless and leader-follower structures.\n\nCorrect Answer: A\n\nExplanation: The key contribution summarized in the documentation is that the paper proposes a new translation-adaptive strategy to achieve distributed guaranteed-performance consensus control without needing information about the Laplacian matrix or its minimum eigenvalue, which existing works require. This allows for completely distributed control."}, "31": {"documentation": {"title": "A secure key transfer protocol for group communication", "source": "R. Velumadhava Rao, K. Selvamani, R. Elakkiya", "docs_id": "1212.2720", "section": ["cs.CR", "cs.DC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A secure key transfer protocol for group communication. Providing security for messages in group communication is more essential and critical nowadays. In group oriented applications such as Video conferencing and entertainment applications, it is necessary to secure the confidential data in such a way that intruders are not able to modify or transmit the data. Key transfer protocols fully rely on trusted Key Generation Center (KGC) to compute group key and to transport the group keys to all communication parties in a secured and secret manner. In this paper, an efficient key generation and key transfer protocol has been proposed where KGC can broadcast group key information to all group members in a secure way. Hence, only authorized group members will be able to retrieve the secret key and unauthorized members cannot retrieve the secret key. Hence, inorder to maintain the forward and backward secrecy, the group keys are updated whenever a new member joins or leaves the communication group. The proposed algorithm is more efficient and relies on NP class. In addition, the keys are distributed to the group users in a safe and secure way. Moreover, the key generated is also very strong since it uses cryptographic techniques which provide efficient computation."}, "answer": " Here is a 4-option multiple choice question with explanations for the given text:\n\nQuestion: Which of the following statements is true about the proposed secure key transfer protocol discussed in the text?\n\nA) It relies on a trusted Key Generation Center to compute and distribute keys to all group members.\n\nB) It updates group keys whenever a member joins but not when a member leaves the group. \n\nC) It uses asymmetric cryptography to generate strong keys.\n\nD) It allows unauthorized members to also retrieve the secret group key.\n\nCorrect Answer: A\n\nExplanations: \nOption A is correct according to the text which states that the protocol \"fully rely on trusted Key Generation Center (KGC) to compute group key and to transport the group keys to all communication parties in a secured and secret manner.\"\n\nOption B is incorrect as the text mentions updating keys when members join or leave the group to maintain forward and backward secrecy. \n\nOption C is incorrect as the text states that the protocol uses cryptographic techniques to generate strong keys, not specifically asymmetric cryptography. \n\nOption D is contradicted by the text which says unauthorized members cannot retrieve the secret key."}, "32": {"documentation": {"title": "On Unimodality of Independence Polynomials of Trees", "source": "Ron Yosef, Matan Mizrachi and Ohr Kadrawi", "docs_id": "2101.06744", "section": ["cs.DM", "math.CO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On Unimodality of Independence Polynomials of Trees. An independent set in a graph is a set of pairwise non-adjacent vertices. The independence number $\\alpha{(G)}$ is the size of a maximum independent set in the graph $G$. The independence polynomial of a graph is the generating function for the sequence of numbers of independent sets of each size. In other words, the $k$-th coefficient of the independence polynomial equals the number of independent sets comprised of $k$ vertices. For instance, the degree of the independence polynomial of the graph $G$ is equal to $\\alpha{(G)}$. In 1987, Alavi, Malde, Schwenk, and Erd{\\\"o}s conjectured that the independence polynomial of a tree is unimodal. In what follows, we provide support to this assertion considering trees with up to $20$ vertices. Moreover, we show that the corresponding independence polynomials are log-concave and, consequently, unimodal. The algorithm computing the independence polynomial of a given tree makes use of a database of non-isomorphic unlabeled trees to prevent repeated computations."}, "answer": " Here is a 4-option multiple choice question on the given documentation:\n\nQuestion: Which of the following statements is true about the independence polynomial of a tree, according to the given documentation?\n\nA) The independence polynomial of a tree is always log-concave.\n\nB) The independence polynomial of a tree has been proven to be unimodal for trees with up to 20 vertices. \n\nC) The degree of the independence polynomial of a graph G is equal to the independence number \u03b1(G).\n\nD) The algorithm to compute the independence polynomial of a tree utilizes dynamic programming.\n\nCorrect Answer: C\n\nExplanation: \nOption C is correct according to the statement \"In other words, the $k$-th coefficient of the independence polynomial equals the number of independent sets comprised of $k$ vertices. For instance, the degree of the independence polynomial of the graph G is equal to \u03b1(G).\"\n\nOption A is incorrect, the documentation states the independence polynomials are log-concave, not always log-concave. \n\nOption B is incorrect, the documentation only provides support for unimodality of trees up to 20 vertices, not a proof.\n\nOption D is incorrect, the algorithm described uses a database of trees, not dynamic programming."}, "33": {"documentation": {"title": "Training Confidence-calibrated Classifiers for Detecting\n  Out-of-Distribution Samples", "source": "Kimin Lee, Honglak Lee, Kibok Lee, Jinwoo Shin", "docs_id": "1711.09325", "section": ["stat.ML", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Training Confidence-calibrated Classifiers for Detecting\n  Out-of-Distribution Samples. The problem of detecting whether a test sample is from in-distribution (i.e., training distribution by a classifier) or out-of-distribution sufficiently different from it arises in many real-world machine learning applications. However, the state-of-art deep neural networks are known to be highly overconfident in their predictions, i.e., do not distinguish in- and out-of-distributions. Recently, to handle this issue, several threshold-based detectors have been proposed given pre-trained neural classifiers. However, the performance of prior works highly depends on how to train the classifiers since they only focus on improving inference procedures. In this paper, we develop a novel training method for classifiers so that such inference algorithms can work better. In particular, we suggest two additional terms added to the original loss (e.g., cross entropy). The first one forces samples from out-of-distribution less confident by the classifier and the second one is for (implicitly) generating most effective training samples for the first one. In essence, our method jointly trains both classification and generative neural networks for out-of-distribution. We demonstrate its effectiveness using deep convolutional neural networks on various popular image datasets."}, "answer": " Here is a 4-option multiple choice question on the document:\n\nQuestion: Which of the following statements is true about the method proposed in the paper?\n\nA) It uses two additional loss terms - one to make the classifier less confident on out-of-distribution samples, and another to implicitly generate effective training samples for the first term.\n\nB) It trains a classifier to distinguish between in-distribution and out-of-distribution samples by modifying the inference procedure alone without changing the training objective. \n\nC) It uses generative adversarial networks along with the classifier to improve out-of-distribution detection.\n\nD) It relies on thresholding the classifier's predictions to detect out-of-distribution samples.\n\nCorrect Answer: A\n\nExplanation: The paper proposes a training method that adds two extra terms to the loss function - one to reduce confidence on out-of-distribution samples and another to generate effective training samples for it. So option A is correct. Options B, C and D refer to other techniques, not the one proposed in this paper."}, "34": {"documentation": {"title": "Finite-Sample Concentration of the Multinomial in Relative Entropy", "source": "Rohit Agrawal", "docs_id": "1904.02291", "section": ["cs.IT", "math.IT", "math.PR", "math.ST", "stat.TH"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Finite-Sample Concentration of the Multinomial in Relative Entropy. We show that the moment generating function of the Kullback-Leibler divergence (relative entropy) between the empirical distribution of $n$ independent samples from a distribution $P$ over a finite alphabet of size $k$ (i.e. a multinomial distribution) and $P$ itself is no more than that of a gamma distribution with shape $k - 1$ and rate $n$. The resulting exponential concentration inequality becomes meaningful (less than 1) when the divergence $\\varepsilon$ is larger than $(k-1)/n$, whereas the standard method of types bound requires $\\varepsilon > \\frac{1}{n} \\cdot \\log{\\binom{n+k-1}{k-1}} \\geq (k-1)/n \\cdot \\log(1 + n/(k-1))$, thus saving a factor of order $\\log(n/k)$ in the standard regime of parameters where $n\\gg k$. As a consequence, we also obtain finite-sample bounds on all the moments of the empirical divergence (equivalently, the discrete likelihood-ratio statistic), which are within constant factors (depending on the moment) of their asymptotic values. Our proof proceeds via a simple reduction to the case $k = 2$ of a binary alphabet (i.e. a binomial distribution), and has the property that improvements in the case of $k = 2$ directly translate to improvements for general $k$. In particular, we conjecture a bound on the binomial moment generating function that would almost close the quadratic gap between our finite-sample bound and the asymptotic moment generating function bound from Wilks' theorem (which does not hold for finite samples)."}, "answer": " Here is a 4-option multiple choice question based on the given text:\n\nQuestion: According to the text, what is the main advantage of the concentration inequality for the multinomial relative entropy derived in the paper compared to the standard method of types bound?\n\nA) It requires a smaller divergence \u03b5 to become meaningful. \n\nB) It provides tighter bounds on the higher moments of the empirical divergence.\n\nC) It applies to alphabets of any size k, not just binary.\n\nD) It leads to an exponential rather than quadratic concentration.\n\nCorrect Answer: A\n\nExplanation: The text states that the new concentration inequality becomes meaningful for \u03b5 > (k-1)/n, whereas the standard method requires \u03b5 > (k-1)/n \u2219 log(1 + n/(k-1)), thus saving a factor of order log(n/k). So the key advantage is requiring a smaller \u03b5 to yield a meaningful bound."}, "35": {"documentation": {"title": "Facial Makeup Transfer Combining Illumination Transfer", "source": "Xin Jin, Rui Han, Ning Ning, Xiaodong Li, Xiaokun Zhang", "docs_id": "1907.03398", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Facial Makeup Transfer Combining Illumination Transfer. To meet the women appearance needs, we present a novel virtual experience approach of facial makeup transfer, developed into windows platform application software. The makeup effects could present on the user's input image in real time, with an only single reference image. The input image and reference image are divided into three layers by facial feature points landmarked: facial structure layer, facial color layer, and facial detail layer. Except for the above layers are processed by different algorithms to generate output image, we also add illumination transfer, so that the illumination effect of the reference image is automatically transferred to the input image. Our approach has the following three advantages: (1) Black or dark and white facial makeup could be effectively transferred by introducing illumination transfer; (2) Efficiently transfer facial makeup within seconds compared to those methods based on deep learning frameworks; (3) Reference images with the air-bangs could transfer makeup perfectly."}, "answer": " Here is a 4-option multiple choice question with explanations for the given documentation:\n\nQuestion: Which of the following is NOT listed as an advantage of the facial makeup transfer approach presented in the documentation?\n\nA) The makeup effects can be presented on the user's input image in real time with only a single reference image. \n\nB) The approach can effectively transfer black or dark and white facial makeup by introducing illumination transfer.\n\nC) The approach can efficiently transfer facial makeup within minutes compared to deep learning methods.\n\nD) The approach requires a large dataset of facial images for training the makeup transfer model.\n\nCorrect Answer: D\n\nExplanations: \nOption A is listed as advantage (1) in the documentation. \nOption B is listed as advantage (1) in the documentation.\nOption C is listed as advantage (2) in the documentation.\nOption D is not mentioned in the documentation as an advantage of the presented approach. The documentation states it can transfer makeup with a single reference image, implying no training data is needed."}, "36": {"documentation": {"title": "Enumerating all maximal biclusters in numerical datasets", "source": "Rosana Veroneze, Arindam Banerjee and Fernando J. Von Zuben", "docs_id": "1403.3562", "section": ["cs.DM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Enumerating all maximal biclusters in numerical datasets. Biclustering has proved to be a powerful data analysis technique due to its wide success in various application domains. However, the existing literature presents efficient solutions only for enumerating maximal biclusters with constant values, or heuristic-based approaches which can not find all biclusters or even support the maximality of the obtained biclusters. Here, we present a general family of biclustering algorithms for enumerating all maximal biclusters with (i) constant values on rows, (ii) constant values on columns, or (iii) coherent values. Versions for perfect and for perturbed biclusters are provided. Our algorithms have four key properties (just the algorithm for perturbed biclusters with coherent values fails to exhibit the first property): they are (1) efficient (take polynomial time per pattern), (2) complete (find all maximal biclusters), (3) correct (all biclusters attend the user-defined measure of similarity), and (4) non-redundant (all the obtained biclusters are maximal and the same bicluster is not enumerated twice). They are based on a generalization of an efficient formal concept analysis algorithm called In-Close2. Experimental results point to the necessity of having efficient enumerative biclustering algorithms and provide a valuable insight into the scalability of our family of algorithms and its sensitivity to user-defined parameters."}, "answer": " Here is a 4-option multiple choice question on the biclustering documentation:\n\nQuestion: According to the documentation, which of the following properties does the algorithm for perturbed biclusters with coherent values fail to exhibit?\n\nA) The algorithm runs in polynomial time per pattern.\n\nB) The algorithm is able to find all maximal biclusters. \n\nC) All biclusters found by the algorithm satisfy the user-defined measure of similarity.\n\nD) The algorithm ensures that all obtained biclusters are maximal and no bicluster is enumerated twice.\n\nCorrect Answer: A\n\nExplanation: The documentation states that \"Our algorithms have four key properties (just the algorithm for perturbed biclusters with coherent values fails to exhibit the first property): they are (1) efficient (take polynomial time per pattern), (2) complete (find all maximal biclusters), (3) correct (all biclusters attend the user-defined measure of similarity), and (4) non-redundant (all the obtained biclusters are maximal and the same bicluster is not enumerated twice).\" Therefore, the algorithm for perturbed biclusters with coherent values fails to exhibit efficient polynomial time per pattern, which is option A."}, "37": {"documentation": {"title": "Highly intensive data dissemination in complex networks", "source": "Gabriele D'Angelo, Stefano Ferretti", "docs_id": "1507.08417", "section": ["cs.NI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Highly intensive data dissemination in complex networks. This paper presents a study on data dissemination in unstructured Peer-to-Peer (P2P) network overlays. The absence of a structure in unstructured overlays eases the network management, at the cost of non-optimal mechanisms to spread messages in the network. Thus, dissemination schemes must be employed that allow covering a large portion of the network with a high probability (e.g.~gossip based approaches). We identify principal metrics, provide a theoretical model and perform the assessment evaluation using a high performance simulator that is based on a parallel and distributed architecture. A main point of this study is that our simulation model considers implementation technical details, such as the use of caching and Time To Live (TTL) in message dissemination, that are usually neglected in simulations, due to the additional overhead they cause. Outcomes confirm that these technical details have an important influence on the performance of dissemination schemes and that the studied schemes are quite effective to spread information in P2P overlay networks, whatever their topology. Moreover, the practical usage of such dissemination mechanisms requires a fine tuning of many parameters, the choice between different network topologies and the assessment of behaviors such as free riding. All this can be done only using efficient simulation tools to support both the network design phase and, in some cases, at runtime."}, "answer": " Here is a 4-option multiple choice question on the paper:\n\nQuestion: Which of the following statements is true about the paper's findings on data dissemination in unstructured P2P networks?\n\nA) The absence of structure in unstructured overlays makes dissemination more optimal than in structured overlays.\n\nB) Caching and TTL have little influence on the performance of dissemination schemes. \n\nC) Gossip-based approaches allow reaching a large portion of the network with high probability.\n\nD) Free riding behaviors can be fully assessed using simulation tools without runtime monitoring.\n\nCorrect Answer: C\n\nExplanation: The paper finds that gossip-based approaches allow effective spreading of information across unstructured P2P networks. It also notes that caching and TTL details impact performance, contradicting option B. The paper does not compare structured vs unstructured overlays (option A) or claim simulation tools can fully assess free riding without runtime monitoring (option D)."}, "38": {"documentation": {"title": "Gini Index based Initial Coin Offering Mechanism", "source": "Mingyu Guo, Zhenghui Wang, Yuko Sakurai", "docs_id": "2002.11387", "section": ["cs.GT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Gini Index based Initial Coin Offering Mechanism. As a fundraising method, Initial Coin Offering (ICO) has raised billions of dollars for thousands of startups in the past two years. Existing ICO mechanisms place more emphasis on the short-term benefits of maximal fundraising while ignoring the problem of unbalanced token allocation, which negatively impacts subsequent fundraising and has bad effects on introducing new investors and resources. We propose a new ICO mechanism which uses the concept of Gini index for the very first time as a mechanism design constraint to control allocation inequality. Our mechanism maintains an elegant and straightforward structure. It allows the agents to modify their bids as a price discovery process, while limiting the bids of whales. We analyze the agents' equilibrium behaviors under our mechanism. Under natural technical assumptions, we show that most agents have simple dominant strategies and the equilibrium revenue approaches the optimal revenue asymptotically in the number of agents. We verify our mechanism using real ICO dataset we collected, and confirm that our mechanism performs well in terms of both allocation fairness and revenue."}, "answer": " Here is a 4-option multiple choice question on the Gini index based ICO mechanism paper:\n\nQuestion: Which of the following statements is true about the Gini index based ICO mechanism proposed in the paper?\n\nA) It uses the Gini index to maximize fundraising revenue while ignoring allocation inequality.\n\nB) It allows whales to place unlimited bids in order to maximize revenue. \n\nC) It uses the Gini index as a constraint to control allocation inequality.\n\nD) It does not allow agents to modify their bids during the ICO.\n\nCorrect Answer: C\n\nExplanation: The paper proposes using the Gini index as a mechanism design constraint to control allocation inequality in an ICO, while still allowing price discovery through bid modifications. Options A, B, and D are incorrect based on details provided in the summary."}, "39": {"documentation": {"title": "Interleaved Polar (I-Polar) Codes", "source": "Mao-Ching Chiu", "docs_id": "1908.00708", "section": ["cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Interleaved Polar (I-Polar) Codes. By inserting interleavers between intermediate stages of the polar encoder, a new class of polar codes, termed interleaved polar (i-polar) codes, is proposed. By the uniform interleaver assumption, we derive the weight enumerating function (WEF) and input-output weight enumerating function (IOWEF) averaged over the ensemble of i-polar codes. The average WEF can be used to calculate the upper bound on the average block error rate (BLER) of a code selected at random from the ensemble of i-polar codes. Also, we propose a concatenated coding scheme that employs P high rate codes as the outer code and Q i-polar codes as the inner code with an interleaver in between. The average WEF of the concatenated code is derived based on the uniform interleaver assumption. Simulation results show that BLER upper bounds can well predict BLER performance levels of the concatenated codes. The results show that the performance of the proposed concatenated code with P=Q=2 is better than that of the CRC-aided i-polar code with P=Q=1 of the same length and code rate at high signal-to-noise ratios (SNRs). Moreover, the proposed concatenated code allows multiple decoders to operate in parallel, which can reduce the decoding latency and hence is suitable for ultra-reliable low-latency communications (URLLC)."}, "answer": " Here is a 4-option multiple choice question on the I-Polar codes paper:\n\nQuestion: Which of the following statements is true about the proposed concatenated coding scheme in the I-Polar codes paper?\n\nA) It employs P low rate codes as the outer code and Q I-Polar codes as the inner code.\n\nB) It has worse BLER performance compared to CRC-aided I-Polar codes at high SNRs. \n\nC) The uniform interleaver assumption is used to derive the average WEF of the concatenated code.\n\nD) It does not allow for parallel decoding, making it unsuitable for URLLC.\n\nCorrect Answer: C\n\nExplanation: The paper proposes a concatenated coding scheme with P high rate codes as the outer code and Q I-Polar codes as the inner code. It shows better BLER performance than CRC-aided codes at high SNRs. The uniform interleaver assumption is used to analyze the average WEF of the concatenated code. It also allows for parallel decoding, making it suitable for URLLC applications."}, "40": {"documentation": {"title": "SLAYER: Spike Layer Error Reassignment in Time", "source": "Sumit Bam Shrestha and Garrick Orchard", "docs_id": "1810.08646", "section": ["cs.NE", "cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "SLAYER: Spike Layer Error Reassignment in Time. Configuring deep Spiking Neural Networks (SNNs) is an exciting research avenue for low power spike event based computation. However, the spike generation function is non-differentiable and therefore not directly compatible with the standard error backpropagation algorithm. In this paper, we introduce a new general backpropagation mechanism for learning synaptic weights and axonal delays which overcomes the problem of non-differentiability of the spike function and uses a temporal credit assignment policy for backpropagating error to preceding layers. We describe and release a GPU accelerated software implementation of our method which allows training both fully connected and convolutional neural network (CNN) architectures. Using our software, we compare our method against existing SNN based learning approaches and standard ANN to SNN conversion techniques and show that our method achieves state of the art performance for an SNN on the MNIST, NMNIST, DVS Gesture, and TIDIGITS datasets."}, "answer": " Question: What is the key innovation proposed in the SLAYER paper to enable error backpropagation in spiking neural networks?\n\nA) Using a temporal credit assignment policy to assign error responsibility across time to deal with the non-differentiability of spike functions.\n\nB) Converting artificial neural networks to spiking neural networks before training. \n\nC) Using GPU acceleration to speed up training of spiking neural networks.\n\nD) Training fully connected and convolutional spiking neural networks on image datasets.\n\nCorrect Answer: A\n\nExplanation: The key innovation proposed in the SLAYER paper is using a temporal credit assignment policy to assign error responsibility across time, which allows backpropagating error in spiking neural networks despite the non-differentiability of spike functions. Options B, C, and D describe other aspects of the paper but do not address the core contribution for enabling backpropagation."}, "41": {"documentation": {"title": "Clue Me In: Semi-Supervised FGVC with Out-of-Distribution Data", "source": "Ruoyi Du, Dongliang Chang, Zhanyu Ma, Yi-Zhe Song, Jun Guo", "docs_id": "2112.02825", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Clue Me In: Semi-Supervised FGVC with Out-of-Distribution Data. Despite great strides made on fine-grained visual classification (FGVC), current methods are still heavily reliant on fully-supervised paradigms where ample expert labels are called for. Semi-supervised learning (SSL) techniques, acquiring knowledge from unlabeled data, provide a considerable means forward and have shown great promise for coarse-grained problems. However, exiting SSL paradigms mostly assume in-distribution (i.e., category-aligned) unlabeled data, which hinders their effectiveness when re-proposed on FGVC. In this paper, we put forward a novel design specifically aimed at making out-of-distribution data work for semi-supervised FGVC, i.e., to \"clue them in\". We work off an important assumption that all fine-grained categories naturally follow a hierarchical structure (e.g., the phylogenetic tree of \"Aves\" that covers all bird species). It follows that, instead of operating on individual samples, we can instead predict sample relations within this tree structure as the optimization goal of SSL. Beyond this, we further introduced two strategies uniquely brought by these tree structures to achieve inter-sample consistency regularization and reliable pseudo-relation. Our experimental results reveal that (i) the proposed method yields good robustness against out-of-distribution data, and (ii) it can be equipped with prior arts, boosting their performance thus yielding state-of-the-art results. Code is available at https://github.com/PRIS-CV/RelMatch."}, "answer": " Question: Which of the following statements best summarizes the key ideas presented in the given documentation?\n\nA) Current fine-grained visual classification methods rely heavily on fully-supervised learning with ample expert labels. The paper proposes a semi-supervised approach that leverages hierarchical relationships between categories to make use of out-of-distribution unlabeled data. \n\nB) The paper introduces a new deep learning architecture for fine-grained visual classification that uses a hierarchical tree structure to model relationships between categories. This allows the model to learn from both labeled and unlabeled out-of-distribution data.\n\nC) Semi-supervised learning techniques have shown great promise for coarse-grained classification problems. The paper presents a method to adapt these techniques to fine-grained problems by using hierarchical category relationships and out-of-distribution data.\n\nD) Expert labels are critical for fine-grained visual classification. The paper shows how to augment labeled data with unlabeled in-distribution data to improve classification accuracy. A hierarchical category structure is used to enforce consistency between labeled and unlabeled data.\n\nCorrect Answer: C\n\nExplanation: Option C best summarizes the key ideas from the documentation - that semi-supervised learning has worked well for coarse-grained problems, and the paper introduces a technique to adapt it to fine-grained problems by using hierarchical category relationships and out-of-distribution unlabeled data."}, "42": {"documentation": {"title": "Face Alignment Robust to Pose, Expressions and Occlusions", "source": "Vishnu Naresh Boddeti, Myung-Cheol Roh, Jongju Shin, Takaharu Oguri,\n  Takeo Kanade", "docs_id": "1707.05938", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Face Alignment Robust to Pose, Expressions and Occlusions. We propose an Ensemble of Robust Constrained Local Models for alignment of faces in the presence of significant occlusions and of any unknown pose and expression. To account for partial occlusions we introduce, Robust Constrained Local Models, that comprises of a deformable shape and local landmark appearance model and reasons over binary occlusion labels. Our occlusion reasoning proceeds by a hypothesize-and-test search over occlusion labels. Hypotheses are generated by Constrained Local Model based shape fitting over randomly sampled subsets of landmark detector responses and are evaluated by the quality of face alignment. To span the entire range of facial pose and expression variations we adopt an ensemble of independent Robust Constrained Local Models to search over a discretized representation of pose and expression. We perform extensive evaluation on a large number of face images, both occluded and unoccluded. We find that our face alignment system trained entirely on facial images captured \"in-the-lab\" exhibits a high degree of generalization to facial images captured \"in-the-wild\". Our results are accurate and stable over a wide spectrum of occlusions, pose and expression variations resulting in excellent performance on many real-world face datasets."}, "answer": " Question: What method does the paper propose for aligning faces in the presence of occlusions and varying pose and expression?\n\nA) A single robust constrained local model trained on a large dataset of occluded and unoccluded faces.\n\nB) An ensemble of independent robust constrained local models that search over discretized pose and expression. \n\nC) A generative adversarial network trained on synthetic occlusions.\n\nD) A convolutional neural network pretrained on ImageNet.\n\nCorrect Answer: B\n\nExplanation: The paper proposes using an ensemble of robust constrained local models to handle occlusion and variation in pose and expression. Each local model handles occlusion reasoning and alignment for a specific pose and expression. \n\nThe other options are not mentioned in the summary. The paper uses an ensemble of local models rather than a single model trained on all data (A). It does not use GANs or CNNs pretrained on ImageNet (C and D)."}, "43": {"documentation": {"title": "Network dismantling", "source": "Alfredo Braunstein, Luca Dall'Asta, Guilhem Semerjian, Lenka\n  Zdeborov\\'a", "docs_id": "1603.08883", "section": ["physics.soc-ph", "cond-mat.stat-mech", "cs.DS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Network dismantling. We study the network dismantling problem, which consists in determining a minimal set of vertices whose removal leaves the network broken into connected components of sub-extensive size. For a large class of random graphs, this problem is tightly connected to the decycling problem (the removal of vertices leaving the graph acyclic). Exploiting this connection and recent works on epidemic spreading we present precise predictions for the minimal size of a dismantling set in a large random graph with a prescribed (light-tailed) degree distribution. Building on the statistical mechanics perspective we propose a three-stage Min-Sum algorithm for efficiently dismantling networks, including heavy-tailed ones for which the dismantling and decycling problems are not equivalent. We also provide further insights into the dismantling problem concluding that it is an intrinsically collective problem and that optimal dismantling sets cannot be viewed as a collection of individually well performing nodes."}, "answer": " Here is a 4-option multiple choice question on the network dismantling paper:\n\nQuestion: Which of the following statements is true about the network dismantling problem according to the paper?\n\nA) It consists in determining a minimal set of vertices whose removal leaves the network broken into connected components of extensive size. \n\nB) For random graphs with light-tailed degree distributions, it is loosely connected to the decycling problem.\n\nC) The dismantling and decycling problems are equivalent even for heavy-tailed degree distributions.\n\nD) Optimal dismantling sets can be viewed as a collection of individually well performing nodes.\n\nCorrect Answer: B\n\nExplanation: The paper states that for a large class of random graphs, the network dismantling problem is tightly connected to the decycling problem, not loosely connected as stated in option B. Options A, C and D are incorrect based on statements made in the abstract."}, "44": {"documentation": {"title": "Inspiration through Observation: Demonstrating the Influence of\n  Automatically Generated Text on Creative Writing", "source": "Melissa Roemmele", "docs_id": "2107.04007", "section": ["cs.CL", "cs.AI", "cs.HC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Inspiration through Observation: Demonstrating the Influence of\n  Automatically Generated Text on Creative Writing. Getting machines to generate text perceived as creative is a long-pursued goal. A growing body of research directs this goal towards augmenting the creative writing abilities of human authors. In this paper, we pursue this objective by analyzing how observing examples of automatically generated text influences writing. In particular, we examine a task referred to as sentence infilling, which involves transforming a list of words into a complete sentence. We emphasize \"storiability\" as a desirable feature of the resulting sentences, where \"storiable\" sentences are those that suggest a story a reader would be curious to hear about. Both humans and an automated system (based on a neural language model) performed this sentence infilling task. In one setting, people wrote sentences on their own; in a different setting, people observed the sentences produced by the model while writing their own sentences. Readers then assigned storiability preferences to the resulting sentences in a subsequent evaluation. We find that human-authored sentences were judged as more storiable when authors observed the generated examples, and that storiability increased as authors derived more semantic content from the examples. This result gives evidence of an \"inspiration through observation\" paradigm for human-computer collaborative writing, through which human writing can be enhanced by text generation models without directly copying their output."}, "answer": " Here is a 4-option multiple choice question on the given text:\n\nQuestion: What was the main finding from the study described in the text regarding the impact of showing automatically generated text examples to human writers?\n\nA) Observing the computer-generated text caused the human writers to directly copy the examples into their own writing.\n\nB) The computer-generated text was judged by readers to be more creative than the human-written text. \n\nC) Human-authored sentences were rated as more storiable when the authors observed the generated examples before writing.\n\nD) There was no measurable effect on human writing from observing the computer-generated text.\n\nCorrect Answer: C\n\nExplanation: The text states that \"We find that human-authored sentences were judged as more storiable when authors observed the generated examples, and that storiability increased as authors derived more semantic content from the examples.\" So the main finding was that observing the computer-generated examples increased the perceived creativity/storiability of the human writing."}, "45": {"documentation": {"title": "Real-Time Monocular Human Depth Estimation and Segmentation on Embedded\n  Systems", "source": "Shan An, Fangru Zhou, Mei Yang, Haogang Zhu, Changhong Fu, and\n  Konstantinos A. Tsintotas", "docs_id": "2108.10506", "section": ["cs.CV", "cs.RO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Real-Time Monocular Human Depth Estimation and Segmentation on Embedded\n  Systems. Estimating a scene's depth to achieve collision avoidance against moving pedestrians is a crucial and fundamental problem in the robotic field. This paper proposes a novel, low complexity network architecture for fast and accurate human depth estimation and segmentation in indoor environments, aiming to applications for resource-constrained platforms (including battery-powered aerial, micro-aerial, and ground vehicles) with a monocular camera being the primary perception module. Following the encoder-decoder structure, the proposed framework consists of two branches, one for depth prediction and another for semantic segmentation. Moreover, network structure optimization is employed to improve its forward inference speed. Exhaustive experiments on three self-generated datasets prove our pipeline's capability to execute in real-time, achieving higher frame rates than contemporary state-of-the-art frameworks (114.6 frames per second on an NVIDIA Jetson Nano GPU with TensorRT) while maintaining comparable accuracy."}, "answer": " Question: What is the primary goal of the proposed network architecture described in the paper?\n\nA) To achieve high accuracy in human depth estimation and segmentation\n\nB) To be deployable on resource-constrained robotic platforms with monocular cameras \n\nC) To execute in real-time on embedded systems with limited compute resources\n\nD) To outperform state-of-the-art frameworks on benchmark datasets\n\nCorrect Answer: C\n\nExplanation: The key goal stated in the abstract is to develop a low complexity network that can provide fast and accurate human depth estimation and segmentation on embedded systems with limited compute resources like battery-powered aerial, micro-aerial and ground vehicles that rely on a single monocular camera. While accuracy is important, the primary aim is real-time performance on such resource-constrained platforms."}, "46": {"documentation": {"title": "PHD-GIFs: Personalized Highlight Detection for Automatic GIF Creation", "source": "Ana Garc\\'ia del Molino and Michael Gygli", "docs_id": "1804.06604", "section": ["cs.CV", "cs.MM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "PHD-GIFs: Personalized Highlight Detection for Automatic GIF Creation. Highlight detection models are typically trained to identify cues that make visual content appealing or interesting for the general public, with the objective of reducing a video to such moments. However, the \"interestingness\" of a video segment or image is subjective. Thus, such highlight models provide results of limited relevance for the individual user. On the other hand, training one model per user is inefficient and requires large amounts of personal information which is typically not available. To overcome these limitations, we present a global ranking model which conditions on each particular user's interests. Rather than training one model per user, our model is personalized via its inputs, which allows it to effectively adapt its predictions, given only a few user-specific examples. To train this model, we create a large-scale dataset of users and the GIFs they created, giving us an accurate indication of their interests. Our experiments show that using the user history substantially improves the prediction accuracy. On our test set of 850 videos, our model improves the recall by 8% with respect to generic highlight detectors. Furthermore, our method proves more precise than the user-agnostic baselines even with just one person-specific example."}, "answer": " Question: What does the highlight detection model presented in the paper do to adapt its predictions to individual users' interests?\n\nA) It trains a separate model for each user, using large amounts of personal data.\n\nB) It takes the user's interests as inputs, allowing it to adjust its predictions without training separate models.\n\nC) It uses a generic model trained only on data from the general public. \n\nD) It requires the user to manually label many examples of their interests.\n\nCorrect Answer: B\n\nExplanation: The paper presents a global ranking model that is personalized via its inputs, which are a few examples specific to each user's interests. This allows it to adapt its predictions to users without training separate models for each user."}, "47": {"documentation": {"title": "ZS-BERT: Towards Zero-Shot Relation Extraction with Attribute\n  Representation Learning", "source": "Chih-Yao Chen, Cheng-Te Li", "docs_id": "2104.04697", "section": ["cs.CL", "cs.IR", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "ZS-BERT: Towards Zero-Shot Relation Extraction with Attribute\n  Representation Learning. While relation extraction is an essential task in knowledge acquisition and representation, and new-generated relations are common in the real world, less effort is made to predict unseen relations that cannot be observed at the training stage. In this paper, we formulate the zero-shot relation extraction problem by incorporating the text description of seen and unseen relations. We propose a novel multi-task learning model, zero-shot BERT (ZS-BERT), to directly predict unseen relations without hand-crafted attribute labeling and multiple pairwise classifications. Given training instances consisting of input sentences and the descriptions of their relations, ZS-BERT learns two functions that project sentences and relation descriptions into an embedding space by jointly minimizing the distances between them and classifying seen relations. By generating the embeddings of unseen relations and new-coming sentences based on such two functions, we use nearest neighbor search to obtain the prediction of unseen relations. Experiments conducted on two well-known datasets exhibit that ZS-BERT can outperform existing methods by at least 13.54\\% improvement on F1 score."}, "answer": " Question: Which of the following statements is true about the ZS-BERT model proposed in the paper?\n\nA) ZS-BERT uses hand-crafted attribute labeling and multiple pairwise classifications to predict unseen relations.\n\nB) ZS-BERT learns to project sentences and relation descriptions into an embedding space by jointly minimizing the distances between them and classifying seen relations. \n\nC) ZS-BERT uses nearest neighbor search to obtain predictions for seen relations observed during training.\n\nD) ZS-BERT achieves worse performance compared to existing methods on predicting unseen relations.\n\nCorrect Answer: B\n\nExplanation: According to the paper, ZS-BERT learns two functions that project sentences and relation descriptions into an embedding space by jointly minimizing the distances between them and classifying seen relations. It then uses these learned functions to generate embeddings for unseen relations and new sentences, and leverages nearest neighbor search to predict unseen relations. Thus, option B summarizing this key aspect of the ZS-BERT model is the correct answer. Options A, C, and D contradict information provided in the paper."}, "48": {"documentation": {"title": "Working Locally Thinking Globally - Part I: Theoretical Guarantees for\n  Convolutional Sparse Coding", "source": "Vardan Papyan, Jeremias Sulam and Michael Elad", "docs_id": "1607.02005", "section": ["cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Working Locally Thinking Globally - Part I: Theoretical Guarantees for\n  Convolutional Sparse Coding. The celebrated sparse representation model has led to remarkable results in various signal processing tasks in the last decade. However, despite its initial purpose of serving as a global prior for entire signals, it has been commonly used for modeling low dimensional patches due to the computational constraints it entails when deployed with learned dictionaries. A way around this problem has been proposed recently, adopting a convolutional sparse representation model. This approach assumes that the global dictionary is a concatenation of banded Circulant matrices. Although several works have presented algorithmic solutions to the global pursuit problem under this new model, very few truly-effective guarantees are known for the success of such methods. In the first of this two-part work, we address the theoretical aspects of the sparse convolutional model, providing the first meaningful answers to corresponding questions of uniqueness of solutions and success of pursuit algorithms. To this end, we generalize mathematical quantities, such as the $\\ell_0$ norm, the mutual coherence and the Spark, to their counterparts in the convolutional setting, which intrinsically capture local measures of the global model. In a companion paper, we extend the analysis to a noisy regime, addressing the stability of the sparsest solutions and pursuit algorithms, and demonstrate practical approaches for solving the global pursuit problem via simple local processing."}, "answer": " Here is a 4-option multiple choice question on the abstract:\n\nQuestion: Which of the following statements is true about the sparse convolutional model discussed in the abstract?\n\nA) It assumes the global dictionary is a concatenation of diagonal matrices. \n\nB) It has been commonly used for modeling entire signals due to its computational efficiency.\n\nC) Very few theoretical guarantees are known for the success of algorithms based on this model.\n\nD) It generalizes quantities like the l0 norm and mutual coherence to capture global measures.\n\nCorrect Answer: C\n\nExplanation: The abstract states that the sparse convolutional model assumes the global dictionary is a concatenation of banded Circulant matrices, not diagonal matrices. It also mentions this model has been used for modeling patches, not entire signals. Further, it says very few truly-effective guarantees are known for the success of algorithms based on this model. Finally, it mentions generalizing quantities like l0 norm and mutual coherence to capture local, not global, measures."}, "49": {"documentation": {"title": "Incentivizing High-quality Content from Heterogeneous Users: On the\n  Existence of Nash Equilibrium", "source": "Yingce Xia, Tao Qin, Nenghai Yu, Tie-Yan Liu", "docs_id": "1404.5155", "section": ["cs.GT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Incentivizing High-quality Content from Heterogeneous Users: On the\n  Existence of Nash Equilibrium. We study the existence of pure Nash equilibrium (PNE) for the mechanisms used in Internet services (e.g., online reviews and question-answer websites) to incentivize users to generate high-quality content. Most existing work assumes that users are homogeneous and have the same ability. However, real-world users are heterogeneous and their abilities can be very different from each other due to their diverse background, culture, and profession. In this work, we consider heterogeneous users with the following framework: (1) the users are heterogeneous and each of them has a private type indicating the best quality of the content she can generate; (2) there is a fixed amount of reward to allocate to the participated users. Under this framework, we study the existence of pure Nash equilibrium of several mechanisms composed by different allocation rules, action spaces, and information settings. We prove the existence of PNE for some mechanisms and the non-existence of PNE for some mechanisms. We also discuss how to find a PNE for those mechanisms with PNE either through a constructive way or a search algorithm."}, "answer": " Here is a 4-option multiple choice question on the content:\n\nQuestion: Which of the following statements is true about the framework studied in the paper for analyzing the existence of pure Nash equilibrium in mechanisms for incentivizing high-quality content from heterogeneous users?\n\nA) The users are assumed to be homogeneous with the same ability to generate high-quality content.\n\nB) There is an unlimited reward budget to allocate to the participated users. \n\nC) The users have private types indicating the quality of content they can generate.\n\nD) The mechanisms studied allow continuous action spaces for the users.\n\nCorrect Answer: C\n\nExplanation: According to the documentation, the framework studied in the paper assumes that users are heterogeneous with different abilities based on their backgrounds. It also assumes a fixed reward budget and studies mechanisms with different allocation rules, action spaces and information settings. So the statement that users have private types indicating the content quality they can generate is true as per the framework description."}, "50": {"documentation": {"title": "Implications of Ocular Pathologies for Iris Recognition Reliability", "source": "Mateusz Trokielewicz and Adam Czajka and Piotr Maciejewicz", "docs_id": "1809.00168", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Implications of Ocular Pathologies for Iris Recognition Reliability. This paper presents an analysis of how iris recognition is influenced by eye disease and an appropriate dataset comprising 2996 images of irises taken from 230 distinct eyes (including 184 affected by more than 20 different eye conditions). The images were collected in near infrared and visible light during routine ophthalmological examination. The experimental study carried out utilizing four independent iris recognition algorithms (MIRLIN, VeriEye, OSIRIS and IriCore) renders four valuable results. First, the enrollment process is highly sensitive to those eye conditions that obstruct the iris or cause geometrical distortions. Second, even those conditions that do not produce visible changes to the structure of the iris may increase the dissimilarity between samples of the same eyes. Third, eye conditions affecting the geometry or the tissue structure of the iris or otherwise producing obstructions significantly decrease same-eye similarity and have a lower, yet still statistically significant, influence on impostor comparison scores. Fourth, for unhealthy eyes, the most prominent effect of disease on iris recognition is to cause segmentation errors. To our knowledge this paper describes the largest database of iris images for disease-affected eyes made publicly available to researchers and offers the most comprehensive study of what we can expect when iris recognition is employed for diseased eyes."}, "answer": " Question: What are the four main findings from the analysis of how iris recognition is influenced by eye disease presented in this paper?\n\nA) The enrollment process is highly sensitive to conditions that obstruct the iris or cause distortions, even conditions that do not visibly affect the iris can increase dissimilarity between samples, conditions affecting the iris structure significantly decrease similarity, and segmentation errors are the most prominent effect for unhealthy eyes.\n\nB) Eye conditions never affect the iris tissue structure, have no influence on impostor comparison scores, do not cause any segmentation errors, and the enrollment process is unaffected. \n\nC) The enrollment process is insensitive to obstructions and distortions, visible changes to the iris increase same-eye similarity, conditions affecting the iris geometry improve segmentation, and impostor scores are unchanged.\n\nD) Segmentation errors are rare, the enrollment process is resistant to disease, same-eye similarity increases with conditions affecting the iris, and impostor scores are significantly impacted.\n\nCorrect Answer: A\n\nExplanation: According to the paper, the four main findings are: 1) The enrollment process is sensitive to obstructions and distortions, 2) Invisible changes can increase dissimilarity between same-eye samples, 3) Conditions affecting the iris structure decrease same-eye similarity and impact impostor scores, though less significantly, and 4) Segmentation errors are the most prominent effect of disease."}, "51": {"documentation": {"title": "Superpixels Based Segmentation and SVM Based Classification Method to\n  Distinguish Five Diseases from Normal Regions in Wireless Capsule Endoscopy", "source": "Omid Haji Maghsoudi", "docs_id": "1711.06616", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Superpixels Based Segmentation and SVM Based Classification Method to\n  Distinguish Five Diseases from Normal Regions in Wireless Capsule Endoscopy. Wireless Capsule Endoscopy (WCE) is relatively a new technology to examine the entire GI trace. During an examination, it captures more than 55,000 frames. Reviewing all these images is time-consuming and prone to human error. It has been a challenge to develop intelligent methods assisting physicians to review the frames. The WCE frames are captured in 8-bit color depths which provides enough a color range to detect abnormalities. Here, superpixel based methods are proposed to segment five diseases including: bleeding, Crohn's disease, Lymphangiectasia, Xanthoma, and Lymphoid hyperplasia. Two superpixels methods are compared to provide semantic segmentation of these prolific diseases: simple linear iterative clustering (SLIC) and quick shift (QS). The segmented superpixels were classified into two classes (normal and abnormal) by support vector machine (SVM) using texture and color features. For both superpixel methods, the accuracy, specificity, sensitivity, and precision (SLIC, QS) were around 92%, 93%, 93%, and 88%, respectively. However, SLIC was dramatically faster than QS."}, "answer": " Question: Which of the following statements is true about the superpixel-based segmentation and SVM classification method for distinguishing diseases from normal regions in wireless capsule endoscopy images?\n\nA) The method used k-means clustering to generate superpixels.\n\nB) The method classified superpixels into 5 different disease classes using SVM. \n\nC) The SLIC superpixel method was slower but more accurate than the quick shift method.\n\nD) The method achieved around 90% accuracy in distinguishing normal from abnormal regions.\n\nCorrect Answer: D\n\nExplanation: \nThe paper proposed using SLIC and quick shift superpixel methods to segment regions in wireless capsule endoscopy images, then classifying the superpixels into normal vs abnormal using SVM and texture/color features. The accuracy reported was around 90-93% for both methods, with SLIC being faster. The superpixels were not classified into 5 disease classes, only normal vs abnormal. And SLIC was faster than quick shift, not slower."}, "52": {"documentation": {"title": "Communication in a Poisson Field of Interferers -- Part I: Interference\n  Distribution and Error Probability", "source": "Pedro C. Pinto, Moe Z. Win", "docs_id": "1001.4519", "section": ["cs.IT", "cs.NI", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Communication in a Poisson Field of Interferers -- Part I: Interference\n  Distribution and Error Probability. We present a mathematical model for communication subject to both network interference and noise. We introduce a framework where the interferers are scattered according to a spatial Poisson process, and are operating asynchronously in a wireless environment subject to path loss, shadowing, and multipath fading. We consider both cases of slow and fast-varying interferer positions. The paper is comprised of two separate parts. In Part I, we determine the distribution of the aggregate network interference at the output of a linear receiver. We characterize the error performance of the link, in terms of average and outage probabilities. The proposed model is valid for any linear modulation scheme (e.g., M-ary phase shift keying or M-ary quadrature amplitude modulation), and captures all the essential physical parameters that affect network interference. Our work generalizes the conventional analysis of communication in the presence of additive white Gaussian noise and fast fading, allowing the traditional results to be extended to include the effect of network interference. In Part II of the paper, we derive the capacity of the link when subject to network interference and noise, and characterize the spectrum of the aggregate interference."}, "answer": " Here is a 4-option multiple choice question on the paper:\n\nQuestion: According to the paper, what distribution does the aggregate network interference follow at the output of a linear receiver when the interferers are scattered according to a spatial Poisson process?\n\nA) Gaussian distribution\nB) Log-normal distribution \nC) Poisson distribution\nD) None of the above\n\nCorrect Answer: C) Poisson distribution\n\nExplanation: The paper states that when interferers are scattered according to a spatial Poisson process, the aggregate network interference follows a Poisson distribution at the output of a linear receiver. This is a key result derived in Part I of the paper."}, "53": {"documentation": {"title": "Characterizing A Database of Sequential Behaviors with Latent Dirichlet\n  Hidden Markov Models", "source": "Yin Song, Longbing Cao, Xuhui Fan, Wei Cao and Jian Zhang", "docs_id": "1305.5734", "section": ["stat.ML", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Characterizing A Database of Sequential Behaviors with Latent Dirichlet\n  Hidden Markov Models. This paper proposes a generative model, the latent Dirichlet hidden Markov models (LDHMM), for characterizing a database of sequential behaviors (sequences). LDHMMs posit that each sequence is generated by an underlying Markov chain process, which are controlled by the corresponding parameters (i.e., the initial state vector, transition matrix and the emission matrix). These sequence-level latent parameters for each sequence are modeled as latent Dirichlet random variables and parameterized by a set of deterministic database-level hyper-parameters. Through this way, we expect to model the sequence in two levels: the database level by deterministic hyper-parameters and the sequence-level by latent parameters. To learn the deterministic hyper-parameters and approximate posteriors of parameters in LDHMMs, we propose an iterative algorithm under the variational EM framework, which consists of E and M steps. We examine two different schemes, the fully-factorized and partially-factorized forms, for the framework, based on different assumptions. We present empirical results of behavior modeling and sequence classification on three real-world data sets, and compare them to other related models. The experimental results prove that the proposed LDHMMs produce better generalization performance in terms of log-likelihood and deliver competitive results on the sequence classification problem."}, "answer": " Here is a multi-form question on the paper:\n\nQuestion: Which of the following statements is true about the latent Dirichlet hidden Markov models (LDHMMs) proposed in the paper?\n\nA) LDHMMs model each sequence using a single set of latent parameters shared across all sequences. The latent parameters are modeled as Dirichlet distributions.\n\nB) LDHMMs model each sequence using sequence-specific latent parameters drawn from Dirichlet distributions. The Dirichlet distributions have shared deterministic hyper-parameters at the database level. \n\nC) LDHMMs use Bayesian networks to model the dependencies between observed and latent variables in each sequence. The model parameters are point estimates learned through maximum likelihood estimation.\n\nD) LDHMMs posit that the sequences are generated from Gaussian emissions conditioned on latent states. The model parameters are learned through variational inference.\n\nCorrect Answer: B) LDHMMs model each sequence using sequence-specific latent parameters drawn from Dirichlet distributions. The Dirichlet distributions have shared deterministic hyper-parameters at the database level.\n\nExplanation: The paper proposes LDHMMs where each sequence has its own latent parameters (initial state, transition matrix, emission matrix) drawn from Dirichlet distributions. The Dirichlet distributions have shared database-level hyper-parameters. So B is the correct option. A is incorrect as LDHMMs do not share latent parameters across sequences. C and D mention incorrect technical details not proposed in the LDHMM model."}, "54": {"documentation": {"title": "Two-Stream Video Classification with Cross-Modality Attention", "source": "Lu Chi and Guiyu Tian and Yadong Mu and Qi Tian", "docs_id": "1908.00497", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Two-Stream Video Classification with Cross-Modality Attention. Fusing multi-modality information is known to be able to effectively bring significant improvement in video classification. However, the most popular method up to now is still simply fusing each stream's prediction scores at the last stage. A valid question is whether there exists a more effective method to fuse information cross modality. With the development of attention mechanism in natural language processing, there emerge many successful applications of attention in the field of computer vision. In this paper, we propose a cross-modality attention operation, which can obtain information from other modality in a more effective way than two-stream. Correspondingly we implement a compatible block named CMA block, which is a wrapper of our proposed attention operation. CMA can be plugged into many existing architectures. In the experiments, we comprehensively compare our method with two-stream and non-local models widely used in video classification. All experiments clearly demonstrate strong performance superiority by our proposed method. We also analyze the advantages of the CMA block by visualizing the attention map, which intuitively shows how the block helps the final prediction."}, "answer": " Question: Which of the following statements best summarizes the key contribution of the paper \"Two-Stream Video Classification with Cross-Modality Attention\"?\n\nA) The paper proposes a new deep learning architecture called CMA that effectively fuses information from different modalities in video data using an attention mechanism.\n\nB) The paper shows that combining prediction scores from different modalities at the last stage is the most effective fusion method for video classification. \n\nC) The paper introduces a new large-scale video classification dataset that advances the state-of-the-art results.\n\nD) The paper demonstrates that attention mechanisms commonly used in NLP can be directly applied to computer vision tasks like video classification.\n\nCorrect Answer: A\n\nExplanation: The paper proposes a cross-modality attention (CMA) operation and block that can fuse information across modalities in video data more effectively than simple late fusion of predictions. The CMA block is shown to improve performance when integrated into existing models."}, "55": {"documentation": {"title": "Contextual Action Recognition with R*CNN", "source": "Georgia Gkioxari, Ross Girshick, Jitendra Malik", "docs_id": "1505.01197", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Contextual Action Recognition with R*CNN. There are multiple cues in an image which reveal what action a person is performing. For example, a jogger has a pose that is characteristic for jogging, but the scene (e.g. road, trail) and the presence of other joggers can be an additional source of information. In this work, we exploit the simple observation that actions are accompanied by contextual cues to build a strong action recognition system. We adapt RCNN to use more than one region for classification while still maintaining the ability to localize the action. We call our system R*CNN. The action-specific models and the feature maps are trained jointly, allowing for action specific representations to emerge. R*CNN achieves 90.2% mean AP on the PASAL VOC Action dataset, outperforming all other approaches in the field by a significant margin. Last, we show that R*CNN is not limited to action recognition. In particular, R*CNN can also be used to tackle fine-grained tasks such as attribute classification. We validate this claim by reporting state-of-the-art performance on the Berkeley Attributes of People dataset."}, "answer": " Here is a 4-option multiple choice question on the given Arxiv documentation:\n\nQuestion: Which of the following statements is true about R*CNN according to the given documentation?\n\nA) R*CNN uses a single region for classification and cannot localize actions. \n\nB) R*CNN achieves worse performance than other approaches on the PASCAL VOC Action dataset.\n\nC) R*CNN can only be used for action recognition tasks.\n\nD) R*CNN adapts RCNN to use multiple regions for classification while maintaining localization ability.\n\nCorrect Answer: D) R*CNN adapts RCNN to use multiple regions for classification while maintaining localization ability.\n\nExplanation: The documentation states that \"We adapt RCNN to use more than one region for classification while still maintaining the ability to localize the action.\" It also states that R*CNN achieves state-of-the-art performance on the PASCAL VOC Action dataset and can be used for tasks beyond just action recognition like attribute classification."}, "56": {"documentation": {"title": "Towards Axiomatic Explanations for Neural Ranking Models", "source": "Michael V\\\"olske, Alexander Bondarenko, Maik Fr\\\"obe, Matthias Hagen,\n  Benno Stein, Jaspreet Singh, Avishek Anand", "docs_id": "2106.08019", "section": ["cs.IR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Towards Axiomatic Explanations for Neural Ranking Models. Recently, neural networks have been successfully employed to improve upon state-of-the-art performance in ad-hoc retrieval tasks via machine-learned ranking functions. While neural retrieval models grow in complexity and impact, little is understood about their correspondence with well-studied IR principles. Recent work on interpretability in machine learning has provided tools and techniques to understand neural models in general, yet there has been little progress towards explaining ranking models. We investigate whether one can explain the behavior of neural ranking models in terms of their congruence with well understood principles of document ranking by using established theories from axiomatic IR. Axiomatic analysis of information retrieval models has formalized a set of constraints on ranking decisions that reasonable retrieval models should fulfill. We operationalize this axiomatic thinking to reproduce rankings based on combinations of elementary constraints. This allows us to investigate to what extent the ranking decisions of neural rankers can be explained in terms of retrieval axioms, and which axioms apply in which situations. Our experimental study considers a comprehensive set of axioms over several representative neural rankers. While the existing axioms can already explain the particularly confident ranking decisions rather well, future work should extend the axiom set to also cover the other still \"unexplainable\" neural IR rank decisions."}, "answer": " Question: Which of the following statements best summarizes the key findings from the given Arxiv documentation?\n\nA) Neural ranking models are well understood in terms of their correspondence with IR principles, as shown by recent interpretability techniques. The existing axioms explain neural rankers' decisions quite comprehensively.\n\nB) While neural rankers achieve state-of-the-art performance, their behavior is not well explained by existing IR axioms. The axioms can explain some confident decisions but many rank decisions remain unexplainable. \n\nC) Neural ranking models completely align with existing IR axioms, providing a full explanation of their behavior. No extensions to the axiom set are needed to cover all neural rank decisions.\n\nD) IR axioms are useless for explaining neural models. Recent interpretability work provides all the tools needed to understand neural rankers.\n\nCorrect Answer: B\n\nExplanation: The key findings are that while neural rankers achieve high performance, their behavior does not fully correspond to existing IR axioms. The axioms can explain some but not all confident rank decisions, with many decisions still unexplainable. So B summarizes this best. A is incorrect as neural rankers are not well explained by current axioms. C is incorrect as not all decisions are explainable. D is incorrect as axioms do explain some decisions, and interpretability tools alone are not enough."}, "57": {"documentation": {"title": "Parichayana: An Eclipse Plugin for Detecting Exception Handling\n  Anti-Patterns and Code Smells in Java Programs", "source": "Ashish Sureka", "docs_id": "1701.00108", "section": ["cs.SE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Parichayana: An Eclipse Plugin for Detecting Exception Handling\n  Anti-Patterns and Code Smells in Java Programs. Anti-patterns and code-smells are signs in the source code which are not defects (does not prevent the program from functioning and does not cause compile errors) and are rather indicators of deeper and bigger problems. Exception handling is a programming construct de- signed to handle the occurrence of anomalous or exceptional conditions (that changes the normal flow of program execution). In this paper, we present an Eclipse plug-in (called as Parichayana) for detecting exception handling anti-patterns and code smells in Java programs. Parichayana is capable of automatically detecting several commonly occurring excep- tion handling programming mistakes. We extend the Eclipse IDE and create new menu entries and associated action via the Parichayana plug- in (free and open-source hosted on GitHub). We compare and contrast Parichayana with several code smell detection tools and demonstrate that our tool provides unique capabilities in context to existing tools. We have created an update site and developers can use the Eclipse up- date manager to install Parichayana from our site. We used Parichyana on several large open-source Java based projects and detected presence of exception handling anti-patterns"}, "answer": " Here is a multi-form question based on the given documentation:\n\nQuestion: Which of the following statements is true about Parichayana according to the documentation?\n\nA) Parichayana is an Eclipse plugin that detects defects and compile errors related to exception handling in Java programs.\n\nB) Parichayana can automatically detect common exception handling anti-patterns and code smells in Java programs. \n\nC) Parichayana is not open source and has to be purchased to use with Eclipse.\n\nD) Parichayana only focuses on code optimization and does not detect problematic patterns.\n\nCorrect Answer: B\n\nExplanation: The documentation states that Parichayana is an open source Eclipse plugin that detects exception handling anti-patterns and code smells, which are indicators of problems but not defects that prevent functioning. It does not detect compile errors and is available on GitHub, not purchased."}, "58": {"documentation": {"title": "AB-Mapper: Attention and BicNet Based Multi-agent Path Finding for\n  Dynamic Crowded Environment", "source": "Huifeng Guan, Yuan Gao, Min Zhao, Yong Yang, Fuqin Deng, Tin Lun Lam", "docs_id": "2110.00760", "section": ["cs.RO", "cs.MA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "AB-Mapper: Attention and BicNet Based Multi-agent Path Finding for\n  Dynamic Crowded Environment. Multi-agent path finding in dynamic crowded environments is of great academic and practical value for multi-robot systems in the real world. To improve the effectiveness and efficiency of communication and learning process during path planning in dynamic crowded environments, we introduce an algorithm called Attention and BicNet based Multi-agent path planning with effective reinforcement (AB-Mapper)under the actor-critic reinforcement learning framework. In this framework, on the one hand, we utilize the BicNet with communication function in the actor-network to achieve intra team coordination. On the other hand, we propose a centralized critic network that can selectively allocate attention weights to surrounding agents. This attention mechanism allows an individual agent to automatically learn a better evaluation of actions by also considering the behaviours of its surrounding agents. Compared with the state-of-the-art method Mapper,our AB-Mapper is more effective (85.86% vs. 81.56% in terms of success rate) in solving the general path finding problems with dynamic obstacles. In addition, in crowded scenarios, our method outperforms the Mapper method by a large margin,reaching a stunning gap of more than 40% for each experiment."}, "answer": " Question: Which of the following statements accurately describes a key contribution of the AB-Mapper algorithm presented in the paper?\n\nA) It uses a centralized critic network with an attention mechanism to enable agents to consider behaviors of surrounding agents when evaluating actions.\n\nB) It relies solely on a BicNet architecture for intra-team coordination during path planning. \n\nC) It focuses on solving path finding in static environments crowded with obstacles.\n\nD) It is unable to effectively handle crowded scenarios, where it performs worse than prior methods.\n\nCorrect Answer: A\n\nExplanation: The paper states that a key contribution of AB-Mapper is the centralized critic network with an attention mechanism that allows agents to consider behaviors of surrounding agents when evaluating actions, not just relying on BicNet for coordination. The method is also said to be effective in crowded scenarios, outperforming prior methods by a large margin."}, "59": {"documentation": {"title": "Heterogeneity-aware Twitter Bot Detection with Relational Graph\n  Transformers", "source": "Shangbin Feng, Zhaoxuan Tan, Rui Li, Minnan Luo", "docs_id": "2109.02927", "section": ["cs.SI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Heterogeneity-aware Twitter Bot Detection with Relational Graph\n  Transformers. Twitter bot detection has become an important and challenging task to combat misinformation and protect the integrity of the online discourse. State-of-the-art approaches generally leverage the topological structure of the Twittersphere, while they neglect the heterogeneity of relations and influence among users. In this paper, we propose a novel bot detection framework to alleviate this problem, which leverages the topological structure of user-formed heterogeneous graphs and models varying influence intensity between users. Specifically, we construct a heterogeneous information network with users as nodes and diversified relations as edges. We then propose relational graph transformers to model heterogeneous influence between users and learn node representations. Finally, we use semantic attention networks to aggregate messages across users and relations and conduct heterogeneity-aware Twitter bot detection. Extensive experiments demonstrate that our proposal outperforms state-of-the-art methods on a comprehensive Twitter bot detection benchmark. Additional studies also bear out the effectiveness of our proposed relational graph transformers, semantic attention networks and the graph-based approach in general."}, "answer": " Question: Which of the following statements is true about the proposed Twitter bot detection framework in the paper?\n\nA) It models homogeneous influence between users in the Twitter network.\n\nB) It constructs a heterogeneous graph with users as nodes and relations as edges. \n\nC) It uses convolutional networks to model relations between users.\n\nD) It relies solely on the textual content generated by users.\n\nCorrect Answer: B\n\nExplanation: The paper proposes a heterogeneity-aware Twitter bot detection framework that constructs a heterogeneous information network with users as nodes and diversified relations as edges. It does not assume homogeneous influence between users. It uses relational graph transformers rather than convolutional networks to model relations. And it leverages the topological structure rather than just textual content."}}