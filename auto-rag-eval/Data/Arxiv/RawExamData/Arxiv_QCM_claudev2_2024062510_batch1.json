{"0": {"documentation": {"title": "Effective Model Sparsification by Scheduled Grow-and-Prune Methods", "source": "Xiaolong Ma, Minghai Qin, Fei Sun, Zejiang Hou, Kun Yuan, Yi Xu,\n  Yanzhi Wang, Yen-Kuang Chen, Rong Jin, Yuan Xie", "docs_id": "2106.09857", "section": ["cs.CV", "cs.AI", "cs.LG", "cs.NE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Effective Model Sparsification by Scheduled Grow-and-Prune Methods. Deep neural networks (DNNs) are effective in solving many real-world problems. Larger DNN models usually exhibit better quality (e.g., accuracy) but their excessive computation results in long inference time. Model sparsification can reduce the computation and memory cost while maintaining model quality. Most existing sparsification algorithms unidirectionally remove weights, while others randomly or greedily explore a small subset of weights in each layer for pruning. The limitations of these algorithms reduce the level of achievable sparsity. In addition, many algorithms still require pre-trained dense models and thus suffer from large memory footprint. In this paper, we propose a novel scheduled grow-and-prune (GaP) methodology without having to pre-train a dense model. It addresses the shortcomings of the previous works by repeatedly growing a subset of layers to dense and then pruning them back to sparse after some training. Experiments show that the models pruned using the proposed methods match or beat the quality of the highly optimized dense models at 80% sparsity on a variety of tasks, such as image classification, objective detection, 3D object part segmentation, and translation. They also outperform other state-of-the-art (SOTA) methods for model sparsification. As an example, a 90% non-uniform sparse ResNet-50 model obtained via GaP achieves 77.9% top-1 accuracy on ImageNet, improving the previous SOTA results by 1.5%. All code will be publicly released."}, "answer": " Question: Which of the following statements is true about the scheduled grow-and-prune (GaP) methodology proposed in the paper?\n\nA) It requires pre-training a dense model before sparsification, leading to large memory footprint.\n\nB) It unidirectionally removes weights during sparsification like most existing methods. \n\nC) It repeatedly grows a subset of layers to dense and then prunes them back to sparse after some training.\n\nD) It randomly explores a small subset of weights in each layer for pruning.\n\nCorrect Answer: C) It repeatedly grows a subset of layers to dense and then prunes them back to sparse after some training.\n\nExplanation: According to the documentation, the proposed GaP methodology does not require pre-training a dense model, and it repeatedly grows and prunes layers, unlike existing unidirectional pruning methods or those that randomly explore weights. So C is the correct option."}, "1": {"documentation": {"title": "Revised Progressive-Hedging-Algorithm Based Two-layer Solution Scheme\n  for Bayesian Reinforcement Learning", "source": "Xin Huang, Duan Li, Daniel Zhuoyu Long", "docs_id": "1906.09035", "section": ["eess.SY", "cs.LG", "cs.SY"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Revised Progressive-Hedging-Algorithm Based Two-layer Solution Scheme\n  for Bayesian Reinforcement Learning. Stochastic control with both inherent random system noise and lack of knowledge on system parameters constitutes the core and fundamental topic in reinforcement learning (RL), especially under non-episodic situations where online learning is much more demanding. This challenge has been notably addressed in Bayesian RL recently where some approximation techniques have been developed to find suboptimal policies. While existing approaches mainly focus on approximating the value function, or on involving Thompson sampling, we propose a novel two-layer solution scheme in this paper to approximate the optimal policy directly, by combining the time-decomposition based dynamic programming (DP) at the lower layer and the scenario-decomposition based revised progressive hedging algorithm (PHA) at the upper layer, for a type of Bayesian RL problem. The key feature of our approach is to separate reducible system uncertainty from irreducible one at two different layers, thus decomposing and conquering. We demonstrate our solution framework more especially via the linear-quadratic-Gaussian problem with unknown gain, which, although seemingly simple, has been a notorious subject over more than half century in dual control."}, "answer": " Here is a multi-form question on the document:\n\nQuestion: Which of the following statements accurately describes a key feature of the proposed two-layer solution scheme for Bayesian reinforcement learning in the paper?\n\nA) It separates reducible system uncertainty from irreducible system uncertainty by using dynamic programming at the lower layer and progressive hedging algorithm at the upper layer. \n\nB) It approximates the optimal policy directly using progressive hedging algorithm at the lower layer and dynamic programming at the upper layer.\n\nC) It combines dynamic programming and progressive hedging algorithm at the lower layer to approximate the value function.\n\nD) It uses Thompson sampling to address the challenge of online learning in non-episodic situations.\n\nCorrect Answer: A\n\nExplanation: \nThe key feature of the proposed two-layer solution scheme is that it separates reducible system uncertainty from irreducible system uncertainty at two different layers, using dynamic programming at the lower layer and progressive hedging algorithm at the upper layer to approximate the optimal policy directly. Options B, C, and D describe different approaches not used in the proposed scheme."}, "2": {"documentation": {"title": "Identifying Object States in Cooking-Related Images", "source": "Ahmad Babaeian Jelodar, Md Sirajus Salekin, Yu Sun", "docs_id": "1805.06956", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Identifying Object States in Cooking-Related Images. Understanding object states is as important as object recognition for robotic task planning and manipulation. To our knowledge, this paper explicitly introduces and addresses the state identification problem in cooking related images for the first time. In this paper, objects and ingredients in cooking videos are explored and the most frequent objects are analyzed. Eleven states from the most frequent cooking objects are examined and a dataset of images containing those objects and their states is created. As a solution to the state identification problem, a Resnet based deep model is proposed. The model is initialized with Imagenet weights and trained on the dataset of eleven classes. The trained state identification model is evaluated on a subset of the Imagenet dataset and state labels are provided using a combination of the model with manual checking. Moreover, an individual model is fine-tuned for each object in the dataset using the weights from the initially trained model and object-specific images, where significant improvement is demonstrated."}, "answer": " Here is a 4-option multiple choice question on the given text:\n\nQuestion: Which of the following statements is true about the dataset created in the paper?\n\nA) The dataset contains images of the most frequent cooking objects in their various states. \n\nB) The dataset has images of 10 different object states.\n\nC) The dataset focuses on rare and unusual cooking objects. \n\nD) The dataset only covers 3 common cooking objects.\n\nCorrect Answer: A\n\nExplanation: According to the text, the authors created a dataset containing images of the most frequent cooking objects in various states. 11 states of the most common objects were examined and included in the dataset. Therefore, option A is correct. Options B, C and D are incorrect based on the information provided in the text."}, "3": {"documentation": {"title": "Managing Recurrent Virtual Network Updates in Multi-Tenant Datacenters:\n  A System Perspective", "source": "Zhuotao Liu and Yuan Cao and Xuewu Zhang and Changping Zhu and Fan\n  Zhang", "docs_id": "1903.09465", "section": ["cs.CR", "cs.NI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Managing Recurrent Virtual Network Updates in Multi-Tenant Datacenters:\n  A System Perspective. With the advent of software-defined networking, network configuration through programmable interfaces becomes practical, leading to various on-demand opportunities for network routing update in multi-tenant datacenters, where tenants have diverse requirements on network routings such as short latency, low path inflation, large bandwidth, high reliability, etc. Conventional solutions that rely on topology search coupled with an objective function https:// www.overleaf.com/project/5beb742041ab9c0e3caec84f to find desired routings have at least two shortcomings: (i) they run into scalability issues when handling consistent and frequent routing updates and (ii) they restrict the flexibility and capability to satisfy various routing requirements. To address these issues, this paper proposes a novel search and optimization decoupled design, which not only saves considerable topology search costs via search result reuse, but also avoids possible sub-optimality in greedy routing search algorithms by making decisions based on the global view of all possible routings. We implement a prototype of our proposed system, OpReduce, and perform extensive evaluations to validate its design goals."}, "answer": " Question: What are the two main shortcomings of conventional solutions for managing recurrent virtual network updates in multi-tenant datacenters according to the paper?\n\nA) They have scalability issues when handling frequent routing updates and they restrict flexibility in satisfying various routing requirements.\n\nB) They rely too much on topology search and do not consider tenant requirements like latency and bandwidth. \n\nC) They do not properly optimize the objective function to find the best network routings.\n\nD) They lack integration with software-defined networking and have limited programmable interfaces.\n\nCorrect Answer: A\n\nExplanation: The passage states that conventional solutions \"run into scalability issues when handling consistent and frequent routing updates and they restrict the flexibility and capability to satisfy various routing requirements.\""}, "4": {"documentation": {"title": "A performance study of some approximation algorithms for minimum\n  dominating set in a graph", "source": "Jonathan S. Li, Rohan Potru, Farhad Shahrokhi", "docs_id": "2009.04636", "section": ["cs.DS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A performance study of some approximation algorithms for minimum\n  dominating set in a graph. We implement and test the performances of several approximation algorithms for computing the minimum dominating set of a graph. These algorithms are the standard greedy algorithm, the recent LP rounding algorithms and a hybrid algorithm that we design by combining the greedy and LP rounding algorithms. All algorithms perform better than anticipated in their theoretical analysis, and have small performance ratios, measured as the size of output divided by the LP objective lower-bound. However, each may have advantages over the others. For instance, LP rounding algorithm normally outperforms the other algorithms on sparse real-world graphs. On a graph with 400,000+ vertices, LP rounding took less than 15 seconds of CPU time to generate a solution with performance ratio 1.011, while the greedy and hybrid algorithms generated solutions of performance ratio 1.12 in similar time. For synthetic graphs, the hybrid algorithm normally outperforms the others, whereas for hypercubes and k-Queens graphs, greedy outperforms the rest. Another advantage of the hybrid algorithm is to solve very large problems where LP solvers crash, as demonstrated on a real-world graph with 7.7 million+ vertices."}, "answer": " Here is a multi-form question based on the given text:\n\nQuestion: Which algorithm performed the best on sparse real-world graphs with over 400,000 vertices in the study?\n\nA) The greedy algorithm, which generated a solution with a performance ratio of 1.12.\n\nB) The LP rounding algorithm, which generated a solution with a performance ratio of 1.011 in under 15 seconds. \n\nC) The hybrid algorithm, which normally outperformed the others.\n\nD) All algorithms performed similarly on these graphs.\n\nCorrect Answer: B\n\nExplanation: According to the text, on a sparse real-world graph with over 400,000 vertices, the LP rounding algorithm generated a solution with a performance ratio of 1.011 in under 15 seconds. This was the best performance on this type of graph in the study."}, "5": {"documentation": {"title": "Efficient Spatio-Temporal Recurrent Neural Network for Video Deblurring", "source": "Zhihang Zhong, Ye Gao, Yinqiang Zheng, Bo Zheng, and Imari Sato", "docs_id": "2106.16028", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Efficient Spatio-Temporal Recurrent Neural Network for Video Deblurring. Real-time video deblurring still remains a challenging task due to the complexity of spatially and temporally varying blur itself and the requirement of low computational cost. To improve the network efficiency, we adopt residual dense blocks into RNN cells, so as to efficiently extract the spatial features of the current frame. Furthermore, a global spatio-temporal attention module is proposed to fuse the effective hierarchical features from past and future frames to help better deblur the current frame. Another issue needs to be addressed urgently is the lack of a real-world benchmark dataset. Thus, we contribute a novel dataset (BSD) to the community, by collecting paired blurry/sharp video clips using a co-axis beam splitter acquisition system. Experimental results show that the proposed method (ESTRNN) can achieve better deblurring performance both quantitatively and qualitatively with less computational cost against state-of-the-art video deblurring methods. In addition, cross-validation experiments between datasets illustrate the high generality of BSD over the synthetic datasets. The code and dataset are released at https://github.com/zzh-tech/ESTRNN."}, "answer": " Question: What method does the paper propose for efficient video deblurring?\n\nA) A spatio-temporal convolutional network with residual connections\nB) A recurrent neural network with dense blocks in the RNN cells \nC) A generative adversarial network with a spatio-temporal discriminator\nD) A transformer network with attention over spatial and temporal dimensions\n\nCorrect Answer: B) A recurrent neural network with dense blocks in the RNN cells\n\nExplanation: The paper proposes using residual dense blocks in the RNN cells to efficiently extract spatial features in each frame. It also uses a global spatio-temporal attention module to fuse features across frames. So the core of the method is a recurrent neural network with dense blocks, making B the correct answer."}, "6": {"documentation": {"title": "Dense Registration and Mosaicking of Fingerprints by Training an\n  End-to-End Network", "source": "Zhe Cui, Jianjiang Feng, Jie Zhou", "docs_id": "2004.05972", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Dense Registration and Mosaicking of Fingerprints by Training an\n  End-to-End Network. Dense registration of fingerprints is a challenging task due to elastic skin distortion, low image quality, and self-similarity of ridge pattern. To overcome the limitation of handcraft features, we propose to train an end-to-end network to directly output pixel-wise displacement field between two fingerprints. The proposed network includes a siamese network for feature embedding, and a following encoder-decoder network for regressing displacement field. By applying displacement fields reliably estimated by tracing high quality fingerprint videos to challenging fingerprints, we synthesize a large number of training fingerprint pairs with ground truth displacement fields. In addition, based on the proposed registration algorithm, we propose a fingerprint mosaicking method based on optimal seam selection. Registration and matching experiments on FVC2004 databases, Tsinghua Distorted Fingerprint (TDF) database, and NIST SD27 latent fingerprint database show that our registration method outperforms previous dense registration methods in accuracy and efficiency. Mosaicking experiment on FVC2004 DB1 demonstrates that the proposed algorithm produced higher quality fingerprints than other algorithms which also validates the performance of our registration algorithm."}, "answer": " Question: What method does the paper propose for fingerprint registration to overcome limitations of handcrafted features?\n\nA) Using a convolutional neural network trained on synthesized fingerprint pairs with ground truth displacement fields.\n\nB) Extracting scale invariant feature transform (SIFT) keypoints and matching them. \n\nC) Applying optical flow on binarized fingerprint images.\n\nD) Matching minutiae points extracted using a minutiae extraction algorithm.\n\nCorrect Answer: A\n\nExplanation: The paper proposes training an end-to-end network on synthesized fingerprint pairs with ground truth displacement fields to directly estimate pixel-wise displacement between fingerprints. This overcomes limitations of handcrafted features like SIFT or minutiae points."}, "7": {"documentation": {"title": "Automated Crowdturfing Attacks and Defenses in Online Review Systems", "source": "Yuanshun Yao, Bimal Viswanath, Jenna Cryan, Haitao Zheng, Ben Y. Zhao", "docs_id": "1708.08151", "section": ["cs.CR", "cs.SI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Automated Crowdturfing Attacks and Defenses in Online Review Systems. Malicious crowdsourcing forums are gaining traction as sources of spreading misinformation online, but are limited by the costs of hiring and managing human workers. In this paper, we identify a new class of attacks that leverage deep learning language models (Recurrent Neural Networks or RNNs) to automate the generation of fake online reviews for products and services. Not only are these attacks cheap and therefore more scalable, but they can control rate of content output to eliminate the signature burstiness that makes crowdsourced campaigns easy to detect. Using Yelp reviews as an example platform, we show how a two phased review generation and customization attack can produce reviews that are indistinguishable by state-of-the-art statistical detectors. We conduct a survey-based user study to show these reviews not only evade human detection, but also score high on \"usefulness\" metrics by users. Finally, we develop novel automated defenses against these attacks, by leveraging the lossy transformation introduced by the RNN training and generation cycle. We consider countermeasures against our mechanisms, show that they produce unattractive cost-benefit tradeoffs for attackers, and that they can be further curtailed by simple constraints imposed by online service providers."}, "answer": " Here is a multi-form question based on the given text:\n\nQuestion: What technique do the authors propose to automate the generation of fake online reviews?\n\nA) Using crowdsourcing forums to hire human workers to write fake reviews.\n\nB) Leveraging deep learning language models like RNNs to automatically generate fake reviews. \n\nC) Manually writing fake reviews and posting them in bursts to avoid detection.\n\nD) Having human workers customize automatically generated fake reviews before posting.\n\nCorrect Answer: B\n\nExplanation: The authors propose using deep learning language models like RNNs to automatically generate fake online reviews, which is cheaper and more scalable than hiring human workers."}, "8": {"documentation": {"title": "Causal Feature Selection with Dimension Reduction for Interpretable Text\n  Classification", "source": "Guohou Shan, James Foulds, Shimei Pan", "docs_id": "2010.04609", "section": ["cs.LG", "cs.CL", "cs.IR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Causal Feature Selection with Dimension Reduction for Interpretable Text\n  Classification. Text features that are correlated with class labels, but do not directly cause them, are sometimesuseful for prediction, but they may not be insightful. As an alternative to traditional correlation-basedfeature selection, causal inference could reveal more principled, meaningful relationships betweentext features and labels. To help researchers gain insight into text data, e.g. for social scienceapplications, in this paper we investigate a class of matching-based causal inference methods fortext feature selection. Features used in document classification are often high dimensional, howeverexisting causal feature selection methods use Propensity Score Matching (PSM) which is known to beless effective in high-dimensional spaces. We propose a new causal feature selection framework thatcombines dimension reduction with causal inference to improve text feature selection. Experiments onboth synthetic and real-world data demonstrate the promise of our methods in improving classificationand enhancing interpretability."}, "answer": " Here is a 4-option multiple choice question on the given text:\n\nQuestion: Which of the following statements accurately describes a key contribution of the paper?\n\nA) The paper proposes using causal inference instead of correlation to select text features that directly cause class labels, which enhances interpretability. \n\nB) The paper investigates using Propensity Score Matching for causal feature selection in high-dimensional text data, which improves classification performance.\n\nC) The paper combines dimension reduction with causal inference for text feature selection, addressing limitations of existing methods in high dimensions.\n\nD) The paper shows that traditional correlation-based methods are superior to causal inference for insightful text classification and feature selection.\n\nCorrect Answer: C\n\nExplanation: The paper proposes a new causal feature selection framework that combines dimension reduction with causal inference to address limitations of existing causal methods like Propensity Score Matching in high dimensional text data. This improves interpretability by revealing meaningful relationships between text features and labels."}, "9": {"documentation": {"title": "A dynamic mode decomposition extension for the forecasting of parametric\n  dynamical systems", "source": "Francesco Andreuzzi and Nicola Demo and Gianluigi Rozza", "docs_id": "2110.09155", "section": ["math.NA", "cs.NA", "math.DS"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A dynamic mode decomposition extension for the forecasting of parametric\n  dynamical systems. Dynamic mode decomposition (DMD) has recently become a popular tool for the non-intrusive analysis of dynamical systems. Exploiting the proper orthogonal decomposition as dimensionality reduction technique, DMD is able to approximate a dynamical system as a sum of (spatial) basis evolving linearly in time, allowing for a better understanding of the physical phenomena or for a future forecasting. We propose in this contribution an extension of the DMD to parametrized dynamical systems, focusing on the future forecasting of the output of interest in a parametric context. Initially, all the snapshots -- for different parameters and different time instants -- are projected to the reduced space, employing the DMD (or one of its variants) to approximate the reduced snapshots for a future instants. Still exploiting the low dimension of the reduced space, the predicted reduced snapshots are then combined using a regression technique, enabling the possibility to approximate any untested parametric configuration in any future instant. We are going to present here the algorithmic core of the aforementioned method, presenting at the end three different test cases with incremental complexity: a simple dynamical system with a linear parameter dependency, a heat problem with nonlinear parameter dependency and a fluid dynamics problem with nonlinear parameter dependency."}, "answer": " Here is a 4-option multiple choice question on the document:\n\nQuestion: Which of the following statements is true about the proposed extension of dynamic mode decomposition (DMD) described in the document?\n\nA) It focuses on approximating the future states of a dynamical system by combining predicted reduced snapshots using regression.\n\nB) It employs proper orthogonal decomposition to reduce the dimensionality of the snapshots before applying standard DMD. \n\nC) It is able to handle dynamical systems with nonlinear parameter dependencies, as shown in the heat and fluid examples.\n\nD) It approximates the dynamical system as a sum of spatial basis functions evolving linearly in time.\n\nCorrect Answer: C\n\nExplanation: \nOption A is true according to the document - the proposed extension combines predicted reduced snapshots using regression to approximate future states.\nOption B is also stated in the document - proper orthogonal decomposition is used for dimensionality reduction before DMD.\nOption C is correct - the examples show the method can handle nonlinear parameter dependencies. \nOption D describes standard DMD, not the proposed extension."}, "10": {"documentation": {"title": "Graph partitions and cluster synchronization in networks of oscillators", "source": "Michael T. Schaub, Neave O'Clery, Yazan N. Billeh, Jean-Charles\n  Delvenne, Renaud Lambiotte and Mauricio Barahona", "docs_id": "1608.04283", "section": ["physics.soc-ph", "cs.SI", "cs.SY", "nlin.CD"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Graph partitions and cluster synchronization in networks of oscillators. Synchronization over networks depends strongly on the structure of the coupling between the oscillators. When the coupling presents certain regularities, the dynamics can be coarse-grained into clusters by means of External Equitable Partitions of the network graph and their associated quotient graphs. We exploit this graph-theoretical concept to study the phenomenon of cluster synchronization, in which different groups of nodes converge to distinct behaviors. We derive conditions and properties of networks in which such clustered behavior emerges, and show that the ensuing dynamics is the result of the localization of the eigenvectors of the associated graph Laplacians linked to the existence of invariant subspaces. The framework is applied to both linear and non-linear models, first for the standard case of networks with positive edges, before being generalized to the case of signed networks with both positive and negative interactions. We illustrate our results with examples of both signed and unsigned graphs for consensus dynamics and for partial synchronization of oscillator networks under the master stability function as well as Kuramoto oscillators."}, "answer": " Question: Which of the following statements best summarizes a key finding from the given documentation on graph partitions and cluster synchronization in networks of oscillators?\n\nA) Cluster synchronization emerges when networks have external equitable partitions and associated quotient graphs that allow coarse-graining of the dynamics into clusters.\n\nB) The phenomenon of cluster synchronization depends only on the structure of the coupling between oscillators, not on the specifics of the oscillator dynamics. \n\nC) Synchronization over networks is completely determined by the signs (positive or negative) of the couplings between oscillators.\n\nD) Cluster synchronization occurs when different groups of nodes converge to the same behavior, not distinct behaviors.\n\nCorrect Answer: A\n\nExplanation: The documentation states that \"when the coupling presents certain regularities, the dynamics can be coarse-grained into clusters by means of External Equitable Partitions of the network graph and their associated quotient graphs.\" This suggests that these graph partitions and quotient graphs enable cluster synchronization where groups of nodes converge to distinct behaviors."}, "11": {"documentation": {"title": "Towards Better Models of Externalities in Sponsored Search Auctions", "source": "Nicola Gatti, Marco Rocco, Paolo Serafino, Carmine Ventre", "docs_id": "1604.04095", "section": ["cs.GT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Towards Better Models of Externalities in Sponsored Search Auctions. Sponsored Search Auctions (SSAs) arguably represent the problem at the intersection of computer science and economics with the deepest applications in real life. Within the realm of SSAs, the study of the effects that showing one ad has on the other ads, a.k.a. externalities in economics, is of utmost importance and has so far attracted the attention of much research. However, even the basic question of modeling the problem has so far escaped a definitive answer. The popular cascade model is arguably too idealized to really describe the phenomenon yet it allows a good comprehension of the problem. Other models, instead, describe the setting more adequately but are too complex to permit a satisfactory theoretical analysis. In this work, we attempt to get the best of both approaches: firstly, we define a number of general mathematical formulations for the problem in the attempt to have a rich description of externalities in SSAs and, secondly, prove a host of results drawing a nearly complete picture about the computational complexity of the problem. We complement these approximability results with some considerations about mechanism design in our context."}, "answer": " Here is a 4-option multiple choice question on the abstract:\n\nQuestion: Which of the following statements best summarizes the key contributions of the paper \"Towards Better Models of Externalities in Sponsored Search Auctions\"?\n\nA) The paper proposes new mathematical models of externalities in sponsored search auctions that better capture real-world effects compared to prior models, and analyzes the computational complexity of these new models.\n\nB) The paper shows that the cascade model is the best way to model externalities in sponsored search auctions, and uses this model to design new auction mechanisms. \n\nC) The paper proves that modeling externalities in sponsored search auctions is computationally intractable no matter what mathematical model is used.\n\nD) The paper argues that externalities have little impact on sponsored search auctions, so complex models are unnecessary.\n\nCorrect Answer: A\n\nExplanation: The abstract states that the paper defines new general mathematical models to better capture externalities compared to prior simplified models like the cascade model, and analyzes the computational complexity of these new models. It does not claim the cascade model is definitively the best (B is incorrect), that the problem is completely intractable (C is incorrect), or that externalities are unimportant (D is incorrect)."}, "12": {"documentation": {"title": "Few-Shot Scene Adaptive Crowd Counting Using Meta-Learning", "source": "Mahesh Kumar Krishna Reddy, Mohammad Hossain, Mrigank Rochan and Yang\n  Wang", "docs_id": "2002.00264", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Few-Shot Scene Adaptive Crowd Counting Using Meta-Learning. We consider the problem of few-shot scene adaptive crowd counting. Given a target camera scene, our goal is to adapt a model to this specific scene with only a few labeled images of that scene. The solution to this problem has potential applications in numerous real-world scenarios, where we ideally like to deploy a crowd counting model specially adapted to a target camera. We accomplish this challenge by taking inspiration from the recently introduced learning-to-learn paradigm in the context of few-shot regime. In training, our method learns the model parameters in a way that facilitates the fast adaptation to the target scene. At test time, given a target scene with a small number of labeled data, our method quickly adapts to that scene with a few gradient updates to the learned parameters. Our extensive experimental results show that the proposed approach outperforms other alternatives in few-shot scene adaptive crowd counting. Code is available at https://github.com/maheshkkumar/fscc."}, "answer": " Question: What is the goal of the proposed few-shot scene adaptive crowd counting method described in the paper?\n\nA) To count the number of people in a few sample images from a target camera scene.\n\nB) To adapt a pre-trained model to a new target camera scene using just a few labeled images from that scene. \n\nC) To train a model on a large labeled dataset of crowd images that generalizes well to any new scene.\n\nD) To label a few images from a new scene and use them to fine-tune a pre-trained model through several epochs of training.\n\nCorrect Answer: B\n\nExplanation: According to the documentation, the goal is to adapt a model to a specific new target camera scene using just a few labeled images from that scene, through a meta-learning based few-shot learning approach."}, "13": {"documentation": {"title": "MigrantStore: Leveraging Virtual Memory in DRAM-PCM Memory Architecture", "source": "Hamza Bin Sohail, Balajee Vamanan, T. N. Vijaykumar", "docs_id": "1504.04297", "section": ["cs.AR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "MigrantStore: Leveraging Virtual Memory in DRAM-PCM Memory Architecture. With the imminent slowing down of DRAM scaling, Phase Change Memory (PCM) is emerging as a lead alternative for main memory technology. While PCM achieves low energy due to various technology-specific advantages, PCM is significantly slower than DRAM (especially for writes) and can endure far fewer writes before wearing out. Previous work has proposed to use a large, DRAM-based hardware cache to absorb writes and provide faster access. However, due to ineffectual caching where blocks are evicted before sufficient number of accesses, hardware caches incur significant overheads in energy and bandwidth, two key but scarce resources in modern multicores. Because using hardware for detecting and removing such ineffectual caching would incur additional hardware cost and complexity, we leverage the OS virtual memory support for this purpose. We propose a DRAM-PCM hybrid memory architecture where the OS migrates pages on demand from the PCM to DRAM. We call the DRAM part of our memory as MigrantStore which includes two ideas. First, to reduce the energy, bandwidth, and wear overhead of ineffectual migrations, we propose migration hysteresis. Second, to reduce the software overhead of good replacement policies, we propose recently- accessed-page-id (RAPid) buffer, a hardware buffer to track the addresses of recently-accessed MigrantStore pages."}, "answer": " Here is a multi-form question on the given documentation:\n\nQuestion: Which of the following is a key idea proposed in MigrantStore to reduce the overhead of ineffectual page migrations between DRAM and PCM?\n\nA) Using a hardware cache to absorb writes and provide faster access to PCM.\n\nB) Leveraging the OS virtual memory support to migrate pages on demand. \n\nC) Migration hysteresis to reduce the overhead of migrating pages that will soon be evicted.\n\nD) A buffer to track recently accessed pages to guide page replacement.\n\nCorrect Answer: C\n\nExplanation: The documentation states that MigrantStore proposes migration hysteresis to reduce the energy, bandwidth, and wear overhead of ineffectual migrations where pages are evicted before sufficient accesses. Options B and D relate to other ideas in MigrantStore but not specifically reducing ineffectual migrations."}, "14": {"documentation": {"title": "Tradeoffs Between Information and Ordinal Approximation for Bipartite\n  Matching", "source": "Elliot Anshelevich and Wennan Zhu", "docs_id": "1707.01608", "section": ["cs.GT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Tradeoffs Between Information and Ordinal Approximation for Bipartite\n  Matching. We study ordinal approximation algorithms for maximum-weight bipartite matchings. Such algorithms only know the ordinal preferences of the agents/nodes in the graph for their preferred matches, but must compete with fully omniscient algorithms which know the true numerical edge weights (utilities). %instead of only their relative orderings. Ordinal approximation is all about being able to produce good results with only limited information. Because of this, one important question is how much better the algorithms can be as the amount of information increases. To address this question for forming high-utility matchings between agents in $\\mathcal{X}$ and $\\mathcal{Y}$, we consider three ordinal information types: when we know the preference order of only nodes in $\\mathcal{X}$ for nodes in $\\mathcal{Y}$, when we know the preferences of both $\\mathcal{X}$ and $\\mathcal{Y}$, and when we know the total order of the edge weights in the entire graph, although not the weights themselves. We also consider settings where only the top preferences of the agents are known to us, instead of their full preference orderings. We design new ordinal approximation algorithms for each of these settings, and quantify how well such algorithms perform as the amount of information given to them increases."}, "answer": " Here is a multi-form question on the document:\n\nQuestion: Which of the following statements accurately summarizes a key contribution of the paper?\n\nA) The paper studies ordinal approximation algorithms for maximum-weight bipartite matchings, where the algorithms only know the ordinal preferences of agents rather than the true numerical edge weights. It analyzes the performance of algorithms given different amounts of ordinal information.\n\nB) The paper proposes new techniques for learning the true numerical edge weights in a bipartite matching problem given only ordinal preferences. It shows these techniques can recover near optimal matchings. \n\nC) The paper proves that knowing only the top preferences of agents is sufficient to find the optimal maximum-weight bipartite matching. Algorithms only need top preferences rather than full ordinal information.\n\nD) The paper shows that ordinal approximation algorithms can perform just as well as algorithms that know the true numerical edge weights for bipartite matching problems. No information is lost with ordinal preferences.\n\nCorrect Answer: A\n\nExplanation: A accurately summarizes that the paper studies ordinal approximation algorithms with limited information and analyzes their performance given different amounts of ordinal preference information. B is incorrect as the paper does not propose techniques for learning true edge weights. C is incorrect as the paper does not prove top preferences are sufficient for optimality. D is incorrect as the paper shows ordinal algorithms generally perform worse than with full numerical information."}, "15": {"documentation": {"title": "Fooling Detection Alone is Not Enough: First Adversarial Attack against\n  Multiple Object Tracking", "source": "Yunhan Jia, Yantao Lu, Junjie Shen, Qi Alfred Chen, Zhenyu Zhong, Tao\n  Wei", "docs_id": "1905.11026", "section": ["cs.CV", "cs.CR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Fooling Detection Alone is Not Enough: First Adversarial Attack against\n  Multiple Object Tracking. Recent work in adversarial machine learning started to focus on the visual perception in autonomous driving and studied Adversarial Examples (AEs) for object detection models. However, in such visual perception pipeline the detected objects must also be tracked, in a process called Multiple Object Tracking (MOT), to build the moving trajectories of surrounding obstacles. Since MOT is designed to be robust against errors in object detection, it poses a general challenge to existing attack techniques that blindly target objection detection: we find that a success rate of over 98% is needed for them to actually affect the tracking results, a requirement that no existing attack technique can satisfy. In this paper, we are the first to study adversarial machine learning attacks against the complete visual perception pipeline in autonomous driving, and discover a novel attack technique, tracker hijacking, that can effectively fool MOT using AEs on object detection. Using our technique, successful AEs on as few as one single frame can move an existing object in to or out of the headway of an autonomous vehicle to cause potential safety hazards. We perform evaluation using the Berkeley Deep Drive dataset and find that on average when 3 frames are attacked, our attack can have a nearly 100% success rate while attacks that blindly target object detection only have up to 25%."}, "answer": " Question: What is the key finding of the research described in the given passage?\n\nA) Adversarial attacks on object detection alone are not sufficient to fool multiple object tracking in autonomous driving systems. A new attack technique called tracker hijacking can more effectively attack the full perception pipeline.\n\nB) Multiple object tracking systems are very robust to errors in object detection, so existing adversarial attack techniques that target object detection cannot fool tracking. \n\nC) Attacking 3 frames with adversarial examples can achieve close to 100% attack success on multiple object tracking, while attacking object detection alone has only 25% success.\n\nD) Autonomous driving systems are vulnerable to adversarial attacks that can move objects into the vehicle's path and cause safety issues.\n\nCorrect Answer: C\n\nExplanation: The key finding is that attacking just object detection is not enough, but the new tracker hijacking technique that targets the full pipeline can attack multiple object tracking with close to 100% success by attacking just 3 frames, while attacking object detection alone has only 25% success."}, "16": {"documentation": {"title": "Makespan Minimization via Posted Prices", "source": "Michal Feldman, Amos Fiat, Alan Roytman", "docs_id": "1705.01965", "section": ["cs.GT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Makespan Minimization via Posted Prices. We consider job scheduling settings, with multiple machines, where jobs arrive online and choose a machine selfishly so as to minimize their cost. Our objective is the classic makespan minimization objective, which corresponds to the completion time of the last job to complete. The incentives of the selfish jobs may lead to poor performance. To reconcile the differing objectives, we introduce posted machine prices. The selfish job seeks to minimize the sum of its completion time on the machine and the posted price for the machine. Prices may be static (i.e., set once and for all before any arrival) or dynamic (i.e., change over time), but they are determined only by the past, assuming nothing about upcoming events. Obviously, such schemes are inherently truthful. We consider the competitive ratio: the ratio between the makespan achievable by the pricing scheme and that of the optimal algorithm. We give tight bounds on the competitive ratio for both dynamic and static pricing schemes for identical, restricted, related, and unrelated machine settings. Our main result is a dynamic pricing scheme for related machines that gives a constant competitive ratio, essentially matching the competitive ratio of online algorithms for this setting. In contrast, dynamic pricing gives poor performance for unrelated machines. This lower bound also exhibits a gap between what can be achieved by pricing versus what can be achieved by online algorithms."}, "answer": " Here is a 4-option multiple choice question on the document:\n\nQuestion: Which of the following statements is true about the dynamic pricing scheme described in the document?\n\nA) It achieves a constant competitive ratio for unrelated machines, matching the performance of online algorithms.\n\nB) It gives poor performance for unrelated machines, exhibiting a gap from online algorithms. \n\nC) It achieves a good competitive ratio for restricted machines, similar to online algorithms.\n\nD) It results in a poor competitive ratio for related machines compared to online algorithms.\n\nCorrect Answer: B\n\nExplanation: The document states that the main result is that the dynamic pricing scheme achieves a constant competitive ratio for related machines, essentially matching online algorithms. However, it also states that dynamic pricing gives poor performance for unrelated machines, exhibiting a gap from online algorithms."}, "17": {"documentation": {"title": "SIMD Vectorization for the Lennard-Jones Potential with AVX2 and AVX-512\n  instructions", "source": "Hiroshi Watanabe and Koh M. Nakagawa", "docs_id": "1806.05713", "section": ["cs.MS", "cs.CE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "SIMD Vectorization for the Lennard-Jones Potential with AVX2 and AVX-512\n  instructions. This work describes the SIMD vectorization of the force calculation of the Lennard-Jones potential with Intel AVX2 and AVX-512 instruction sets. Since the force-calculation kernel of the molecular dynamics method involves indirect access to memory, the data layout is one of the most important factors in vectorization. We find that the Array of Structures (AoS) with padding exhibits better performance than Structure of Arrays (SoA) with appropriate vectorization and optimizations. In particular, AoS with 512-bit width exhibits the best performance among the architectures. While the difference in performance between AoS and SoA is significant for the vectorization with AVX2, that with AVX-512 is minor. The effect of other optimization techniques, such as software pipelining together with vectorization, is also discussed. We present results for benchmarks on three CPU architectures: Intel Haswell (HSW), Knights Landing (KNL), and Skylake (SKL). The performance gains by vectorization are about 42\\% on HSW compared with the code optimized without vectorization. On KNL, the hand-vectorized codes exhibit 34\\% better performance than the codes vectorized automatically by the Intel compiler. On SKL, the code vectorized with AVX2 exhibits slightly better performance than that with vectorized AVX-512."}, "answer": " Here is a multi-form question on the SIMD vectorization of the Lennard-Jones potential:\n\nQuestion: Which CPU architecture exhibited the best performance for the AoS data layout with 512-bit vector width in the SIMD vectorization of the Lennard-Jones potential?\n\nA) Intel Haswell (HSW)\nB) Intel Knights Landing (KNL) \nC) Intel Skylake (SKL)\nD) Intel Ivy Bridge (IVB)\n\nCorrect Answer: B\n\nExplanation: \nThe documentation states that \"AoS with 512-bit width exhibits the best performance among the architectures.\" It also mentions benchmark results were presented for Haswell, Knights Landing, and Skylake. Since Knights Landing supports 512-bit vector operations with AVX-512, it would perform the best with the AoS layout using 512-bit vectors."}, "18": {"documentation": {"title": "An Extensible and Personalizable Multi-Modal Trip Planner", "source": "Xudong Liu, Christian Fritz, Matthew Klenk", "docs_id": "1909.11604", "section": ["cs.AI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "An Extensible and Personalizable Multi-Modal Trip Planner. Despite a tremendous amount of work in the literature and in the commercial sectors, current approaches to multi-modal trip planning still fail to consistently generate plans that users deem optimal in practice. We believe that this is due to the fact that current planners fail to capture the true preferences of users, e.g., their preferences depend on aspects that are not modeled. An example of this could be a preference not to walk through an unsafe area at night. We present a novel multi-modal trip planner that allows users to upload auxiliary geographic data (e.g., crime rates) and to specify temporal constraints and preferences over these data in combination with typical metrics such as time and cost. Concretely, our planner supports the modes walking, biking, driving, public transit, and taxi, uses linear temporal logic to capture temporal constraints, and preferential cost functions to represent preferences. We show by examples that this allows the expression of very interesting preferences and constraints that, naturally, lead to quite diverse optimal plans."}, "answer": " Question: Which of the following statements best summarizes a key limitation of current multi-modal trip planning systems according to the given text?\n\nA) Current systems fail to find the fastest route.\n\nB) Current systems do not allow users to specify temporal constraints and auxiliary data. \n\nC) Current systems fail to capture users' true preferences that depend on unmodeled aspects.\n\nD) Current systems do not support enough transportation modes.\n\nCorrect Answer: C\n\nExplanation: The text states that \"current approaches to multi-modal trip planning still fail to consistently generate plans that users deem optimal in practice\" because \"current planners fail to capture the true preferences of users, e.g., their preferences depend on aspects that are not modeled.\" This suggests a key limitation is that current systems fail to capture users' true preferences that depend on unmodeled aspects."}, "19": {"documentation": {"title": "Variational Noise-Contrastive Estimation", "source": "Benjamin Rhodes, Michael Gutmann", "docs_id": "1810.08010", "section": ["stat.ML", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Variational Noise-Contrastive Estimation. Unnormalised latent variable models are a broad and flexible class of statistical models. However, learning their parameters from data is intractable, and few estimation techniques are currently available for such models. To increase the number of techniques in our arsenal, we propose variational noise-contrastive estimation (VNCE), building on NCE which is a method that only applies to unnormalised models. The core idea is to use a variational lower bound to the NCE objective function, which can be optimised in the same fashion as the evidence lower bound (ELBO) in standard variational inference (VI). We prove that VNCE can be used for both parameter estimation of unnormalised models and posterior inference of latent variables. The developed theory shows that VNCE has the same level of generality as standard VI, meaning that advances made there can be directly imported to the unnormalised setting. We validate VNCE on toy models and apply it to a realistic problem of estimating an undirected graphical model from incomplete data."}, "answer": " Question: Which of the following statements is true about Variational Noise-Contrastive Estimation (VNCE) according to the given text?\n\nA) VNCE is a technique that applies to both normalised and unnormalised latent variable models.\n\nB) VNCE lower bounds the evidence lower bound (ELBO) used in standard variational inference. \n\nC) VNCE can be used for parameter estimation but not for posterior inference of latent variables.\n\nD) VNCE has limited generality compared to standard variational inference.\n\nCorrect Answer: B\n\nExplanation: The text states that VNCE builds on NCE, which only applies to unnormalised models. It uses a \"variational lower bound to the NCE objective function\", similar to how the ELBO is a lower bound in standard VI. The text also states that VNCE can be used for both parameter estimation and posterior inference. Therefore, option B is correct."}, "20": {"documentation": {"title": "GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond", "source": "Yue Cao and Jiarui Xu and Stephen Lin and Fangyun Wei and Han Hu", "docs_id": "1904.11492", "section": ["cs.CV", "cs.AI", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond. The Non-Local Network (NLNet) presents a pioneering approach for capturing long-range dependencies, via aggregating query-specific global context to each query position. However, through a rigorous empirical analysis, we have found that the global contexts modeled by non-local network are almost the same for different query positions within an image. In this paper, we take advantage of this finding to create a simplified network based on a query-independent formulation, which maintains the accuracy of NLNet but with significantly less computation. We further observe that this simplified design shares similar structure with Squeeze-Excitation Network (SENet). Hence we unify them into a three-step general framework for global context modeling. Within the general framework, we design a better instantiation, called the global context (GC) block, which is lightweight and can effectively model the global context. The lightweight property allows us to apply it for multiple layers in a backbone network to construct a global context network (GCNet), which generally outperforms both simplified NLNet and SENet on major benchmarks for various recognition tasks. The code and configurations are released at https://github.com/xvjiarui/GCNet."}, "answer": " Question: What is the main finding from the empirical analysis of Non-Local Networks that motivated the creation of the simplified network in the paper?\n\nA) Non-local networks are computationally expensive and slow to train.\n\nB) The global contexts modeled by non-local networks are almost the same for different query positions within an image. \n\nC) Non-local networks do not effectively capture long-range dependencies.\n\nD) Squeeze-Excitation Networks outperform non-local networks on major benchmarks.\n\nCorrect Answer: B) The global contexts modeled by non-local networks are almost the same for different query positions within an image.\n\nExplanation: According to the documentation, the empirical analysis found that the global contexts modeled by non-local networks are almost the same for different query positions within an image. This finding motivated the creation of a simplified network based on a query-independent formulation, which maintains the accuracy of the original non-local network but with less computation."}, "21": {"documentation": {"title": "Practical and Fast Momentum-Based Power Methods", "source": "Tahseen Rabbani, Apollo Jain, Arjun Rajkumar, Furong Huang", "docs_id": "2108.09264", "section": ["cs.LG", "cs.NA", "math.NA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Practical and Fast Momentum-Based Power Methods. The power method is a classical algorithm with broad applications in machine learning tasks, including streaming PCA, spectral clustering, and low-rank matrix approximation. The distilled purpose of the vanilla power method is to determine the largest eigenvalue (in absolute modulus) and its eigenvector of a matrix. A momentum-based scheme can be used to accelerate the power method, but achieving an optimal convergence rate with existing algorithms critically relies on additional spectral information that is unavailable at run-time, and sub-optimal initializations can result in divergence. In this paper, we provide a pair of novel momentum-based power methods, which we call the delayed momentum power method (DMPower) and a streaming variant, the delayed momentum streaming method (DMStream). Our methods leverage inexact deflation and are capable of achieving near-optimal convergence with far less restrictive hyperparameter requirements. We provide convergence analyses for both algorithms through the lens of perturbation theory. Further, we experimentally demonstrate that DMPower routinely outperforms the vanilla power method and that both algorithms match the convergence speed of an oracle running existing accelerated methods with perfect spectral knowledge."}, "answer": " Here is a 4-option multiple choice question on the paper:\n\nQuestion: Which of the following statements is true about the delayed momentum power method (DMPower) presented in the paper?\n\nA) It requires additional spectral information about the matrix at run-time to achieve optimal convergence.\n\nB) It relies on exact deflation to accelerate the power method. \n\nC) It can achieve near-optimal convergence speed with less restrictive hyperparameter requirements compared to existing accelerated power methods.\n\nD) It always converges slower than the vanilla power method.\n\nCorrect Answer: C\n\nExplanation: The paper states that DMPower uses inexact deflation and can achieve near-optimal convergence with far less restrictive hyperparameter requirements compared to existing accelerated power methods that rely on additional spectral information about the matrix that is unavailable at run-time. So C is the correct option."}, "22": {"documentation": {"title": "A Unified Particle-Optimization Framework for Scalable Bayesian Sampling", "source": "Changyou Chen, Ruiyi Zhang, Wenlin Wang, Bai Li and Liqun Chen", "docs_id": "1805.11659", "section": ["stat.ML", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Unified Particle-Optimization Framework for Scalable Bayesian Sampling. There has been recent interest in developing scalable Bayesian sampling methods such as stochastic gradient MCMC (SG-MCMC) and Stein variational gradient descent (SVGD) for big-data analysis. A standard SG-MCMC algorithm simulates samples from a discrete-time Markov chain to approximate a target distribution, thus samples could be highly correlated, an undesired property for SG-MCMC. In contrary, SVGD directly optimizes a set of particles to approximate a target distribution, and thus is able to obtain good approximations with relatively much fewer samples. In this paper, we propose a principle particle-optimization framework based on Wasserstein gradient flows to unify SG-MCMC and SVGD, and to allow new algorithms to be developed. Our framework interprets SG-MCMC as particle optimization on the space of probability measures, revealing a strong connection between SG-MCMC and SVGD. The key component of our framework is several particle-approximate techniques to efficiently solve the original partial differential equations on the space of probability measures. Extensive experiments on both synthetic data and deep neural networks demonstrate the effectiveness and efficiency of our framework for scalable Bayesian sampling."}, "answer": " Here is a 4-option multiple choice question on the document:\n\nQuestion: Which of the following statements accurately describes a key difference between stochastic gradient MCMC (SG-MCMC) and Stein variational gradient descent (SVGD) according to the document?\n\nA) SG-MCMC simulates samples from a discrete-time Markov chain while SVGD directly optimizes a set of particles. \n\nB) SG-MCMC approximates a target distribution better than SVGD with fewer samples.\n\nC) SG-MCMC samples are less correlated than SVGD samples.\n\nD) SVGD relies on partial differential equations while SG-MCMC does not.\n\nCorrect Answer: A\n\nExplanation: The document states that a standard SG-MCMC algorithm simulates samples from a discrete-time Markov chain to approximate a target distribution, while SVGD directly optimizes a set of particles to approximate a target distribution. This is the key difference highlighted between the two methods."}, "23": {"documentation": {"title": "Interpreting multi-variate models with setPCA", "source": "Nordine Aouni, Luc Linders, David Robinson, Len Vandelaer, Jessica\n  Wiezorek, Geetesh Gupta, Rachel Cavill", "docs_id": "2111.09138", "section": ["q-bio.GN", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Interpreting multi-variate models with setPCA. Principal Component Analysis (PCA) and other multi-variate models are often used in the analysis of \"omics\" data. These models contain much information which is currently neither easily accessible nor interpretable. Here we present an algorithmic method which has been developed to integrate this information with existing databases of background knowledge, stored in the form of known sets (for instance genesets or pathways). To make this accessible we have produced a Graphical User Interface (GUI) in Matlab which allows the overlay of known set information onto the loadings plot and thus improves the interpretability of the multi-variate model. For each known set the optimal convex hull, covering a subset of elements from the known set, is found through a search algorithm and displayed. In this paper we discuss two main topics; the details of the search algorithm for the optimal convex hull for this problem and the GUI interface which is freely available for download for academic use."}, "answer": " Question: What algorithm is used in the paper to integrate multivariate model information with existing databases of background knowledge?\n\nA) A greedy search algorithm to find optimal convex hulls covering elements from known sets\n\nB) An expectation maximization algorithm to cluster elements into known sets \n\nC) A dynamic programming algorithm to align elements to known pathways\n\nD) A simulated annealing algorithm to optimize mappings between elements and known sets\n\nCorrect Answer: A\n\nExplanation: The paper discusses using a search algorithm to find optimal convex hulls covering subsets of elements from known sets (such as genesets or pathways) to integrate multivariate model information with background knowledge databases. This allows known set information to be overlaid onto multivariate model loadings plots to improve interpretability."}, "24": {"documentation": {"title": "Fine-Grained Prediction of Syntactic Typology: Discovering Latent\n  Structure with Supervised Learning", "source": "Dingquan Wang, Jason Eisner", "docs_id": "1710.03877", "section": ["cs.CL"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Fine-Grained Prediction of Syntactic Typology: Discovering Latent\n  Structure with Supervised Learning. We show how to predict the basic word-order facts of a novel language given only a corpus of part-of-speech (POS) sequences. We predict how often direct objects follow their verbs, how often adjectives follow their nouns, and in general the directionalities of all dependency relations. Such typological properties could be helpful in grammar induction. While such a problem is usually regarded as unsupervised learning, our innovation is to treat it as supervised learning, using a large collection of realistic synthetic languages as training data. The supervised learner must identify surface features of a language's POS sequence (hand-engineered or neural features) that correlate with the language's deeper structure (latent trees). In the experiment, we show: 1) Given a small set of real languages, it helps to add many synthetic languages to the training data. 2) Our system is robust even when the POS sequences include noise. 3) Our system on this task outperforms a grammar induction baseline by a large margin."}, "answer": " Here is a 4-option multiple choice question with explanations for the given research summary:\n\nQuestion: Which of the following statements best describes the innovation presented in the research?\n\nA) Using unsupervised learning to predict typological properties from POS sequences \n\nB) Identifying surface features of POS sequences that correlate with deeper syntactic structure\n\nC) Showing it is possible to accurately predict word order facts from small sets of real languages\n\nD) Demonstrating a grammar induction system that outperforms the proposed supervised approach\n\nCorrect Answer: B\n\nExplanations:\n\nA) Incorrect, the research uses supervised learning, not unsupervised learning. \n\nB) Correct. The key innovation is framing typology prediction as a supervised task and identifying POS features that correlate with syntax.\n\nC) Incorrect. The research shows it helps to add synthetic languages, not that small real datasets suffice. \n\nD) Incorrect. The proposed system outperforms grammar induction, not vice versa."}, "25": {"documentation": {"title": "Playing against the fittest: A simple strategy that promotes the\n  emergence of cooperation", "source": "M. Brede", "docs_id": "1104.4532", "section": ["cs.GT", "physics.bio-ph", "physics.soc-ph", "q-bio.PE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Playing against the fittest: A simple strategy that promotes the\n  emergence of cooperation. Understanding the emergence and sustainability of cooperation is a fundamental problem in evolutionary biology and is frequently studied by the framework of evolutionary game theory. A very powerful mechanism to promote cooperation is network reciprocity, where the interaction patterns and opportunities for strategy spread of agents are constrained to limited sets of permanent interactions partners. Cooperation survives because it is possible for close-knit communities of cooperation to be shielded from invasion by defectors. Here we show that parameter ranges in which cooperation can survive are strongly expanded if game play on networks is skewed towards more frequent interactions with more successful neighbours. In particular, if agents exclusively select neighbors for game play that are more successful than themselves, cooperation can even dominate in situations in which it would die out if interaction neighbours were chosen without a bias or with a preference for less successful opponents. We demonstrate that the \"selecting fitter neighbours\" strategy is evolutionarily stable. Moreover, it will emerge as the dominant strategy out of an initially random population of agents."}, "answer": " Question: According to the passage, which of the following statements is true about the \"selecting fitter neighbours\" strategy?\n\nA) It allows cooperation to survive only when agents play games exclusively with neighbours who are more successful than themselves.\n\nB) It promotes the emergence of cooperation even in situations where cooperation would die out without biased partner selection. \n\nC) It is an evolutionarily stable strategy that can emerge from an initially random population of agents.\n\nD) It shields communities of cooperators from invasion by defectors by constraining interactions to limited sets of partners.\n\nCorrect Answer: C\n\nExplanation: The passage states that the \"selecting fitter neighbours\" strategy allows cooperation to dominate even when it would die out without biased partner selection. It also notes that this strategy is evolutionarily stable and can emerge from a random population of agents. Therefore, option C is correct. Options A, B, and D refer to other aspects of the passage that are not specifically attributed to the \"selecting fitter neighbours\" strategy."}, "26": {"documentation": {"title": "Teaching the Old Dog New Tricks: Supervised Learning with Constraints", "source": "Fabrizio Detassis, Michele Lombardi, Michela Milano", "docs_id": "2002.10766", "section": ["cs.LG", "cs.AI", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Teaching the Old Dog New Tricks: Supervised Learning with Constraints. Adding constraint support in Machine Learning has the potential to address outstanding issues in data-driven AI systems, such as safety and fairness. Existing approaches typically apply constrained optimization techniques to ML training, enforce constraint satisfaction by adjusting the model design, or use constraints to correct the output. Here, we investigate a different, complementary, strategy based on \"teaching\" constraint satisfaction to a supervised ML method via the direct use of a state-of-the-art constraint solver: this enables taking advantage of decades of research on constrained optimization with limited effort. In practice, we use a decomposition scheme alternating master steps (in charge of enforcing the constraints) and learner steps (where any supervised ML model and training algorithm can be employed). The process leads to approximate constraint satisfaction in general, and convergence properties are difficult to establish; despite this fact, we found empirically that even a na\\\"ive setup of our approach performs well on ML tasks with fairness constraints, and on classical datasets with synthetic constraints."}, "answer": " Question: Which of the following statements best summarizes the key idea presented in the given text?\n\nA) Existing approaches to incorporating constraints into machine learning typically modify the model architecture or training process. The proposed approach instead teaches constraint satisfaction directly to a supervised learning model using a constraint solver.\n\nB) The proposed approach for incorporating constraints into machine learning is based on alternating steps between a master module that enforces constraints and a learner module that trains a supervised model.\n\nC) The proposed constrained learning approach has proven convergence properties and empirically achieves perfect constraint satisfaction on tasks with fairness constraints.\n\nD) Decades of research on constrained optimization can be leveraged for machine learning by simply alternating steps between existing solvers and learners.\n\nCorrect Answer: A\n\nExplanation: The key idea is that rather than modifying the model or training process to enforce constraints, the proposed approach teaches constraint satisfaction directly to a supervised learning model by leveraging a constraint solver. Options B and D focus too narrowly on implementation details. Option C contradicts the text which states convergence is difficult to establish and satisfaction is approximate."}, "27": {"documentation": {"title": "Fault-Tolerant Perception for Automated Driving A Lightweight Monitoring\n  Approach", "source": "Cornelius Buerkle, Florian Geissler, Michael Paulitsch, Kay-Ulrich\n  Scholl", "docs_id": "2111.12360", "section": ["cs.RO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Fault-Tolerant Perception for Automated Driving A Lightweight Monitoring\n  Approach. While the most visible part of the safety verification process of automated vehicles concerns the planning and control system, it is often overlooked that safety of the latter crucially depends on the fault-tolerance of the preceding environment perception. Modern perception systems feature complex and often machine-learning-based components with various failure modes that can jeopardize the overall safety. At the same time, a verification by for example redundant execution is not always feasible due to resource constraints. In this paper, we address the need for feasible and efficient perception monitors and propose a lightweight approach that helps to protect the integrity of the perception system while keeping the additional compute overhead minimal. In contrast to existing solutions, the monitor is realized by a well-balanced combination of sensor checks -- here using LiDAR information -- and plausibility checks on the object motion history. It is designed to detect relevant errors in the distance and velocity of objects in the environment of the automated vehicle. In conjunction with an appropriate planning system, such a monitor can help to make safe automated driving feasible."}, "answer": " Question: What is a key challenge for verifying the safety of automated vehicles that the paper aims to address?\n\nA) The need for efficient perception monitors to protect the integrity of the perception system while minimizing compute overhead.\n\nB) The difficulty of replicating machine learning components to enable redundant execution for verification. \n\nC) The lack of feasible approaches to verify the planning and control systems of automated vehicles.\n\nD) The oversight of the importance of verifying the fault-tolerance of environment perception systems.\n\nCorrect Answer: A\n\nExplanation: The paper discusses the need for feasible and efficient perception monitors to protect the integrity of the perception system while keeping additional compute overhead minimal. This addresses the key challenge of verifying automated vehicle safety despite resource constraints."}, "28": {"documentation": {"title": "Reliable Prediction of Channel Assignment Performance in Wireless Mesh\n  Networks", "source": "Srikant Manas Kala, Ranadheer Musham, M Pavan Kumar Reddy, and\n  Bheemarjuna Reddy Tamma", "docs_id": "1508.03605", "section": ["cs.NI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Reliable Prediction of Channel Assignment Performance in Wireless Mesh\n  Networks. The advancements in wireless mesh networks (WMN), and the surge in multi-radio multi-channel (MRMC) WMN deployments have spawned a multitude of network performance issues. These issues are intricately linked to the adverse impact of endemic interference. Thus, interference mitigation is a primary design objective in WMNs. Interference alleviation is often effected through efficient channel allocation (CA) schemes which fully utilize the potential of MRMC environment and also restrain the detrimental impact of interference. However, numerous CA schemes have been proposed in research literature and there is a lack of CA performance prediction techniques which could assist in choosing a suitable CA for a given WMN. In this work, we propose a reliable interference estimation and CA performance prediction approach. We demonstrate its efficacy by substantiating the CA performance predictions for a given WMN with experimental data obtained through rigorous simulations on an ns-3 802.11g environment."}, "answer": " Here is a 4-option multiple choice question with explanations for the given text:\n\nQuestion: Which of the following statements is true about channel allocation (CA) schemes for wireless mesh networks (WMNs) according to the text?\n\nA) CA schemes are not effective at mitigating interference in WMNs.\n\nB) There is a lack of reliable techniques to predict the performance of different CA schemes. \n\nC) CA schemes utilize the potential of multi-radio multi-channel environments to introduce more interference.\n\nD) Numerous CA schemes have been proposed but their performance is very similar across different WMNs.\n\nCorrect Answer: B\n\nExplanations:\n\nA) Incorrect. The text states that interference mitigation is a primary design objective achieved through efficient CA schemes. \n\nB) Correct. The text mentions a lack of CA performance prediction techniques to choose a suitable CA scheme for a given WMN.\n\nC) Incorrect. The text states CA schemes utilize the MRMC potential to restrain the detrimental impact of interference. \n\nD) Incorrect. The text mentions numerous CA schemes have been proposed implying their performance may vary across WMNs."}, "29": {"documentation": {"title": "Learning to Infer Semantic Parameters for 3D Shape Editing", "source": "Fangyin Wei, Elena Sizikova, Avneesh Sud, Szymon Rusinkiewicz, Thomas\n  Funkhouser", "docs_id": "2011.04755", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Learning to Infer Semantic Parameters for 3D Shape Editing. Many applications in 3D shape design and augmentation require the ability to make specific edits to an object's semantic parameters (e.g., the pose of a person's arm or the length of an airplane's wing) while preserving as much existing details as possible. We propose to learn a deep network that infers the semantic parameters of an input shape and then allows the user to manipulate those parameters. The network is trained jointly on shapes from an auxiliary synthetic template and unlabeled realistic models, ensuring robustness to shape variability while relieving the need to label realistic exemplars. At testing time, edits within the parameter space drive deformations to be applied to the original shape, which provides semantically-meaningful manipulation while preserving the details. This is in contrast to prior methods that either use autoencoders with a limited latent-space dimensionality, failing to preserve arbitrary detail, or drive deformations with purely-geometric controls, such as cages, losing the ability to update local part regions. Experiments with datasets of chairs, airplanes, and human bodies demonstrate that our method produces more natural edits than prior work."}, "answer": " Here is a 4-option multiple choice question on the paper:\n\nQuestion: Which of the following statements is true about the proposed method in the paper \"Learning to Infer Semantic Parameters for 3D Shape Editing\"?\n\nA) It uses autoencoders to edit 3D shapes while preserving details.\n\nB) It drives deformations using purely geometric controls like cages. \n\nC) It requires labeling realistic 3D models to train the network.\n\nD) It allows editing semantic parameters of 3D shapes while retaining existing details.\n\nCorrect Answer: D) It allows editing semantic parameters of 3D shapes while retaining existing details.\n\nExplanation: The paper proposes a method to edit the semantic parameters of 3D shapes like pose of a person's arm while preserving existing shape details. It does this by training a network on synthetic and unlabeled realistic data, avoiding the need to manually label real data. The network infers semantic parameters that can be edited at test time to deform the shape naturally. This is unlike autoencoders that lack detail preservation or cage-based methods that cannot make semantic edits."}, "30": {"documentation": {"title": "A 3-D Spatial Model for In-building Wireless Networks with Correlated\n  Shadowing", "source": "Junse Lee, Xinchen Zhang, and Francois Baccelli", "docs_id": "1603.07072", "section": ["cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A 3-D Spatial Model for In-building Wireless Networks with Correlated\n  Shadowing. Consider orthogonal planes in the 3-D space representing floors and walls in a large building. These planes divide the space into rooms where a wireless infrastructure is deployed. This paper is focused on the analysis of the correlated shadowing field created by this wireless infrastructure through the set of walls and floors. When the locations of the planes and of the wireless nodes are governed by Poisson processes, we obtain a simple stochastic model which captures the non-uniform nature of node deployment and room sizes. This model, which we propose to call the Poisson building, captures the complex in-building shadowing correlations, is scalable in the number of dimensions and is tractable for network performance analysis. It allows an exact mathematical characterization of the interference distribution in both infinite and finite buildings, which further leads to closed-form expressions for the coverage probabilities in in-building cellular networks and the success probability of in-building underlay D2D transmissions."}, "answer": " Here is a 4-option multiple choice question on the paper:\n\nQuestion: Which of the following statements is true about the Poisson building model proposed in the paper?\n\nA) It assumes the wireless nodes are placed uniformly in the rooms created by orthogonal planes representing floors and walls.\n\nB) It cannot analyze interference distribution in finite buildings. \n\nC) It ignores correlated shadowing effects between rooms.\n\nD) It models the rooms and node locations using binomial point processes.\n\nCorrect Answer: A\n\nExplanation: \nThe paper proposes the Poisson building model where orthogonal planes representing floors and walls divide the space into rooms. The locations of these planes and wireless nodes are modeled as Poisson processes, capturing the non-uniform nature of room sizes and node deployment. The model analyzes interference distribution in both infinite and finite buildings. It also captures complex in-building shadowing correlations between rooms."}, "31": {"documentation": {"title": "Multiscale dynamical embeddings of complex networks", "source": "Michael T. Schaub and Jean-Charles Delvenne and Renaud Lambiotte and\n  Mauricio Barahona", "docs_id": "1804.03733", "section": ["cs.SI", "cs.SY", "physics.soc-ph"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Multiscale dynamical embeddings of complex networks. Complex systems and relational data are often abstracted as dynamical processes on networks. To understand, predict and control their behavior, a crucial step is to extract reduced descriptions of such networks. Inspired by notions from Control Theory, we propose a time-dependent dynamical similarity measure between nodes, which quantifies the effect a node-input has on the network. This dynamical similarity induces an embedding that can be employed for several analysis tasks. Here we focus on (i)~dimensionality reduction, i.e., projecting nodes onto a low dimensional space that captures dynamic similarity at different time scales, and (ii)~how to exploit our embeddings to uncover functional modules. We exemplify our ideas through case studies focusing on directed networks without strong connectivity, and signed networks. We further highlight how certain ideas from community detection can be generalized and linked to Control Theory, by using the here developed dynamical perspective."}, "answer": " Here is a 4-option multiple choice question on the given text:\n\nQuestion: Which of the following statements accurately describes a key contribution of the paper?\n\nA) The paper proposes a new dimensionality reduction technique for complex networks based on dynamical similarity between nodes.\n\nB) The paper develops a method to identify functional modules in networks by exploiting embeddings that capture dynamic similarity. \n\nC) The paper shows how to apply ideas from community detection to control theory through the lens of a dynamical perspective.\n\nD) The paper focuses on analyzing directed and signed networks using notions of controllability from control theory.\n\nCorrect Answer: B\n\nExplanation: The paper proposes a dynamical similarity measure between nodes to generate embeddings that can be used for dimensionality reduction and uncovering functional modules. While it provides case studies on directed and signed networks, the key contribution is using the embeddings for module detection, not applying control theory. Community detection is mentioned as related work, not a contribution."}, "32": {"documentation": {"title": "Deep Networks Can Resemble Human Feed-forward Vision in Invariant Object\n  Recognition", "source": "Saeed Reza Kheradpisheh, Masoud Ghodrati, Mohammad Ganjtabesh,\n  Timoth\\'ee Masquelier", "docs_id": "1508.03929", "section": ["cs.CV", "q-bio.NC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Deep Networks Can Resemble Human Feed-forward Vision in Invariant Object\n  Recognition. Deep convolutional neural networks (DCNNs) have attracted much attention recently, and have shown to be able to recognize thousands of object categories in natural image databases. Their architecture is somewhat similar to that of the human visual system: both use restricted receptive fields, and a hierarchy of layers which progressively extract more and more abstracted features. Yet it is unknown whether DCNNs match human performance at the task of view-invariant object recognition, whether they make similar errors and use similar representations for this task, and whether the answers depend on the magnitude of the viewpoint variations. To investigate these issues, we benchmarked eight state-of-the-art DCNNs, the HMAX model, and a baseline shallow model and compared their results to those of humans with backward masking. Unlike in all previous DCNN studies, we carefully controlled the magnitude of the viewpoint variations to demonstrate that shallow nets can outperform deep nets and humans when variations are weak. When facing larger variations, however, more layers were needed to match human performance and error distributions, and to have representations that are consistent with human behavior. A very deep net with 18 layers even outperformed humans at the highest variation level, using the most human-like representations."}, "answer": " Question: What was the main finding from benchmarking deep convolutional neural networks (DCNNs) against humans on the task of view-invariant object recognition?\n\nA) DCNNs matched human performance across all levels of viewpoint variation.\n\nB) Shallow nets outperformed deep nets and humans only when viewpoint variations were small. \n\nC) An 18-layer DCNN matched human performance and representations across all viewpoint variations.\n\nD) More layers were needed for DCNNs to match human performance and error distributions as viewpoint variations increased.\n\nCorrect Answer: D\n\nExplanation: The passage states that shallow nets could outperform deep nets and humans when viewpoint variations were weak, but more layers were needed in DCNNs to match human performance and error distributions as the magnitude of viewpoint variations increased. The 18-layer net only outperformed humans at the highest variation level."}, "33": {"documentation": {"title": "Data-efficient Deep Reinforcement Learning for Dexterous Manipulation", "source": "Ivaylo Popov, Nicolas Heess, Timothy Lillicrap, Roland Hafner, Gabriel\n  Barth-Maron, Matej Vecerik, Thomas Lampe, Yuval Tassa, Tom Erez, Martin\n  Riedmiller", "docs_id": "1704.03073", "section": ["cs.LG", "cs.RO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Data-efficient Deep Reinforcement Learning for Dexterous Manipulation. Deep learning and reinforcement learning methods have recently been used to solve a variety of problems in continuous control domains. An obvious application of these techniques is dexterous manipulation tasks in robotics which are difficult to solve using traditional control theory or hand-engineered approaches. One example of such a task is to grasp an object and precisely stack it on another. Solving this difficult and practically relevant problem in the real world is an important long-term goal for the field of robotics. Here we take a step towards this goal by examining the problem in simulation and providing models and techniques aimed at solving it. We introduce two extensions to the Deep Deterministic Policy Gradient algorithm (DDPG), a model-free Q-learning based method, which make it significantly more data-efficient and scalable. Our results show that by making extensive use of off-policy data and replay, it is possible to find control policies that robustly grasp objects and stack them. Further, our results hint that it may soon be feasible to train successful stacking policies by collecting interactions on real robots."}, "answer": " Question: Which of the following statements accurately summarizes a key contribution of the paper \"Data-efficient Deep Reinforcement Learning for Dexterous Manipulation\"?\n\nA) The paper introduces a new deep learning architecture that is more sample efficient for dexterous robotic manipulation tasks like grasping and stacking objects.\n\nB) The paper shows it is possible to solve complex dexterous manipulation tasks like grasping and stacking using model-free deep reinforcement learning algorithms. \n\nC) The paper proves deep reinforcement learning methods will never be able to solve real-world dexterous manipulation tasks.\n\nD) The paper introduces a new robotic hand design optimized for dexterous in-hand manipulation.\n\nCorrect Answer: B\n\nExplanation: The paper demonstrates that by making modifications to the DDPG algorithm to improve sample efficiency, it is possible to use model-free deep reinforcement learning to learn policies that can robustly grasp and stack objects, taking a step towards solving real-world dexterous manipulation."}, "34": {"documentation": {"title": "Temporal Second Difference Traces", "source": "Mitchell Keith Bloch", "docs_id": "1104.4664", "section": ["cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Temporal Second Difference Traces. Q-learning is a reliable but inefficient off-policy temporal-difference method, backing up reward only one step at a time. Replacing traces, using a recency heuristic, are more efficient but less reliable. In this work, we introduce model-free, off-policy temporal difference methods that make better use of experience than Watkins' Q(\\lambda). We introduce both Optimistic Q(\\lambda) and the temporal second difference trace (TSDT). TSDT is particularly powerful in deterministic domains. TSDT uses neither recency nor frequency heuristics, storing (s,a,r,s',\\delta) so that off-policy updates can be performed after apparently suboptimal actions have been taken. There are additional advantages when using state abstraction, as in MAXQ. We demonstrate that TSDT does significantly better than both Q-learning and Watkins' Q(\\lambda) in a deterministic cliff-walking domain. Results in a noisy cliff-walking domain are less advantageous for TSDT, but demonstrate the efficacy of Optimistic Q(\\lambda), a replacing trace with some of the advantages of TSDT."}, "answer": " Question: Which of the following statements best summarizes the key ideas presented in the given text?\n\nA) Q-learning is inefficient for off-policy learning. Traces using recency heuristics are more efficient but less reliable. The temporal second difference trace (TSDT) method combines the reliability of Q-learning with the efficiency of traces by storing experience tuples to enable off-policy updates after suboptimal actions. \n\nB) Q-learning is an on-policy temporal difference method. Traces using frequency heuristics are more reliable but less efficient. The optimistic Q(lambda) method uses recency heuristics to enable off-policy updates after optimal actions.\n\nC) Q-learning backs up rewards slowly. Traces using recency heuristics backup faster but are unreliable. The temporal second difference trace (TSDT) method uses neither recency nor frequency heuristics, enabling efficient and reliable off-policy updates.\n\nD) Q-learning is inefficient and unreliable. Traces using frequency heuristics are efficient and reliable. The optimistic Q(lambda) method uses frequency heuristics to enable on-policy updates after suboptimal actions.\n\nCorrect Answer: C"}, "35": {"documentation": {"title": "Artistic style transfer for videos and spherical images", "source": "Manuel Ruder, Alexey Dosovitskiy, Thomas Brox", "docs_id": "1708.04538", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Artistic style transfer for videos and spherical images. Manually re-drawing an image in a certain artistic style takes a professional artist a long time. Doing this for a video sequence single-handedly is beyond imagination. We present two computational approaches that transfer the style from one image (for example, a painting) to a whole video sequence. In our first approach, we adapt to videos the original image style transfer technique by Gatys et al. based on energy minimization. We introduce new ways of initialization and new loss functions to generate consistent and stable stylized video sequences even in cases with large motion and strong occlusion. Our second approach formulates video stylization as a learning problem. We propose a deep network architecture and training procedures that allow us to stylize arbitrary-length videos in a consistent and stable way, and nearly in real time. We show that the proposed methods clearly outperform simpler baselines both qualitatively and quantitatively. Finally, we propose a way to adapt these approaches also to 360 degree images and videos as they emerge with recent virtual reality hardware."}, "answer": " Question: What technique did Gatys et al. originally propose for artistic style transfer for images?\n\nA) A deep network architecture and training procedures for style transfer\nB) Energy minimization between the content and style images \nC) A way to adapt style transfer to 360 degree images and videos\nD) New loss functions to generate consistent stylized video sequences\n\nCorrect Answer: B) Energy minimization between the content and style images\n\nExplanations: \nThe documentation mentions that Gatys et al. originally proposed an image style transfer technique based on energy minimization between the content and style images. The other answer choices refer to techniques proposed in this new work, not the original work by Gatys et al."}, "36": {"documentation": {"title": "Crosstalk Noise based Configurable Computing: A New Paradigm for Digital\n  Electronics", "source": "Naveen Kumar Macha, Md Arif Iqbal, Bhavana Tejaswini Repalle, Sehtab\n  Hossain, Mostafizur Rahman", "docs_id": "2004.08040", "section": ["cs.ET"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Crosstalk Noise based Configurable Computing: A New Paradigm for Digital\n  Electronics. The past few decades have seen exponential growth in capabilities of digital electronics primarily due to the ability to scale Integrated Circuits (ICs) to smaller dimensions while attaining power and performance benefits. That scalability is now being challenged due to the lack of scaled transistor performance and also manufacturing complexities [1]-[5]. In addition, the growing cyber threat in fabless manufacturing era poses a new front that modern ICs need to withstand. We present a new noise based computing where the interconnect interference between nanoscale metal lines is intentionally engineered to exhibit programmable Boolean logic behavior. The reliance on just coupling between metal lines and not on transistors for computing, and the programmability are the foundations for better scalability, and security by obscurity. Here, we show experimental evidence of a functioning Crosstalk computing chip at 65nm technology. Our demonstration of computing constructs, gate level configurability and utilization of foundry processes show feasibility. These results in conjunction with our simulation results at 7nm for various benchmarks, which show over 48%, 57%, and 10% density, power and performance respectively, gains over equivalent CMOS in the best case, show potentials. The benefits of Crosstalk circuits and inherent programmable features set it apart and make it a promising prospect for future electronics."}, "answer": " Here is a 4-option multiple choice question on the Crosstalk Noise based Configurable Computing paper:\n\nQuestion: Which of the following statements is true about Crosstalk Noise based Configurable Computing according to the paper?\n\nA) It relies on coupling between transistors rather than metal lines for computing.\n\nB) It shows worse density, power and performance compared to CMOS in simulations. \n\nC) It utilizes foundry compatible processes and demonstrates gate-level configurability.\n\nD) It is less scalable and secure than conventional CMOS based computing.\n\nCorrect Answer: C\n\nExplanation: According to the paper, Crosstalk Noise based Configurable Computing relies on coupling between nanoscale metal lines rather than transistors for computing (option A is incorrect). The paper states that simulations show gains in density, power and performance over CMOS (option B is incorrect). It also demonstrates utilization of foundry processes and gate-level configurability (option C is correct). The paper argues it has better scalability and security compared to CMOS (option D is incorrect)."}, "37": {"documentation": {"title": "Chat More If You Like: Dynamic Cue Words Planning to Flow Longer\n  Conversations", "source": "Lili Yao, Ruijian Xu, Chao Li, Dongyan Zhao and Rui Yan", "docs_id": "1811.07631", "section": ["cs.CL"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Chat More If You Like: Dynamic Cue Words Planning to Flow Longer\n  Conversations. To build an open-domain multi-turn conversation system is one of the most interesting and challenging tasks in Artificial Intelligence. Many research efforts have been dedicated to building such dialogue systems, yet few shed light on modeling the conversation flow in an ongoing dialogue. Besides, it is common for people to talk about highly relevant aspects during a conversation. And the topics are coherent and drift naturally, which demonstrates the necessity of dialogue flow modeling. To this end, we present the multi-turn cue-words driven conversation system with reinforcement learning method (RLCw), which strives to select an adaptive cue word with the greatest future credit, and therefore improve the quality of generated responses. We introduce a new reward to measure the quality of cue words in terms of effectiveness and relevance. To further optimize the model for long-term conversations, a reinforcement approach is adopted in this paper. Experiments on real-life dataset demonstrate that our model consistently outperforms a set of competitive baselines in terms of simulated turns, diversity and human evaluation."}, "answer": " Question: What method does the paper present for selecting cue words to improve the quality of generated responses in a multi-turn open-domain conversation system?\n\nA) A reinforcement learning method that selects cue words to maximize future reward \n\nB) A supervised learning method that selects cue words based on human annotations\n\nC) A rule-based method that selects cue words from a predefined list\n\nD) A generative model that samples cue words from a learned distribution\n\nCorrect Answer: A\n\nExplanation: The paper presents a reinforcement learning method called RLCw that selects cue words to maximize a reward function measuring effectiveness and relevance. This allows the model to adaptively choose cue words that improve the quality of generated responses in multi-turn conversations."}, "38": {"documentation": {"title": "Gamifying the Escape from the Engineering Method Prison - An Innovative\n  Board Game to Teach the Essence Theory to Future Project Managers and\n  Software Engineers", "source": "Kai-Kristian Kemell, Juhani Risku, Arthur Evensen, Pekka Abrahamsson,\n  Aleksander Madsen Dahl, Lars Henrik Grytten, Agata Jedryszek, Petter Rostrup,\n  Anh Nguyen-Duc", "docs_id": "1809.08656", "section": ["cs.SE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Gamifying the Escape from the Engineering Method Prison - An Innovative\n  Board Game to Teach the Essence Theory to Future Project Managers and\n  Software Engineers. Software Engineering is an engineering discipline but lacks a solid theoretical foundation. One effort in remedying this situation has been the SEMAT Essence specification. Essence consists of a language for modeling Software Engineering (SE) practices and methods and a kernel containing what its authors describe as being elements that are present in every software development project. In practice, it is a method agnostic project management tool for SE Projects. Using the language of the specification, Essence can be used to model any software development method or practice. Thus, the specification can potentially be applied to any software development context, making it a powerful tool. However, due to the manual work and the learning process involved in modeling practices with Essence, its initial adoption can be tasking for development teams. Due to the importance of project management in SE projects, new project management tools such as Essence are valuable, and facilitating their adoption is consequently important. To tackle this issue in the case of Essence, we present a game-based approach to teaching the use Essence. In this paper, we gamify the learning process by means of an innovative board game. The game is empirically validated in a study involving students from the IT faculty of University of Jyv\\\"askyl\\\"a (n=61). Based on the results, we report the effectiveness of the game-based approach to teaching both Essence and SE project work."}, "answer": " Here is a multi-form question based on the given text:\n\nQuestion: Which of the following statements accurately describes the Essence specification according to the text?\n\nA) Essence is a new software development methodology that provides a theoretical foundation for software engineering. \n\nB) Essence is a language and kernel for modeling software engineering practices in a method-agnostic way.\n\nC) Essence can only be used to model specific software development methods like agile or waterfall.\n\nD) Essence is a project management tool that cannot be applied to different software development contexts.\n\nCorrect Answer: B\n\nExplanation: The text states that Essence is \"a language for modeling Software Engineering (SE) practices and methods and a kernel containing what its authors describe as being elements that are present in every software development project.\" It also mentions that \"Using the language of the specification, Essence can be used to model any software development method or practice.\" Therefore, option B most accurately describes Essence as a method-agnostic modeling language and kernel for software engineering practices."}, "39": {"documentation": {"title": "Near-Optimal Algorithms for Minimax Optimization", "source": "Tianyi Lin, Chi Jin and Michael. I. Jordan", "docs_id": "2002.02417", "section": ["math.OC", "cs.LG", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Near-Optimal Algorithms for Minimax Optimization. This paper resolves a longstanding open question pertaining to the design of near-optimal first-order algorithms for smooth and strongly-convex-strongly-concave minimax problems. Current state-of-the-art first-order algorithms find an approximate Nash equilibrium using $\\tilde{O}(\\kappa_{\\mathbf x}+\\kappa_{\\mathbf y})$ or $\\tilde{O}(\\min\\{\\kappa_{\\mathbf x}\\sqrt{\\kappa_{\\mathbf y}}, \\sqrt{\\kappa_{\\mathbf x}}\\kappa_{\\mathbf y}\\})$ gradient evaluations, where $\\kappa_{\\mathbf x}$ and $\\kappa_{\\mathbf y}$ are the condition numbers for the strong-convexity and strong-concavity assumptions. A gap still remains between these results and the best existing lower bound $\\tilde{\\Omega}(\\sqrt{\\kappa_{\\mathbf x}\\kappa_{\\mathbf y}})$. This paper presents the first algorithm with $\\tilde{O}(\\sqrt{\\kappa_{\\mathbf x}\\kappa_{\\mathbf y}})$ gradient complexity, matching the lower bound up to logarithmic factors. Our algorithm is designed based on an accelerated proximal point method and an accelerated solver for minimax proximal steps. It can be easily extended to the settings of strongly-convex-concave, convex-concave, nonconvex-strongly-concave, and nonconvex-concave functions. This paper also presents algorithms that match or outperform all existing methods in these settings in terms of gradient complexity, up to logarithmic factors."}, "answer": " Here is a multi-form question on the paper:\n\nQuestion: Which of the following statements is true about the paper \"Near-Optimal Algorithms for Minimax Optimization\"?\n\nA) The paper presents the first algorithm that matches the lower bound of \u0398(\u221a\u03bax\u03bay) for finding an approximate Nash equilibrium, up to logarithmic factors. \n\nB) The paper shows how to extend existing algorithms to achieve the optimal \u0398(\u03bax+\u03bay) gradient complexity for strongly-convex-strongly-concave functions.\n\nC) The paper proves a new lower bound of \u03a9(min{\u03bax\u221a\u03bay, \u221a\u03bax\u03bay}) for first-order methods.\n\nD) The paper gives an algorithm with O(\u03bax+\u03bay) gradient complexity for nonconvex-nonconcave functions.\n\nCorrect Answer: A\n\nExplanation: The paper presents an accelerated proximal point method that achieves ~O(\u221a\u03bax\u03bay) gradient complexity for finding an approximate Nash equilibrium, matching the existing lower bound up to logarithmic factors. Options B and C are incorrect statements about the results. Option D is incorrect because the paper does not consider nonconvex-nonconcave functions."}, "40": {"documentation": {"title": "Achievable DoF Regions of Three-User MIMO Broadcast Channel with Delayed\n  CSIT", "source": "Tong Zhang, and Rui Wang", "docs_id": "2001.05134", "section": ["cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Achievable DoF Regions of Three-User MIMO Broadcast Channel with Delayed\n  CSIT. For the two-user multiple-input multiple-output (MIMO) broadcast channel with delayed channel state information at the transmitter (CSIT) and arbitrary antenna configurations, all the degrees-of-freedom (DoF) regions are obtained. However, for the three-user MIMO broadcast channel with delayed CSIT and arbitrary antenna configurations, the DoF region of order-2 messages is still unclear and only a partial achievable DoF region of order-1 messages is obtained, where the order-2 messages and order-1 messages are desired by two receivers and one receiver, respectively. In this paper, for the three-user MIMO broadcast channel with delayed CSIT and arbitrary antenna configurations, we first design transmission schemes for order-2 messages and order-1 messages. Next, we propose to analyze the achievable DoF region of transmission scheme by transformation approach. In particular, we transform the decoding condition of transmission scheme w.r.t. phase duration into the achievable DoF region w.r.t. achievable DoF, through achievable DoF tuple expression connecting phase duration and achievable DoF. As a result, the DoF region of order-2 messages is characterized and an achievable DoF region of order-1 messages is completely expressed. Besides, for order-1 messages, we derive the sufficient condition, under which the proposed achievable DoF region is the DoF region."}, "answer": " Here is a multi-form question on the document:\n\nQuestion: For the three-user MIMO broadcast channel with delayed CSIT and arbitrary antenna configurations, which of the following statements is true about the achievable DoF region of order-1 messages obtained in the paper?\n\nA) The complete achievable DoF region of order-1 messages is characterized. \n\nB) Only a partial achievable DoF region of order-1 messages is obtained.\n\nC) The sufficient condition is derived under which the achievable DoF region of order-1 messages is the complete DoF region.\n\nD) The achievable DoF region of order-1 messages is shown to be larger than that of order-2 messages.\n\nCorrect Answer: C\n\nExplanation: According to the document, a complete achievable DoF region of order-1 messages is expressed. Additionally, the sufficient condition is derived under which the proposed achievable DoF region of order-1 messages is the complete DoF region. Therefore, option C is the correct answer."}, "41": {"documentation": {"title": "Arbitrary Pattern Formation by Opaque Fat Robots with Lights", "source": "Kaustav Bose, Ranendu Adhikary, Manash Kumar Kundu, Buddhadeb Sau", "docs_id": "1910.02706", "section": ["cs.DC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Arbitrary Pattern Formation by Opaque Fat Robots with Lights. Arbitrary Pattern Formation is a widely studied problem in autonomous robot systems. The problem asks to design a distributed algorithm that moves a team of autonomous, anonymous and identical mobile robots to form any arbitrary pattern given as input. The majority of the existing literature investigates this problem for robots with unobstructed visibility. In a few recent works, the problem has been studied in the obstructed visibility model, where the view of a robot can be obstructed by the presence of other robots. However, in these works, the robots have been modelled as dimensionless points in the plane. In this paper, we have considered the problem in the more realistic setting where the robots have a physical extent. In particular, the robots are modelled as opaque disks. Furthermore, the robots operate under a fully asynchronous scheduler. They do not have access to any global coordinate system, but agree on the direction and orientation of one coordinate axis. Each robot is equipped with an externally visible light which can assume a constant number of predefined colors. In this setting, we have given a complete characterization of initial configurations from where any arbitrary pattern can be formed by a deterministic distributed algorithm."}, "answer": " Here is a multi-form question on the Arxiv documentation:\n\nQuestion: Which of the following statements is true about the arbitrary pattern formation problem studied in the paper?\n\nA) The robots are modeled as dimensionless points in the plane.\n\nB) The robots have obstructed visibility and physical extent as opaque disks. \n\nC) The robots operate under a synchronous scheduler.\n\nD) The robots have access to a global coordinate system.\n\nCorrect Answer: B\n\nExplanation: The paper studies the arbitrary pattern formation problem where the robots have obstructed visibility due to their physical extent as opaque disks. The robots operate under an asynchronous scheduler and do not have access to a global coordinate system."}, "42": {"documentation": {"title": "Avoiding Implementation Pitfalls of \"Matrix Capsules with EM Routing\" by\n  Hinton et al", "source": "Ashley Daniel Gritzman", "docs_id": "1907.00652", "section": ["cs.LG", "cs.CV", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Avoiding Implementation Pitfalls of \"Matrix Capsules with EM Routing\" by\n  Hinton et al. The recent progress on capsule networks by Hinton et al. has generated considerable excitement in the machine learning community. The idea behind a capsule is inspired by a cortical minicolumn in the brain, whereby a vertically organised group of around 100 neurons receive common inputs, have common outputs, are interconnected, and may well constitute a fundamental computation unit of the cerebral cortex. However, Hinton's paper on \"Matrix Capsule with EM Routing'\" was unfortunately not accompanied by a release of source code, which left interested researchers attempting to implement the architecture and reproduce the benchmarks on their own. This has certainly slowed the progress of research building on this work. While writing our own implementation, we noticed several common mistakes in other open source implementations that we came across. In this paper we share some of these learnings, specifically focusing on three implementation pitfalls and how to avoid them: (1) parent capsules with only one child; (2) normalising the amount of data assigned to parent capsules; (3) parent capsules at different positions compete for child capsules. While our implementation is a considerable improvement over currently available implementations, it still falls slightly short of the performance reported by Hinton et al. (2018). The source code for this implementation is available on GitHub at the following URL: https://github.com/IBM/matrix-capsules-with-em-routing."}, "answer": " Here is a multi-form question based on the given documentation:\n\nQuestion: Which of the following is NOT mentioned as an implementation pitfall that the authors share learnings on how to avoid?\n\nA) Parent capsules with only one child\nB) Normalising the amount of data assigned to parent capsules \nC) Parent capsules at different positions compete for the same child capsules\nD) Using a softmax function for routing between capsule layers\n\nCorrect Answer: D\n\nExplanation: The documentation mentions avoiding pitfalls related to parent capsules with only one child, normalising data assignment to parent capsules, and parent capsules competing for child capsules. It does not mention anything about using a softmax function for routing between capsule layers."}, "43": {"documentation": {"title": "On the minimal ranks of matrix pencils and the existence of a best\n  approximate block-term tensor decomposition", "source": "Jos\\'e Henrique de Morais Goulart, Pierre Comon", "docs_id": "1712.05742", "section": ["math.NA", "cs.NA", "math.AG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "On the minimal ranks of matrix pencils and the existence of a best\n  approximate block-term tensor decomposition. Under the action of the general linear group with tensor structure, the ranks of matrices $A$ and $B$ forming an $m \\times n$ pencil $A + \\lambda B$ can change, but in a restricted manner. Specifically, with every pencil one can associate a pair of minimal ranks, which is unique up to a permutation. This notion can be defined for matrix pencils and, more generally, also for matrix polynomials of arbitrary degree. In this paper, we provide a formal definition of the minimal ranks, discuss its properties and the natural hierarchy it induces in a pencil space. Then, we show how the minimal ranks of a pencil can be determined from its Kronecker canonical form. For illustration, we classify the orbits according to their minimal ranks (under the action of the general linear group) in the case of real pencils with $m, n \\le 4$. Subsequently, we show that real regular $2k \\times 2k$ pencils having only complex-valued eigenvalues, which form an open positive-volume set, do not admit a best approximation (in the norm topology) on the set of real pencils whose minimal ranks are bounded by $2k-1$. Our results can be interpreted from a tensor viewpoint, where the minimal ranks of a degree-$(d-1)$ matrix polynomial characterize the minimal ranks of matrices constituting a block-term decomposition of an $m \\times n \\times d$ tensor into a sum of matrix-vector tensor products."}, "answer": " Here is a possible multi-form question based on the given text:\n\nQuestion: According to the text, which of the following statements about the minimal ranks of a matrix pencil $A + \\lambda B$ is true?\n\nA) The minimal ranks are unique up to a permutation and invariant under the action of the general linear group. \n\nB) The minimal ranks can be determined from the Jordan canonical form of the pencil.\n\nC) Real regular $2k \\times 2k$ pencils with only complex eigenvalues do not have best low-rank approximations. \n\nD) The minimal ranks induce a hierarchy in the space of matrix polynomials of arbitrary degree.\n\nCorrect Answer: C\n\nExplanation: The text states that the minimal ranks of a matrix pencil are unique up to permutation and invariant under the action of the general linear group, ruling out option B. It also states that the minimal ranks induce a hierarchy in the space of matrix pencils, not matrix polynomials of arbitrary degree, ruling out D. Option C summarizes the result that real regular $2k \\times 2k$ pencils with only complex eigenvalues do not have best low-rank approximations bounded by 2k-1."}, "44": {"documentation": {"title": "Balancing transparency, efficiency and security in pervasive systems", "source": "Mark Wenstrom, Eloisa Bentivegna and Ali Hurson (Pennsylvania State\n  University)", "docs_id": "0801.3102", "section": ["cs.HC", "cs.IR"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Balancing transparency, efficiency and security in pervasive systems. This chapter will survey pervasive computing with a look at how its constraint for transparency affects issues of resource management and security. The goal of pervasive computing is to render computing transparent, such that computing resources are ubiquitously offered to the user and services are proactively performed for a user without his or her intervention. The task of integrating computing infrastructure into everyday life without making it excessively invasive brings about tradeoffs between flexibility and robustness, efficiency and effectiveness, as well as autonomy and reliability. As the feasibility of ubiquitous computing and its real potential for mass applications are still a matter of controversy, this chapter will look into the underlying issues of resource management and authentication to discover how these can be handled in a least invasive fashion. The discussion will be closed by an overview of the solutions proposed by current pervasive computing efforts, both in the area of generic platforms and for dedicated applications such as pervasive education and healthcare."}, "answer": " Question: What is the goal of pervasive computing according to the given text?\n\nA) To render computing excessively invasive in everyday life {candidate A}  \n\nB) To provide ubiquitous computing resources without user intervention {candidate B}\n\nC) To make computing infrastructure robust but not very flexible {candidate C}  \n\nD) To make tradeoffs between autonomy, efficiency and security {candidate D}\n\nCorrect Answer: B\n\nExplanation: The text states \"The goal of pervasive computing is to render computing transparent, such that computing resources are ubiquitously offered to the user and services are proactively performed for a user without his or her intervention.\" Therefore, option B is the correct answer."}, "45": {"documentation": {"title": "A novel mutation operator based on the union of fitness and design\n  spaces information for Differential Evolution", "source": "H. Sharifi Noghabi, H. Rajabi Mashhadi, K. Shojaei", "docs_id": "1510.02513", "section": ["cs.NE"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A novel mutation operator based on the union of fitness and design\n  spaces information for Differential Evolution. Differential Evolution (DE) is one of the most successful and powerful evolutionary algorithms for global optimization problem. The most important operator in this algorithm is mutation operator which parents are selected randomly to participate in it. Recently, numerous papers are tried to make this operator more intelligent by selection of parents for mutation intelligently. The intelligent selection for mutation vectors is performed by applying design space (also known as decision space) criterion or fitness space criterion, however, in both cases, half of valuable information of the problem space is disregarded. In this article, a Universal Differential Evolution (UDE) is proposed which takes advantage of both design and fitness spaces criteria for intelligent selection of mutation vectors. The experimental analysis on UDE are performed on CEC2005 benchmarks and the results stated that UDE significantly improved the performance of differential evolution in comparison with other methods that only use one criterion for intelligent selection."}, "answer": " Question: What is the key difference between the proposed Universal Differential Evolution (UDE) approach and previous Differential Evolution (DE) approaches for parent selection during mutation?\n\nA) UDE uses only the fitness space criterion for parent selection, while previous DE approaches used only the design space criterion.\n\nB) UDE uses both the fitness space and design space criteria for parent selection, while previous DE approaches used only one of these criteria. \n\nC) UDE uses only the design space criterion for parent selection, while previous DE approaches used both criteria.\n\nD) UDE ignores both the fitness and design spaces during parent selection, while previous DE approaches used one or both.\n\nCorrect Answer: B\n\nExplanation: The key difference is that UDE utilizes both the fitness space and design space criteria for intelligently selecting mutation vectors, whereas previous DE approaches used only one of these criteria, disregarding the other. By using both criteria, UDE takes advantage of all the available information in the problem space."}, "46": {"documentation": {"title": "Communication-Compressed Adaptive Gradient Method for Distributed\n  Nonconvex Optimization", "source": "Yujia Wang, Lu Lin and Jinghui Chen", "docs_id": "2111.00705", "section": ["cs.LG", "cs.AI", "cs.DC", "math.OC", "stat.ML"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Communication-Compressed Adaptive Gradient Method for Distributed\n  Nonconvex Optimization. Due to the explosion in the size of the training datasets, distributed learning has received growing interest in recent years. One of the major bottlenecks is the large communication cost between the central server and the local workers. While error feedback compression has been proven to be successful in reducing communication costs with stochastic gradient descent (SGD), there are much fewer attempts in building communication-efficient adaptive gradient methods with provable guarantees, which are widely used in training large-scale machine learning models. In this paper, we propose a new communication-compressed AMSGrad for distributed nonconvex optimization problem, which is provably efficient. Our proposed distributed learning framework features an effective gradient compression strategy and a worker-side model update design. We prove that the proposed communication-efficient distributed adaptive gradient method converges to the first-order stationary point with the same iteration complexity as uncompressed vanilla AMSGrad in the stochastic nonconvex optimization setting. Experiments on various benchmarks back up our theory."}, "answer": " Here is a 4-option multiple choice question on the document:\n\nQuestion: Which of the following statements is true about the proposed communication-compressed AMSGrad method described in the document?\n\nA) It uses a worker-side model update design but does not use any gradient compression strategy.\n\nB) It converges to the first-order stationary point with lower iteration complexity compared to uncompressed vanilla AMSGrad. \n\nC) It uses an effective gradient compression strategy and a worker-side model update design.\n\nD) It has been proven to work well for convex optimization problems but not for nonconvex problems.\n\nCorrect Answer: C\n\nExplanation: According to the document, the proposed communication-compressed AMSGrad method features an effective gradient compression strategy and a worker-side model update design. It has been proven to converge to the first-order stationary point with the same iteration complexity as uncompressed vanilla AMSGrad in stochastic nonconvex optimization settings."}, "47": {"documentation": {"title": "More on zeros and approximation of the Ising partition function", "source": "Alexander Barvinok and Nicholas Barvinok", "docs_id": "2005.11232", "section": ["math.PR", "cs.DS", "math-ph", "math.CO", "math.MP"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "More on zeros and approximation of the Ising partition function. We consider the problem of computing the partition function $\\sum_x e^{f(x)}$, where $f: \\{-1, 1\\}^n \\longrightarrow {\\Bbb R}$ is a quadratic or cubic polynomial on the Boolean cube $\\{-1, 1\\}^n$. In the case of a quadratic polynomial $f$, we show that the partition function can be approximated within relative error $0 < \\epsilon < 1$ in quasi-polynomial $n^{O(\\ln n - \\ln \\epsilon)}$ time if the Lipschitz constant of the non-linear part of $f$ with respect to the $\\ell^1$ metric on the Boolean cube does not exceed $1-\\delta$, for any $\\delta >0$, fixed in advance. For a cubic polynomial $f$, we get the same result under a somewhat stronger condition. We apply the method of polynomial interpolation, for which we prove that $\\sum_x e^{\\tilde{f}(x)} \\ne 0$ for complex-valued polynomials $\\tilde{f}$ in a neighborhood of a real-valued $f$ satisfying the above mentioned conditions. The bounds are asymptotically optimal. Results on the zero-free region are interpreted as the absence of a phase transition in the Lee - Yang sense in the corresponding Ising model. The novel feature of the bounds is that they control the total interaction of each vertex but not every single interaction of sets of vertices."}, "answer": " Here is a multi-form question based on the given text:\n\nQuestion: According to the text, what is the main result for approximating the partition function of a quadratic polynomial f on the Boolean cube?\n\nA) The partition function can be approximated in quasi-polynomial time if the Lipschitz constant of f does not exceed 1. \n\nB) The partition function can be approximated within error \u03b5 in time n^{O(ln n - ln \u03b5)} if the Lipschitz constant of f is at most 1-\u03b4.\n\nC) The partition function can be approximated in polynomial time for any quadratic f.\n\nD) The partition function cannot be approximated efficiently for any non-trivial f.\n\nCorrect Answer: B\n\nThe text states that for a quadratic f, the partition function can be approximated within error \u03b5 in quasi-polynomial time n^{O(ln n - ln \u03b5)} if the Lipschitz constant does not exceed 1-\u03b4."}, "48": {"documentation": {"title": "Round-Robin Streaming with Generations", "source": "Yao Li and P\\'eter Vingelmann and Morten Videb{\\ae}k Pedersen and\n  Emina Soljanin", "docs_id": "1206.3014", "section": ["cs.IT", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Round-Robin Streaming with Generations. We consider three types of application layer coding for streaming over lossy links: random linear coding, systematic random linear coding, and structured coding. The file being streamed is divided into sub-blocks (generations). Code symbols are formed by combining data belonging to the same generation, and transmitted in a round-robin fashion. We compare the schemes based on delivery packet count, net throughput, and energy consumption for a range of generation sizes. We determine these performance measures both analytically and in an experimental configuration. We find our analytical predictions to match the experimental results. We show that coding at the application layer brings about a significant increase in net data throughput, and thereby reduction in energy consumption due to reduced communication time. On the other hand, on devices with constrained computing resources, heavy coding operations cause packet drops in higher layers and negatively affect the net throughput. We find from our experimental results that low-rate MDS codes are best for small generation sizes, whereas systematic random linear coding has the best net throughput and lowest energy consumption for larger generation sizes due to its low decoding complexity."}, "answer": " Question: Which coding scheme was found to have the best net throughput and lowest energy consumption for larger generation sizes in the round-robin streaming experiment described in the paper?\n\nA) Random linear coding\nB) Systematic random linear coding \nC) Structured coding\nD) Low-rate MDS codes\n\nCorrect Answer: B) Systematic random linear coding\n\nExplanations:\nThe paper states that \"systematic random linear coding has the best net throughput and lowest energy consumption for larger generation sizes due to its low decoding complexity.\" So for larger generation sizes, systematic random linear coding performed the best in terms of net throughput and energy consumption."}, "49": {"documentation": {"title": "PaPy: Parallel and Distributed Data-processing Pipelines in Python", "source": "Marcin Cieslik and Cameron Mura", "docs_id": "1407.4378", "section": ["cs.PL", "q-bio.QM"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "PaPy: Parallel and Distributed Data-processing Pipelines in Python. PaPy, which stands for parallel pipelines in Python, is a highly flexible framework that enables the construction of robust, scalable workflows for either generating or processing voluminous datasets. A workflow is created from user-written Python functions (nodes) connected by 'pipes' (edges) into a directed acyclic graph. These functions are arbitrarily definable, and can make use of any Python modules or external binaries. Given a user-defined topology and collection of input data, functions are composed into nested higher-order maps, which are transparently and robustly evaluated in parallel on a single computer or on remote hosts. Local and remote computational resources can be flexibly pooled and assigned to functional nodes, thereby allowing facile load-balancing and pipeline optimization to maximize computational throughput. Input items are processed by nodes in parallel, and traverse the graph in batches of adjustable size -- a trade-off between lazy-evaluation, parallelism, and memory consumption. The processing of a single item can be parallelized in a scatter/gather scheme. The simplicity and flexibility of distributed workflows using PaPy bridges the gap between desktop -> grid, enabling this new computing paradigm to be leveraged in the processing of large scientific datasets."}, "answer": " Question: What is the main purpose of the PaPy framework?\n\nA) To enable construction of scalable workflows for processing large datasets using Python functions connected in a directed acyclic graph.\n\nB) To provide a desktop application for visualizing scientific datasets. \n\nC) To implement grid computing workflows using Python.\n\nD) To create parallel processing pipelines in C++.\n\nCorrect Answer: A\n\nExplanation: The documentation states that PaPy \"enables the construction of robust, scalable workflows for either generating or processing voluminous datasets\" using Python functions connected in a directed acyclic graph. So the main purpose is to enable scalable workflows for processing large datasets."}, "50": {"documentation": {"title": "Conceptual Framework for Internet of Things' Virtualization via OpenFlow\n  in Context-aware Networks", "source": "Theo Kanter, Rahim Rahmani, and Arif Mahmud", "docs_id": "1401.7437", "section": ["cs.NI", "cs.DC"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Conceptual Framework for Internet of Things' Virtualization via OpenFlow\n  in Context-aware Networks. A novel conceptual framework is presented in this paper with an aim to standardize and virtualize Internet of Things(IoT) infrastructure through deploying OpenFlow technology. The framework can receivee services based on context information leaving the current infrastructure unchanged. This framework allows the active collaboration of heterogeneous devices and protocols. Moreover it is capable to model placement of physical objects, manage the system and to collect information for services deployed on an IoT infrastructure. Our proposed IoT virtualization is applicable to a random topology scenario which makes it possible to 1) share flow sensors resources 2) establish multioperational sensor networks, and 3) extend reachability within the framework without establishing any further physical networks. Flow sensors achieve better results comparable to the typical sensors with respect to packet generation, reachability, simulation time, throughput, energy consumption point of view. Even better results are possible through utilizing multicast groups in large scale networks."}, "answer": " Here is a 4-option multiple choice question with explanations for the given text:\n\nQuestion: Which of the following is NOT a capability of the proposed IoT virtualization framework?\n\nA) Sharing flow sensor resources \nB) Establishing multioperational sensor networks\nC) Extending reachability without new physical networks\nD) Eliminating the need for heterogeneous devices and protocols\n\nCorrect Answer: D\n\nExplanations:\n\nA) The framework allows sharing of flow sensor resources through virtualization and OpenFlow.\n\nB) It can establish multioperational sensor networks on the same infrastructure. \n\nC) It extends reachability within the framework without establishing additional physical networks.\n\nD) The framework allows active collaboration of heterogeneous devices and protocols. So eliminating heterogeneity is not a capability."}, "51": {"documentation": {"title": "NVIDIA NeMo Neural Machine Translation Systems for English-German and\n  English-Russian News and Biomedical Tasks at WMT21", "source": "Sandeep Subramanian, Oleksii Hrinchuk, Virginia Adams, Oleksii\n  Kuchaiev", "docs_id": "2111.08634", "section": ["cs.CL", "cs.LG"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "NVIDIA NeMo Neural Machine Translation Systems for English-German and\n  English-Russian News and Biomedical Tasks at WMT21. This paper provides an overview of NVIDIA NeMo's neural machine translation systems for the constrained data track of the WMT21 News and Biomedical Shared Translation Tasks. Our news task submissions for English-German (En-De) and English-Russian (En-Ru) are built on top of a baseline transformer-based sequence-to-sequence model. Specifically, we use a combination of 1) checkpoint averaging 2) model scaling 3) data augmentation with backtranslation and knowledge distillation from right-to-left factorized models 4) finetuning on test sets from previous years 5) model ensembling 6) shallow fusion decoding with transformer language models and 7) noisy channel re-ranking. Additionally, our biomedical task submission for English-Russian uses a biomedically biased vocabulary and is trained from scratch on news task data, medically relevant text curated from the news task dataset, and biomedical data provided by the shared task. Our news system achieves a sacreBLEU score of 39.5 on the WMT'20 En-De test set outperforming the best submission from last year's task of 38.8. Our biomedical task Ru-En and En-Ru systems reach BLEU scores of 43.8 and 40.3 respectively on the WMT'20 Biomedical Task Test set, outperforming the previous year's best submissions."}, "answer": " Question: Which of the following techniques was NOT used by NVIDIA NeMo's neural machine translation systems for the WMT21 news and biomedical shared translation tasks?\n\nA) Checkpoint averaging\nB) Model scaling \nC) Data augmentation with backtranslation and knowledge distillation from right-to-left factorized models\nD) Adversarial training\n\nCorrect Answer: D\n\nExplanation: The paper states that NVIDIA NeMo's systems used techniques such as checkpoint averaging, model scaling, data augmentation with backtranslation and knowledge distillation, etc. However, it does not mention using adversarial training, so that is the technique not used by the systems."}, "52": {"documentation": {"title": "Optimal Control Theory in Intelligent Transportation Systems Research -\n  A Review", "source": "Jimmy SJ. Ren, Wei Wang, Stephen Shaoyi Liao", "docs_id": "1304.3778", "section": ["cs.SY", "cs.NI"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Optimal Control Theory in Intelligent Transportation Systems Research -\n  A Review. Continuous motorization and urbanization around the globe leads to an expansion of population in major cities. Therefore, ever-growing pressure imposed on the existing mass transit systems calls for a better technology, Intelligent Transportation Systems (ITS), to solve many new and demanding management issues. Many studies in the extant ITS literature attempted to address these issues within which various research methodologies were adopted. However, there is very few paper summarized what does optimal control theory (OCT), one of the sharpest tools to tackle management issues in engineering, do in solving these issues. It{\\textquoteright}s both important and interesting to answer the following two questions. (1) How does OCT contribute to ITS research objectives? (2) What are the research gaps and possible future research directions? We searched 11 top transportation and control journals and reviewed 41 research articles in ITS area in which OCT was used as the main research methodology. We categorized the articles by four different ways to address our research questions. We can conclude from the review that OCT is widely used to address various aspects of management issues in ITS within which a large portion of the studies aimed to reduce traffic congestion. We also critically discussed these studies and pointed out some possible future research directions towards which OCT can be used."}, "answer": " Here is a 4-option multiple choice question on the given passage:\n\nQuestion: According to the passage, which of the following is NOT mentioned as a contribution of optimal control theory to intelligent transportation systems research?\n\nA) Reducing traffic congestion\nB) Improving mass transit system technology \nC) Minimizing transportation system costs\nD) Enhancing transportation safety\n\nCorrect Answer: C\n\nExplanation: The passage states that a large portion of studies using optimal control theory in intelligent transportation systems research aimed to reduce traffic congestion. It does not mention using optimal control theory to minimize transportation system costs, so C is the correct answer. A, B, and D are all mentioned in the passage as ways optimal control theory contributes to intelligent transportation systems research objectives."}, "53": {"documentation": {"title": "Continuous and Discrete-Time Survival Prediction with Neural Networks", "source": "H{\\aa}vard Kvamme and {\\O}rnulf Borgan", "docs_id": "1910.06724", "section": ["stat.ML", "cs.LG", "stat.ME"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Continuous and Discrete-Time Survival Prediction with Neural Networks. Application of discrete-time survival methods for continuous-time survival prediction is considered. For this purpose, a scheme for discretization of continuous-time data is proposed by considering the quantiles of the estimated event-time distribution, and, for smaller data sets, it is found to be preferable over the commonly used equidistant scheme. Furthermore, two interpolation schemes for continuous-time survival estimates are explored, both of which are shown to yield improved performance compared to the discrete-time estimates. The survival methods considered are based on the likelihood for right-censored survival data, and parameterize either the probability mass function (PMF) or the discrete-time hazard rate, both with neural networks. Through simulations and study of real-world data, the hazard rate parametrization is found to perform slightly better than the parametrization of the PMF. Inspired by these investigations, a continuous-time method is proposed by assuming that the continuous-time hazard rate is piecewise constant. The method, named PC-Hazard, is found to be highly competitive with the aforementioned methods in addition to other methods for survival prediction found in the literature."}, "answer": " Question: Which of the following statements accurately summarizes a key finding from the research described in the given documentation?\n\nA) The PC-Hazard method was found to significantly outperform all other methods considered in the study in terms of survival prediction accuracy.\n\nB) Parametrizing the probability mass function with neural networks led to better performance than parametrizing the discrete-time hazard rate. \n\nC) Using quantile-based discretization outperformed equidistant discretization, especially for smaller datasets.\n\nD) Interpolating continuous-time estimates from discrete-time neural network models improved performance over just using the discrete-time estimates.\n\nCorrect Answer: D\n\nExplanation: The documentation states that exploring two interpolation schemes for continuous-time survival estimates based on discrete-time neural network models \"are shown to yield improved performance compared to the discrete-time estimates.\" While other findings are mentioned, this summarizes a key result of interpolating from discrete to continuous time."}, "54": {"documentation": {"title": "Assessing Threat of Adversarial Examples on Deep Neural Networks", "source": "Abigail Graese, Andras Rozsa, Terrance E. Boult", "docs_id": "1610.04256", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Assessing Threat of Adversarial Examples on Deep Neural Networks. Deep neural networks are facing a potential security threat from adversarial examples, inputs that look normal but cause an incorrect classification by the deep neural network. For example, the proposed threat could result in hand-written digits on a scanned check being incorrectly classified but looking normal when humans see them. This research assesses the extent to which adversarial examples pose a security threat, when one considers the normal image acquisition process. This process is mimicked by simulating the transformations that normally occur in acquiring the image in a real world application, such as using a scanner to acquire digits for a check amount or using a camera in an autonomous car. These small transformations negate the effect of the carefully crafted perturbations of adversarial examples, resulting in a correct classification by the deep neural network. Thus just acquiring the image decreases the potential impact of the proposed security threat. We also show that the already widely used process of averaging over multiple crops neutralizes most adversarial examples. Normal preprocessing, such as text binarization, almost completely neutralizes adversarial examples. This is the first paper to show that for text driven classification, adversarial examples are an academic curiosity, not a security threat."}, "answer": " Here is a multi-form question based on the given text:\n\nQuestion: Which of the following statements accurately summarizes the main findings of the research described in the text?\n\nA) Adversarial examples pose a serious security threat to deep neural networks, as they can cause misclassifications while looking normal to humans.\n\nB) The normal image acquisition process introduces transformations that negate the effects of adversarial examples, reducing their threat. \n\nC) Averaging over multiple crops is an effective defense against most adversarial examples.\n\nD) Text-driven classification tasks are immune to adversarial examples due to preprocessing steps like binarization.\n\nCorrect Answer: B\n\nThe text states that the normal image acquisition process introduces transformations that negate the effect of adversarial examples, resulting in correct classifications. Thus it finds that just acquiring the image decreases the potential impact of adversarial examples."}, "55": {"documentation": {"title": "Information Update: TDMA or FDMA?", "source": "Haoyuan Pan, Soung Chang Liew", "docs_id": "1911.02241", "section": ["cs.IT", "eess.SP", "math.IT"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Information Update: TDMA or FDMA?. This paper studies information freshness in information update systems operated with TDMA and FDMA. Information freshness is characterized by a recently introduced metric, age of information (AoI), defined as the time elapsed since the generation of the last successfully received update. In an update system with multiple users sharing the same wireless channel to send updates to a common receiver, how to divide the channel among users affects information freshness. We investigate the AoI performances of two fundamental multiple access schemes, TDMA and FDMA. We first derive the time-averaged AoI by estimating the packet error rate of short update packets based on Gallager's random coding bound. For time-critical systems, we further define a new AoI metric, termed bounded AoI, which corresponds to an AoI threshold for the instantaneous AoI. Specifically, the instantaneous AoI is below the bounded AoI a large percentage of the time. We give a theoretical upper bound for bounded AoI. Our simulation results are consistent with our theoretical analysis. Although TDMA outperforms FDMA in terms of average AoI, FDMA is more robust against varying channel conditions since it gives a more stable bounded AoI across different received powers. Overall, our findings give insight to the design of practical multiple access systems with AoI requirements."}, "answer": " Here is a 4-option multiple choice question on the paper:\n\nQuestion: Which multiple access scheme gives a more stable bounded AoI across different received powers according to the paper?\n\nA) TDMA\nB) CSMA \nC) FDMA\nD) CDMA\n\nCorrect Answer: C) FDMA \n\nExplanation: The paper states that \"FDMA is more robust against varying channel conditions since it gives a more stable bounded AoI across different received powers.\" So FDMA provides a more stable bounded AoI as received power changes, compared to TDMA."}, "56": {"documentation": {"title": "Motion Basis Learning for Unsupervised Deep Homography Estimation with\n  Subspace Projection", "source": "Nianjin Ye, Chuan Wang, Haoqiang Fan, Shuaicheng Liu", "docs_id": "2103.15346", "section": ["cs.CV"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Motion Basis Learning for Unsupervised Deep Homography Estimation with\n  Subspace Projection. In this paper, we introduce a new framework for unsupervised deep homography estimation. Our contributions are 3 folds. First, unlike previous methods that regress 4 offsets for a homography, we propose a homography flow representation, which can be estimated by a weighted sum of 8 pre-defined homography flow bases. Second, considering a homography contains 8 Degree-of-Freedoms (DOFs) that is much less than the rank of the network features, we propose a Low Rank Representation (LRR) block that reduces the feature rank, so that features corresponding to the dominant motions are retained while others are rejected. Last, we propose a Feature Identity Loss (FIL) to enforce the learned image feature warp-equivariant, meaning that the result should be identical if the order of warp operation and feature extraction is swapped. With this constraint, the unsupervised optimization is achieved more effectively and more stable features are learned. Extensive experiments are conducted to demonstrate the effectiveness of all the newly proposed components, and results show that our approach outperforms the state-of-the-art on the homography benchmark datasets both qualitatively and quantitatively. Code is available at https://github.com/megvii-research/BasesHomo."}, "answer": " Here is a 4-option multiple choice question on the paper:\n\nQuestion: Which of the following is NOT a contribution of the paper \"Motion Basis Learning for Unsupervised Deep Homography Estimation with Subspace Projection\"?\n\nA) Proposing a homography flow representation that can be estimated by a weighted sum of pre-defined homography flow bases. \n\nB) Introducing a Low Rank Representation block to reduce the feature rank and retain features corresponding to dominant motions.\n\nC) Using a convolutional neural network to directly regress the 4 offsets of a homography.\n\nD) Proposing a Feature Identity Loss to enforce warp-equivariance of learned image features.\n\nCorrect Answer: C\n\nExplanation: The paper does NOT propose using a CNN to directly regress the 4 offsets of a homography. Instead, it represents homography as a weighted sum of predefined flow bases and uses a Low Rank Representation block to reduce feature rank. Options A, B, and D refer to actual contributions of the paper."}, "57": {"documentation": {"title": "A twin error gauge for Kaczmarz's iterations", "source": "Bart S. van Lith, Per Christian Hansen, Michiel E. Hochstenbach", "docs_id": "1906.07470", "section": ["math.NA", "cs.NA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A twin error gauge for Kaczmarz's iterations. We propose two new algebraic reconstruction techniques based on Kaczmarz's method that produce a regularized solution to noisy tomography problems. Tomography problems exhibit semi-convergence when iterative methods are employed, and the aim is therefore to stop near the semi-convergence point. Our approach is based on an error gauge that is constructed by pairing standard down-sweep Kaczmarz's method with its up-sweep version; we stop the iterations when this error gauge is minimal. The reconstructions of the new methods differ from standard Kaczmarz iterates in that our final result is the average of the stopped up- and down-sweeps. Even when Kaczmarz's method is supplied with an oracle that provides the exact error -- and is thereby able to stop at the best possible iterate -- our methods have a lower two-norm error in the vast majority of our test cases. In terms of computational cost, our methods are a little cheaper than standard Kaczmarz equipped with a statistical stopping rule."}, "answer": " Here is a 4-option multiple choice question on the given documentation:\n\nQuestion: Which of the following statements is true about the proposed methods in the paper?\n\nA) The methods use only the down-sweep version of Kaczmarz's iterations.\n\nB) The methods stop the iterations when the error gauge constructed from up-sweep and down-sweep iterations is maximal. \n\nC) The final reconstruction is the average of the up-sweep and down-sweep iterations at the stopping point.\n\nD) The methods always have a higher two-norm error compared to standard Kaczmarz with an optimal stopping rule.\n\nCorrect Answer: C\n\nExplanation: The paper proposes new methods based on Kaczmarz's iterations that stop near the semi-convergence point. The error gauge is constructed by pairing the up-sweep and down-sweep versions, and iterations are stopped when this gauge is minimal. The final reconstruction averages the up-sweep and down-sweep iterations at the stopping point. So option C is correct."}, "58": {"documentation": {"title": "Higher-dimension Tensor Completion via Low-rank Tensor Ring\n  Decomposition", "source": "Longhao Yuan, Jianting Cao, Qiang Wu and Qibin Zhao", "docs_id": "1807.01589", "section": ["cs.NA"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "Higher-dimension Tensor Completion via Low-rank Tensor Ring\n  Decomposition. The problem of incomplete data is common in signal processing and machine learning. Tensor completion algorithms aim to recover the incomplete data from its partially observed entries. In this paper, taking advantages of high compressibility and flexibility of recently proposed tensor ring (TR) decomposition, we propose a new tensor completion approach named tensor ring weighted optimization (TR-WOPT). It finds the latent factors of the incomplete tensor by gradient descent algorithm, then the latent factors are employed to predict the missing entries of the tensor. We conduct various tensor completion experiments on synthetic data and real-world data. The simulation results show that TR-WOPT performs well in various high-dimension tensors. Furthermore, image completion results show that our proposed algorithm outperforms the state-of-the-art algorithms in many situations. Especially when the missing rate of the test images is high (e.g., over 0.9), the performance of our TR-WOPT is significantly better than the compared algorithms."}, "answer": " Question: What method does the paper propose for tensor completion of high-dimensional incomplete data?\n\nA) A gradient descent algorithm to find the latent factors of the incomplete tensor, then using the factors to predict missing entries.\n\nB) A convolutional neural network trained on complete tensors to predict missing entries. \n\nC) A nearest neighbors approach that uses low-rank matrix completion on unfolding of the tensor.\n\nD) An alternating least squares approach with trace norm regularization.\n\nCorrect Answer: A) A gradient descent algorithm to find the latent factors of the incomplete tensor, then using the factors to predict missing entries.\n\nExplanation: The paper proposes a new tensor completion approach named tensor ring weighted optimization (TR-WOPT) that finds the latent factors of the incomplete tensor by gradient descent, then uses the factors to fill in missing entries."}, "59": {"documentation": {"title": "A Robotic Line Scan System with Adaptive ROI for Inspection of Defects\n  over Convex Free-form Specular Surfaces", "source": "Shengzeng Huo, David Navarro-Alarcon, David Chik", "docs_id": "2008.10816", "section": ["cs.RO"], "start_character": "N/A", "end_character": "N/A", "date": "N/A", "text": "A Robotic Line Scan System with Adaptive ROI for Inspection of Defects\n  over Convex Free-form Specular Surfaces. In this paper, we present a new robotic system to perform defect inspection tasks over free-form specular surfaces. The autonomous procedure is achieved by a six-DOF manipulator, equipped with a line scan camera and a high-intensity lighting system. Our method first uses the object's CAD mesh model to implement a K-means unsupervised learning algorithm that segments the object's surface into areas with similar curvature. Then, the scanning path is computed by using an adaptive algorithm that adjusts the camera's ROI to observe regions with irregular shapes properly. A novel iterative closest point-based projection registration method that robustly localizes the object in the robot's coordinate frame system is proposed to deal with the blind spot problem of specular objects captured by depth sensors. Finally, an image processing pipeline automatically detects surface defects in the captured high-resolution images. A detailed experimental study with a vision-guided robotic scanning system is reported to validate the proposed methodology."}, "answer": " Question: What method is proposed in the paper to deal with the blind spot problem of specular objects captured by depth sensors?\n\nA) An iterative closest point-based projection registration method that robustly localizes the object in the robot's coordinate frame system\n\nB) A K-means unsupervised learning algorithm that segments the object's surface into areas with similar curvature\n\nC) An adaptive algorithm that adjusts the camera's ROI to observe regions with irregular shapes properly\n\nD) A novel image processing pipeline that automatically detects surface defects in the captured high-resolution images\n\nCorrect Answer: A"}}